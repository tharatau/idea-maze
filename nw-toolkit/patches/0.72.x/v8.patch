Only in nw/v8: .git
diff -r -u --color up/v8/.ycm_extra_conf.py nw/v8/.ycm_extra_conf.py
--- up/v8/.ycm_extra_conf.py	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/.ycm_extra_conf.py	2023-01-19 16:46:35.929776315 -0500
@@ -42,7 +42,6 @@
 # Flags from YCM's default config.
 flags = [
 '-DUSE_CLANG_COMPLETER',
-'-std=gnu++14',
 '-x',
 'c++',
 ]
@@ -143,25 +142,27 @@
   # Parse flags that are important for YCM's purposes.
   for flag in clang_line.split(' '):
     if flag.startswith('-I'):
-      # Relative paths need to be resolved, because they're relative to the
-      # output dir, not the source.
-      if flag[2] == '/':
-        v8_flags.append(flag)
-      else:
-        abs_path = os.path.normpath(os.path.join(out_dir, flag[2:]))
-        v8_flags.append('-I' + abs_path)
-    elif flag.startswith('-std'):
+      v8_flags.append(MakeIncludePathAbsolute(flag, "-I", out_dir))
+    elif flag.startswith('-isystem'):
+      v8_flags.append(MakeIncludePathAbsolute(flag, "-isystem", out_dir))
+    elif flag.startswith('-std') or flag.startswith(
+        '-pthread') or flag.startswith('-no'):
       v8_flags.append(flag)
-    elif flag.startswith('-') and flag[1] in 'DWFfmO':
-      if flag == '-Wno-deprecated-register' or flag == '-Wno-header-guard':
-        # These flags causes libclang (3.3) to crash. Remove it until things
-        # are fixed.
-        continue
+    elif flag.startswith('-') and flag[1] in 'DWFfmgOX':
       v8_flags.append(flag)
-
   return v8_flags
 
 
+def MakeIncludePathAbsolute(flag, prefix, out_dir):
+  # Relative paths need to be resolved, because they're relative to the
+  # output dir, not the source.
+  if flag[len(prefix)] == '/':
+    return flag
+  else:
+    abs_path = os.path.normpath(os.path.join(out_dir, flag[len(prefix):]))
+    return prefix + abs_path
+
+
 def FlagsForFile(filename):
   """This is the main entry point for YCM. Its interface is fixed.
 
@@ -180,3 +181,9 @@
     'flags': final_flags,
     'do_cache': True
   }
+
+
+def Settings(**kwargs):
+  if kwargs['language'] == 'cfamily':
+    return FlagsForFile(kwargs['filename'])
+  return {}
diff -r -u --color up/v8/AUTHORS nw/v8/AUTHORS
--- up/v8/AUTHORS	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/AUTHORS	2023-01-19 16:46:35.929776315 -0500
@@ -60,6 +60,7 @@
 Amos Lim <eui-sang.lim@samsung.com>
 Andreas Anyuru <andreas.anyuru@gmail.com>
 Andrei Kashcha <anvaka@gmail.com>
+Andreu Botella <andreu@andreubotella.com>
 Andrew Paprocki <andrew@ishiboo.com>
 Anna Henningsen <anna@addaleax.net>
 Antoine du Hamel <duhamelantoine1995@gmail.com>
@@ -113,6 +114,7 @@
 Felix Geisendörfer <haimuiba@gmail.com>
 Feng Yu <f3n67u@gmail.com>
 Filipe David Manana <fdmanana@gmail.com>
+Frank Lemanschik <frank@dspeed.eu>
 Franziska Hinkelmann <franziska.hinkelmann@gmail.com>
 Gao Sheng <gaosheng08@meituan.com>
 Geoffrey Garside <ggarside@gmail.com>
@@ -151,6 +153,7 @@
 Joel Stanley <joel@jms.id.au>
 Johan Bergström <johan@bergstroem.nu>
 Jonathan Liu <net147@gmail.com>
+Juan Arboleda <soyjuanarbol@gmail.com>
 Julien Brianceau <jbriance@cisco.com>
 JunHo Seo <sejunho@gmail.com>
 Junha Park <jpark3@scu.edu>
diff -r -u --color up/v8/BUILD.bazel nw/v8/BUILD.bazel
--- up/v8/BUILD.bazel	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/BUILD.bazel	2023-01-19 16:46:35.929776315 -0500
@@ -39,6 +39,7 @@
 # v8_enable_trace_baseline_exec
 # v8_enable_trace_feedback_updates
 # v8_enable_atomic_object_field_writes
+# v8_enable_conservative_stack_scanning
 # v8_enable_concurrent_marking
 # v8_enable_ignition_dispatch_counting
 # v8_enable_builtins_profiling
@@ -1381,6 +1382,8 @@
         "src/handles/global-handles-inl.h",
         "src/handles/global-handles.cc",
         "src/handles/global-handles.h",
+        "src/handles/traced-handles.cc",
+        "src/handles/traced-handles.h",
         "src/handles/handles-inl.h",
         "src/handles/handles.cc",
         "src/handles/handles.h",
@@ -1963,6 +1966,7 @@
         "src/profiler/heap-snapshot-generator-inl.h",
         "src/profiler/heap-snapshot-generator.cc",
         "src/profiler/heap-snapshot-generator.h",
+        "src/profiler/output-stream-writer.h",
         "src/profiler/profile-generator-inl.h",
         "src/profiler/profile-generator.cc",
         "src/profiler/profile-generator.h",
@@ -2664,6 +2668,8 @@
         "src/compiler/all-nodes.h",
         "src/compiler/allocation-builder.h",
         "src/compiler/allocation-builder-inl.h",
+        "src/compiler/backend/bitcast-elider.cc",
+        "src/compiler/backend/bitcast-elider.h",
         "src/compiler/backend/code-generator.cc",
         "src/compiler/backend/code-generator.h",
         "src/compiler/backend/code-generator-impl.h",
@@ -2887,7 +2893,7 @@
         "src/compiler/turboshaft/graph.h",
         "src/compiler/turboshaft/graph-visualizer.cc",
         "src/compiler/turboshaft/graph-visualizer.h",
-        "src/compiler/turboshaft/machine-optimization-assembler.h",
+        "src/compiler/turboshaft/machine-optimization-reducer.h",
         "src/compiler/turboshaft/operations.cc",
         "src/compiler/turboshaft/operations.h",
         "src/compiler/turboshaft/operation-matching.h",
@@ -2897,12 +2903,14 @@
         "src/compiler/turboshaft/recreate-schedule.h",
         "src/compiler/turboshaft/representations.cc",
         "src/compiler/turboshaft/representations.h",
+        "src/compiler/turboshaft/select-lowering-reducer.h",
         "src/compiler/turboshaft/sidetable.h",
         "src/compiler/turboshaft/simplify-tf-loops.cc",
         "src/compiler/turboshaft/simplify-tf-loops.h",
+        "src/compiler/turboshaft/snapshot-table.h",
         "src/compiler/turboshaft/utils.cc",
         "src/compiler/turboshaft/utils.h",
-        "src/compiler/turboshaft/value-numbering-assembler.h",
+        "src/compiler/turboshaft/value-numbering-reducer.h",
         "src/compiler/type-cache.cc",
         "src/compiler/type-cache.h",
         "src/compiler/type-narrowing-reducer.cc",
@@ -3152,16 +3160,16 @@
         # Note these cannot be v8_target_is_* selects because these contain
         # inline assembly that runs inside the executable. Since these are
         # linked directly into mksnapshot, they must use the actual target cpu.
-        "@v8//bazel/config:is_inline_asm_ia32": ["src/heap/base/asm/ia32/push_registers_asm.cc"],
-        "@v8//bazel/config:is_inline_asm_x64": ["src/heap/base/asm/x64/push_registers_asm.cc"],
-        "@v8//bazel/config:is_inline_asm_arm": ["src/heap/base/asm/arm/push_registers_asm.cc"],
-        "@v8//bazel/config:is_inline_asm_arm64": ["src/heap/base/asm/arm64/push_registers_asm.cc"],
-        "@v8//bazel/config:is_inline_asm_s390x": ["src/heap/base/asm/s390/push_registers_asm.cc"],
-        "@v8//bazel/config:is_inline_asm_riscv64": ["src/heap/base/asm/riscv64/push_registers_asm.cc"],
-        "@v8//bazel/config:is_inline_asm_ppc64le": ["src/heap/base/asm/ppc/push_registers_asm.cc"],
-        "@v8//bazel/config:is_msvc_asm_ia32": ["src/heap/base/asm/ia32/push_registers_masm.asm"],
-        "@v8//bazel/config:is_msvc_asm_x64": ["src/heap/base/asm/x64/push_registers_masm.asm"],
-        "@v8//bazel/config:is_msvc_asm_arm64": ["src/heap/base/asm/arm64/push_registers_masm.S"],
+        "@v8//bazel/config:is_inline_asm_ia32": ["src/heap/base/asm/ia32/save_registers_asm.cc"],
+        "@v8//bazel/config:is_inline_asm_x64": ["src/heap/base/asm/x64/save_registers_asm.cc"],
+        "@v8//bazel/config:is_inline_asm_arm": ["src/heap/base/asm/arm/save_registers_asm.cc"],
+        "@v8//bazel/config:is_inline_asm_arm64": ["src/heap/base/asm/arm64/save_registers_asm.cc"],
+        "@v8//bazel/config:is_inline_asm_s390x": ["src/heap/base/asm/s390/save_registers_asm.cc"],
+        "@v8//bazel/config:is_inline_asm_riscv64": ["src/heap/base/asm/riscv64/save_registers_asm.cc"],
+        "@v8//bazel/config:is_inline_asm_ppc64le": ["src/heap/base/asm/ppc/save_registers_asm.cc"],
+        "@v8//bazel/config:is_msvc_asm_ia32": ["src/heap/base/asm/ia32/save_registers_masm.asm"],
+        "@v8//bazel/config:is_msvc_asm_x64": ["src/heap/base/asm/x64/save_registers_masm.asm"],
+        "@v8//bazel/config:is_msvc_asm_arm64": ["src/heap/base/asm/arm64/save_registers_masm.S"],
     }),
 )
 
diff -r -u --color up/v8/BUILD.gn nw/v8/BUILD.gn
--- up/v8/BUILD.gn	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/BUILD.gn	2023-01-19 16:46:35.940609645 -0500
@@ -59,7 +59,7 @@
   v8_deprecation_warnings = true
 
   # Enable compiler warnings when using V8_DEPRECATE_SOON apis.
-  v8_imminent_deprecation_warnings = true
+  v8_imminent_deprecation_warnings = false
 
   # Embeds the given script into the snapshot.
   v8_embed_script = ""
@@ -308,7 +308,7 @@
 
   # Enable the experimental V8 sandbox.
   # Sets -DV8_ENABLE_SANDBOX.
-  v8_enable_sandbox = ""
+  v8_enable_sandbox = false
 
   # Enable all available sandbox features. Implies v8_enable_sandbox.
   v8_enable_sandbox_future = false
@@ -340,7 +340,7 @@
   v8_enable_map_packing = false
 
   # Allow for JS promise hooks (instead of just C++).
-  v8_enable_javascript_promise_hooks = false
+  v8_enable_javascript_promise_hooks = true
 
   # Enable allocation folding globally (sets -dV8_ALLOCATION_FOLDING).
   # When it's disabled, the --turbo-allocation-folding runtime flag will be ignored.
@@ -356,7 +356,7 @@
   # Enable legacy mode for ScriptOrModule's lifetime. By default it's a
   # temporary object, if enabled it will be kept alive by the parent Script.
   # This is only used by nodejs.
-  v8_scriptormodule_legacy_lifetime = false
+  v8_scriptormodule_legacy_lifetime = true
 
   # Change code emission and runtime features to be CET shadow-stack compliant
   # (incomplete and experimental).
@@ -625,7 +625,7 @@
 
 config("internal_config") {
   defines = []
-
+  cflags = []
   # Only targets in this file and its subdirs can depend on this.
   visibility = [ "./*" ]
 
@@ -636,13 +636,17 @@
     ":cppgc_header_features",
   ]
 
-  if (is_component_build) {
+  if (true) {
     defines += [ "BUILDING_V8_SHARED" ]
   }
 
   if (v8_current_cpu == "riscv64" || v8_current_cpu == "riscv32") {
     libs = [ "atomic" ]
   }
+
+  if (is_win) {
+    cflags += [ "/Zc:dllexportInlines-" ]
+  }
 }
 
 # Should be applied to all targets that write trace events.
@@ -659,14 +663,14 @@
 # This config should be applied to code using the libplatform.
 config("libplatform_config") {
   include_dirs = [ "include" ]
-  if (is_component_build) {
+  if (false) {
     defines = [ "USING_V8_PLATFORM_SHARED" ]
   }
 }
 
 # This config should be applied to code using the libbase.
 config("libbase_config") {
-  if (is_component_build) {
+  if (false) {
     defines = [ "USING_V8_BASE_SHARED" ]
   }
   libs = []
@@ -709,7 +713,9 @@
   configs = [ ":headers_config" ]
   defines = []
   if (is_component_build) {
-    defines += [ "USING_V8_SHARED" ]
+    defines += [ "USING_V8_SHARED", "V8_SHARED", "USING_V8_PLATFORM_SHARED" ]
+  } else {
+    defines += [ "V8_SHARED" ]
   }
 
   if (current_cpu == "riscv64" || current_cpu == "riscv32") {
@@ -1460,6 +1466,10 @@
       "/wd4715",  # 'function' : not all control paths return a value'
                   # MSVC does not analyze switch (enum) for completeness.
     ]
+
+    # TODO(https://crbug.com/1377771): Keep MSVC on C++17 until source code is
+    # made compatible with C++20.
+    cflags_cc = [ "/std:c++17" ]
   }
 
   if (!is_clang && !is_win) {
@@ -1476,6 +1486,11 @@
       # Disable gcc warnings for using enum constant in boolean context.
       # https://gcc.gnu.org/bugzilla/show_bug.cgi?id=97266
       "-Wno-int-in-bool-context",
+
+      # Disable gcc deprecation warnings, which are firing on implicit capture
+      # of `this` in capture-by-value lambdas and preventing a build roll which
+      # enables C++20 (see https://crbug.com/1374227).
+      "-Wno-deprecated",
     ]
   }
 
@@ -2268,6 +2283,8 @@
     "v8_current_cpu=\"$v8_current_cpu\"",
     "v8_enable_atomic_object_field_writes=" +
         "$v8_enable_atomic_object_field_writes",
+    "v8_enable_conservative_stack_scanning=" +
+        "$v8_enable_conservative_stack_scanning",
     "v8_enable_concurrent_marking=$v8_enable_concurrent_marking",
     "v8_enable_single_generation=$v8_enable_single_generation",
     "v8_enable_i18n_support=$v8_enable_i18n_support",
@@ -2347,6 +2364,8 @@
   visibility = [
     ":*",
     "test/cctest:*",
+    "//tools/v8_context_snapshot:*",
+    "//chrome:*",
   ]
 
   allow_circular_includes_from = [ ":torque_generated_initializers" ]
@@ -2803,6 +2822,7 @@
     "src/compiler/all-nodes.h",
     "src/compiler/allocation-builder-inl.h",
     "src/compiler/allocation-builder.h",
+    "src/compiler/backend/bitcast-elider.h",
     "src/compiler/backend/code-generator-impl.h",
     "src/compiler/backend/code-generator.h",
     "src/compiler/backend/frame-elider.h",
@@ -2923,16 +2943,18 @@
     "src/compiler/turboshaft/graph-builder.h",
     "src/compiler/turboshaft/graph-visualizer.h",
     "src/compiler/turboshaft/graph.h",
-    "src/compiler/turboshaft/machine-optimization-assembler.h",
+    "src/compiler/turboshaft/machine-optimization-reducer.h",
     "src/compiler/turboshaft/operation-matching.h",
     "src/compiler/turboshaft/operations.h",
     "src/compiler/turboshaft/optimization-phase.h",
     "src/compiler/turboshaft/recreate-schedule.h",
     "src/compiler/turboshaft/representations.h",
+    "src/compiler/turboshaft/select-lowering-reducer.h",
     "src/compiler/turboshaft/sidetable.h",
     "src/compiler/turboshaft/simplify-tf-loops.h",
+    "src/compiler/turboshaft/snapshot-table.h",
     "src/compiler/turboshaft/utils.h",
-    "src/compiler/turboshaft/value-numbering-assembler.h",
+    "src/compiler/turboshaft/value-numbering-reducer.h",
     "src/compiler/type-cache.h",
     "src/compiler/type-narrowing-reducer.h",
     "src/compiler/typed-optimization.h",
@@ -3024,6 +3046,7 @@
     "src/handles/maybe-handles.h",
     "src/handles/persistent-handles.h",
     "src/handles/shared-object-conveyor-handles.h",
+    "src/handles/traced-handles.h",
     "src/heap/allocation-observer.h",
     "src/heap/allocation-result.h",
     "src/heap/allocation-stats.h",
@@ -3422,6 +3445,7 @@
     "src/profiler/heap-profiler.h",
     "src/profiler/heap-snapshot-generator-inl.h",
     "src/profiler/heap-snapshot-generator.h",
+    "src/profiler/output-stream-writer.h",
     "src/profiler/profile-generator-inl.h",
     "src/profiler/profile-generator.h",
     "src/profiler/profiler-listener.h",
@@ -4041,6 +4065,7 @@
   "src/compiler/access-info.cc",
   "src/compiler/add-type-assertions-reducer.cc",
   "src/compiler/all-nodes.cc",
+  "src/compiler/backend/bitcast-elider.cc",
   "src/compiler/backend/code-generator.cc",
   "src/compiler/backend/frame-elider.cc",
   "src/compiler/backend/gap-resolver.cc",
@@ -4429,6 +4454,7 @@
     "src/handles/local-handles.cc",
     "src/handles/persistent-handles.cc",
     "src/handles/shared-object-conveyor-handles.cc",
+    "src/handles/traced-handles.cc",
     "src/heap/allocation-observer.cc",
     "src/heap/array-buffer-sweeper.cc",
     "src/heap/base-space.cc",
@@ -4737,6 +4763,7 @@
 
   if (v8_enable_maglev) {
     sources += [
+      "src/maglev/maglev-assembler.cc",
       "src/maglev/maglev-code-generator.cc",
       "src/maglev/maglev-compilation-info.cc",
       "src/maglev/maglev-compilation-unit.cc",
@@ -5335,7 +5362,8 @@
   }
 }
 
-v8_component("v8_libbase") {
+v8_static_lib("v8_libbase") {
+
   sources = [
     "src/base/address-region.h",
     "src/base/atomic-utils.h",
@@ -5464,7 +5492,7 @@
 
   defines = []
 
-  if (is_component_build) {
+  if (false) {
     defines = [ "BUILDING_V8_BASE_SHARED" ]
   }
 
@@ -5596,7 +5624,7 @@
   # TODO(infra): Add support for qnx, freebsd, openbsd, netbsd, and solaris.
 }
 
-v8_component("v8_libplatform") {
+v8_static_lib("v8_libplatform") {
   sources = [
     "//base/trace_event/common/trace_event_common.h",
     "include/libplatform/libplatform-export.h",
@@ -5627,7 +5655,7 @@
 
   configs = [ ":internal_config_base" ]
 
-  if (is_component_build) {
+  if (true) {
     defines = [ "BUILDING_V8_PLATFORM_SHARED" ]
   }
 
@@ -5745,31 +5773,31 @@
 
   if (is_clang || !is_win) {
     if (current_cpu == "x64") {
-      sources += [ "src/heap/base/asm/x64/push_registers_asm.cc" ]
+      sources += [ "src/heap/base/asm/x64/save_registers_asm.cc" ]
     } else if (current_cpu == "x86") {
-      sources += [ "src/heap/base/asm/ia32/push_registers_asm.cc" ]
+      sources += [ "src/heap/base/asm/ia32/save_registers_asm.cc" ]
     } else if (current_cpu == "arm") {
-      sources += [ "src/heap/base/asm/arm/push_registers_asm.cc" ]
+      sources += [ "src/heap/base/asm/arm/save_registers_asm.cc" ]
     } else if (current_cpu == "arm64") {
-      sources += [ "src/heap/base/asm/arm64/push_registers_asm.cc" ]
+      sources += [ "src/heap/base/asm/arm64/save_registers_asm.cc" ]
     } else if (current_cpu == "ppc64") {
-      sources += [ "src/heap/base/asm/ppc/push_registers_asm.cc" ]
+      sources += [ "src/heap/base/asm/ppc/save_registers_asm.cc" ]
     } else if (current_cpu == "s390x") {
-      sources += [ "src/heap/base/asm/s390/push_registers_asm.cc" ]
+      sources += [ "src/heap/base/asm/s390/save_registers_asm.cc" ]
     } else if (current_cpu == "mips64el") {
-      sources += [ "src/heap/base/asm/mips64/push_registers_asm.cc" ]
+      sources += [ "src/heap/base/asm/mips64/save_registers_asm.cc" ]
     } else if (current_cpu == "loong64") {
-      sources += [ "src/heap/base/asm/loong64/push_registers_asm.cc" ]
+      sources += [ "src/heap/base/asm/loong64/save_registers_asm.cc" ]
     } else if (current_cpu == "riscv64" || current_cpu == "riscv32") {
-      sources += [ "src/heap/base/asm/riscv/push_registers_asm.cc" ]
+      sources += [ "src/heap/base/asm/riscv/save_registers_asm.cc" ]
     }
   } else if (is_win) {
     if (current_cpu == "x64") {
-      sources += [ "src/heap/base/asm/x64/push_registers_masm.asm" ]
+      sources += [ "src/heap/base/asm/x64/save_registers_masm.asm" ]
     } else if (current_cpu == "x86") {
-      sources += [ "src/heap/base/asm/ia32/push_registers_masm.asm" ]
+      sources += [ "src/heap/base/asm/ia32/save_registers_masm.asm" ]
     } else if (current_cpu == "arm64") {
-      sources += [ "src/heap/base/asm/arm64/push_registers_masm.S" ]
+      sources += [ "src/heap/base/asm/arm64/save_registers_masm.S" ]
     }
   }
 
@@ -6059,6 +6087,25 @@
 ###############################################################################
 # Executables
 #
+v8_executable("nwjc") {
+
+    sources = [
+      "src/nwjc.cc",
+    ]
+
+    configs = [
+      ":internal_config",
+    ]
+
+    deps = [
+      ":v8_base",
+      ":v8_libplatform",
+      ":v8_libbase",
+      ":v8_snapshot",
+      ":v8_initializers",
+      "//build/win:default_exe_manifest",
+    ]
+}
 
 if (current_toolchain == v8_generator_toolchain) {
   v8_executable("bytecode_builtins_list_generator") {
diff -r -u --color up/v8/DEPS nw/v8/DEPS
--- up/v8/DEPS	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/DEPS	2023-01-19 16:46:35.940609645 -0500
@@ -26,6 +26,7 @@
   # most commonly useful for developers. Bots and developers that need to use
   # other images (e.g., qemu.arm64) can override this with additional images.
   'checkout_fuchsia_boot_images': "qemu.x64",
+  'checkout_fuchsia_product_bundles': '"{checkout_fuchsia_boot_images}" != ""',
 
   'checkout_instrumented_libraries': False,
   'checkout_ittapi': False,
@@ -42,22 +43,22 @@
   'fuchsia_sdk_cipd_prefix': 'fuchsia/sdk/gn/',
 
   # reclient CIPD package version
-  'reclient_version': 're_client_version:0.69.0.458df98-gomaip',
+  'reclient_version': 're_client_version:0.83.0.da55f4f-gomaip',
 
   # GN CIPD package version.
-  'gn_version': 'git_revision:cc28efe62ef0c2fb32455f414a29c4a55bb7fbc4',
+  'gn_version': 'git_revision:a4d67be044b42963de801001e7146f9657c7fad4',
 
   # ninja CIPD package version
   # https://chrome-infra-packages.appspot.com/p/infra/3pp/tools/ninja
   'ninja_version': 'version:2@1.8.2.chromium.3',
 
   # luci-go CIPD package version.
-  'luci_go': 'git_revision:20c50aa39686d91330c2daceccaa4ef1a0a72ee4',
+  'luci_go': 'git_revision:f8f64a8c560d2bf68a3ad1137979d17cffb36d30',
 
   # Three lines of non-changing comments so that
   # the commit queue can handle CLs rolling Fuchsia sdk
   # and whatever else without interference from each other.
-  'fuchsia_version': 'version:9.20220919.2.1',
+  'fuchsia_version': 'version:10.20221109.1.1',
 
   # Three lines of non-changing comments so that
   # the commit queue can handle CLs rolling android_sdk_build-tools_version
@@ -97,9 +98,9 @@
   'base/trace_event/common':
     Var('chromium_url') + '/chromium/src/base/trace_event/common.git' + '@' + '521ac34ebd795939c7e16b37d9d3ddb40e8ed556',
   'build':
-    Var('chromium_url') + '/chromium/src/build.git' + '@' + '7e7c21a9ac34c4fc2b255aa44d639efec9c33b90',
+    Var('chromium_url') + '/chromium/src/build.git' + '@' + '875cb19167f2e0d7b1eca89a4d5b5693421424c6',
   'buildtools':
-    Var('chromium_url') + '/chromium/src/buildtools.git' + '@' + '9174abb6ac087b46f22248dc713b6c0328b8f774',
+    Var('chromium_url') + '/chromium/src/buildtools.git' + '@' + '49ac7cf34ab2e59a10629a7a722cfb94348c4996',
   'buildtools/clang_format/script':
     Var('chromium_url') + '/external/github.com/llvm/llvm-project/clang/tools/clang-format.git' + '@' + '8b525d2747f2584fc35d8c7e612e66f377858df7',
   'buildtools/linux64': {
@@ -123,11 +124,11 @@
     'condition': 'host_os == "mac"',
   },
   'buildtools/third_party/libc++/trunk':
-    Var('chromium_url') + '/external/github.com/llvm/llvm-project/libcxx.git' + '@' + '2e919977e0030ce61bd19c40cefe31b995f1e2d4',
+    Var('chromium_url') + '/external/github.com/llvm/llvm-project/libcxx.git' + '@' + '4218f3525ad438b22b0e173d963515a09d143398',
   'buildtools/third_party/libc++abi/trunk':
-    Var('chromium_url') + '/external/github.com/llvm/llvm-project/libcxxabi.git' + '@' + 'db2a783a7d1ef0f0ef31da4b6e3de0c31fcfd93f',
+    Var('chromium_url') + '/external/github.com/llvm/llvm-project/libcxxabi.git' + '@' + '1a32724f721e1c3b6c590a07fe4a954344f15e48',
   'buildtools/third_party/libunwind/trunk':
-    Var('chromium_url') + '/external/github.com/llvm/llvm-project/libunwind.git' + '@' + '08ebcbe7b672a04e341cb3a88d8bf4276f96ac6e',
+    Var('chromium_url') + '/external/github.com/llvm/llvm-project/libunwind.git' + '@' + 'a318d6a4c283a9d342d2a1e20292c1496fe12997',
   'buildtools/win': {
     'packages': [
       {
@@ -153,13 +154,13 @@
   'test/mozilla/data':
     Var('chromium_url') + '/v8/deps/third_party/mozilla-tests.git' + '@' + 'f6c578a10ea707b1a8ab0b88943fe5115ce2b9be',
   'test/test262/data':
-    Var('chromium_url') + '/external/github.com/tc39/test262.git' + '@' + '58b7a2358286b918efd38eac4b2facbc8ada1206',
+    Var('chromium_url') + '/external/github.com/tc39/test262.git' + '@' + 'ade328d530525333751e8a3b58f02e18624da085',
   'third_party/android_ndk': {
     'url': Var('chromium_url') + '/android_ndk.git' + '@' + '8388a2be5421311dc75c5f937aae13d821a27f3d',
     'condition': 'checkout_android',
   },
   'third_party/android_platform': {
-    'url': Var('chromium_url') + '/chromium/src/third_party/android_platform.git' + '@' + '04b33506bfd9d0e866bd8bd62f4cbf323d84dc79',
+    'url': Var('chromium_url') + '/chromium/src/third_party/android_platform.git' + '@' + '1bf9b932433ebb78828bf3c8cd0ccc86b9ef4787',
     'condition': 'checkout_android',
   },
   'third_party/android_sdk/public': {
@@ -201,7 +202,7 @@
       'dep_type': 'cipd',
   },
   'third_party/catapult': {
-    'url': Var('chromium_url') + '/catapult.git' + '@' + 'ff03621a71c01a6f2b0f3bf2677cf815291a9e85',
+    'url': Var('chromium_url') + '/catapult.git' + '@' + 'f0b11967c94cba8f7cca91d2da20c98d4420fc25',
     'condition': 'checkout_android',
   },
   'third_party/colorama/src': {
@@ -209,7 +210,7 @@
     'condition': 'checkout_android',
   },
   'third_party/depot_tools':
-    Var('chromium_url') + '/chromium/tools/depot_tools.git' + '@' + 'a724859f7a9b3531c0373d86886a42314e772532',
+    Var('chromium_url') + '/chromium/tools/depot_tools.git' + '@' + 'ae1a70891738fb14f64fbb884e00b87ac663aa15',
   'third_party/fuchsia-sdk/sdk': {
     'packages': [
         {
@@ -226,9 +227,9 @@
   'third_party/googletest/src':
     Var('chromium_url') + '/external/github.com/google/googletest.git' + '@' + 'af29db7ec28d6df1c7f0f745186884091e602e07',
   'third_party/icu':
-    Var('chromium_url') + '/chromium/deps/icu.git' + '@' + '20f8ac695af59b6c830def7d4e95bfeb13dd7be5',
+    Var('chromium_url') + '/chromium/deps/icu.git' + '@' + 'da07448619763d1cde255b361324242646f5b268',
   'third_party/instrumented_libraries':
-    Var('chromium_url') + '/chromium/src/third_party/instrumented_libraries.git' + '@' + 'e09c4b66b6e87116eb190651421f1a6e2f3b9c52',
+    Var('chromium_url') + '/chromium/src/third_party/instrumented_libraries.git' + '@' + '7bb87a375ffc3effd17a50f690099dcfb9ee280b',
   'third_party/ittapi': {
     # Force checkout ittapi libraries to pass v8 header includes check on
     # bots that has check_v8_header_includes enabled.
@@ -236,13 +237,13 @@
     'condition': "checkout_ittapi or check_v8_header_includes",
   },
   'third_party/jinja2':
-    Var('chromium_url') + '/chromium/src/third_party/jinja2.git' + '@' + 'ee69aa00ee8536f61db6a451f3858745cf587de6',
+    Var('chromium_url') + '/chromium/src/third_party/jinja2.git' + '@' + '4633bf431193690c3491244f5a0acbe9ac776233',
   'third_party/jsoncpp/source':
     Var('chromium_url') + '/external/github.com/open-source-parsers/jsoncpp.git'+ '@' + '42e892d96e47b1f6e29844cc705e148ec4856448',
   'third_party/logdog/logdog':
     Var('chromium_url') + '/infra/luci/luci-py/client/libs/logdog' + '@' + '0b2078a90f7a638d576b3a7c407d136f2fb62399',
   'third_party/markupsafe':
-    Var('chromium_url') + '/chromium/src/third_party/markupsafe.git' + '@' + '1b882ef6372b58bfd55a3285f37ed801be9137cd',
+    Var('chromium_url') + '/chromium/src/third_party/markupsafe.git' + '@' + '13f4e8c9e206567eeb13bf585406ddc574005748',
   'third_party/ninja': {
     'packages': [
       {
@@ -262,9 +263,9 @@
       'condition': 'checkout_android',
   },
   'third_party/zlib':
-    Var('chromium_url') + '/chromium/src/third_party/zlib.git'+ '@' + 'd689fca54d7b43154f7cf77f785d19f2628fa133',
+    Var('chromium_url') + '/chromium/src/third_party/zlib.git'+ '@' + '8bbd6c3129b5146489f2321f054e855c347857f4',
   'tools/clang':
-    Var('chromium_url') + '/chromium/src/tools/clang.git' + '@' + 'a5e0d72349d028a4023927d6d166a8478355fac3',
+    Var('chromium_url') + '/chromium/src/tools/clang.git' + '@' + 'd3df9cc5362e0af4cda798b0612dde39783b3dc0',
   'tools/luci-go': {
       'packages': [
         {
@@ -573,11 +574,11 @@
   {
     'name': 'Download Fuchsia system images',
     'pattern': '.',
-    'condition': 'checkout_fuchsia',
+    'condition': 'checkout_fuchsia and checkout_fuchsia_product_bundles',
     'action': [
       'python3',
-      'build/fuchsia/update_images.py',
-      '--boot-images={checkout_fuchsia_boot_images}',
+      'build/fuchsia/update_product_bundles.py',
+      '{checkout_fuchsia_boot_images}',
     ],
   },
   {
diff -r -u --color up/v8/OWNERS nw/v8/OWNERS
--- up/v8/OWNERS	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/OWNERS	2023-01-19 16:46:35.940609645 -0500
@@ -5,6 +5,7 @@
 per-file .*=file:INFRA_OWNERS
 per-file .bazelrc=file:COMMON_OWNERS
 per-file .mailmap=file:COMMON_OWNERS
+per-file .ycm_extra_conf.py=file:COMMON_OWNERS
 per-file codereview.settings=file:INFRA_OWNERS
 per-file AUTHORS=file:COMMON_OWNERS
 per-file BUILD.bazel=file:COMMON_OWNERS
diff -r -u --color up/v8/bazel/defs.bzl nw/v8/bazel/defs.bzl
--- up/v8/bazel/defs.bzl	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/bazel/defs.bzl	2023-01-19 16:46:35.940609645 -0500
@@ -151,14 +151,6 @@
                 "-fno-integrated-as",
             ],
             "//conditions:default": [],
-        }) + select({
-            "@v8//bazel/config:is_debug":[
-                "-fvisibility=default",
-            ],
-            "//conditions:default": [
-                "-fvisibility=hidden",
-                "-fvisibility-inlines-hidden",
-            ],
         }),
         includes = ["include"],
         linkopts = select({
@@ -518,6 +510,7 @@
         ("v8_current_cpu", cpu),
         ("v8_dict_property_const_tracking", "false"),
         ("v8_enable_atomic_object_field_writes", "false"),
+        ("v8_enable_conservative_stack_scanning", "false"),
         ("v8_enable_concurrent_marking", "false"),
         ("v8_enable_i18n_support", icu),
         ("v8_enable_verify_predictable", "false"),
diff -r -u --color up/v8/gni/snapshot_toolchain.gni nw/v8/gni/snapshot_toolchain.gni
--- up/v8/gni/snapshot_toolchain.gni	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/gni/snapshot_toolchain.gni	2023-01-19 16:46:35.940609645 -0500
@@ -44,10 +44,6 @@
 #
 # There are test cases for this code posted as an attachment to
 # https://crbug.com/625353.
-#
-# TODO(GYP): Currently only regular (non-cross) compiles, and cross-compiles
-# from x64 hosts to Intel, ARM, or MIPS targets, are implemented. Add support
-# for the other supported configurations.
 
 if (v8_snapshot_toolchain == "") {
   if (current_os == host_os && current_cpu == host_cpu) {
@@ -69,22 +65,21 @@
     # therefore snapshots will need to be built using native mksnapshot
     # in combination with qemu
     v8_snapshot_toolchain = current_toolchain
+  } else if (host_cpu == current_cpu) {
+    # Cross-build from same ISA on one OS to another. For example:
+    # * targeting win/x64 on a linux/x64 host
+    # * targeting win/arm64 on a mac/arm64 host
+    v8_snapshot_toolchain = host_toolchain
   } else if (host_cpu == "arm64" && current_cpu == "x64") {
     # Cross-build from arm64 to intel (likely on an Apple Silicon mac).
     v8_snapshot_toolchain =
         "//build/toolchain/${host_os}:clang_arm64_v8_$v8_current_cpu"
   } else if (host_cpu == "x64") {
     # This is a cross-compile from an x64 host to either a non-Intel target
-    # cpu or a different target OS. Clang will always be used by default on the
-    # host, unless this is a ChromeOS build, in which case the same toolchain
-    # (Clang or GCC) will be used for target and host by default.
-    if (is_chromeos && !is_clang) {
-      _clang = ""
-    } else {
-      _clang = "clang_"
-    }
+    # cpu or to 32-bit x86 on a different target OS.
 
-    if (v8_current_cpu == "x64" || v8_current_cpu == "x86") {
+    assert(v8_current_cpu != "x64", "handled by host_cpu == current_cpu branch")
+    if (v8_current_cpu == "x86") {
       _cpus = v8_current_cpu
     } else if (v8_current_cpu == "arm64" || v8_current_cpu == "mips64el" ||
                v8_current_cpu == "riscv64" || v8_current_cpu == "loong64") {
@@ -104,7 +99,7 @@
     }
 
     if (_cpus != "") {
-      v8_snapshot_toolchain = "//build/toolchain/${host_os}:${_clang}${_cpus}"
+      v8_snapshot_toolchain = "//build/toolchain/${host_os}:clang_${_cpus}"
     } else if (is_win && v8_current_cpu == "arm64") {
       # cross compile Windows arm64 with host toolchain.
       v8_snapshot_toolchain = host_toolchain
diff -r -u --color up/v8/gni/v8.gni nw/v8/gni/v8.gni
--- up/v8/gni/v8.gni	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/gni/v8.gni	2023-01-19 16:46:35.940609645 -0500
@@ -242,6 +242,15 @@
   }
 }
 
+template("v8_static_lib") {
+  static_library(target_name) {
+    forward_variables_from(invoker, "*", [ "configs" ])
+    configs += invoker.configs
+    configs -= v8_remove_configs
+    configs += v8_add_configs
+  }
+}
+
 template("v8_executable") {
   executable(target_name) {
     forward_variables_from(invoker,
Only in nw/v8: gypfiles
diff -r -u --color up/v8/include/cppgc/README.md nw/v8/include/cppgc/README.md
--- up/v8/include/cppgc/README.md	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/cppgc/README.md	2023-01-19 16:46:35.940609645 -0500
@@ -26,6 +26,8 @@
 References to objects belonging to another thread's heap are modeled using cross-thread roots.
 This is even true for on-heap to on-heap references.
 
+Oilpan heaps may generally not be accessed from different threads unless otherwise noted.
+
 ## Heap partitioning
 
 Oilpan's heaps are partitioned into spaces.
diff -r -u --color up/v8/include/cppgc/heap-handle.h nw/v8/include/cppgc/heap-handle.h
--- up/v8/include/cppgc/heap-handle.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/cppgc/heap-handle.h	2023-01-19 16:46:35.940609645 -0500
@@ -19,6 +19,11 @@
  * Opaque handle used for additional heap APIs.
  */
 class HeapHandle {
+ public:
+  // Deleted copy ctor to avoid treating the type by value.
+  HeapHandle(const HeapHandle&) = delete;
+  HeapHandle& operator=(const HeapHandle&) = delete;
+
  private:
   HeapHandle() = default;
 
diff -r -u --color up/v8/include/cppgc/platform.h nw/v8/include/cppgc/platform.h
--- up/v8/include/cppgc/platform.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/cppgc/platform.h	2023-01-19 16:46:35.940609645 -0500
@@ -33,8 +33,9 @@
   virtual ~Platform() = default;
 
   /**
-   * Returns the allocator used by cppgc to allocate its heap and various
-   * support structures.
+   * \returns the allocator used by cppgc to allocate its heap and various
+   * support structures. Returning nullptr results in using the `PageAllocator`
+   * provided by `cppgc::InitializeProcess()` instead.
    */
   virtual PageAllocator* GetPageAllocator() = 0;
 
@@ -133,9 +134,10 @@
  * Can be called multiple times when paired with `ShutdownProcess()`.
  *
  * \param page_allocator The allocator used for maintaining meta data. Must stay
- *   always alive and not change between multiple calls to InitializeProcess.
+ *   always alive and not change between multiple calls to InitializeProcess. If
+ *   no allocator is provided, a default internal version will be used.
  */
-V8_EXPORT void InitializeProcess(PageAllocator* page_allocator);
+V8_EXPORT void InitializeProcess(PageAllocator* page_allocator = nullptr);
 
 /**
  * Must be called after destroying the last used heap. Some process-global
diff -r -u --color up/v8/include/js_protocol.pdl nw/v8/include/js_protocol.pdl
--- up/v8/include/js_protocol.pdl	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/js_protocol.pdl	2023-01-19 16:46:35.940609645 -0500
@@ -458,13 +458,14 @@
       # New value for breakpoints active state.
       boolean active
 
-  # Defines pause on exceptions state. Can be set to stop on all exceptions, uncaught exceptions or
-  # no exceptions. Initial pause on exceptions state is `none`.
+  # Defines pause on exceptions state. Can be set to stop on all exceptions, uncaught exceptions,
+  # or caught exceptions, no exceptions. Initial pause on exceptions state is `none`.
   command setPauseOnExceptions
     parameters
       # Pause on exceptions mode.
       enum state
         none
+        caught
         uncaught
         all
 
diff -r -u --color up/v8/include/libplatform/libplatform.h nw/v8/include/libplatform/libplatform.h
--- up/v8/include/libplatform/libplatform.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/libplatform/libplatform.h	2023-01-19 16:46:35.940609645 -0500
@@ -36,7 +36,7 @@
  * If |tracing_controller| is nullptr, the default platform will create a
  * v8::platform::TracingController instance and use it.
  */
-V8_PLATFORM_EXPORT std::unique_ptr<v8::Platform> NewDefaultPlatform(
+V8_EXPORT std::unique_ptr<v8::Platform> NewDefaultPlatform(
     int thread_pool_size = 0,
     IdleTaskSupport idle_task_support = IdleTaskSupport::kDisabled,
     InProcessStackDumping in_process_stack_dumping =
@@ -74,7 +74,8 @@
  * the |behavior| parameter, this call does not block if no task is pending. The
  * |platform| has to be created using |NewDefaultPlatform|.
  */
-V8_PLATFORM_EXPORT bool PumpMessageLoop(
+
+V8_EXPORT bool PumpMessageLoop(
     v8::Platform* platform, v8::Isolate* isolate,
     MessageLoopBehavior behavior = MessageLoopBehavior::kDoNotWait);
 
@@ -85,7 +86,7 @@
  * This call does not block if no task is pending. The |platform| has to be
  * created using |NewDefaultPlatform|.
  */
-V8_PLATFORM_EXPORT void RunIdleTasks(v8::Platform* platform,
+V8_EXPORT void RunIdleTasks(v8::Platform* platform,
                                      v8::Isolate* isolate,
                                      double idle_time_in_seconds);
 
diff -r -u --color up/v8/include/v8-array-buffer.h nw/v8/include/v8-array-buffer.h
--- up/v8/include/v8-array-buffer.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/v8-array-buffer.h	2023-01-19 16:46:35.940609645 -0500
@@ -170,7 +170,8 @@
      * while kReservation is for larger allocations with the ability to set
      * access permissions.
      */
-    enum class AllocationMode { kNormal, kReservation };
+    enum class AllocationMode { kNormal, kReservation, kNodeJS };
+    virtual void Free(void* data, size_t length, AllocationMode mode);
 
     /**
      * Convenience allocator.
@@ -211,6 +212,8 @@
    */
   static Local<ArrayBuffer> New(Isolate* isolate,
                                 std::shared_ptr<BackingStore> backing_store);
+  static Local<ArrayBuffer> NewNode(Isolate* isolate,
+                                std::shared_ptr<BackingStore> backing_store);
 
   /**
    * Returns a new standalone BackingStore that is allocated using the array
@@ -241,18 +244,46 @@
   bool IsDetachable() const;
 
   /**
+   * Returns true if this ArrayBuffer has been detached.
+   */
+  bool WasDetached() const;
+
+  /**
    * Detaches this ArrayBuffer and all its views (typed arrays).
    * Detaching sets the byte length of the buffer and all typed arrays to zero,
    * preventing JavaScript from ever accessing underlying backing store.
    * ArrayBuffer should have been externalized and must be detachable.
    */
+  V8_DEPRECATE_SOON(
+      "Use the version which takes a key parameter (passing a null handle is "
+      "ok).")
   void Detach();
 
+  void set_nodejs(bool);
+
+  /**
+   * Detaches this ArrayBuffer and all its views (typed arrays).
+   * Detaching sets the byte length of the buffer and all typed arrays to zero,
+   * preventing JavaScript from ever accessing underlying backing store.
+   * ArrayBuffer should have been externalized and must be detachable. Returns
+   * Nothing if the key didn't pass the [[ArrayBufferDetachKey]] check,
+   * Just(true) otherwise.
+   */
+  V8_WARN_UNUSED_RESULT Maybe<bool> Detach(v8::Local<v8::Value> key);
+
+  /**
+   * Sets the ArrayBufferDetachKey.
+   */
+  void SetDetachKey(v8::Local<v8::Value> key);
+
   /**
    * Get a shared pointer to the backing store of this array buffer. This
    * pointer coordinates the lifetime management of the internal storage
    * with any live ArrayBuffers on the heap, even across isolates. The embedder
    * should not attempt to manage lifetime of the storage through other means.
+   *
+   * The returned shared pointer will not be empty, even if the ArrayBuffer has
+   * been detached. Use |WasDetached| to tell if it has been detached instead.
    */
   std::shared_ptr<BackingStore> GetBackingStore();
 
diff -r -u --color up/v8/include/v8-callbacks.h nw/v8/include/v8-callbacks.h
--- up/v8/include/v8-callbacks.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/v8-callbacks.h	2023-01-19 16:46:35.940609645 -0500
@@ -237,7 +237,8 @@
 enum class CrashKeyId {
   kIsolateAddress,
   kReadonlySpaceFirstPageAddress,
-  kMapSpaceFirstPageAddress,
+  kMapSpaceFirstPageAddress V8_ENUM_DEPRECATE_SOON("Map space got removed"),
+  kOldSpaceFirstPageAddress,
   kCodeRangeBaseAddress,
   kCodeSpaceFirstPageAddress,
   kDumpType,
diff -r -u --color up/v8/include/v8-context.h nw/v8/include/v8-context.h
--- up/v8/include/v8-context.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/v8-context.h	2023-01-19 16:46:35.940609645 -0500
@@ -169,6 +169,9 @@
   /** Returns the microtask queue associated with a current context. */
   MicrotaskQueue* GetMicrotaskQueue();
 
+  /** Sets the microtask queue associated with the current context. */
+  void SetMicrotaskQueue(MicrotaskQueue* queue);
+
   /**
    * The field at kDebugIdIndex used to be reserved for the inspector.
    * It now serves no purpose.
diff -r -u --color up/v8/include/v8-cppgc.h nw/v8/include/v8-cppgc.h
--- up/v8/include/v8-cppgc.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/v8-cppgc.h	2023-01-19 16:46:35.940609645 -0500
@@ -95,6 +95,10 @@
 
 /**
  * A heap for allocating managed C++ objects.
+ *
+ * Similar to v8::Isolate, the heap may only be accessed from one thread at a
+ * time. The heap may be used from different threads using the
+ * v8::Locker/v8::Unlocker APIs which is different from generic Oilpan.
  */
 class V8_EXPORT CppHeap {
  public:
diff -r -u --color up/v8/include/v8-data.h nw/v8/include/v8-data.h
--- up/v8/include/v8-data.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/v8-data.h	2023-01-19 16:46:35.940609645 -0500
@@ -53,7 +53,7 @@
   bool IsContext() const;
 
  private:
-  Data();
+  Data() = delete;
 };
 
 /**
diff -r -u --color up/v8/include/v8-debug.h nw/v8/include/v8-debug.h
--- up/v8/include/v8-debug.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/v8-debug.h	2023-01-19 16:46:35.940609645 -0500
@@ -130,6 +130,8 @@
     kDetailed = kOverview | kIsEval | kIsConstructor | kScriptNameOrSourceURL
   };
 
+  V8_DEPRECATED("Use Isolate version")
+                Local<StackFrame> GetFrame(uint32_t index) const;
   /**
    * Returns a StackFrame at a particular index.
    */
diff -r -u --color up/v8/include/v8-embedder-heap.h nw/v8/include/v8-embedder-heap.h
--- up/v8/include/v8-embedder-heap.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/v8-embedder-heap.h	2023-01-19 16:46:35.940609645 -0500
@@ -72,7 +72,7 @@
 class V8_EXPORT
 // GCC doesn't like combining __attribute__(()) with [[deprecated]].
 #ifdef __clang__
-V8_DEPRECATE_SOON("Use CppHeap when working with v8::TracedReference.")
+V8_DEPRECATED("Use CppHeap when working with v8::TracedReference.")
 #endif  // __clang__
     EmbedderHeapTracer {
  public:
diff -r -u --color up/v8/include/v8-initialization.h nw/v8/include/v8-initialization.h
--- up/v8/include/v8-initialization.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/v8-initialization.h	2023-01-19 16:46:35.951442975 -0500
@@ -151,6 +151,7 @@
    */
   static bool InitializeICUDefaultLocation(const char* exec_path,
                                            const char* icu_data_file = nullptr);
+  static void* RawICUData();
 
   /**
    * Initialize the external startup data. The embedder only needs to
diff -r -u --color up/v8/include/v8-internal.h nw/v8/include/v8-internal.h
--- up/v8/include/v8-internal.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/v8-internal.h	2023-01-19 16:46:35.951442975 -0500
@@ -581,6 +581,8 @@
   static const int kNodeStateMask = 0x3;
   static const int kNodeStateIsWeakValue = 2;
 
+  static const int kTracedNodeClassIdOffset = kApiSystemPointerSize;
+
   static const int kFirstNonstringType = 0x80;
   static const int kOddballType = 0x83;
   static const int kForeignType = 0xcc;
diff -r -u --color up/v8/include/v8-isolate.h nw/v8/include/v8-isolate.h
--- up/v8/include/v8-isolate.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/v8-isolate.h	2023-01-19 16:46:35.951442975 -0500
@@ -209,6 +209,7 @@
  */
 class V8_EXPORT Isolate {
  public:
+  ArrayBuffer::Allocator* array_buffer_allocator();
   /**
    * Initial configuration parameters for a new Isolate.
    */
@@ -296,6 +297,7 @@
     OOMErrorCallback oom_error_callback = nullptr;
   };
 
+  void SetArrayBufferAllocatorShared(std::shared_ptr<ArrayBuffer::Allocator> allocator);
   /**
    * Stack-allocated class which sets the isolate for all operations
    * executed within a local scope.
@@ -954,22 +956,20 @@
    * Attaches a managed C++ heap as an extension to the JavaScript heap. The
    * embedder maintains ownership of the CppHeap. At most one C++ heap can be
    * attached to V8.
+   *
    * AttachCppHeap cannot be used simultaneously with SetEmbedderHeapTracer.
    *
-   * This is an experimental feature and may still change significantly.
+   * Multi-threaded use requires the use of v8::Locker/v8::Unlocker, see
+   * CppHeap.
    */
   void AttachCppHeap(CppHeap*);
 
   /**
    * Detaches a managed C++ heap if one was attached using `AttachCppHeap()`.
-   *
-   * This is an experimental feature and may still change significantly.
    */
   void DetachCppHeap();
 
   /**
-   * This is an experimental feature and may still change significantly.
-
    * \returns the C++ heap managed by V8. Only available if such a heap has been
    *   attached using `AttachCppHeap()`.
    */
@@ -1526,8 +1526,10 @@
 
   void SetWasmLoadSourceMapCallback(WasmLoadSourceMapCallback callback);
 
+  V8_DEPRECATED("Wasm SIMD is always enabled")
   void SetWasmSimdEnabledCallback(WasmSimdEnabledCallback callback);
 
+  V8_DEPRECATED("Wasm exceptions are always enabled")
   void SetWasmExceptionsEnabledCallback(WasmExceptionsEnabledCallback callback);
 
   void SetSharedArrayBufferConstructorEnabledCallback(
diff -r -u --color up/v8/include/v8-microtask-queue.h nw/v8/include/v8-microtask-queue.h
--- up/v8/include/v8-microtask-queue.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/v8-microtask-queue.h	2023-01-19 16:46:35.951442975 -0500
@@ -118,7 +118,12 @@
  public:
   enum Type { kRunMicrotasks, kDoNotRunMicrotasks };
 
+  V8_DEPRECATE_SOON(
+      "May be incorrect if context was created with non-default microtask "
+      "queue")
   MicrotasksScope(Isolate* isolate, Type type);
+
+  MicrotasksScope(Local<Context> context, Type type);
   MicrotasksScope(Isolate* isolate, MicrotaskQueue* microtask_queue, Type type);
   ~MicrotasksScope();
 
diff -r -u --color up/v8/include/v8-platform.h nw/v8/include/v8-platform.h
--- up/v8/include/v8-platform.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/v8-platform.h	2023-01-19 16:46:35.951442975 -0500
@@ -10,6 +10,37 @@
 #include <stdlib.h>  // For abort.
 #include <memory>
 #include <string>
+#include "v8config.h"
+
+#ifdef V8_OS_WIN
+
+// Setup for Windows DLL export/import. When building the V8 DLL the
+// BUILDING_V8_SHARED needs to be defined. When building a program which uses
+// the V8 DLL USING_V8_SHARED needs to be defined. When either building the V8
+// static library or building a program which uses the V8 static library neither
+// BUILDING_V8_SHARED nor USING_V8_SHARED should be defined.
+#ifdef BUILDING_V8_SHARED
+# define V8_EXPORT __declspec(dllexport)
+#elif USING_V8_SHARED
+# define V8_EXPORT __declspec(dllimport)
+#else
+# define V8_EXPORT
+#endif  // BUILDING_V8_SHARED
+
+#else  // V8_OS_WIN
+
+// Setup for Linux shared library export.
+#if V8_HAS_ATTRIBUTE_VISIBILITY
+# ifdef BUILDING_V8_SHARED
+#  define V8_EXPORT __attribute__ ((visibility("default")))
+# else
+#  define V8_EXPORT
+# endif
+#else
+# define V8_EXPORT
+#endif
+
+#endif  // V8_OS_WIN
 
 #include "v8config.h"  // NOLINT(build/include_directory)
 
@@ -923,6 +954,7 @@
 
   /**
    * Allows the embedder to manage memory page allocations.
+   * Returning nullptr will cause V8 to use the default page allocator.
    */
   virtual PageAllocator* GetPageAllocator() = 0;
 
@@ -1115,6 +1147,7 @@
    */
   V8_EXPORT static double SystemClockTimeMillis();
 };
+void V8_EXPORT SetTLSPlatform(Platform* platform);
 
 }  // namespace v8
 
diff -r -u --color up/v8/include/v8-primitive-object.h nw/v8/include/v8-primitive-object.h
--- up/v8/include/v8-primitive-object.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/v8-primitive-object.h	2023-01-19 16:46:35.951442975 -0500
@@ -78,6 +78,8 @@
  */
 class V8_EXPORT StringObject : public Object {
  public:
+  V8_DEPRECATED("Use Isolate* version") static
+                       Local<Value> New(Local<String> value);
   static Local<Value> New(Isolate* isolate, Local<String> value);
 
   Local<String> ValueOf() const;
diff -r -u --color up/v8/include/v8-primitive.h nw/v8/include/v8-primitive.h
--- up/v8/include/v8-primitive.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/v8-primitive.h	2023-01-19 16:46:35.951442975 -0500
@@ -69,6 +69,9 @@
     return reinterpret_cast<PrimitiveArray*>(data);
   }
 
+  V8_DEPRECATED("Use Isolate version")
+                void Set(int index, Local<Primitive> item);
+  V8_DEPRECATED("Use Isolate version") Local<Primitive> Get(int index);
  private:
   static void CheckCast(Data* obj);
 };
@@ -140,6 +143,7 @@
    * Returns the number of bytes in the UTF-8 encoded
    * representation of this string.
    */
+  V8_DEPRECATED("Use Isolate version instead") int Utf8Length() const;
   int Utf8Length(Isolate* isolate) const;
 
   /**
@@ -196,13 +200,24 @@
   // 16-bit character codes.
   int Write(Isolate* isolate, uint16_t* buffer, int start = 0, int length = -1,
             int options = NO_OPTIONS) const;
+  V8_DEPRECATED("Use Isolate* version")
+                int Write(uint16_t* buffer, int start = 0, int length = -1,
+                          int options = NO_OPTIONS) const;
   // One byte characters.
   int WriteOneByte(Isolate* isolate, uint8_t* buffer, int start = 0,
                    int length = -1, int options = NO_OPTIONS) const;
+  V8_DEPRECATED("Use Isolate* version")
+                int WriteOneByte(uint8_t* buffer, int start = 0,
+                                 int length = -1, int options = NO_OPTIONS)
+                    const;
   // UTF-8 encoded characters.
   int WriteUtf8(Isolate* isolate, char* buffer, int length = -1,
                 int* nchars_ref = nullptr, int options = NO_OPTIONS) const;
 
+  V8_DEPRECATED("Use Isolate* version")
+                int WriteUtf8(char* buffer, int length = -1,
+                              int* nchars_ref = NULL, int options = NO_OPTIONS)
+                    const;
   /**
    * A zero length string.
    */
@@ -446,6 +461,9 @@
   static Local<String> Concat(Isolate* isolate, Local<String> left,
                               Local<String> right);
 
+  V8_DEPRECATED("Use Isolate* version") static
+                       Local<String> Concat(Local<String> left,
+                                            Local<String> right);
   /**
    * Creates a new external string using the data defined in the given
    * resource. When the external string is no longer live on V8's heap the
@@ -509,6 +527,8 @@
    */
   class V8_EXPORT Utf8Value {
    public:
+   V8_DEPRECATED("Use Isolate version")
+                  explicit Utf8Value(Local<v8::Value> obj);
     Utf8Value(Isolate* isolate, Local<v8::Value> obj);
     ~Utf8Value();
     char* operator*() { return str_; }
@@ -532,6 +552,7 @@
    */
   class V8_EXPORT Value {
    public:
+   V8_DEPRECATED("Use Isolate version") explicit Value(Local<v8::Value> obj);
     Value(Isolate* isolate, Local<v8::Value> obj);
     ~Value();
     uint16_t* operator*() { return str_; }
diff -r -u --color up/v8/include/v8-profiler.h nw/v8/include/v8-profiler.h
--- up/v8/include/v8-profiler.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/v8-profiler.h	2023-01-19 16:46:35.951442975 -0500
@@ -175,6 +175,32 @@
   static const int kNoColumnNumberInfo = Message::kNoColumnInfo;
 };
 
+/**
+ * An interface for exporting data from V8, using "push" model.
+ */
+class V8_EXPORT OutputStream {
+ public:
+  enum WriteResult { kContinue = 0, kAbort = 1 };
+  virtual ~OutputStream() = default;
+  /** Notify about the end of stream. */
+  virtual void EndOfStream() = 0;
+  /** Get preferred output chunk size. Called only once. */
+  virtual int GetChunkSize() { return 1024; }
+  /**
+   * Writes the next chunk of snapshot data into the stream. Writing
+   * can be stopped by returning kAbort as function result. EndOfStream
+   * will not be called in case writing was aborted.
+   */
+  virtual WriteResult WriteAsciiChunk(char* data, int size) = 0;
+  /**
+   * Writes the next chunk of heap stats data into the stream. Writing
+   * can be stopped by returning kAbort as function result. EndOfStream
+   * will not be called in case writing was aborted.
+   */
+  virtual WriteResult WriteHeapStatsChunk(HeapStatsUpdate* data, int count) {
+    return kAbort;
+  }
+};
 
 /**
  * CpuProfile contains a CPU profile in a form of top-down call tree
@@ -182,6 +208,9 @@
  */
 class V8_EXPORT CpuProfile {
  public:
+  enum SerializationFormat {
+    kJSON = 0  // See format description near 'Serialize' method.
+  };
   /** Returns CPU profile title. */
   Local<String> GetTitle() const;
 
@@ -235,6 +264,25 @@
    * All pointers to nodes previously returned become invalid.
    */
   void Delete();
+
+  /**
+   * Prepare a serialized representation of the profile. The result
+   * is written into the stream provided in chunks of specified size.
+   *
+   * For the JSON format, heap contents are represented as an object
+   * with the following structure:
+   *
+   *  {
+   *    nodes: [nodes array],
+   *    startTime: number,
+   *    endTime: number
+   *    samples: [strings array]
+   *    timeDeltas: [numbers array]
+   *  }
+   *
+   */
+  void Serialize(OutputStream* stream,
+                 SerializationFormat format = kJSON) const;
 };
 
 enum CpuProfilingMode {
@@ -576,37 +624,6 @@
   const HeapGraphEdge* GetChild(int index) const;
 };
 
-
-/**
- * An interface for exporting data from V8, using "push" model.
- */
-class V8_EXPORT OutputStream {
- public:
-  enum WriteResult {
-    kContinue = 0,
-    kAbort = 1
-  };
-  virtual ~OutputStream() = default;
-  /** Notify about the end of stream. */
-  virtual void EndOfStream() = 0;
-  /** Get preferred output chunk size. Called only once. */
-  virtual int GetChunkSize() { return 1024; }
-  /**
-   * Writes the next chunk of snapshot data into the stream. Writing
-   * can be stopped by returning kAbort as function result. EndOfStream
-   * will not be called in case writing was aborted.
-   */
-  virtual WriteResult WriteAsciiChunk(char* data, int size) = 0;
-  /**
-   * Writes the next chunk of heap stats data into the stream. Writing
-   * can be stopped by returning kAbort as function result. EndOfStream
-   * will not be called in case writing was aborted.
-   */
-  virtual WriteResult WriteHeapStatsChunk(HeapStatsUpdate* data, int count) {
-    return kAbort;
-  }
-};
-
 /**
  * HeapSnapshots record the state of the JS heap at some moment.
  */
diff -r -u --color up/v8/include/v8-script.h nw/v8/include/v8-script.h
--- up/v8/include/v8-script.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/v8-script.h	2023-01-19 16:46:35.951442975 -0500
@@ -621,6 +621,7 @@
       CompileOptions options = kNoCompileOptions,
       NoCacheReason no_cache_reason = kNoCacheNoReason);
 
+  static MaybeLocal<Module> CompileModuleWithCache(Isolate* isolate, Source* source);
   /**
    * Returns a task which streams script data into V8, or NULL if the script
    * cannot be streamed. The user is responsible for running the task on a
@@ -798,6 +799,9 @@
   return reinterpret_cast<Module*>(data);
 }
 
+void V8_EXPORT FixSourceNWBin(Isolate* v8_isolate, Local<UnboundScript> script);
+void V8_EXPORT FixSourceNWBin(Isolate* v8_isolate, Local<Module> module);
+
 }  // namespace v8
 
 #endif  // INCLUDE_V8_SCRIPT_H_
diff -r -u --color up/v8/include/v8-traced-handle.h nw/v8/include/v8-traced-handle.h
--- up/v8/include/v8-traced-handle.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/v8-traced-handle.h	2023-01-19 16:46:35.951442975 -0500
@@ -403,7 +403,7 @@
   using I = internal::Internals;
   if (IsEmpty()) return;
   internal::Address* obj = reinterpret_cast<internal::Address*>(val_);
-  uint8_t* addr = reinterpret_cast<uint8_t*>(obj) + I::kNodeClassIdOffset;
+  uint8_t* addr = reinterpret_cast<uint8_t*>(obj) + I::kTracedNodeClassIdOffset;
   *reinterpret_cast<uint16_t*>(addr) = class_id;
 }
 
@@ -411,7 +411,7 @@
   using I = internal::Internals;
   if (IsEmpty()) return 0;
   internal::Address* obj = reinterpret_cast<internal::Address*>(val_);
-  uint8_t* addr = reinterpret_cast<uint8_t*>(obj) + I::kNodeClassIdOffset;
+  uint8_t* addr = reinterpret_cast<uint8_t*>(obj) + I::kTracedNodeClassIdOffset;
   return *reinterpret_cast<uint16_t*>(addr);
 }
 
diff -r -u --color up/v8/include/v8-value.h nw/v8/include/v8-value.h
--- up/v8/include/v8-value.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/v8-value.h	2023-01-19 16:46:35.951442975 -0500
@@ -245,6 +245,11 @@
   bool IsWeakSet() const;
 
   /**
+   * Returns true if this value is a WeakRef.
+   */
+  bool IsWeakRef() const;
+
+  /**
    * Returns true if this value is an ArrayBuffer.
    */
   bool IsArrayBuffer() const;
@@ -397,6 +402,10 @@
    */
   Local<Boolean> ToBoolean(Isolate* isolate) const;
 
+  V8_DEPRECATED("Use maybe version") Local<Boolean> ToBoolean() const;
+  V8_DEPRECATED("Use maybe version") Local<String> ToString() const;
+  V8_DEPRECATED("Use maybe version") Local<Object> ToObject() const;
+  V8_DEPRECATED("Use maybe version") Local<Integer> ToInteger() const;
   /**
    * Attempts to convert a string to an array index.
    * Returns an empty handle if the conversion fails.
@@ -418,7 +427,14 @@
   /** Returns the equivalent of `ToInt32()->Value()`. */
   V8_WARN_UNUSED_RESULT Maybe<int32_t> Int32Value(Local<Context> context) const;
 
+  V8_DEPRECATED("Use maybe version") bool BooleanValue() const;
+  V8_DEPRECATED("Use maybe version") double NumberValue() const;
+  V8_DEPRECATED("Use maybe version") int64_t IntegerValue() const;
+  V8_DEPRECATED("Use maybe version") uint32_t Uint32Value() const;
+  V8_DEPRECATED("Use maybe version") int32_t Int32Value() const;
+
   /** JS == */
+  V8_DEPRECATED("Use maybe version") bool Equals(Local<Value> that) const;
   V8_WARN_UNUSED_RESULT Maybe<bool> Equals(Local<Context> context,
                                            Local<Value> that) const;
   bool StrictEquals(Local<Value> that) const;
diff -r -u --color up/v8/include/v8-version.h nw/v8/include/v8-version.h
--- up/v8/include/v8-version.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/v8-version.h	2023-01-19 16:46:35.951442975 -0500
@@ -9,9 +9,9 @@
 // NOTE these macros are used by some of the tool scripts and the build
 // system so their names cannot be changed without changing the scripts.
 #define V8_MAJOR_VERSION 10
-#define V8_MINOR_VERSION 8
-#define V8_BUILD_NUMBER 168
-#define V8_PATCH_LEVEL 21
+#define V8_MINOR_VERSION 9
+#define V8_BUILD_NUMBER 194
+#define V8_PATCH_LEVEL 9
 
 // Use 1 for candidates and 0 otherwise.
 // (Boolean macro values are not supported by all preprocessors.)
diff -r -u --color up/v8/include/v8-wasm.h nw/v8/include/v8-wasm.h
--- up/v8/include/v8-wasm.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/v8-wasm.h	2023-01-19 16:46:35.951442975 -0500
@@ -121,6 +121,9 @@
   static void CheckCast(Value* obj);
 };
 
+V8_DEPRECATED("Use WasmModuleObject")
+              typedef WasmModuleObject WasmCompiledModule;
+
 /**
  * The V8 interface for WebAssembly streaming compilation. When streaming
  * compilation is initiated, V8 passes a {WasmStreaming} object to the embedder
diff -r -u --color up/v8/include/v8config.h nw/v8/include/v8config.h
--- up/v8/include/v8config.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/include/v8config.h	2023-01-19 16:46:35.951442975 -0500
@@ -288,6 +288,9 @@
 //
 //  V8_HAS_ATTRIBUTE_ALWAYS_INLINE      - __attribute__((always_inline))
 //                                        supported
+//  V8_HAS_ATTRIBUTE_CONSTINIT          - __attribute__((require_constant_
+//                                                       initialization))
+//                                        supported
 //  V8_HAS_ATTRIBUTE_NONNULL            - __attribute__((nonnull)) supported
 //  V8_HAS_ATTRIBUTE_NOINLINE           - __attribute__((noinline)) supported
 //  V8_HAS_ATTRIBUTE_UNUSED             - __attribute__((unused)) supported
@@ -334,6 +337,8 @@
 #endif
 
 # define V8_HAS_ATTRIBUTE_ALWAYS_INLINE (__has_attribute(always_inline))
+# define V8_HAS_ATTRIBUTE_CONSTINIT \
+    (__has_attribute(require_constant_initialization))
 # define V8_HAS_ATTRIBUTE_NONNULL (__has_attribute(nonnull))
 # define V8_HAS_ATTRIBUTE_NOINLINE (__has_attribute(noinline))
 # define V8_HAS_ATTRIBUTE_UNUSED (__has_attribute(unused))
@@ -450,6 +455,16 @@
 #endif
 
 
+// A macro to mark a declaration as requiring constant initialization.
+// Use like:
+//   int* foo V8_CONSTINIT;
+#if V8_HAS_ATTRIBUTE_CONSTINIT
+# define V8_CONSTINIT __attribute__((require_constant_initialization))
+#else
+# define V8_CONSTINIT
+#endif
+
+
 // A macro to mark specific arguments as non-null.
 // Use like:
 //   int add(int* x, int y, int* z) V8_NONNULL(1, 3) { return *x + y + *z; }
diff -r -u --color up/v8/infra/testing/builders.pyl nw/v8/infra/testing/builders.pyl
--- up/v8/infra/testing/builders.pyl	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/infra/testing/builders.pyl	2023-01-19 16:46:35.951442975 -0500
@@ -161,7 +161,6 @@
     'tests': [
       {'name': 'benchmarks'},
       {'name': 'benchmarks', 'variant': 'extra'},
-      {'name': 'gcmole'},
       {'name': 'mjsunit_sp_frame_access'},
       {'name': 'mozilla'},
       {'name': 'mozilla', 'variant': 'extra'},
@@ -180,6 +179,7 @@
         ],
         'shards': 4,
       },
+      {'name': 'gcmole'},
     ],
   },
   'v8_linux_optional_rel': {
@@ -847,7 +847,7 @@
       {'name': 'mozilla'},
       {'name': 'test262', 'variant': 'default', 'shards': 2},
       {'name': 'v8testing', 'shards': 2},
-      {'name': 'v8testing', 'variant': 'extra'},
+      {'name': 'v8testing', 'variant': 'extra', 'shards': 2},
     ],
   },
   ##############################################################################
@@ -989,7 +989,6 @@
     'tests': [
       {'name': 'benchmarks'},
       {'name': 'benchmarks', 'variant': 'extra'},
-      {'name': 'gcmole'},
       {'name': 'mjsunit_sp_frame_access'},
       {'name': 'mozilla'},
       {'name': 'mozilla', 'variant': 'extra'},
@@ -1051,6 +1050,7 @@
         'test_args': ['--extra-flags', '--noenable-avx'],
         'shards': 2
       },
+      {'name': 'gcmole'},
     ],
   },
   'V8 Linux - arm64 - sim - CFI': {
@@ -1807,8 +1807,8 @@
     'tests': [
       {'name': 'mozilla'},
       {'name': 'test262', 'variant': 'default'},
-      {'name': 'v8testing'},
-      {'name': 'v8testing', 'variant': 'extra'},
+      {'name': 'v8testing', 'shards': 2},
+      {'name': 'v8testing', 'variant': 'extra', 'shards': 2},
     ],
   },
   'V8 Win64 - debug': {
@@ -1829,7 +1829,7 @@
     'tests': [
       {'name': 'mozilla'},
       {'name': 'test262', 'variant': 'default'},
-      {'name': 'v8testing'},
+      {'name': 'v8testing', 'shards': 2},
     ],
   },
   'V8 Win64 ASAN': {
diff -r -u --color up/v8/src/api/api-natives.cc nw/v8/src/api/api-natives.cc
--- up/v8/src/api/api-natives.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/api/api-natives.cc	2023-01-19 16:46:35.951442975 -0500
@@ -83,6 +83,8 @@
         InstantiateFunction(isolate,
                             Handle<FunctionTemplateInfo>::cast(getter)),
         Object);
+    Handle<CodeT> trampoline = BUILTIN_CODE(isolate, DebugBreakTrampoline);
+    Handle<JSFunction>::cast(getter)->set_code(*trampoline);
   }
   if (setter->IsFunctionTemplateInfo() &&
       FunctionTemplateInfo::cast(*setter).BreakAtEntry()) {
@@ -91,6 +93,8 @@
         InstantiateFunction(isolate,
                             Handle<FunctionTemplateInfo>::cast(setter)),
         Object);
+    Handle<CodeT> trampoline = BUILTIN_CODE(isolate, DebugBreakTrampoline);
+    Handle<JSFunction>::cast(setter)->set_code(*trampoline);
   }
   RETURN_ON_EXCEPTION(
       isolate,
diff -r -u --color up/v8/src/api/api.cc nw/v8/src/api/api.cc
--- up/v8/src/api/api.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/api/api.cc	2023-01-19 16:46:35.962276304 -0500
@@ -63,6 +63,7 @@
 #include "src/handles/global-handles.h"
 #include "src/handles/persistent-handles.h"
 #include "src/handles/shared-object-conveyor-handles.h"
+#include "src/handles/traced-handles.h"
 #include "src/heap/embedder-tracing.h"
 #include "src/heap/heap-inl.h"
 #include "src/heap/heap-write-barrier.h"
@@ -174,6 +175,31 @@
 namespace v8 {
 
 static OOMErrorCallback g_oom_error_callback = nullptr;
+namespace {
+
+// TODO(delphick): Remove this completely when the deprecated functions that use
+// it are removed.
+// DO NOT USE THIS IN NEW CODE!
+i::Isolate* UnsafeIsolateFromHeapObject(i::Handle<i::HeapObject> obj) {
+  // Use MemoryChunk directly instead of Isolate::FromWritableHeapObject to
+  // temporarily allow isolate access from read-only space objects.
+  i::MemoryChunk* chunk = i::MemoryChunk::FromHeapObject(*obj);
+  return chunk->heap()->isolate();
+}
+
+// TODO(delphick): Remove this completely when the deprecated functions that use
+// it are removed.
+// DO NOT USE THIS IN NEW CODE!
+Local<Context> UnsafeContextFromHeapObject(i::Handle<i::Object> obj) {
+  // Use MemoryChunk directly instead of Isolate::FromWritableHeapObject to
+  // temporarily allow isolate access from read-only space objects.
+  i::MemoryChunk* chunk =
+      i::MemoryChunk::FromHeapObject(i::HeapObject::cast(*obj));
+  return reinterpret_cast<Isolate*>(chunk->heap()->isolate())
+      ->GetCurrentContext();
+}
+
+}  // namespace
 
 static ScriptOrigin GetScriptOriginForScript(i::Isolate* i_isolate,
                                              i::Handle<i::Script> script) {
@@ -354,6 +380,11 @@
   i::V8::SetSnapshotBlob(snapshot_blob);
 }
 
+void v8::ArrayBuffer::Allocator::Free(void* data, size_t length,
+                                      AllocationMode mode) {
+  UNIMPLEMENTED();
+}
+
 namespace {
 
 #ifdef V8_ENABLE_SANDBOX
@@ -616,7 +647,10 @@
   i::Snapshot::ClearReconstructableDataForSerialization(
       i_isolate, function_code_handling == FunctionCodeHandling::kClear);
 
-  i::GlobalSafepointScope global_safepoint(i_isolate);
+  i::SafepointKind safepoint_kind = i_isolate->has_shared_heap()
+                                        ? i::SafepointKind::kGlobal
+                                        : i::SafepointKind::kIsolate;
+  i::SafepointScope safepoint_scope(i_isolate, safepoint_kind);
   i::DisallowGarbageCollection no_gc_from_here_on;
 
   // Create a vector with all contexts and clear associated Persistent fields.
@@ -654,7 +688,7 @@
 
   data->created_ = true;
   return i::Snapshot::Create(i_isolate, &contexts, embedder_fields_serializers,
-                             global_safepoint, no_gc_from_here_on);
+                             safepoint_scope, no_gc_from_here_on);
 }
 
 bool StartupData::CanBeRehashed() const {
@@ -792,8 +826,7 @@
   Utils::ApiCheck((slot != nullptr), "v8::GlobalizeTracedReference",
                   "the address slot must be not null");
 #endif
-  i::Handle<i::Object> result =
-      i_isolate->global_handles()->CreateTraced(*obj, slot, store_mode);
+  auto result = i_isolate->traced_handles()->Create(*obj, slot, store_mode);
 #ifdef VERIFY_HEAP
   if (i::v8_flags.verify_heap) {
     i::Object(*obj).ObjectVerify(i_isolate);
@@ -803,16 +836,16 @@
 }
 
 void MoveTracedReference(internal::Address** from, internal::Address** to) {
-  GlobalHandles::MoveTracedReference(from, to);
+  TracedHandles::Move(from, to);
 }
 
 void CopyTracedReference(const internal::Address* const* from,
                          internal::Address** to) {
-  GlobalHandles::CopyTracedReference(from, to);
+  TracedHandles::Copy(from, to);
 }
 
 void DisposeTracedReference(internal::Address* location) {
-  GlobalHandles::DestroyTracedReference(location);
+  TracedHandles::Destroy(location);
 }
 
 }  // namespace internal
@@ -2225,6 +2258,12 @@
   array->set(index, *i_item);
 }
 
+void PrimitiveArray::Set(int index, Local<Primitive> item) {
+  i::Handle<i::FixedArray> array = Utils::OpenHandle(this);
+  i::Isolate* isolate = UnsafeIsolateFromHeapObject(array);
+  Set(reinterpret_cast<Isolate*>(isolate), index, item);
+}
+
 Local<Primitive> PrimitiveArray::Get(Isolate* v8_isolate, int index) {
   i::Isolate* i_isolate = reinterpret_cast<i::Isolate*>(v8_isolate);
   i::Handle<i::FixedArray> array = Utils::OpenHandle(this);
@@ -2275,6 +2314,12 @@
       i::handle(self->import_assertions(), i_isolate));
 }
 
+Local<Primitive> PrimitiveArray::Get(int index) {
+  i::Handle<i::FixedArray> array = Utils::OpenHandle(this);
+  i::Isolate* isolate = UnsafeIsolateFromHeapObject(array);
+  return Get(reinterpret_cast<Isolate*>(isolate), index);
+}
+
 Module::Status Module::GetStatus() const {
   i::Handle<i::Module> self = Utils::OpenHandle(this);
   switch (self->status()) {
@@ -2618,6 +2663,21 @@
   return ToApiHandle<Module>(i_isolate->factory()->NewSourceTextModule(shared));
 }
 
+MaybeLocal<Module> ScriptCompiler::CompileModuleWithCache(Isolate* isolate,
+                                                          Source* source) {
+  i::Isolate* i_isolate = reinterpret_cast<i::Isolate*>(isolate);
+
+  Utils::ApiCheck(source->GetResourceOptions().IsModule(),
+                  "v8::ScriptCompiler::CompileModule",
+                  "Invalid ScriptOrigin: is_module must be true");
+  auto maybe = CompileUnboundInternal(isolate, source, kConsumeCodeCache, kNoCacheNoReason);
+  Local<UnboundScript> unbound;
+  if (!maybe.ToLocal(&unbound)) return MaybeLocal<Module>();
+
+  i::Handle<i::SharedFunctionInfo> shared = Utils::OpenHandle(*unbound);
+  return ToApiHandle<Module>(i_isolate->factory()->NewSourceTextModule(shared));
+}
+
 // static
 V8_WARN_UNUSED_RESULT MaybeLocal<Function> ScriptCompiler::CompileFunction(
     Local<Context> context, Source* source, size_t arguments_count,
@@ -3227,6 +3287,11 @@
   return Utils::StackFrameToLocal(info);
 }
 
+Local<StackFrame> StackTrace::GetFrame(uint32_t index) const {
+  i::Isolate* isolate = UnsafeIsolateFromHeapObject(Utils::OpenHandle(this));
+  return GetFrame(reinterpret_cast<Isolate*>(isolate), index);
+}
+
 int StackTrace::GetFrameCount() const {
   return Utils::OpenHandle(this)->length();
 }
@@ -3256,9 +3321,9 @@
   i::Isolate* i_isolate = self->GetIsolate();
   i::Handle<i::Script> script(self->script(), i_isolate);
   i::Script::PositionInfo info;
-  CHECK(i::Script::GetPositionInfo(script,
+  i::Script::GetPositionInfo(script,
                                    i::StackFrameInfo::GetSourcePosition(self),
-                                   &info, i::Script::WITH_OFFSET));
+                                   &info, i::Script::WITH_OFFSET);
   if (script->HasSourceURLComment()) {
     info.line -= script->line_offset();
     if (info.line == 0) {
@@ -3623,6 +3688,25 @@
 
 // --- D a t a ---
 
+Local<Boolean> Value::ToBoolean() const {
+  return ToBoolean(Isolate::GetCurrent());
+}
+
+Local<String> Value::ToString() const {
+  return ToString(Isolate::GetCurrent()->GetCurrentContext())
+      .FromMaybe(Local<String>());
+}
+
+Local<Object> Value::ToObject() const {
+  return ToObject(Isolate::GetCurrent()->GetCurrentContext())
+      .FromMaybe(Local<Object>());
+}
+
+Local<Integer> Value::ToInteger() const {
+  return ToInteger(Isolate::GetCurrent()->GetCurrentContext())
+      .FromMaybe(Local<Integer>());
+}
+
 bool Value::FullIsUndefined() const {
   i::Handle<i::Object> object = Utils::OpenHandle(this);
   bool result = object->IsUndefined();
@@ -3732,6 +3816,7 @@
 #endif  // V8_ENABLE_WEBASSEMBLY
 VALUE_IS_SPECIFIC_TYPE(WeakMap, JSWeakMap)
 VALUE_IS_SPECIFIC_TYPE(WeakSet, JSWeakSet)
+VALUE_IS_SPECIFIC_TYPE(WeakRef, JSWeakRef)
 
 #undef VALUE_IS_SPECIFIC_TYPE
 
@@ -4216,6 +4301,15 @@
                   "Value is not a RegExp");
 }
 
+bool Value::BooleanValue() const {
+  auto obj = Utils::OpenHandle(this);
+  if (obj->IsSmi()) return *obj != i::Smi::zero();
+  DCHECK(obj->IsHeapObject());
+  i::Isolate* isolate =
+      UnsafeIsolateFromHeapObject(i::Handle<i::HeapObject>::cast(obj));
+  return obj->BooleanValue(isolate);
+}
+
 Maybe<double> Value::NumberValue(Local<Context> context) const {
   auto obj = Utils::OpenHandle(this);
   if (obj->IsNumber()) return Just(obj->Number());
@@ -4228,6 +4322,13 @@
   return Just(num->Number());
 }
 
+double Value::NumberValue() const {
+  auto obj = Utils::OpenHandle(this);
+  if (obj->IsNumber()) return obj->Number();
+  return NumberValue(UnsafeContextFromHeapObject(obj))
+      .FromMaybe(std::numeric_limits<double>::quiet_NaN());
+}
+
 Maybe<int64_t> Value::IntegerValue(Local<Context> context) const {
   auto obj = Utils::OpenHandle(this);
   if (obj->IsNumber()) {
@@ -4242,6 +4343,18 @@
   return Just(NumberToInt64(*num));
 }
 
+int64_t Value::IntegerValue() const {
+  auto obj = Utils::OpenHandle(this);
+  if (obj->IsNumber()) {
+    if (obj->IsSmi()) {
+      return i::Smi::ToInt(*obj);
+    } else {
+      return static_cast<int64_t>(obj->Number());
+    }
+  }
+  return IntegerValue(UnsafeContextFromHeapObject(obj)).FromMaybe(0);
+}
+
 Maybe<int32_t> Value::Int32Value(Local<Context> context) const {
   auto obj = Utils::OpenHandle(this);
   if (obj->IsNumber()) return Just(NumberToInt32(*obj));
@@ -4255,6 +4368,12 @@
                            : static_cast<int32_t>(num->Number()));
 }
 
+int32_t Value::Int32Value() const {
+  auto obj = Utils::OpenHandle(this);
+  if (obj->IsNumber()) return NumberToInt32(*obj);
+  return Int32Value(UnsafeContextFromHeapObject(obj)).FromMaybe(0);
+}
+
 Maybe<uint32_t> Value::Uint32Value(Local<Context> context) const {
   auto obj = Utils::OpenHandle(this);
   if (obj->IsNumber()) return Just(NumberToUint32(*obj));
@@ -4268,6 +4387,12 @@
                            : static_cast<uint32_t>(num->Number()));
 }
 
+uint32_t Value::Uint32Value() const {
+  auto obj = Utils::OpenHandle(this);
+  if (obj->IsNumber()) return NumberToUint32(*obj);
+  return Uint32Value(UnsafeContextFromHeapObject(obj)).FromMaybe(0);
+}
+
 MaybeLocal<Uint32> Value::ToArrayIndex(Local<Context> context) const {
   auto self = Utils::OpenHandle(this);
   if (self->IsSmi()) {
@@ -4304,6 +4429,20 @@
   return result;
 }
 
+bool Value::Equals(Local<Value> that) const {
+  auto self = Utils::OpenHandle(this);
+  auto other = Utils::OpenHandle(*that);
+  if (self->IsSmi() && other->IsSmi()) {
+    return self->Number() == other->Number();
+  }
+  if (self->IsJSObject() && other->IsJSObject()) {
+    return *self == *other;
+  }
+  auto heap_object = self->IsSmi() ? other : self;
+  auto context = UnsafeContextFromHeapObject(heap_object);
+  return Equals(context, that).FromMaybe(false);
+}
+
 bool Value::StrictEquals(Local<Value> that) const {
   auto self = Utils::OpenHandle(this);
   auto other = Utils::OpenHandle(*that);
@@ -5613,6 +5752,11 @@
   return helper.Check(*str);
 }
 
+int String::Utf8Length() const {
+  i::Isolate* isolate = UnsafeIsolateFromHeapObject(Utils::OpenHandle(this));
+  return Utf8Length(reinterpret_cast<Isolate*>(isolate));
+}
+
 int String::Utf8Length(Isolate* v8_isolate) const {
   i::Handle<i::String> str = Utils::OpenHandle(this);
   str = i::String::Flatten(reinterpret_cast<i::Isolate*>(v8_isolate), str);
@@ -5765,6 +5909,7 @@
 }
 }  // anonymous namespace
 
+
 int String::WriteUtf8(Isolate* v8_isolate, char* buffer, int capacity,
                       int* nchars_ref, int options) const {
   i::Handle<i::String> str = Utils::OpenHandle(this);
@@ -5783,6 +5928,14 @@
   }
 }
 
+int String::WriteUtf8(char* buffer, int capacity, int* nchars_ref,
+                      int options) const {
+  i::Handle<i::String> str = Utils::OpenHandle(this);
+  i::Isolate* isolate = UnsafeIsolateFromHeapObject(str);
+  return WriteUtf8(reinterpret_cast<Isolate*>(isolate), buffer, capacity,
+                   nchars_ref, options);
+}
+
 template <typename CharType>
 static inline int WriteHelper(i::Isolate* i_isolate, const String* string,
                               CharType* buffer, int start, int length,
@@ -5804,12 +5957,23 @@
   return write_length;
 }
 
+int String::WriteOneByte(uint8_t* buffer, int start, int length,
+                         int options) const {
+  i::Isolate* isolate = UnsafeIsolateFromHeapObject(Utils::OpenHandle(this));
+  return WriteHelper(isolate, this, buffer, start, length, options);
+}
+
 int String::WriteOneByte(Isolate* v8_isolate, uint8_t* buffer, int start,
                          int length, int options) const {
   return WriteHelper(reinterpret_cast<i::Isolate*>(v8_isolate), this, buffer,
                      start, length, options);
 }
 
+int String::Write(uint16_t* buffer, int start, int length, int options) const {
+  i::Isolate* isolate = UnsafeIsolateFromHeapObject(Utils::OpenHandle(this));
+  return WriteHelper(isolate, this, buffer, start, length, options);
+}
+
 int String::Write(Isolate* v8_isolate, uint16_t* buffer, int start, int length,
                   int options) const {
   return WriteHelper(reinterpret_cast<i::Isolate*>(v8_isolate), this, buffer,
@@ -6313,6 +6477,10 @@
   return i::InitializeICUDefaultLocation(exec_path, icu_data_file);
 }
 
+void* v8::V8::RawICUData() {
+  return i::RawICUData();
+}
+
 void v8::V8::InitializeExternalStartupData(const char* directory_path) {
   i::InitializeExternalStartupData(directory_path);
 }
@@ -6611,10 +6779,31 @@
 v8::MicrotaskQueue* Context::GetMicrotaskQueue() {
   i::Handle<i::Context> env = Utils::OpenHandle(this);
   Utils::ApiCheck(env->IsNativeContext(), "v8::Context::GetMicrotaskQueue",
-                  "Must be calld on a native context");
+                  "Must be called on a native context");
   return i::Handle<i::NativeContext>::cast(env)->microtask_queue();
 }
 
+void Context::SetMicrotaskQueue(v8::MicrotaskQueue* queue) {
+  i::Handle<i::Context> context = Utils::OpenHandle(this);
+  i::Isolate* i_isolate = context->GetIsolate();
+  Utils::ApiCheck(context->IsNativeContext(), "v8::Context::SetMicrotaskQueue",
+                  "Must be called on a native context");
+  i::Handle<i::NativeContext> native_context =
+      i::Handle<i::NativeContext>::cast(context);
+  i::HandleScopeImplementer* impl = i_isolate->handle_scope_implementer();
+  Utils::ApiCheck(!native_context->microtask_queue()->IsRunningMicrotasks(),
+                  "v8::Context::SetMicrotaskQueue",
+                  "Must not be running microtasks");
+  Utils::ApiCheck(
+      native_context->microtask_queue()->GetMicrotasksScopeDepth() == 0,
+      "v8::Context::SetMicrotaskQueue", "Must not have microtask scope pushed");
+  Utils::ApiCheck(impl->EnteredContextCount() == 0,
+                  "v8::Context::SetMicrotaskQueue()",
+                  "Cannot set Microtask Queue with an entered context");
+  native_context->set_microtask_queue(
+      i_isolate, static_cast<const i::MicrotaskQueue*>(queue));
+}
+
 v8::Local<v8::Object> Context::Global() {
   i::Handle<i::Context> context = Utils::OpenHandle(this);
   i::Isolate* i_isolate = context->GetIsolate();
@@ -7037,6 +7226,12 @@
   return Utils::ToLocal(result);
 }
 
+Local<String> v8::String::Concat(Local<String> left, Local<String> right) {
+  i::Handle<i::String> left_string = Utils::OpenHandle(*left);
+  i::Isolate* isolate = UnsafeIsolateFromHeapObject(left_string);
+  return Concat(reinterpret_cast<Isolate*>(isolate), left, right);
+}
+
 MaybeLocal<String> v8::String::NewExternalTwoByte(
     Isolate* v8_isolate, v8::String::ExternalStringResource* resource) {
   CHECK(resource && resource->data());
@@ -7338,6 +7533,12 @@
   return js_primitive_wrapper.value().IsTrue(i_isolate);
 }
 
+Local<v8::Value> v8::StringObject::New(Local<String> value) {
+  i::Handle<i::String> string = Utils::OpenHandle(*value);
+  i::Isolate* isolate = UnsafeIsolateFromHeapObject(string);
+  return New(reinterpret_cast<Isolate*>(isolate), value);
+}
+
 Local<v8::Value> v8::StringObject::New(Isolate* v8_isolate,
                                        Local<String> value) {
   i::Handle<i::String> string = Utils::OpenHandle(*value);
@@ -8075,6 +8276,10 @@
   return Utils::OpenHandle(this)->is_detachable();
 }
 
+bool v8::ArrayBuffer::WasDetached() const {
+  return Utils::OpenHandle(this)->was_detached();
+}
+
 namespace {
 std::shared_ptr<i::BackingStore> ToInternal(
     std::shared_ptr<i::BackingStoreBase> backing_store) {
@@ -8082,14 +8287,32 @@
 }
 }  // namespace
 
-void v8::ArrayBuffer::Detach() {
+Maybe<bool> v8::ArrayBuffer::Detach(v8::Local<v8::Value> key) {
   i::Handle<i::JSArrayBuffer> obj = Utils::OpenHandle(this);
   i::Isolate* i_isolate = obj->GetIsolate();
   Utils::ApiCheck(obj->is_detachable(), "v8::ArrayBuffer::Detach",
                   "Only detachable ArrayBuffers can be detached");
-  API_RCS_SCOPE(i_isolate, ArrayBuffer, Detach);
-  ENTER_V8_NO_SCRIPT_NO_EXCEPTION(i_isolate);
-  obj->Detach();
+  ENTER_V8_NO_SCRIPT(
+      i_isolate, reinterpret_cast<v8::Isolate*>(i_isolate)->GetCurrentContext(),
+      ArrayBuffer, Detach, Nothing<bool>(), i::HandleScope);
+  if (!key.IsEmpty()) {
+    i::Handle<i::Object> i_key = Utils::OpenHandle(*key);
+    constexpr bool kForceForWasmMemory = false;
+    has_pending_exception =
+        i::JSArrayBuffer::Detach(obj, kForceForWasmMemory, i_key).IsNothing();
+  } else {
+    has_pending_exception = i::JSArrayBuffer::Detach(obj).IsNothing();
+  }
+  RETURN_ON_FAILED_EXECUTION_PRIMITIVE(bool);
+  return Just(true);
+}
+
+void v8::ArrayBuffer::Detach() { Detach(Local<Value>()).Check(); }
+
+void v8::ArrayBuffer::SetDetachKey(v8::Local<v8::Value> key) {
+  i::Handle<i::JSArrayBuffer> obj = Utils::OpenHandle(this);
+  i::Handle<i::Object> i_key = Utils::OpenHandle(*key);
+  obj->set_detach_key(*i_key);
 }
 
 size_t v8::ArrayBuffer::ByteLength() const {
@@ -8171,6 +8394,25 @@
       static_cast<v8::BackingStore*>(backing_store.release()));
 }
 
+Local<ArrayBuffer> v8::ArrayBuffer::NewNode(
+    Isolate* isolate, std::shared_ptr<BackingStore> backing_store) {
+  CHECK_IMPLIES(backing_store->ByteLength() != 0,
+                backing_store->Data() != nullptr);
+  i::Isolate* i_isolate = reinterpret_cast<i::Isolate*>(isolate);
+  API_RCS_SCOPE(i_isolate, ArrayBuffer, New);
+  ENTER_V8_NO_SCRIPT_NO_EXCEPTION(i_isolate);
+  std::shared_ptr<i::BackingStore> i_backing_store(
+      ToInternal(std::move(backing_store)));
+  Utils::ApiCheck(
+      !i_backing_store->is_shared(), "v8_ArrayBuffer_New",
+      "Cannot construct ArrayBuffer with a BackingStore of SharedArrayBuffer");
+  i_backing_store->set_nodejs(true);
+  i::Handle<i::JSArrayBuffer> obj =
+      i_isolate->factory()->NewJSArrayBuffer(std::move(i_backing_store));
+  obj->set_is_node_js(true);
+  return Utils::ToLocal(obj);
+}
+
 Local<ArrayBuffer> v8::ArrayBufferView::Buffer() {
   i::Handle<i::JSArrayBufferView> obj = Utils::OpenHandle(this);
   i::Handle<i::JSArrayBuffer> buffer;
@@ -8555,6 +8797,11 @@
   i_isolate->ClearKeptObjects();
 }
 
+ArrayBuffer::Allocator* Isolate::array_buffer_allocator() {
+  i::Isolate* isolate = reinterpret_cast<i::Isolate*>(this);
+  return isolate->array_buffer_allocator();
+}
+
 v8::Local<v8::Context> Isolate::GetCurrentContext() {
   i::Isolate* i_isolate = reinterpret_cast<i::Isolate*>(this);
   i::Context context = i_isolate->context();
@@ -8767,6 +9014,13 @@
 
 Isolate::CreateParams::~CreateParams() = default;
 
+void Isolate::SetArrayBufferAllocatorShared(
+                                            std::shared_ptr<ArrayBuffer::Allocator> allocator) {
+  i::Isolate* isolate = reinterpret_cast<i::Isolate*>(this);
+  CHECK_EQ(allocator.get(), isolate->array_buffer_allocator());
+  isolate->set_array_buffer_allocator_shared(std::move(allocator));
+}
+
 // static
 // This is separate so that tests can provide a different |isolate|.
 void Isolate::Initialize(Isolate* v8_isolate,
@@ -9553,16 +9807,19 @@
 CALLBACK_SETTER(WasmLoadSourceMapCallback, WasmLoadSourceMapCallback,
                 wasm_load_source_map_callback)
 
-CALLBACK_SETTER(WasmSimdEnabledCallback, WasmSimdEnabledCallback,
-                wasm_simd_enabled_callback)
-
-CALLBACK_SETTER(WasmExceptionsEnabledCallback, WasmExceptionsEnabledCallback,
-                wasm_exceptions_enabled_callback)
-
 CALLBACK_SETTER(SharedArrayBufferConstructorEnabledCallback,
                 SharedArrayBufferConstructorEnabledCallback,
                 sharedarraybuffer_constructor_enabled_callback)
 
+void Isolate::SetWasmExceptionsEnabledCallback(
+    WasmExceptionsEnabledCallback callback) {
+  // Exceptions are always enabled
+}
+
+void Isolate::SetWasmSimdEnabledCallback(WasmSimdEnabledCallback callback) {
+  // SIMD is always enabled
+}
+
 void Isolate::InstallConditionalFeatures(Local<Context> context) {
   v8::HandleScope handle_scope(this);
   v8::Context::Scope context_scope(context);
@@ -9721,6 +9978,11 @@
                                  MicrotasksScope::Type type)
     : MicrotasksScope(v8_isolate, nullptr, type) {}
 
+MicrotasksScope::MicrotasksScope(Local<Context> v8_context,
+                                 MicrotasksScope::Type type)
+    : MicrotasksScope(v8_context->GetIsolate(), v8_context->GetMicrotaskQueue(),
+                      type) {}
+
 MicrotasksScope::MicrotasksScope(Isolate* v8_isolate,
                                  MicrotaskQueue* microtask_queue,
                                  MicrotasksScope::Type type)
@@ -9772,6 +10034,9 @@
   return microtask_queue->IsRunningMicrotasks();
 }
 
+String::Utf8Value::Utf8Value(v8::Local<v8::Value> obj)
+  : Utf8Value(Isolate::GetCurrent(), obj) {}
+
 String::Utf8Value::Utf8Value(v8::Isolate* v8_isolate, v8::Local<v8::Value> obj)
     : str_(nullptr), length_(0) {
   if (obj.IsEmpty()) return;
@@ -9789,6 +10054,9 @@
 
 String::Utf8Value::~Utf8Value() { i::DeleteArray(str_); }
 
+String::Value::Value(v8::Local<v8::Value> obj)
+  : Value(Isolate::GetCurrent(), obj) {}
+
 String::Value::Value(v8::Isolate* v8_isolate, v8::Local<v8::Value> obj)
     : str_(nullptr), length_(0) {
   if (obj.IsEmpty()) return;
@@ -10042,6 +10310,21 @@
   return profile->end_time().since_origin().InMicroseconds();
 }
 
+static i::CpuProfile* ToInternal(const CpuProfile* profile) {
+  return const_cast<i::CpuProfile*>(
+      reinterpret_cast<const i::CpuProfile*>(profile));
+}
+
+void CpuProfile::Serialize(OutputStream* stream,
+                           CpuProfile::SerializationFormat format) const {
+  Utils::ApiCheck(format == kJSON, "v8::CpuProfile::Serialize",
+                  "Unknown serialization format");
+  Utils::ApiCheck(stream->GetChunkSize() > 0, "v8::CpuProfile::Serialize",
+                  "Invalid stream chunk size");
+  i::CpuProfileJSONSerializer serializer(ToInternal(this));
+  serializer.Serialize(stream);
+}
+
 int CpuProfile::GetSamplesCount() const {
   return reinterpret_cast<const i::CpuProfile*>(this)->samples_count();
 }
@@ -10504,9 +10787,10 @@
     TracedGlobalHandleVisitor* visitor) {
   i::Isolate* i_isolate = reinterpret_cast<i::Isolate*>(v8_isolate_);
   i::DisallowGarbageCollection no_gc;
-  i_isolate->global_handles()->IterateTracedNodes(visitor);
+  i_isolate->traced_handles()->Iterate(visitor);
 }
 
+
 bool EmbedderHeapTracer::IsRootForNonTracingGC(
     const v8::TracedReference<v8::Value>& handle) {
   return true;
@@ -10623,6 +10907,29 @@
 }
 #endif  // !V8_ENABLE_WEBASSEMBLY
 
+void SetTLSPlatform(Platform* platform) {
+  i::V8::SetTLSPlatform(platform);
+}
+
+void FixSourceNWBin(Isolate* v8_isolate, Local<UnboundScript> script) {
+  i::Isolate* isolate = reinterpret_cast<i::Isolate*>(v8_isolate);
+  i::Handle<i::HeapObject> obj =
+    i::Handle<i::HeapObject>::cast(v8::Utils::OpenHandle(*script));
+  i::Handle<i::SharedFunctionInfo>
+    function_info(i::SharedFunctionInfo::cast(*obj), isolate);
+  i::Handle<i::Script> iscript(i::Script::cast(function_info->script()),
+                              isolate);
+  iscript->set_source(i::ReadOnlyRoots(isolate).undefined_value());
+}
+
+void FixSourceNWBin(Isolate* v8_isolate, Local<Module> module) {
+  i::Isolate* isolate = reinterpret_cast<i::Isolate*>(v8_isolate);
+  i::Handle<i::SourceTextModule> obj =
+    i::Handle<i::SourceTextModule>::cast(v8::Utils::OpenHandle(*module));
+  i::Handle<i::Script> iscript(i::Script::cast(obj->GetScript()), isolate);
+  iscript->set_source(i::ReadOnlyRoots(isolate).undefined_value());
+}
+
 namespace internal {
 
 const size_t HandleScopeImplementer::kEnteredContextsOffset =
diff -r -u --color up/v8/src/asmjs/asm-js.cc nw/v8/src/asmjs/asm-js.cc
--- up/v8/src/asmjs/asm-js.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/asmjs/asm-js.cc	2023-01-19 16:46:35.962276304 -0500
@@ -133,7 +133,7 @@
 // Hook to report successful execution of {AsmJs::CompileAsmViaWasm} phase.
 void ReportCompilationSuccess(Handle<Script> script, int position,
                               double compile_time, size_t module_size) {
-  if (FLAG_suppress_asm_messages || !FLAG_trace_asm_time) return;
+  if (v8_flags.suppress_asm_messages || !v8_flags.trace_asm_time) return;
   base::EmbeddedVector<char, 100> text;
   int length = SNPrintF(text, "success, compile time %0.3f ms, %zu bytes",
                         compile_time, module_size);
@@ -146,7 +146,7 @@
 // Hook to report failed execution of {AsmJs::CompileAsmViaWasm} phase.
 void ReportCompilationFailure(ParseInfo* parse_info, int position,
                               const char* reason) {
-  if (FLAG_suppress_asm_messages) return;
+  if (v8_flags.suppress_asm_messages) return;
   parse_info->pending_error_handler()->ReportWarningAt(
       position, position, MessageTemplate::kAsmJsInvalid, reason);
 }
@@ -154,7 +154,7 @@
 // Hook to report successful execution of {AsmJs::InstantiateAsmWasm} phase.
 void ReportInstantiationSuccess(Handle<Script> script, int position,
                                 double instantiate_time) {
-  if (FLAG_suppress_asm_messages || !FLAG_trace_asm_time) return;
+  if (v8_flags.suppress_asm_messages || !v8_flags.trace_asm_time) return;
   base::EmbeddedVector<char, 50> text;
   int length = SNPrintF(text, "success, %0.3f ms", instantiate_time);
   CHECK_NE(-1, length);
@@ -166,7 +166,7 @@
 // Hook to report failed execution of {AsmJs::InstantiateAsmWasm} phase.
 void ReportInstantiationFailure(Handle<Script> script, int position,
                                 const char* reason) {
-  if (FLAG_suppress_asm_messages) return;
+  if (v8_flags.suppress_asm_messages) return;
   base::Vector<const char> text = base::CStrVector(reason);
   Report(script, position, text, MessageTemplate::kAsmJsLinkingFailed,
          v8::Isolate::kMessageWarning);
@@ -237,7 +237,7 @@
   stream->Seek(compilation_info()->literal()->start_position());
   wasm::AsmJsParser parser(&translate_zone, stack_limit(), stream);
   if (!parser.Run()) {
-    if (!FLAG_suppress_asm_messages) {
+    if (!v8_flags.suppress_asm_messages) {
       ReportCompilationFailure(parse_info(), parser.failure_location(),
                                parser.failure_message());
     }
diff -r -u --color up/v8/src/asmjs/asm-parser.cc nw/v8/src/asmjs/asm-parser.cc
--- up/v8/src/asmjs/asm-parser.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/asmjs/asm-parser.cc	2023-01-19 16:46:35.962276304 -0500
@@ -28,7 +28,7 @@
   failed_ = true;                                                        \
   failure_message_ = msg;                                                \
   failure_location_ = static_cast<int>(scanner_.Position());             \
-  if (FLAG_trace_asm_parser) {                                           \
+  if (v8_flags.trace_asm_parser) {                                       \
     PrintF("[asm.js failure: %s, token: '%s', see: %s:%d]\n", msg,       \
            scanner_.Name(scanner_.Token()).c_str(), __FILE__, __LINE__); \
   }                                                                      \
diff -r -u --color up/v8/src/asmjs/asm-scanner.cc nw/v8/src/asmjs/asm-scanner.cc
--- up/v8/src/asmjs/asm-scanner.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/asmjs/asm-scanner.cc	2023-01-19 16:46:35.962276304 -0500
@@ -67,7 +67,7 @@
   }
 
 #if DEBUG
-  if (FLAG_trace_asm_scanner) {
+  if (v8_flags.trace_asm_scanner) {
     if (Token() == kDouble) {
       PrintF("%lf ", AsDouble());
     } else if (Token() == kUnsigned) {
diff -r -u --color up/v8/src/ast/scopes.cc nw/v8/src/ast/scopes.cc
--- up/v8/src/ast/scopes.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/ast/scopes.cc	2023-01-19 16:46:35.962276304 -0500
@@ -715,7 +715,7 @@
   scope->GetScriptScope()->RewriteReplGlobalVariables();
 
 #ifdef DEBUG
-  if (FLAG_print_scopes) {
+  if (v8_flags.print_scopes) {
     PrintF("Global scope:\n");
     scope->Print();
   }
@@ -929,6 +929,7 @@
   // Move eval calls since Snapshot's creation into new_parent.
   if (outer_scope_->calls_eval_) {
     new_parent->RecordEvalCall();
+    outer_scope_->calls_eval_ = false;
     declaration_scope_->sloppy_eval_can_extend_vars_ = false;
   }
 }
@@ -1754,7 +1755,7 @@
   }
 
 #ifdef DEBUG
-  if (FLAG_print_scopes) {
+  if (v8_flags.print_scopes) {
     PrintF("Inner function scope:\n");
     Print();
   }
diff -r -u --color up/v8/src/base/container-utils.h nw/v8/src/base/container-utils.h
--- up/v8/src/base/container-utils.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/base/container-utils.h	2023-01-19 16:46:35.962276304 -0500
@@ -7,6 +7,7 @@
 
 #include <algorithm>
 #include <optional>
+#include <vector>
 
 namespace v8::base {
 
@@ -77,6 +78,18 @@
   return std::count_if(begin(container), end(container), predicate);
 }
 
+// Helper for std::all_of.
+template <typename C, typename P>
+inline bool all_of(const C& container, const P& predicate) {
+  return std::all_of(begin(container), end(container), predicate);
+}
+
+// Helper for std::none_of.
+template <typename C, typename P>
+inline bool none_of(const C& container, const P& predicate) {
+  return std::none_of(begin(container), end(container), predicate);
+}
+
 // Returns true iff all elements of {container} compare equal using operator==.
 template <typename C>
 inline bool all_equal(const C& container) {
@@ -87,6 +100,21 @@
                      [&](const auto& v) { return v == value; });
 }
 
+// Returns true iff all elements of {container} compare equal to {value} using
+// operator==.
+template <typename C, typename T>
+inline bool all_equal(const C& container, const T& value) {
+  return std::all_of(begin(container), end(container),
+                     [&](const auto& v) { return v == value; });
+}
+
+// Appends to vector {v} all the elements in the range {begin(container)} and
+// {end(container)}.
+template <typename T, typename A, typename C>
+inline void vector_append(std::vector<T, A>& v, const C& container) {
+  v.insert(end(v), begin(container), end(container));
+}
+
 }  // namespace v8::base
 
 #endif  // V8_BASE_CONTAINER_UTILS_H_
diff -r -u --color up/v8/src/base/logging.h nw/v8/src/base/logging.h
--- up/v8/src/base/logging.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/base/logging.h	2023-01-19 16:46:35.973109637 -0500
@@ -48,6 +48,13 @@
 
 #define UNIMPLEMENTED() FATAL("unimplemented code")
 #define UNREACHABLE() FATAL("unreachable code")
+// g++ versions <= 8 cannot use UNREACHABLE() in a constexpr function.
+// TODO(miladfarca): Remove once all compilers handle this properly.
+#if defined(__GNUC__) && !defined(__clang__) && (__GNUC__ <= 8)
+#define CONSTEXPR_UNREACHABLE() abort()
+#else
+#define CONSTEXPR_UNREACHABLE() UNREACHABLE()
+#endif
 
 namespace v8 {
 namespace base {
diff -r -u --color up/v8/src/base/platform/memory.h nw/v8/src/base/platform/memory.h
--- up/v8/src/base/platform/memory.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/base/platform/memory.h	2023-01-19 16:46:35.973109637 -0500
@@ -111,6 +111,8 @@
 // `AllocateAtLeast()` for a safe version.
 inline size_t MallocUsableSize(void* ptr) {
 #if V8_OS_WIN
+  // |_msize| cannot handle a null pointer.
+  if (!ptr) return 0;
   return _msize(ptr);
 #elif V8_OS_DARWIN
   return malloc_size(ptr);
@@ -130,7 +132,7 @@
 
 // Allocates at least `n * sizeof(T)` uninitialized storage but may allocate
 // more which is indicated by the return value. Mimics C++23
-// `allocate_ate_least()`.
+// `allocate_at_least()`.
 template <typename T>
 V8_NODISCARD AllocationResult<T*> AllocateAtLeast(size_t n) {
   const size_t min_wanted_size = n * sizeof(T);
@@ -140,13 +142,14 @@
 #else  // V8_HAS_MALLOC_USABLE_SIZE
   const size_t usable_size = MallocUsableSize(memory);
 #if V8_USE_UNDEFINED_BEHAVIOR_SANITIZER
+  if (memory == nullptr)
+    return {nullptr, 0};
   // UBSan (specifically, -fsanitize=bounds) assumes that any access outside
   // of the requested size for malloc is UB and will trap in ud2 instructions.
   // This can be worked around by using `Realloc()` on the specific memory
-  // region, assuming that the allocator doesn't actually reallocate the
-  // buffer.
+  // region.
   if (usable_size != min_wanted_size) {
-    CHECK_EQ(static_cast<T*>(Realloc(memory, usable_size)), memory);
+    memory = static_cast<T*>(Realloc(memory, usable_size));
   }
 #endif  // V8_USE_UNDEFINED_BEHAVIOR_SANITIZER
   return {memory, usable_size};
diff -r -u --color up/v8/src/base/platform/mutex.h nw/v8/src/base/platform/mutex.h
--- up/v8/src/base/platform/mutex.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/base/platform/mutex.h	2023-01-19 16:46:35.973109637 -0500
@@ -278,7 +278,6 @@
   // pthread_rwlock_t is broken on MacOS when signals are being sent to the
   // process (see https://crbug.com/v8/11399).
   // We thus use std::shared_mutex on MacOS, which does not have this problem.
-  // TODO(13256): Use std::shared_mutex directly, on all platforms.
   using NativeHandle = std::shared_mutex;
 #elif V8_OS_POSIX
   using NativeHandle = pthread_rwlock_t;
diff -r -u --color up/v8/src/baseline/ppc/baseline-assembler-ppc-inl.h nw/v8/src/baseline/ppc/baseline-assembler-ppc-inl.h
--- up/v8/src/baseline/ppc/baseline-assembler-ppc-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/baseline/ppc/baseline-assembler-ppc-inl.h	2023-01-19 16:46:35.983942967 -0500
@@ -8,6 +8,7 @@
 #include "src/baseline/baseline-assembler.h"
 #include "src/codegen/interface-descriptors.h"
 #include "src/codegen/ppc/assembler-ppc-inl.h"
+#include "src/codegen/ppc/register-ppc.h"
 #include "src/objects/literal-objects-inl.h"
 
 namespace v8 {
@@ -596,6 +597,7 @@
 
 void BaselineAssembler::LdaContextSlot(Register context, uint32_t index,
                                        uint32_t depth) {
+  ASM_CODE_COMMENT(masm_);
   for (; depth > 0; --depth) {
     LoadTaggedPointerField(context, context, Context::kPreviousOffset);
   }
@@ -605,8 +607,15 @@
 
 void BaselineAssembler::StaContextSlot(Register context, Register value,
                                        uint32_t index, uint32_t depth) {
-  for (; depth > 0; --depth) {
-    LoadTaggedPointerField(context, context, Context::kPreviousOffset);
+  ASM_CODE_COMMENT(masm_);
+  if (depth > 0) {
+    for (; depth > 0; --depth) {
+      LoadTaggedPointerField(context, context, Context::kPreviousOffset);
+    }
+    if (COMPRESS_POINTERS_BOOL) {
+      // Decompress tagged pointer.
+      __ AddS64(context, context, kPtrComprCageBaseRegister);
+    }
   }
   StoreTaggedFieldWithWriteBarrier(context, Context::OffsetOfElementAt(index),
                                    value);
@@ -614,6 +623,7 @@
 
 void BaselineAssembler::LdaModuleVariable(Register context, int cell_index,
                                           uint32_t depth) {
+  ASM_CODE_COMMENT(masm_);
   for (; depth > 0; --depth) {
     LoadTaggedPointerField(context, context, Context::kPreviousOffset);
   }
@@ -636,6 +646,7 @@
 
 void BaselineAssembler::StaModuleVariable(Register context, Register value,
                                           int cell_index, uint32_t depth) {
+  ASM_CODE_COMMENT(masm_);
   for (; depth > 0; --depth) {
     LoadTaggedPointerField(context, context, Context::kPreviousOffset);
   }
@@ -650,6 +661,7 @@
 }
 
 void BaselineAssembler::AddSmi(Register lhs, Smi rhs) {
+  ASM_CODE_COMMENT(masm_);
   if (rhs.value() == 0) return;
   __ LoadSmiLiteral(r0, rhs);
   if (SmiValuesAre31Bits()) {
diff -r -u --color up/v8/src/builtins/arm/builtins-arm.cc nw/v8/src/builtins/arm/builtins-arm.cc
--- up/v8/src/builtins/arm/builtins-arm.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/arm/builtins-arm.cc	2023-01-19 16:46:35.994776297 -0500
@@ -25,6 +25,7 @@
 #include "src/runtime/runtime.h"
 
 #if V8_ENABLE_WEBASSEMBLY
+#include "src/wasm/baseline/liftoff-assembler-defs.h"
 #include "src/wasm/wasm-linkage.h"
 #include "src/wasm/wasm-objects.h"
 #endif  // V8_ENABLE_WEBASSEMBLY
@@ -2532,63 +2533,123 @@
 }
 
 #if V8_ENABLE_WEBASSEMBLY
-void Builtins::Generate_WasmCompileLazy(MacroAssembler* masm) {
-  // The function index was put in a register by the jump table trampoline.
-  // Convert to Smi for the runtime call.
-  __ SmiTag(kWasmCompileLazyFuncIndexRegister);
-  {
-    HardAbortScope hard_abort(masm);  // Avoid calls to Abort.
-    FrameAndConstantPoolScope scope(masm, StackFrame::WASM_COMPILE_LAZY);
 
-    // Save all parameter registers (see wasm-linkage.h). They might be
-    // overwritten in the runtime call below. We don't have any callee-saved
-    // registers in wasm, so no need to store anything else.
-    RegList gp_regs;
+struct SaveWasmParamsScope {
+  explicit SaveWasmParamsScope(MacroAssembler* masm)
+      : lowest_fp_reg(std::begin(wasm::kFpParamRegisters)[0]),
+        highest_fp_reg(std::end(wasm::kFpParamRegisters)[-1]),
+        masm(masm) {
     for (Register gp_param_reg : wasm::kGpParamRegisters) {
       gp_regs.set(gp_param_reg);
     }
-    DwVfpRegister lowest_fp_reg = std::begin(wasm::kFpParamRegisters)[0];
-    DwVfpRegister highest_fp_reg = std::end(wasm::kFpParamRegisters)[-1];
+    gp_regs.set(lr);
     for (DwVfpRegister fp_param_reg : wasm::kFpParamRegisters) {
       CHECK(fp_param_reg.code() >= lowest_fp_reg.code() &&
             fp_param_reg.code() <= highest_fp_reg.code());
     }
 
-    CHECK_EQ(gp_regs.Count(), arraysize(wasm::kGpParamRegisters));
+    CHECK_EQ(gp_regs.Count(), arraysize(wasm::kGpParamRegisters) + 1);
     CHECK_EQ(highest_fp_reg.code() - lowest_fp_reg.code() + 1,
              arraysize(wasm::kFpParamRegisters));
     CHECK_EQ(gp_regs.Count(),
-             WasmCompileLazyFrameConstants::kNumberOfSavedGpParamRegs + 1);
+             WasmLiftoffSetupFrameConstants::kNumberOfSavedGpParamRegs +
+                 1 /* instance */ + 1 /* lr */);
     CHECK_EQ(highest_fp_reg.code() - lowest_fp_reg.code() + 1,
-             WasmCompileLazyFrameConstants::kNumberOfSavedFpParamRegs);
+             WasmLiftoffSetupFrameConstants::kNumberOfSavedFpParamRegs);
 
     __ stm(db_w, sp, gp_regs);
     __ vstm(db_w, sp, lowest_fp_reg, highest_fp_reg);
+  }
+  ~SaveWasmParamsScope() {
+    __ vldm(ia_w, sp, lowest_fp_reg, highest_fp_reg);
+    __ ldm(ia_w, sp, gp_regs);
+  }
+
+  RegList gp_regs;
+  DwVfpRegister lowest_fp_reg;
+  DwVfpRegister highest_fp_reg;
+  MacroAssembler* masm;
+};
+
+// This builtin creates the following stack frame:
+//
+// [  feedback vector  ]  <-- sp  // Added by this builtin.
+// [   Wasm instance   ]          // Added by this builtin.
+// [ WASM frame marker ]          // Already there on entry.
+// [     saved fp      ]  <-- fp  // Already there on entry.
+void Builtins::Generate_WasmLiftoffFrameSetup(MacroAssembler* masm) {
+  Register func_index = wasm::kLiftoffFrameSetupFunctionReg;
+  Register vector = r5;
+  Register scratch = r7;
+  Label allocate_vector, done;
+
+  __ ldr(vector, FieldMemOperand(kWasmInstanceRegister,
+                                 WasmInstanceObject::kFeedbackVectorsOffset));
+  __ add(vector, vector, Operand(func_index, LSL, kTaggedSizeLog2));
+  __ ldr(vector, FieldMemOperand(vector, FixedArray::kHeaderSize));
+  __ JumpIfSmi(vector, &allocate_vector);
+  __ bind(&done);
+  __ push(kWasmInstanceRegister);
+  __ push(vector);
+  __ Ret();
+
+  __ bind(&allocate_vector);
 
-    // Push the Wasm instance as an explicit argument to the runtime function.
+  // Feedback vector doesn't exist yet. Call the runtime to allocate it.
+  // We temporarily change the frame type for this, because we need special
+  // handling by the stack walker in case of GC.
+  __ mov(scratch,
+         Operand(StackFrame::TypeToMarker(StackFrame::WASM_LIFTOFF_SETUP)));
+  __ str(scratch, MemOperand(sp));
+  {
+    SaveWasmParamsScope save_params(masm);
+    // Arguments to the runtime function: instance, func_index.
     __ push(kWasmInstanceRegister);
-    // Push the function index as second argument.
-    __ push(kWasmCompileLazyFuncIndexRegister);
-    // Allocate a stack slot for the NativeModule, the pushed value does not
-    // matter.
+    __ SmiTag(func_index);
+    __ push(func_index);
+    // Allocate a stack slot where the runtime function can spill a pointer
+    // to the {NativeModule}.
     __ push(r8);
-    // Initialize the JavaScript context with 0. CEntry will use it to
-    // set the current context on the isolate.
     __ Move(cp, Smi::zero());
-    __ CallRuntime(Runtime::kWasmCompileLazy, 3);
-    // The runtime function returns the jump table slot offset as a Smi. Use
-    // that to compute the jump target in r8.
-    __ mov(r8, Operand::SmiUntag(kReturnRegister0));
+    __ CallRuntime(Runtime::kWasmAllocateFeedbackVector, 3);
+    __ mov(vector, kReturnRegister0);
+    // Saved parameters are restored at the end of this block.
+  }
+  __ mov(scratch, Operand(StackFrame::TypeToMarker(StackFrame::WASM)));
+  __ str(scratch, MemOperand(sp));
+  __ b(&done);
+}
 
-    // Restore registers.
-    __ vldm(ia_w, sp, lowest_fp_reg, highest_fp_reg);
-    __ ldm(ia_w, sp, gp_regs);
+void Builtins::Generate_WasmCompileLazy(MacroAssembler* masm) {
+  // The function index was put in a register by the jump table trampoline.
+  // Convert to Smi for the runtime call.
+  __ SmiTag(kWasmCompileLazyFuncIndexRegister);
+  {
+    HardAbortScope hard_abort(masm);  // Avoid calls to Abort.
+    FrameAndConstantPoolScope scope(masm, StackFrame::INTERNAL);
+
+    {
+      SaveWasmParamsScope save_params(masm);
+
+      // Push the Wasm instance as an explicit argument to the runtime function.
+      __ push(kWasmInstanceRegister);
+      // Push the function index as second argument.
+      __ push(kWasmCompileLazyFuncIndexRegister);
+      // Initialize the JavaScript context with 0. CEntry will use it to
+      // set the current context on the isolate.
+      __ Move(cp, Smi::zero());
+      __ CallRuntime(Runtime::kWasmCompileLazy, 2);
+      // The runtime function returns the jump table slot offset as a Smi. Use
+      // that to compute the jump target in r8.
+      __ mov(r8, Operand::SmiUntag(kReturnRegister0));
+
+      // Saved parameters are restored at the end of this block.
+    }
 
     // After the instance register has been restored, we can add the jump table
     // start to the jump table offset already stored in r8.
-    __ ldr(r9, MemOperand(
-                   kWasmInstanceRegister,
-                   WasmInstanceObject::kJumpTableStartOffset - kHeapObjectTag));
+    __ ldr(r9, FieldMemOperand(kWasmInstanceRegister,
+                               WasmInstanceObject::kJumpTableStartOffset));
     __ add(r8, r8, r9);
   }
 
diff -r -u --color up/v8/src/builtins/arm64/builtins-arm64.cc nw/v8/src/builtins/arm64/builtins-arm64.cc
--- up/v8/src/builtins/arm64/builtins-arm64.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/arm64/builtins-arm64.cc	2023-01-19 16:46:35.994776297 -0500
@@ -26,11 +26,12 @@
 #include "src/runtime/runtime.h"
 
 #if V8_ENABLE_WEBASSEMBLY
-#include "src/wasm/wasm-linkage.h"
-#include "src/wasm/wasm-objects.h"
+#include "src/wasm/baseline/liftoff-assembler-defs.h"
 #include "src/wasm/object-access.h"
 #include "src/wasm/stacks.h"
 #include "src/wasm/wasm-constants.h"
+#include "src/wasm/wasm-linkage.h"
+#include "src/wasm/wasm-objects.h"
 #endif  // V8_ENABLE_WEBASSEMBLY
 
 #if defined(V8_OS_WIN)
@@ -2922,6 +2923,102 @@
 }
 
 #if V8_ENABLE_WEBASSEMBLY
+// Compute register lists for parameters to be saved. We save all parameter
+// registers (see wasm-linkage.h). They might be overwritten in runtime
+// calls. We don't have any callee-saved registers in wasm, so no need to
+// store anything else.
+constexpr RegList kSavedGpRegs = ([]() constexpr {
+  RegList saved_gp_regs;
+  for (Register gp_param_reg : wasm::kGpParamRegisters) {
+    saved_gp_regs.set(gp_param_reg);
+  }
+  // The instance has already been stored in the fixed part of the frame.
+  saved_gp_regs.clear(kWasmInstanceRegister);
+  // All set registers were unique. The instance is skipped.
+  CHECK_EQ(saved_gp_regs.Count(), arraysize(wasm::kGpParamRegisters) - 1);
+  // We push a multiple of 16 bytes.
+  CHECK_EQ(0, saved_gp_regs.Count() % 2);
+  CHECK_EQ(WasmLiftoffSetupFrameConstants::kNumberOfSavedGpParamRegs,
+           saved_gp_regs.Count());
+  return saved_gp_regs;
+})();
+
+constexpr DoubleRegList kSavedFpRegs = ([]() constexpr {
+  DoubleRegList saved_fp_regs;
+  for (DoubleRegister fp_param_reg : wasm::kFpParamRegisters) {
+    saved_fp_regs.set(fp_param_reg);
+  }
+
+  CHECK_EQ(saved_fp_regs.Count(), arraysize(wasm::kFpParamRegisters));
+  CHECK_EQ(WasmLiftoffSetupFrameConstants::kNumberOfSavedFpParamRegs,
+           saved_fp_regs.Count());
+  return saved_fp_regs;
+})();
+
+// When entering this builtin, we have just created a Wasm stack frame:
+//
+// [   Wasm instance   ]  <-- sp
+// [ WASM frame marker ]
+// [     saved fp      ]  <-- fp
+//
+// Due to stack alignment restrictions, this builtin adds the feedback vector
+// plus a filler to the stack. The stack pointer will be
+// moved an appropriate distance by {PatchPrepareStackFrame}.
+//
+// [     (unused)      ]  <-- sp
+// [  feedback vector  ]
+// [   Wasm instance   ]
+// [ WASM frame marker ]
+// [     saved fp      ]  <-- fp
+void Builtins::Generate_WasmLiftoffFrameSetup(MacroAssembler* masm) {
+  Register func_index = wasm::kLiftoffFrameSetupFunctionReg;
+  Register vector = x9;
+  Register scratch = x10;
+  Label allocate_vector, done;
+
+  __ LoadTaggedPointerField(
+      vector, FieldMemOperand(kWasmInstanceRegister,
+                              WasmInstanceObject::kFeedbackVectorsOffset));
+  __ Add(vector, vector, Operand(func_index, LSL, kTaggedSizeLog2));
+  __ LoadTaggedPointerField(vector,
+                            FieldMemOperand(vector, FixedArray::kHeaderSize));
+  __ JumpIfSmi(vector, &allocate_vector);
+  __ bind(&done);
+  __ Push(vector, xzr);
+  __ Ret();
+
+  __ bind(&allocate_vector);
+  // Feedback vector doesn't exist yet. Call the runtime to allocate it.
+  // We temporarily change the frame type for this, because we need special
+  // handling by the stack walker in case of GC.
+  __ Mov(scratch, StackFrame::TypeToMarker(StackFrame::WASM_LIFTOFF_SETUP));
+  __ Str(scratch, MemOperand(fp, TypedFrameConstants::kFrameTypeOffset));
+  // Save registers.
+  __ PushXRegList(kSavedGpRegs);
+  __ PushQRegList(kSavedFpRegs);
+  __ Push<TurboAssembler::kSignLR>(lr, xzr);  // xzr is for alignment.
+
+  // Arguments to the runtime function: instance, func_index, and an
+  // additional stack slot for the NativeModule. The first pushed register
+  // is for alignment. {x0} and {x1} are picked arbitrarily.
+  __ SmiTag(func_index);
+  __ Push(x0, kWasmInstanceRegister, func_index, x1);
+  __ Mov(cp, Smi::zero());
+  __ CallRuntime(Runtime::kWasmAllocateFeedbackVector, 3);
+  __ Mov(vector, kReturnRegister0);
+
+  // Restore registers and frame type.
+  __ Pop<TurboAssembler::kAuthLR>(xzr, lr);
+  __ PopQRegList(kSavedFpRegs);
+  __ PopXRegList(kSavedGpRegs);
+  // Restore the instance from the frame.
+  __ Ldr(kWasmInstanceRegister,
+         MemOperand(fp, WasmFrameConstants::kWasmInstanceOffset));
+  __ Mov(scratch, StackFrame::TypeToMarker(StackFrame::WASM));
+  __ Str(scratch, MemOperand(fp, TypedFrameConstants::kFrameTypeOffset));
+  __ B(&done);
+}
+
 void Builtins::Generate_WasmCompileLazy(MacroAssembler* masm) {
   // The function index was put in w8 by the jump table trampoline.
   // Sign extend and convert to Smi for the runtime call.
@@ -2929,60 +3026,28 @@
           kWasmCompileLazyFuncIndexRegister.W());
   __ SmiTag(kWasmCompileLazyFuncIndexRegister);
 
-  // Compute register lists for parameters to be saved. We save all parameter
-  // registers (see wasm-linkage.h). They might be overwritten in the runtime
-  // call below. We don't have any callee-saved registers in wasm, so no need to
-  // store anything else.
-  constexpr RegList kSavedGpRegs = ([]() constexpr {
-    RegList saved_gp_regs;
-    for (Register gp_param_reg : wasm::kGpParamRegisters) {
-      saved_gp_regs.set(gp_param_reg);
-    }
-    // Also push x1, because we must push multiples of 16 bytes (see
-    // {TurboAssembler::PushCPURegList}.
-    saved_gp_regs.set(x1);
-    // All set registers were unique.
-    CHECK_EQ(saved_gp_regs.Count(), arraysize(wasm::kGpParamRegisters) + 1);
-    // We push a multiple of 16 bytes.
-    CHECK_EQ(0, saved_gp_regs.Count() % 2);
-    // The Wasm instance must be part of the saved registers.
-    CHECK(saved_gp_regs.has(kWasmInstanceRegister));
-    // + instance + alignment
-    CHECK_EQ(WasmCompileLazyFrameConstants::kNumberOfSavedGpParamRegs + 2,
-             saved_gp_regs.Count());
-    return saved_gp_regs;
-  })();
-
-  constexpr DoubleRegList kSavedFpRegs = ([]() constexpr {
-    DoubleRegList saved_fp_regs;
-    for (DoubleRegister fp_param_reg : wasm::kFpParamRegisters) {
-      saved_fp_regs.set(fp_param_reg);
-    }
-
-    CHECK_EQ(saved_fp_regs.Count(), arraysize(wasm::kFpParamRegisters));
-    CHECK_EQ(WasmCompileLazyFrameConstants::kNumberOfSavedFpParamRegs,
-             saved_fp_regs.Count());
-    return saved_fp_regs;
-  })();
-
   UseScratchRegisterScope temps(masm);
   temps.Exclude(x17);
   {
     HardAbortScope hard_abort(masm);  // Avoid calls to Abort.
-    FrameScope scope(masm, StackFrame::WASM_COMPILE_LAZY);
+    FrameScope scope(masm, StackFrame::INTERNAL);
+    // Manually save the instance (which kSavedGpRegs skips because its
+    // other use puts it into the fixed frame anyway). The stack slot is valid
+    // because the {FrameScope} (via {EnterFrame}) always reserves it (for stack
+    // alignment reasons). The instance is needed because once this builtin is
+    // done, we'll call a regular Wasm function.
+    __ Str(kWasmInstanceRegister,
+           MemOperand(fp, WasmFrameConstants::kWasmInstanceOffset));
 
     // Save registers that we need to keep alive across the runtime call.
     __ PushXRegList(kSavedGpRegs);
     __ PushQRegList(kSavedFpRegs);
 
-    // Pass instance, function index, and an additional stack slot for the
-    // native module, as explicit arguments to the runtime function. The first
-    // pushed register is for alignment. {x0} and {x1} are picked arbitrarily.
-    __ Push(x0, kWasmInstanceRegister, kWasmCompileLazyFuncIndexRegister, x1);
+    __ Push(kWasmInstanceRegister, kWasmCompileLazyFuncIndexRegister);
     // Initialize the JavaScript context with 0. CEntry will use it to
     // set the current context on the isolate.
     __ Mov(cp, Smi::zero());
-    __ CallRuntime(Runtime::kWasmCompileLazy, 3);
+    __ CallRuntime(Runtime::kWasmCompileLazy, 2);
 
     // Untag the returned Smi into into x17 (ip1), for later use.
     static_assert(!kSavedGpRegs.has(x17));
@@ -2991,6 +3056,9 @@
     // Restore registers.
     __ PopQRegList(kSavedFpRegs);
     __ PopXRegList(kSavedGpRegs);
+    // Restore the instance from the frame.
+    __ Ldr(kWasmInstanceRegister,
+           MemOperand(fp, WasmFrameConstants::kWasmInstanceOffset));
   }
 
   // The runtime function returned the jump table slot offset as a Smi (now in
@@ -2998,9 +3066,8 @@
   // target, to be compliant with CFI.
   constexpr Register temp = x8;
   static_assert(!kSavedGpRegs.has(temp));
-  __ ldr(temp, MemOperand(
-                   kWasmInstanceRegister,
-                   WasmInstanceObject::kJumpTableStartOffset - kHeapObjectTag));
+  __ ldr(temp, FieldMemOperand(kWasmInstanceRegister,
+                               WasmInstanceObject::kJumpTableStartOffset));
   __ add(x17, temp, Operand(x17));
   // Finally, jump to the jump table slot for the function.
   __ Jump(x17);
diff -r -u --color up/v8/src/builtins/array-to-sorted.tq nw/v8/src/builtins/array-to-sorted.tq
--- up/v8/src/builtins/array-to-sorted.tq	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/array-to-sorted.tq	2023-01-19 16:46:35.994776297 -0500
@@ -65,7 +65,8 @@
       if (sortState.numberOfUndefined != 0) goto FastObject;
 
       const workArray = sortState.workArray;
-      for (let i: Smi = 0; i < workArray.length; ++i) {
+      dcheck(numberOfNonUndefined <= workArray.length);
+      for (let i: Smi = 0; i < numberOfNonUndefined; ++i) {
         const e = UnsafeCast<JSAny>(workArray.objects[i]);
         // TODO(v8:12764): ArrayTimSortImpl already boxed doubles. Support
         // PACKED_DOUBLE_ELEMENTS.
diff -r -u --color up/v8/src/builtins/base.tq nw/v8/src/builtins/base.tq
--- up/v8/src/builtins/base.tq	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/base.tq	2023-01-19 16:46:35.994776297 -0500
@@ -603,8 +603,11 @@
 
 extern macro Print(constexpr string): void;
 extern macro Print(constexpr string, Object): void;
-extern macro Comment(constexpr string): void;
 extern macro Print(Object): void;
+extern macro PrintErr(constexpr string): void;
+extern macro PrintErr(constexpr string, Object): void;
+extern macro PrintErr(Object): void;
+extern macro Comment(constexpr string): void;
 extern macro DebugBreak(): void;
 
 // ES6 7.1.4 ToInteger ( argument )
diff -r -u --color up/v8/src/builtins/builtins-arraybuffer.cc nw/v8/src/builtins/builtins-arraybuffer.cc
--- up/v8/src/builtins/builtins-arraybuffer.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/builtins-arraybuffer.cc	2023-01-19 16:46:35.994776297 -0500
@@ -53,7 +53,7 @@
   // Ensure that all fields are initialized because BackingStore::Allocate is
   // allowed to GC. Note that we cannot move the allocation of the ArrayBuffer
   // after BackingStore::Allocate because of the spec.
-  array_buffer->Setup(shared, resizable, nullptr);
+  array_buffer->Setup(shared, resizable, nullptr, isolate);
 
   size_t byte_length;
   size_t max_byte_length = 0;
@@ -558,7 +558,8 @@
     // Nothing to do for steps 6-12.
 
     // 13. Perform ? DetachArrayBuffer(O).
-    array_buffer->Detach();
+    MAYBE_RETURN(JSArrayBuffer::Detach(array_buffer),
+                 ReadOnlyRoots(isolate).exception());
 
     // 14. Return new.
     return *isolate->factory()
@@ -581,7 +582,8 @@
     }
 
     // 13. Perform ? DetachArrayBuffer(O).
-    array_buffer->Detach();
+    MAYBE_RETURN(JSArrayBuffer::Detach(array_buffer),
+                 ReadOnlyRoots(isolate).exception());
 
     // 14. Return new.
     return *isolate->factory()->NewJSArrayBuffer(std::move(from_backing_store));
@@ -623,7 +625,8 @@
   }
 
   // 13. Perform ? DetachArrayBuffer(O).
-  array_buffer->Detach();
+  MAYBE_RETURN(JSArrayBuffer::Detach(array_buffer),
+               ReadOnlyRoots(isolate).exception());
 
   // 14. Return new.
   return *new_;
diff -r -u --color up/v8/src/builtins/builtins-bigint-gen.h nw/v8/src/builtins/builtins-bigint-gen.h
--- up/v8/src/builtins/builtins-bigint-gen.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/builtins-bigint-gen.h	2023-01-19 16:46:35.994776297 -0500
@@ -93,6 +93,21 @@
     return return_code;
   }
 
+  TNode<Int32T> CppAbsoluteModAndCanonicalize(TNode<BigInt> result,
+                                              TNode<BigInt> x,
+                                              TNode<BigInt> y) {
+    TNode<ExternalReference> mutable_big_int_absolute_mod_and_canonicalize =
+        ExternalConstant(
+            ExternalReference::
+                mutable_big_int_absolute_mod_and_canonicalize_function());
+    TNode<Int32T> return_code = UncheckedCast<Int32T>(CallCFunction(
+        mutable_big_int_absolute_mod_and_canonicalize, MachineType::Int32(),
+        std::make_pair(MachineType::AnyTagged(), result),
+        std::make_pair(MachineType::AnyTagged(), x),
+        std::make_pair(MachineType::AnyTagged(), y)));
+    return return_code;
+  }
+
   void CppBitwiseAndPosPosAndCanonicalize(TNode<BigInt> result, TNode<BigInt> x,
                                           TNode<BigInt> y) {
     TNode<ExternalReference>
diff -r -u --color up/v8/src/builtins/builtins-bigint.tq nw/v8/src/builtins/builtins-bigint.tq
--- up/v8/src/builtins/builtins-bigint.tq	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/builtins-bigint.tq	2023-01-19 16:46:35.994776297 -0500
@@ -17,6 +17,8 @@
     MutableBigInt, BigIntBase, BigIntBase): int32;
 extern macro BigIntBuiltinsAssembler::CppAbsoluteDivAndCanonicalize(
     MutableBigInt, BigIntBase, BigIntBase): int32;
+extern macro BigIntBuiltinsAssembler::CppAbsoluteModAndCanonicalize(
+    MutableBigInt, BigIntBase, BigIntBase): int32;
 extern macro BigIntBuiltinsAssembler::CppBitwiseAndPosPosAndCanonicalize(
     MutableBigInt, BigIntBase, BigIntBase): void;
 extern macro BigIntBuiltinsAssembler::CppBitwiseAndNegNegAndCanonicalize(
@@ -335,6 +337,70 @@
   } label MixedTypes {
     ThrowTypeError(MessageTemplate::kBigIntMixedTypes);
   } label BigIntDivZero {
+    ThrowRangeError(MessageTemplate::kBigIntDivZero);
+  } label TerminationRequested {
+    TerminateExecution();
+  }
+}
+
+macro BigIntModulusImpl(implicit context: Context)(x: BigInt, y: BigInt):
+    BigInt labels BigIntDivZero, TerminationRequested {
+  const ylength = ReadBigIntLength(y);
+
+  // case: x % 0n
+  if (ylength == 0) {
+    goto BigIntDivZero;
+  }
+
+  // case: x % y, where x < y
+  if (MutableBigIntAbsoluteCompare(x, y) < 0) {
+    return x;
+  }
+
+  // case: x % 1n or x % -1n
+  if (ylength == 1 && LoadBigIntDigit(y, 0) == 1) {
+    const zero = AllocateEmptyBigInt(kPositiveSign, 0);
+    return Convert<BigInt>(zero);
+  }
+
+  // case: x % y
+  const resultSign = ReadBigIntSign(x);
+  const resultLength = ylength;
+  const result = AllocateEmptyBigIntNoThrow(resultSign, resultLength)
+      otherwise unreachable;
+
+  if (CppAbsoluteModAndCanonicalize(result, x, y) == 1) {
+    goto TerminationRequested;
+  }
+
+  return Convert<BigInt>(result);
+}
+
+builtin BigIntModulusNoThrow(implicit context: Context)(
+    x: BigInt, y: BigInt): Numeric {
+  try {
+    return BigIntModulusImpl(x, y) otherwise BigIntDivZero,
+           TerminationRequested;
+  } label BigIntDivZero {
+    // Smi sentinel 0 is used to signal BigIntDivZero exception.
+    return Convert<Smi>(0);
+  } label TerminationRequested {
+    // Smi sentinel 1 is used to signal TerminateExecution exception.
+    return Convert<Smi>(1);
+  }
+}
+
+builtin BigIntModulus(implicit context: Context)(
+    xNum: Numeric, yNum: Numeric): BigInt {
+  try {
+    const x = Cast<BigInt>(xNum) otherwise MixedTypes;
+    const y = Cast<BigInt>(yNum) otherwise MixedTypes;
+
+    return BigIntModulusImpl(x, y) otherwise BigIntDivZero,
+           TerminationRequested;
+  } label MixedTypes {
+    ThrowTypeError(MessageTemplate::kBigIntMixedTypes);
+  } label BigIntDivZero {
     ThrowRangeError(MessageTemplate::kBigIntDivZero);
   } label TerminationRequested {
     TerminateExecution();
diff -r -u --color up/v8/src/builtins/builtins-collections-gen.cc nw/v8/src/builtins/builtins-collections-gen.cc
--- up/v8/src/builtins/builtins-collections-gen.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/builtins-collections-gen.cc	2023-01-19 16:46:35.994776297 -0500
@@ -68,7 +68,7 @@
   }
   BIND(&fast_loop);
   {
-    Label if_exception_during_fast_iteration(this);
+    Label if_exception_during_fast_iteration(this, Label::kDeferred);
     TNode<JSArray> initial_entries_jsarray =
         UncheckedCast<JSArray>(initial_entries);
 #if DEBUG
diff -r -u --color up/v8/src/builtins/builtins-definitions.h nw/v8/src/builtins/builtins-definitions.h
--- up/v8/src/builtins/builtins-definitions.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/builtins-definitions.h	2023-01-19 16:46:35.994776297 -0500
@@ -983,12 +983,11 @@
   IF_WASM(ASM, WasmResume, WasmDummy)                                          \
   IF_WASM(ASM, WasmReject, WasmDummy)                                          \
   IF_WASM(ASM, WasmCompileLazy, WasmDummy)                                     \
+  IF_WASM(ASM, WasmLiftoffFrameSetup, WasmDummy)                               \
   IF_WASM(ASM, WasmDebugBreak, WasmDummy)                                      \
   IF_WASM(ASM, WasmOnStackReplace, WasmDummy)                                  \
   IF_WASM(TFC, WasmFloat32ToNumber, WasmFloat32ToNumber)                       \
   IF_WASM(TFC, WasmFloat64ToNumber, WasmFloat64ToNumber)                       \
-  IF_WASM(TFC, WasmI32AtomicWait32, WasmI32AtomicWait32)                       \
-  IF_WASM(TFC, WasmI64AtomicWait32, WasmI64AtomicWait32)                       \
   IF_WASM(TFC, JSToWasmLazyDeoptContinuation, SingleParameterOnStack)          \
                                                                                \
   /* WeakMap */                                                                \
@@ -1873,7 +1872,7 @@
   /* ES #sec-string.prototype.normalize */                             \
   CPP(StringPrototypeNormalizeIntl)                                    \
   /* ecma402 #sup-string.prototype.tolocalelowercase */                \
-  CPP(StringPrototypeToLocaleLowerCase)                                \
+  TFJ(StringPrototypeToLocaleLowerCase, kDontAdaptArgumentsSentinel)   \
   /* ecma402 #sup-string.prototype.tolocaleuppercase */                \
   CPP(StringPrototypeToLocaleUpperCase)                                \
   /* ES #sec-string.prototype.tolowercase */                           \
diff -r -u --color up/v8/src/builtins/builtins-intl-gen.cc nw/v8/src/builtins/builtins-intl-gen.cc
--- up/v8/src/builtins/builtins-intl-gen.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/builtins-intl-gen.cc	2023-01-19 16:46:36.005609627 -0500
@@ -37,11 +37,80 @@
         BitcastTaggedToWord(seq_string),
         IntPtrConstant(SeqOneByteString::kHeaderSize - kHeapObjectTag));
   }
+
+  TNode<Uint8T> GetChar(TNode<SeqOneByteString> seq_string, int index) {
+    int effective_offset =
+        SeqOneByteString::kHeaderSize - kHeapObjectTag + index;
+    return Load<Uint8T>(seq_string, IntPtrConstant(effective_offset));
+  }
+
+  // Jumps to {target} if the first two characters of {seq_string} equal
+  // {pattern} ignoring case.
+  void JumpIfStartsWithIgnoreCase(TNode<SeqOneByteString> seq_string,
+                                  const char* pattern, Label* target) {
+    int effective_offset = SeqOneByteString::kHeaderSize - kHeapObjectTag;
+    TNode<Uint16T> raw =
+        Load<Uint16T>(seq_string, IntPtrConstant(effective_offset));
+    DCHECK_EQ(strlen(pattern), 2);
+#if V8_TARGET_BIG_ENDIAN
+    int raw_pattern = (pattern[0] << 8) + pattern[1];
+#else
+    int raw_pattern = pattern[0] + (pattern[1] << 8);
+#endif
+    GotoIf(Word32Equal(Word32Or(raw, Int32Constant(0x2020)),
+                       Int32Constant(raw_pattern)),
+           target);
+  }
+
+  TNode<BoolT> IsNonAlpha(TNode<Uint8T> character) {
+    return Uint32GreaterThan(
+        Int32Sub(Word32Or(character, Int32Constant(0x20)), Int32Constant('a')),
+        Int32Constant('z' - 'a'));
+  }
+
+  enum class ToLowerCaseKind {
+    kToLowerCase,
+    kToLocaleLowerCase,
+  };
+  void ToLowerCaseImpl(TNode<String> string, TNode<Object> maybe_locales,
+                       TNode<Context> context, ToLowerCaseKind kind,
+                       std::function<void(TNode<Object>)> ReturnFct);
 };
 
 TF_BUILTIN(StringToLowerCaseIntl, IntlBuiltinsAssembler) {
   const auto string = Parameter<String>(Descriptor::kString);
+  ToLowerCaseImpl(string, TNode<Object>() /*maybe_locales*/, TNode<Context>(),
+                  ToLowerCaseKind::kToLowerCase,
+                  [this](TNode<Object> ret) { Return(ret); });
+}
+
+TF_BUILTIN(StringPrototypeToLowerCaseIntl, IntlBuiltinsAssembler) {
+  auto maybe_string = Parameter<Object>(Descriptor::kReceiver);
+  auto context = Parameter<Context>(Descriptor::kContext);
+
+  TNode<String> string =
+      ToThisString(context, maybe_string, "String.prototype.toLowerCase");
+
+  Return(CallBuiltin(Builtin::kStringToLowerCaseIntl, context, string));
+}
 
+TF_BUILTIN(StringPrototypeToLocaleLowerCase, IntlBuiltinsAssembler) {
+  TNode<Int32T> argc =
+      UncheckedParameter<Int32T>(Descriptor::kJSActualArgumentsCount);
+  CodeStubArguments args(this, argc);
+  TNode<Object> maybe_string = args.GetReceiver();
+  TNode<Context> context = Parameter<Context>(Descriptor::kContext);
+  TNode<Object> maybe_locales = args.GetOptionalArgumentValue(0);
+  TNode<String> string =
+      ToThisString(context, maybe_string, "String.prototype.toLocaleLowerCase");
+  ToLowerCaseImpl(string, maybe_locales, context,
+                  ToLowerCaseKind::kToLocaleLowerCase,
+                  [&args](TNode<Object> ret) { args.PopAndReturn(ret); });
+}
+
+void IntlBuiltinsAssembler::ToLowerCaseImpl(
+    TNode<String> string, TNode<Object> maybe_locales, TNode<Context> context,
+    ToLowerCaseKind kind, std::function<void(TNode<Object>)> ReturnFct) {
   Label call_c(this), return_string(this), runtime(this, Label::kDeferred);
 
   // Early exit on empty strings.
@@ -54,9 +123,40 @@
       state(), string, ToDirectStringAssembler::kDontUnpackSlicedStrings);
   to_direct.TryToDirect(&runtime);
 
+  if (kind == ToLowerCaseKind::kToLocaleLowerCase) {
+    Label fast(this), check_locale(this);
+    // Check for fast locales.
+    GotoIf(IsUndefined(maybe_locales), &fast);
+    // Passing a smi here is equivalent to passing an empty list of locales.
+    GotoIf(TaggedIsSmi(maybe_locales), &fast);
+    GotoIfNot(IsString(CAST(maybe_locales)), &runtime);
+    GotoIfNot(IsSeqOneByteString(CAST(maybe_locales)), &runtime);
+    TNode<SeqOneByteString> locale = CAST(maybe_locales);
+    TNode<Uint32T> locale_length = LoadStringLengthAsWord32(locale);
+    GotoIf(Int32LessThan(locale_length, Int32Constant(2)), &runtime);
+    GotoIf(IsNonAlpha(GetChar(locale, 0)), &runtime);
+    GotoIf(IsNonAlpha(GetChar(locale, 1)), &runtime);
+    GotoIf(Word32Equal(locale_length, Int32Constant(2)), &check_locale);
+    GotoIf(Word32NotEqual(locale_length, Int32Constant(5)), &runtime);
+    GotoIf(Word32NotEqual(GetChar(locale, 2), Int32Constant('-')), &runtime);
+    GotoIf(IsNonAlpha(GetChar(locale, 3)), &runtime);
+    GotoIf(IsNonAlpha(GetChar(locale, 4)), &runtime);
+    Goto(&check_locale);
+
+    Bind(&check_locale);
+    JumpIfStartsWithIgnoreCase(locale, "az", &runtime);
+    JumpIfStartsWithIgnoreCase(locale, "el", &runtime);
+    JumpIfStartsWithIgnoreCase(locale, "lt", &runtime);
+    JumpIfStartsWithIgnoreCase(locale, "tr", &runtime);
+    Goto(&fast);
+
+    Bind(&fast);
+  }
+
   const TNode<Int32T> instance_type = to_direct.instance_type();
   CSA_DCHECK(this,
              Word32BinaryNot(IsIndirectStringInstanceType(instance_type)));
+
   GotoIfNot(IsOneByteStringInstanceType(instance_type), &runtime);
 
   // For short strings, do the conversion in CSA through the lookup table.
@@ -103,7 +203,7 @@
     // hash) on the source string.
     GotoIfNot(var_did_change.value(), &return_string);
 
-    Return(dst);
+    ReturnFct(dst);
   }
 
   // Call into C for case conversion. The signature is:
@@ -121,30 +221,23 @@
         function_addr, type_tagged, std::make_pair(type_tagged, src),
         std::make_pair(type_tagged, dst)));
 
-    Return(result);
+    ReturnFct(result);
   }
 
   BIND(&return_string);
-  Return(string);
+  ReturnFct(string);
 
   BIND(&runtime);
-  {
-    const TNode<Object> result = CallRuntime(Runtime::kStringToLowerCaseIntl,
-                                             NoContextConstant(), string);
-    Return(result);
+  if (kind == ToLowerCaseKind::kToLocaleLowerCase) {
+    ReturnFct(CallRuntime(Runtime::kStringToLocaleLowerCase, context, string,
+                          maybe_locales));
+  } else {
+    DCHECK_EQ(kind, ToLowerCaseKind::kToLowerCase);
+    ReturnFct(CallRuntime(Runtime::kStringToLowerCaseIntl, NoContextConstant(),
+                          string));
   }
 }
 
-TF_BUILTIN(StringPrototypeToLowerCaseIntl, IntlBuiltinsAssembler) {
-  auto maybe_string = Parameter<Object>(Descriptor::kReceiver);
-  auto context = Parameter<Context>(Descriptor::kContext);
-
-  TNode<String> string =
-      ToThisString(context, maybe_string, "String.prototype.toLowerCase");
-
-  Return(CallBuiltin(Builtin::kStringToLowerCaseIntl, context, string));
-}
-
 void IntlBuiltinsAssembler::ListFormatCommon(TNode<Context> context,
                                              TNode<Int32T> argc,
                                              Runtime::FunctionId format_func_id,
diff -r -u --color up/v8/src/builtins/builtins-intl.cc nw/v8/src/builtins/builtins-intl.cc
--- up/v8/src/builtins/builtins-intl.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/builtins-intl.cc	2023-01-19 16:46:36.005609627 -0500
@@ -901,28 +901,39 @@
   return *JSRelativeTimeFormat::ResolvedOptions(isolate, format_holder);
 }
 
-BUILTIN(StringPrototypeToLocaleLowerCase) {
-  HandleScope scope(isolate);
-
-  isolate->CountUsage(v8::Isolate::UseCounterFeature::kStringToLocaleLowerCase);
-
-  TO_THIS_STRING(string, "String.prototype.toLocaleLowerCase");
-
-  RETURN_RESULT_OR_FAILURE(
-      isolate, Intl::StringLocaleConvertCase(isolate, string, false,
-                                             args.atOrUndefined(isolate, 1)));
+bool IsFastLocale(Object maybe_locale) {
+  DisallowGarbageCollection no_gc;
+  if (!maybe_locale.IsSeqOneByteString()) {
+    return false;
+  }
+  auto locale = SeqOneByteString::cast(maybe_locale);
+  uint8_t* chars = locale.GetChars(no_gc);
+  if (locale.length() < 2 || !std::isalpha(chars[0]) ||
+      !std::isalpha(chars[1])) {
+    return false;
+  }
+  if (locale.length() != 2 &&
+      (locale.length() != 5 || chars[2] != '-' || !std::isalpha(chars[3]) ||
+       !std::isalpha(chars[4]))) {
+    return false;
+  }
+  char first = chars[0] | 0x20;
+  char second = chars[1] | 0x20;
+  return (first != 'a' || second != 'z') && (first != 'e' || second != 'l') &&
+         (first != 'l' || second != 't') && (first != 't' || second != 'r');
 }
 
 BUILTIN(StringPrototypeToLocaleUpperCase) {
   HandleScope scope(isolate);
-
-  isolate->CountUsage(v8::Isolate::UseCounterFeature::kStringToLocaleUpperCase);
-
+  Handle<Object> maybe_locale = args.atOrUndefined(isolate, 1);
   TO_THIS_STRING(string, "String.prototype.toLocaleUpperCase");
-
-  RETURN_RESULT_OR_FAILURE(
-      isolate, Intl::StringLocaleConvertCase(isolate, string, true,
-                                             args.atOrUndefined(isolate, 1)));
+  if (maybe_locale->IsUndefined() || IsFastLocale(*maybe_locale)) {
+    string = String::Flatten(isolate, string);
+    RETURN_RESULT_OR_FAILURE(isolate, Intl::ConvertToUpper(isolate, string));
+  } else {
+    RETURN_RESULT_OR_FAILURE(isolate, Intl::StringLocaleConvertCase(
+                                          isolate, string, true, maybe_locale));
+  }
 }
 
 BUILTIN(PluralRulesConstructor) {
diff -r -u --color up/v8/src/builtins/builtins-object-gen.cc nw/v8/src/builtins/builtins-object-gen.cc
--- up/v8/src/builtins/builtins-object-gen.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/builtins-object-gen.cc	2023-01-19 16:46:36.005609627 -0500
@@ -756,7 +756,7 @@
       if_number(this, Label::kDeferred), if_object(this), if_primitive(this),
       if_proxy(this, Label::kDeferred), if_regexp(this), if_string(this),
       if_symbol(this, Label::kDeferred), if_value(this),
-      if_bigint(this, Label::kDeferred);
+      if_bigint(this, Label::kDeferred), if_wasm(this);
 
   auto receiver = Parameter<Object>(Descriptor::kReceiver);
   auto context = Parameter<Context>(Descriptor::kContext);
@@ -776,16 +776,22 @@
   const struct {
     InstanceType value;
     Label* label;
-  } kJumpTable[] = {{JS_OBJECT_TYPE, &if_object},
-                    {JS_ARRAY_TYPE, &if_array},
-                    {JS_REG_EXP_TYPE, &if_regexp},
-                    {JS_ARGUMENTS_OBJECT_TYPE, &if_arguments},
-                    {JS_DATE_TYPE, &if_date},
-                    {JS_API_OBJECT_TYPE, &if_object},
-                    {JS_SPECIAL_API_OBJECT_TYPE, &if_object},
-                    {JS_PROXY_TYPE, &if_proxy},
-                    {JS_ERROR_TYPE, &if_error},
-                    {JS_PRIMITIVE_WRAPPER_TYPE, &if_value}};
+  } kJumpTable[] = {
+    {JS_OBJECT_TYPE, &if_object},
+    {JS_ARRAY_TYPE, &if_array},
+    {JS_REG_EXP_TYPE, &if_regexp},
+    {JS_ARGUMENTS_OBJECT_TYPE, &if_arguments},
+    {JS_DATE_TYPE, &if_date},
+    {JS_API_OBJECT_TYPE, &if_object},
+    {JS_SPECIAL_API_OBJECT_TYPE, &if_object},
+    {JS_PROXY_TYPE, &if_proxy},
+    {JS_ERROR_TYPE, &if_error},
+    {JS_PRIMITIVE_WRAPPER_TYPE, &if_value},
+#if V8_ENABLE_WEBASSEMBLY
+    {WASM_STRUCT_TYPE, &if_wasm},
+    {WASM_ARRAY_TYPE, &if_wasm},
+#endif
+  };
   size_t const kNumCases = arraysize(kJumpTable);
   Label* case_labels[kNumCases];
   int32_t case_values[kNumCases];
@@ -1051,6 +1057,11 @@
       Goto(&loop);
     }
 
+#if V8_ENABLE_WEBASSEMBLY
+    BIND(&if_wasm);
+    ThrowTypeError(context, MessageTemplate::kWasmObjectsAreOpaque);
+#endif
+
     BIND(&return_generic);
     {
       TNode<Object> tag = GetProperty(context, ToObject(context, receiver),
diff -r -u --color up/v8/src/builtins/builtins-struct.cc nw/v8/src/builtins/builtins-struct.cc
--- up/v8/src/builtins/builtins-struct.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/builtins-struct.cc	2023-01-19 16:46:36.005609627 -0500
@@ -2,6 +2,8 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
+#include <unordered_set>
+
 #include "src/builtins/builtins-utils-inl.h"
 #include "src/objects/js-struct-inl.h"
 #include "src/objects/property-details.h"
@@ -15,6 +17,25 @@
 // rely on DescriptorArrays and are hence limited to 1020 fields at most.
 static_assert(kMaxJSStructFields <= kMaxNumberOfDescriptors);
 
+namespace {
+
+struct NameHandleHasher {
+  size_t operator()(Handle<Name> name) const { return name->hash(); }
+};
+
+struct UniqueNameHandleEqual {
+  bool operator()(Handle<Name> x, Handle<Name> y) const {
+    DCHECK(x->IsUniqueName());
+    DCHECK(y->IsUniqueName());
+    return *x == *y;
+  }
+};
+
+using UniqueNameHandleSet =
+    std::unordered_set<Handle<Name>, NameHandleHasher, UniqueNameHandleEqual>;
+
+}  // namespace
+
 BUILTIN(SharedStructTypeConstructor) {
   DCHECK(v8_flags.shared_string_table);
 
@@ -43,6 +64,7 @@
       num_properties, 0, AllocationType::kSharedOld);
 
   // Build up the descriptor array.
+  UniqueNameHandleSet all_field_names;
   for (int i = 0; i < num_properties; ++i) {
     Handle<Object> raw_field_name;
     ASSIGN_RETURN_FAILURE_ON_EXCEPTION(
@@ -53,6 +75,14 @@
                                        Object::ToName(isolate, raw_field_name));
     field_name = factory->InternalizeName(field_name);
 
+    // Check that there are no duplicates.
+    const bool is_duplicate = !all_field_names.insert(field_name).second;
+    if (is_duplicate) {
+      THROW_NEW_ERROR_RETURN_FAILURE(
+          isolate, NewTypeError(MessageTemplate::kDuplicateTemplateProperty,
+                                field_name));
+    }
+
     // Shared structs' fields need to be aligned, so make it all tagged.
     PropertyDetails details(
         PropertyKind::kData, SEALED, PropertyLocation::kField,
@@ -85,7 +115,12 @@
 
   instance_map->InitializeDescriptors(isolate, *descriptors);
   // Structs have fixed layout ahead of time, so there's no slack.
-  instance_map->SetInObjectUnusedPropertyFields(0);
+  int out_of_object_properties = num_properties - in_object_properties;
+  if (out_of_object_properties == 0) {
+    instance_map->SetInObjectUnusedPropertyFields(0);
+  } else {
+    instance_map->SetOutOfObjectUnusedPropertyFields(0);
+  }
   instance_map->set_is_extensible(false);
   JSFunction::SetInitialMap(isolate, constructor, instance_map,
                             factory->null_value());
@@ -94,6 +129,16 @@
   // to it.
   instance_map->set_constructor_or_back_pointer(*factory->null_value());
 
+  // Pre-create the enum cache in the shared space, as otherwise for-in
+  // enumeration will incorrectly create an enum cache in the per-thread heap.
+  if (num_properties == 0) {
+    instance_map->SetEnumLength(0);
+  } else {
+    FastKeyAccumulator::InitializeFastPropertyEnumCache(
+        isolate, instance_map, num_properties, AllocationType::kSharedOld);
+    DCHECK_EQ(num_properties, instance_map->EnumLength());
+  }
+
   return *constructor;
 }
 
diff -r -u --color up/v8/src/builtins/builtins-typed-array-gen.cc nw/v8/src/builtins/builtins-typed-array-gen.cc
--- up/v8/src/builtins/builtins-typed-array-gen.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/builtins-typed-array-gen.cc	2023-01-19 16:46:36.005609627 -0500
@@ -64,6 +64,8 @@
   StoreObjectFieldNoWriteBarrier(buffer, JSArrayBuffer::kBitFieldOffset,
                                  Int32Constant(bitfield_value));
 
+  StoreObjectFieldNoWriteBarrier(buffer, JSArrayBuffer::kDetachKeyOffset,
+                                 UndefinedConstant());
   StoreBoundedSizeToObject(buffer, JSArrayBuffer::kRawByteLengthOffset,
                            UintPtrConstant(0));
   StoreSandboxedPointerToObject(buffer, JSArrayBuffer::kBackingStoreOffset,
diff -r -u --color up/v8/src/builtins/builtins-wasm-gen.cc nw/v8/src/builtins/builtins-wasm-gen.cc
--- up/v8/src/builtins/builtins-wasm-gen.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/builtins-wasm-gen.cc	2023-01-19 16:46:36.005609627 -0500
@@ -52,60 +52,6 @@
   Return(ChangeFloat64ToTagged(val));
 }
 
-TF_BUILTIN(WasmI32AtomicWait32, WasmBuiltinsAssembler) {
-  if (!Is32()) {
-    Unreachable();
-    return;
-  }
-
-  auto address = UncheckedParameter<Uint32T>(Descriptor::kAddress);
-  TNode<Number> address_number = ChangeUint32ToTagged(address);
-
-  auto expected_value = UncheckedParameter<Int32T>(Descriptor::kExpectedValue);
-  TNode<Number> expected_value_number = ChangeInt32ToTagged(expected_value);
-
-  auto timeout_low = UncheckedParameter<IntPtrT>(Descriptor::kTimeoutLow);
-  auto timeout_high = UncheckedParameter<IntPtrT>(Descriptor::kTimeoutHigh);
-  TNode<BigInt> timeout = BigIntFromInt32Pair(timeout_low, timeout_high);
-
-  TNode<WasmInstanceObject> instance = LoadInstanceFromFrame();
-  TNode<Context> context = LoadContextFromInstance(instance);
-
-  TNode<Smi> result_smi =
-      CAST(CallRuntime(Runtime::kWasmI32AtomicWait, context, instance,
-                       address_number, expected_value_number, timeout));
-  Return(Unsigned(SmiToInt32(result_smi)));
-}
-
-TF_BUILTIN(WasmI64AtomicWait32, WasmBuiltinsAssembler) {
-  if (!Is32()) {
-    Unreachable();
-    return;
-  }
-
-  auto address = UncheckedParameter<Uint32T>(Descriptor::kAddress);
-  TNode<Number> address_number = ChangeUint32ToTagged(address);
-
-  auto expected_value_low =
-      UncheckedParameter<IntPtrT>(Descriptor::kExpectedValueLow);
-  auto expected_value_high =
-      UncheckedParameter<IntPtrT>(Descriptor::kExpectedValueHigh);
-  TNode<BigInt> expected_value =
-      BigIntFromInt32Pair(expected_value_low, expected_value_high);
-
-  auto timeout_low = UncheckedParameter<IntPtrT>(Descriptor::kTimeoutLow);
-  auto timeout_high = UncheckedParameter<IntPtrT>(Descriptor::kTimeoutHigh);
-  TNode<BigInt> timeout = BigIntFromInt32Pair(timeout_low, timeout_high);
-
-  TNode<WasmInstanceObject> instance = LoadInstanceFromFrame();
-  TNode<Context> context = LoadContextFromInstance(instance);
-
-  TNode<Smi> result_smi =
-      CAST(CallRuntime(Runtime::kWasmI64AtomicWait, context, instance,
-                       address_number, expected_value, timeout));
-  Return(Unsigned(SmiToInt32(result_smi)));
-}
-
 TF_BUILTIN(JSToWasmLazyDeoptContinuation, WasmBuiltinsAssembler) {
   // Reset thread_in_wasm_flag.
   TNode<ExternalReference> thread_in_wasm_flag_address_address =
diff -r -u --color up/v8/src/builtins/builtins.cc nw/v8/src/builtins/builtins.cc
--- up/v8/src/builtins/builtins.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/builtins.cc	2023-01-19 16:46:36.005609627 -0500
@@ -181,9 +181,6 @@
 
 void Builtins::set_code(Builtin builtin, CodeT code) {
   DCHECK_EQ(builtin, code.builtin_id());
-  if (!V8_REMOVE_BUILTINS_CODE_OBJECTS && V8_EXTERNAL_CODE_SPACE_BOOL) {
-    DCHECK_EQ(builtin, FromCodeT(code).builtin_id());
-  }
   DCHECK(Internals::HasHeapObjectTag(code.ptr()));
   // The given builtin may be uninitialized thus we cannot check its type here.
   isolate_->builtin_table()[Builtins::ToInt(builtin)] = code.ptr();
diff -r -u --color up/v8/src/builtins/ia32/builtins-ia32.cc nw/v8/src/builtins/ia32/builtins-ia32.cc
--- up/v8/src/builtins/ia32/builtins-ia32.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/ia32/builtins-ia32.cc	2023-01-19 16:46:36.016442956 -0500
@@ -2,7 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-#include "src/codegen/register.h"
 #if V8_TARGET_ARCH_IA32
 
 #include "src/api/api-arguments.h"
@@ -27,6 +26,7 @@
 #include "src/objects/smi.h"
 
 #if V8_ENABLE_WEBASSEMBLY
+#include "src/wasm/baseline/liftoff-assembler-defs.h"
 #include "src/wasm/wasm-linkage.h"
 #include "src/wasm/wasm-objects.h"
 #endif  // V8_ENABLE_WEBASSEMBLY
@@ -2763,60 +2763,162 @@
 }
 
 #if V8_ENABLE_WEBASSEMBLY
+
+// Returns the offset beyond the last saved FP register.
+int SaveWasmParams(MacroAssembler* masm) {
+  // Save all parameter registers (see wasm-linkage.h). They might be
+  // overwritten in the subsequent runtime call. We don't have any callee-saved
+  // registers in wasm, so no need to store anything else.
+  static_assert(WasmLiftoffSetupFrameConstants::kNumberOfSavedGpParamRegs + 1 ==
+                    arraysize(wasm::kGpParamRegisters),
+                "frame size mismatch");
+  for (Register reg : wasm::kGpParamRegisters) {
+    __ Push(reg);
+  }
+  static_assert(WasmLiftoffSetupFrameConstants::kNumberOfSavedFpParamRegs ==
+                    arraysize(wasm::kFpParamRegisters),
+                "frame size mismatch");
+  __ AllocateStackSpace(kSimd128Size * arraysize(wasm::kFpParamRegisters));
+  int offset = 0;
+  for (DoubleRegister reg : wasm::kFpParamRegisters) {
+    __ movdqu(Operand(esp, offset), reg);
+    offset += kSimd128Size;
+  }
+  return offset;
+}
+
+// Consumes the offset beyond the last saved FP register (as returned by
+// {SaveWasmParams}).
+void RestoreWasmParams(MacroAssembler* masm, int offset) {
+  for (DoubleRegister reg : base::Reversed(wasm::kFpParamRegisters)) {
+    offset -= kSimd128Size;
+    __ movdqu(reg, Operand(esp, offset));
+  }
+  DCHECK_EQ(0, offset);
+  __ add(esp, Immediate(kSimd128Size * arraysize(wasm::kFpParamRegisters)));
+  for (Register reg : base::Reversed(wasm::kGpParamRegisters)) {
+    __ Pop(reg);
+  }
+}
+
+// When this builtin is called, the topmost stack entry is the calling pc.
+// This is replaced with the following:
+//
+// [    calling pc     ]  <-- esp; popped by {ret}.
+// [  feedback vector  ]
+// [   Wasm instance   ]
+// [ WASM frame marker ]
+// [    saved ebp      ]  <-- ebp; this is where "calling pc" used to be.
+void Builtins::Generate_WasmLiftoffFrameSetup(MacroAssembler* masm) {
+  constexpr Register func_index = wasm::kLiftoffFrameSetupFunctionReg;
+
+  // We have zero free registers at this point. Free up a temp. Its value
+  // could be tagged, but we're only storing it on the stack for a short
+  // while, and no GC or stack walk can happen during this time.
+  Register tmp = eax;  // Arbitrarily chosen.
+  __ Push(tmp);        // This is the "marker" slot.
+  {
+    Operand saved_ebp_slot = Operand(esp, kSystemPointerSize);
+    __ mov(tmp, saved_ebp_slot);  // tmp now holds the "calling pc".
+    __ mov(saved_ebp_slot, ebp);
+    __ lea(ebp, Operand(esp, kSystemPointerSize));
+  }
+  __ Push(tmp);  // This is the "instance" slot.
+
+  // Stack layout is now:
+  // [calling pc]  <-- instance_slot  <-- esp
+  // [saved tmp]   <-- marker_slot
+  // [saved ebp]
+  Operand marker_slot = Operand(ebp, WasmFrameConstants::kFrameTypeOffset);
+  Operand instance_slot = Operand(ebp, WasmFrameConstants::kWasmInstanceOffset);
+
+  // Load the feedback vector.
+  __ mov(tmp, FieldOperand(kWasmInstanceRegister,
+                           WasmInstanceObject::kFeedbackVectorsOffset));
+  __ mov(tmp, FieldOperand(tmp, func_index, times_tagged_size,
+                           FixedArray::kHeaderSize));
+  Label allocate_vector;
+  __ JumpIfSmi(tmp, &allocate_vector);
+
+  // Vector exists. Finish setting up the stack frame.
+  __ Push(tmp);                // Feedback vector.
+  __ mov(tmp, instance_slot);  // Calling PC.
+  __ Push(tmp);
+  __ mov(instance_slot, kWasmInstanceRegister);
+  __ mov(tmp, marker_slot);
+  __ mov(marker_slot, Immediate(StackFrame::TypeToMarker(StackFrame::WASM)));
+  __ ret(0);
+
+  __ bind(&allocate_vector);
+  // Feedback vector doesn't exist yet. Call the runtime to allocate it.
+  // We temporarily change the frame type for this, because we need special
+  // handling by the stack walker in case of GC.
+  // For the runtime call, we create the following stack layout:
+  //
+  // [ reserved slot for NativeModule ]  <-- arg[2]
+  // [  ("declared") function index   ]  <-- arg[1] for runtime func.
+  // [         Wasm instance          ]  <-- arg[0]
+  // [ ...spilled Wasm parameters...  ]
+  // [           calling pc           ]  <-- already in place
+  // [   WASM_LIFTOFF_SETUP marker    ]
+  // [           saved ebp            ]  <-- already in place
+
+  __ mov(tmp, marker_slot);
+  __ mov(marker_slot,
+         Immediate(StackFrame::TypeToMarker(StackFrame::WASM_LIFTOFF_SETUP)));
+
+  int offset = SaveWasmParams(masm);
+
+  // Arguments to the runtime function: instance, func_index.
+  __ Push(kWasmInstanceRegister);
+  __ SmiTag(func_index);
+  __ Push(func_index);
+  // Allocate a stack slot where the runtime function can spill a pointer
+  // to the NativeModule.
+  __ Push(esp);
+  __ Move(kContextRegister, Smi::zero());
+  __ CallRuntime(Runtime::kWasmAllocateFeedbackVector, 3);
+  tmp = func_index;
+  __ mov(tmp, kReturnRegister0);
+
+  RestoreWasmParams(masm, offset);
+
+  // Finish setting up the stack frame:
+  //                                    [   calling pc    ]
+  //                     (tmp reg) ---> [ feedback vector ]
+  // [        calling pc         ]  =>  [  Wasm instance  ]  <-- instance_slot
+  // [ WASM_LIFTOFF_SETUP marker ]      [   WASM marker   ]  <-- marker_slot
+  // [         saved ebp         ]      [    saved ebp    ]
+  __ mov(marker_slot, Immediate(StackFrame::TypeToMarker(StackFrame::WASM)));
+  __ Push(tmp);                // Feedback vector.
+  __ mov(tmp, instance_slot);  // Calling PC.
+  __ Push(tmp);
+  __ mov(instance_slot, kWasmInstanceRegister);
+  __ ret(0);
+}
+
 void Builtins::Generate_WasmCompileLazy(MacroAssembler* masm) {
   // The function index was put in edi by the jump table trampoline.
   // Convert to Smi for the runtime call.
   __ SmiTag(kWasmCompileLazyFuncIndexRegister);
   {
     HardAbortScope hard_abort(masm);  // Avoid calls to Abort.
-    FrameScope scope(masm, StackFrame::WASM_COMPILE_LAZY);
-
-    // Save all parameter registers (see wasm-linkage.h). They might be
-    // overwritten in the runtime call below. We don't have any callee-saved
-    // registers in wasm, so no need to store anything else.
-    static_assert(
-        WasmCompileLazyFrameConstants::kNumberOfSavedGpParamRegs + 1 ==
-            arraysize(wasm::kGpParamRegisters),
-        "frame size mismatch");
-    for (Register reg : wasm::kGpParamRegisters) {
-      __ Push(reg);
-    }
-    static_assert(WasmCompileLazyFrameConstants::kNumberOfSavedFpParamRegs ==
-                      arraysize(wasm::kFpParamRegisters),
-                  "frame size mismatch");
-    __ AllocateStackSpace(kSimd128Size * arraysize(wasm::kFpParamRegisters));
-    int offset = 0;
-    for (DoubleRegister reg : wasm::kFpParamRegisters) {
-      __ movdqu(Operand(esp, offset), reg);
-      offset += kSimd128Size;
-    }
+    FrameScope scope(masm, StackFrame::INTERNAL);
+    int offset = SaveWasmParams(masm);
 
-    // Push the Wasm instance as an explicit argument to the runtime function.
+    // Push arguments for the runtime function.
     __ Push(kWasmInstanceRegister);
-    // Push the function index as second argument.
     __ Push(kWasmCompileLazyFuncIndexRegister);
-    // Allocate a stack slot, where the runtime function can spill a pointer to
-    // the the NativeModule.
-    __ Push(esp);
     // Initialize the JavaScript context with 0. CEntry will use it to
     // set the current context on the isolate.
     __ Move(kContextRegister, Smi::zero());
-    __ CallRuntime(Runtime::kWasmCompileLazy, 3);
+    __ CallRuntime(Runtime::kWasmCompileLazy, 2);
     // The runtime function returns the jump table slot offset as a Smi. Use
     // that to compute the jump target in edi.
     __ SmiUntag(kReturnRegister0);
     __ mov(edi, kReturnRegister0);
 
-    // Restore registers.
-    for (DoubleRegister reg : base::Reversed(wasm::kFpParamRegisters)) {
-      offset -= kSimd128Size;
-      __ movdqu(reg, Operand(esp, offset));
-    }
-    DCHECK_EQ(0, offset);
-    __ add(esp, Immediate(kSimd128Size * arraysize(wasm::kFpParamRegisters)));
-    for (Register reg : base::Reversed(wasm::kGpParamRegisters)) {
-      __ Pop(reg);
-    }
+    RestoreWasmParams(masm, offset);
 
     // After the instance register has been restored, we can add the jump table
     // start to the jump table offset already stored in edi.
diff -r -u --color up/v8/src/builtins/loong64/builtins-loong64.cc nw/v8/src/builtins/loong64/builtins-loong64.cc
--- up/v8/src/builtins/loong64/builtins-loong64.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/loong64/builtins-loong64.cc	2023-01-19 16:46:36.016442956 -0500
@@ -26,6 +26,7 @@
 #include "src/runtime/runtime.h"
 
 #if V8_ENABLE_WEBASSEMBLY
+#include "src/wasm/baseline/liftoff-assembler-defs.h"
 #include "src/wasm/wasm-linkage.h"
 #include "src/wasm/wasm-objects.h"
 #endif  // V8_ENABLE_WEBASSEMBLY
@@ -2562,48 +2563,106 @@
 }
 
 #if V8_ENABLE_WEBASSEMBLY
+// Compute register lists for parameters to be saved. We save all parameter
+// registers (see wasm-linkage.h). They might be overwritten in the runtime
+// call below. We don't have any callee-saved registers in wasm, so no need to
+// store anything else.
+constexpr RegList kSavedGpRegs = ([]() constexpr {
+  RegList saved_gp_regs;
+  for (Register gp_param_reg : wasm::kGpParamRegisters) {
+    saved_gp_regs.set(gp_param_reg);
+  }
+
+  // The instance has already been stored in the fixed part of the frame.
+  saved_gp_regs.clear(kWasmInstanceRegister);
+  // All set registers were unique.
+  CHECK_EQ(saved_gp_regs.Count(), arraysize(wasm::kGpParamRegisters) - 1);
+  CHECK_EQ(WasmLiftoffSetupFrameConstants::kNumberOfSavedGpParamRegs,
+           saved_gp_regs.Count());
+  return saved_gp_regs;
+})();
+
+constexpr DoubleRegList kSavedFpRegs = ([]() constexpr {
+  DoubleRegList saved_fp_regs;
+  for (DoubleRegister fp_param_reg : wasm::kFpParamRegisters) {
+    saved_fp_regs.set(fp_param_reg);
+  }
+
+  CHECK_EQ(saved_fp_regs.Count(), arraysize(wasm::kFpParamRegisters));
+  CHECK_EQ(WasmLiftoffSetupFrameConstants::kNumberOfSavedFpParamRegs,
+           saved_fp_regs.Count());
+  return saved_fp_regs;
+})();
+
+// When entering this builtin, we have just created a Wasm stack frame:
+//
+// [   Wasm instance   ]  <-- sp
+// [ WASM frame marker ]
+// [     saved fp      ]  <-- fp
+//
+// Add the feedback vector to the stack.
+//
+// [  feedback vector  ]  <-- sp
+// [   Wasm instance   ]
+// [ WASM frame marker ]
+// [     saved fp      ]  <-- fp
+void Builtins::Generate_WasmLiftoffFrameSetup(MacroAssembler* masm) {
+  Register func_index = wasm::kLiftoffFrameSetupFunctionReg;
+  Register vector = t1;
+  Register scratch = t2;
+  Label allocate_vector, done;
+
+  __ Ld_d(vector, FieldMemOperand(kWasmInstanceRegister,
+                                  WasmInstanceObject::kFeedbackVectorsOffset));
+  __ Alsl_d(vector, func_index, vector, kTaggedSizeLog2);
+  __ Ld_d(vector, FieldMemOperand(vector, FixedArray::kHeaderSize));
+  __ JumpIfSmi(vector, &allocate_vector);
+  __ bind(&done);
+  __ Push(vector);
+  __ Ret();
+
+  __ bind(&allocate_vector);
+  // Feedback vector doesn't exist yet. Call the runtime to allocate it.
+  // We temporarily change the frame type for this, because we need special
+  // handling by the stack walker in case of GC.
+  __ li(scratch, StackFrame::TypeToMarker(StackFrame::WASM_LIFTOFF_SETUP));
+  __ St_d(scratch, MemOperand(fp, TypedFrameConstants::kFrameTypeOffset));
+
+  // Save registers.
+  __ MultiPush(kSavedGpRegs);
+  __ MultiPushFPU(kSavedFpRegs);
+  __ Push(ra);
+
+  // Arguments to the runtime function: instance, func_index, and an
+  // additional stack slot for the NativeModule.
+  __ SmiTag(func_index);
+  __ Push(kWasmInstanceRegister, func_index, zero_reg);
+  __ Move(cp, Smi::zero());
+  __ CallRuntime(Runtime::kWasmAllocateFeedbackVector, 3);
+  __ mov(vector, kReturnRegister0);
+
+  // Restore registers and frame type.
+  __ Pop(ra);
+  __ MultiPopFPU(kSavedFpRegs);
+  __ MultiPop(kSavedGpRegs);
+  __ Ld_d(kWasmInstanceRegister,
+          MemOperand(fp, WasmFrameConstants::kWasmInstanceOffset));
+  __ li(scratch, StackFrame::TypeToMarker(StackFrame::WASM));
+  __ St_d(scratch, MemOperand(fp, TypedFrameConstants::kFrameTypeOffset));
+  __ Branch(&done);
+}
+
 void Builtins::Generate_WasmCompileLazy(MacroAssembler* masm) {
   // The function index was put in t0 by the jump table trampoline.
   // Convert to Smi for the runtime call
   __ SmiTag(kWasmCompileLazyFuncIndexRegister);
 
-  // Compute register lists for parameters to be saved. We save all parameter
-  // registers (see wasm-linkage.h). They might be overwritten in the runtime
-  // call below. We don't have any callee-saved registers in wasm, so no need to
-  // store anything else.
-  constexpr RegList kSavedGpRegs = ([]() constexpr {
-    RegList saved_gp_regs;
-    for (Register gp_param_reg : wasm::kGpParamRegisters) {
-      saved_gp_regs.set(gp_param_reg);
-    }
-
-    // All set registers were unique.
-    CHECK_EQ(saved_gp_regs.Count(), arraysize(wasm::kGpParamRegisters));
-    // The Wasm instance must be part of the saved registers.
-    CHECK(saved_gp_regs.has(kWasmInstanceRegister));
-    // + instance
-    CHECK_EQ(WasmCompileLazyFrameConstants::kNumberOfSavedGpParamRegs + 1,
-             saved_gp_regs.Count());
-    return saved_gp_regs;
-  })();
-
-  constexpr DoubleRegList kSavedFpRegs = ([]() constexpr {
-    DoubleRegList saved_fp_regs;
-    for (DoubleRegister fp_param_reg : wasm::kFpParamRegisters) {
-      saved_fp_regs.set(fp_param_reg);
-    }
-
-    CHECK_EQ(saved_fp_regs.Count(), arraysize(wasm::kFpParamRegisters));
-    CHECK_EQ(WasmCompileLazyFrameConstants::kNumberOfSavedFpParamRegs,
-             saved_fp_regs.Count());
-    return saved_fp_regs;
-  })();
-
   {
     HardAbortScope hard_abort(masm);  // Avoid calls to Abort.
-    FrameScope scope(masm, StackFrame::WASM_COMPILE_LAZY);
+    FrameScope scope(masm, StackFrame::INTERNAL);
 
     // Save registers that we need to keep alive across the runtime call.
+    __ Push(kWasmInstanceRegister);
     __ MultiPush(kSavedGpRegs);
     __ MultiPushFPU(kSavedFpRegs);
 
@@ -2612,16 +2671,12 @@
     // as if they were saved.
     __ Sub_d(sp, sp, kSavedFpRegs.Count() * kDoubleSize);
 
-    // Pass instance and function index as an explicit arguments to the runtime
-    // function.
-    // Allocate a stack slot, where the runtime function can spill a pointer to
-    // the the NativeModule.
-    __ Push(kWasmInstanceRegister, kWasmCompileLazyFuncIndexRegister, zero_reg);
+    __ Push(kWasmInstanceRegister, kWasmCompileLazyFuncIndexRegister);
 
     // Initialize the JavaScript context with 0. CEntry will use it to
     // set the current context on the isolate.
     __ Move(kContextRegister, Smi::zero());
-    __ CallRuntime(Runtime::kWasmCompileLazy, 3);
+    __ CallRuntime(Runtime::kWasmCompileLazy, 2);
 
     // Untag the returned Smi into into t7, for later use.
     static_assert(!kSavedGpRegs.has(t7));
@@ -2631,14 +2686,14 @@
     // Restore registers.
     __ MultiPopFPU(kSavedFpRegs);
     __ MultiPop(kSavedGpRegs);
+    __ Pop(kWasmInstanceRegister);
   }
 
   // The runtime function returned the jump table slot offset as a Smi (now in
   // t7). Use that to compute the jump target.
   static_assert(!kSavedGpRegs.has(t8));
-  __ Ld_d(t8, MemOperand(
-                  kWasmInstanceRegister,
-                  WasmInstanceObject::kJumpTableStartOffset - kHeapObjectTag));
+  __ Ld_d(t8, FieldMemOperand(kWasmInstanceRegister,
+                              WasmInstanceObject::kJumpTableStartOffset));
   __ Add_d(t7, t8, Operand(t7));
 
   // Finally, jump to the jump table slot for the function.
diff -r -u --color up/v8/src/builtins/mips64/builtins-mips64.cc nw/v8/src/builtins/mips64/builtins-mips64.cc
--- up/v8/src/builtins/mips64/builtins-mips64.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/mips64/builtins-mips64.cc	2023-01-19 16:46:36.016442956 -0500
@@ -26,6 +26,7 @@
 #include "src/runtime/runtime.h"
 
 #if V8_ENABLE_WEBASSEMBLY
+#include "src/wasm/baseline/liftoff-assembler-defs.h"
 #include "src/wasm/wasm-linkage.h"
 #include "src/wasm/wasm-objects.h"
 #endif  // V8_ENABLE_WEBASSEMBLY
@@ -2553,48 +2554,106 @@
 }
 
 #if V8_ENABLE_WEBASSEMBLY
+// Compute register lists for parameters to be saved. We save all parameter
+// registers (see wasm-linkage.h). They might be overwritten in the runtime
+// call below. We don't have any callee-saved registers in wasm, so no need to
+// store anything else.
+constexpr RegList kSavedGpRegs = ([]() constexpr {
+  RegList saved_gp_regs;
+  for (Register gp_param_reg : wasm::kGpParamRegisters) {
+    saved_gp_regs.set(gp_param_reg);
+  }
+
+  // The instance has already been stored in the fixed part of the frame.
+  saved_gp_regs.clear(kWasmInstanceRegister);
+  // All set registers were unique.
+  CHECK_EQ(saved_gp_regs.Count(), arraysize(wasm::kGpParamRegisters) - 1);
+  CHECK_EQ(WasmLiftoffSetupFrameConstants::kNumberOfSavedGpParamRegs,
+           saved_gp_regs.Count());
+  return saved_gp_regs;
+})();
+
+constexpr DoubleRegList kSavedFpRegs = ([]() constexpr {
+  DoubleRegList saved_fp_regs;
+  for (DoubleRegister fp_param_reg : wasm::kFpParamRegisters) {
+    saved_fp_regs.set(fp_param_reg);
+  }
+
+  CHECK_EQ(saved_fp_regs.Count(), arraysize(wasm::kFpParamRegisters));
+  CHECK_EQ(WasmLiftoffSetupFrameConstants::kNumberOfSavedFpParamRegs,
+           saved_fp_regs.Count());
+  return saved_fp_regs;
+})();
+
+// When entering this builtin, we have just created a Wasm stack frame:
+//
+// [   Wasm instance   ]  <-- sp
+// [ WASM frame marker ]
+// [     saved fp      ]  <-- fp
+//
+// Add the feedback vector to the stack.
+//
+// [  feedback vector  ]  <-- sp
+// [   Wasm instance   ]
+// [ WASM frame marker ]
+// [     saved fp      ]  <-- fp
+void Builtins::Generate_WasmLiftoffFrameSetup(MacroAssembler* masm) {
+  Register func_index = wasm::kLiftoffFrameSetupFunctionReg;
+  Register vector = t1;
+  Register scratch = t2;
+  Label allocate_vector, done;
+
+  __ Ld(vector, FieldMemOperand(kWasmInstanceRegister,
+                                WasmInstanceObject::kFeedbackVectorsOffset));
+  __ Dlsa(vector, vector, func_index, kTaggedSizeLog2);
+  __ Ld(vector, FieldMemOperand(vector, FixedArray::kHeaderSize));
+  __ JumpIfSmi(vector, &allocate_vector);
+  __ bind(&done);
+  __ Push(vector);
+  __ Ret();
+
+  __ bind(&allocate_vector);
+  // Feedback vector doesn't exist yet. Call the runtime to allocate it.
+  // We temporarily change the frame type for this, because we need special
+  // handling by the stack walker in case of GC.
+  __ li(scratch, StackFrame::TypeToMarker(StackFrame::WASM_LIFTOFF_SETUP));
+  __ Sd(scratch, MemOperand(fp, TypedFrameConstants::kFrameTypeOffset));
+
+  // Save registers.
+  __ MultiPush(kSavedGpRegs);
+  __ MultiPushFPU(kSavedFpRegs);
+  __ Push(ra);
+
+  // Arguments to the runtime function: instance, func_index, and an
+  // additional stack slot for the NativeModule.
+  __ SmiTag(func_index);
+  __ Push(kWasmInstanceRegister, func_index, zero_reg);
+  __ Move(cp, Smi::zero());
+  __ CallRuntime(Runtime::kWasmAllocateFeedbackVector, 3);
+  __ mov(vector, kReturnRegister0);
+
+  // Restore registers and frame type.
+  __ Pop(ra);
+  __ MultiPopFPU(kSavedFpRegs);
+  __ MultiPop(kSavedGpRegs);
+  __ Ld(kWasmInstanceRegister,
+        MemOperand(fp, WasmFrameConstants::kWasmInstanceOffset));
+  __ li(scratch, StackFrame::TypeToMarker(StackFrame::WASM));
+  __ Sd(scratch, MemOperand(fp, TypedFrameConstants::kFrameTypeOffset));
+  __ Branch(&done);
+}
+
 void Builtins::Generate_WasmCompileLazy(MacroAssembler* masm) {
   // The function index was put in t0 by the jump table trampoline.
   // Convert to Smi for the runtime call
   __ SmiTag(kWasmCompileLazyFuncIndexRegister);
 
-  // Compute register lists for parameters to be saved. We save all parameter
-  // registers (see wasm-linkage.h). They might be overwritten in the runtime
-  // call below. We don't have any callee-saved registers in wasm, so no need to
-  // store anything else.
-  constexpr RegList kSavedGpRegs = ([]() constexpr {
-    RegList saved_gp_regs;
-    for (Register gp_param_reg : wasm::kGpParamRegisters) {
-      saved_gp_regs.set(gp_param_reg);
-    }
-
-    // All set registers were unique.
-    CHECK_EQ(saved_gp_regs.Count(), arraysize(wasm::kGpParamRegisters));
-    // The Wasm instance must be part of the saved registers.
-    CHECK(saved_gp_regs.has(kWasmInstanceRegister));
-    // + instance
-    CHECK_EQ(WasmCompileLazyFrameConstants::kNumberOfSavedGpParamRegs + 1,
-             saved_gp_regs.Count());
-    return saved_gp_regs;
-  })();
-
-  constexpr DoubleRegList kSavedFpRegs = ([]() constexpr {
-    DoubleRegList saved_fp_regs;
-    for (DoubleRegister fp_param_reg : wasm::kFpParamRegisters) {
-      saved_fp_regs.set(fp_param_reg);
-    }
-
-    CHECK_EQ(saved_fp_regs.Count(), arraysize(wasm::kFpParamRegisters));
-    CHECK_EQ(WasmCompileLazyFrameConstants::kNumberOfSavedFpParamRegs,
-             saved_fp_regs.Count());
-    return saved_fp_regs;
-  })();
-
   {
     HardAbortScope hard_abort(masm);  // Avoid calls to Abort.
-    FrameScope scope(masm, StackFrame::WASM_COMPILE_LAZY);
+    FrameScope scope(masm, StackFrame::INTERNAL);
 
     // Save registers that we need to keep alive across the runtime call.
+    __ Push(kWasmInstanceRegister);
     __ MultiPush(kSavedGpRegs);
     // Check if machine has simd enabled, if so push vector registers. If not
     // then only push double registers.
@@ -2617,16 +2676,13 @@
     // as if they were saved.
     __ Dsubu(sp, sp, kSavedFpRegs.Count() * kDoubleSize);
     __ bind(&simd_pushed);
-    // Pass instance and function index as an explicit arguments to the runtime
-    // function.
-    // Allocate a stack slot, where the runtime function can spill a pointer to
-    // the the NativeModule.
-    __ Push(kWasmInstanceRegister, kWasmCompileLazyFuncIndexRegister, zero_reg);
+
+    __ Push(kWasmInstanceRegister, kWasmCompileLazyFuncIndexRegister);
 
     // Initialize the JavaScript context with 0. CEntry will use it to
     // set the current context on the isolate.
     __ Move(kContextRegister, Smi::zero());
-    __ CallRuntime(Runtime::kWasmCompileLazy, 3);
+    __ CallRuntime(Runtime::kWasmCompileLazy, 2);
 
     // Restore registers.
     Label pop_doubles, simd_popped;
@@ -2646,6 +2702,7 @@
     __ MultiPopFPU(kSavedFpRegs);
     __ bind(&simd_popped);
     __ MultiPop(kSavedGpRegs);
+    __ Pop(kWasmInstanceRegister);
   }
 
   // Untag the returned Smi, for later use.
@@ -2655,9 +2712,8 @@
   // The runtime function returned the jump table slot offset as a Smi (now in
   // t8). Use that to compute the jump target.
   static_assert(!kSavedGpRegs.has(t8));
-  __ Ld(t8,
-        MemOperand(kWasmInstanceRegister,
-                   WasmInstanceObject::kJumpTableStartOffset - kHeapObjectTag));
+  __ Ld(t8, FieldMemOperand(kWasmInstanceRegister,
+                            WasmInstanceObject::kJumpTableStartOffset));
   __ Daddu(t8, v0, t8);
 
   // Finally, jump to the jump table slot for the function.
diff -r -u --color up/v8/src/builtins/number.tq nw/v8/src/builtins/number.tq
--- up/v8/src/builtins/number.tq	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/number.tq	2023-01-19 16:46:36.016442956 -0500
@@ -666,8 +666,7 @@
   } label Float64s(left: float64, right: float64) {
     return AllocateHeapNumberWithValue(left % right);
   } label AtLeastOneBigInt(left: Numeric, right: Numeric) {
-    tail runtime::BigIntBinaryOp(
-        context, left, right, SmiTag<Operation>(Operation::kModulus));
+    tail bigint::BigIntModulus(left, right);
   }
 }
 
diff -r -u --color up/v8/src/builtins/ppc/builtins-ppc.cc nw/v8/src/builtins/ppc/builtins-ppc.cc
--- up/v8/src/builtins/ppc/builtins-ppc.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/ppc/builtins-ppc.cc	2023-01-19 16:46:36.016442956 -0500
@@ -24,6 +24,7 @@
 #include "src/runtime/runtime.h"
 
 #if V8_ENABLE_WEBASSEMBLY
+#include "src/wasm/baseline/liftoff-assembler-defs.h"
 #include "src/wasm/wasm-linkage.h"
 #include "src/wasm/wasm-objects.h"
 #endif  // V8_ENABLE_WEBASSEMBLY
@@ -2759,71 +2760,130 @@
 }
 
 #if V8_ENABLE_WEBASSEMBLY
-void Builtins::Generate_WasmCompileLazy(MacroAssembler* masm) {
-  // The function index was put in a register by the jump table trampoline.
-  // Convert to Smi for the runtime call.
-  __ SmiTag(kWasmCompileLazyFuncIndexRegister);
 
-  {
-    HardAbortScope hard_abort(masm);  // Avoid calls to Abort.
-    FrameAndConstantPoolScope scope(masm, StackFrame::WASM_COMPILE_LAZY);
-
-    // Save all parameter registers (see wasm-linkage.h). They might be
-    // overwritten in the runtime call below. We don't have any callee-saved
-    // registers in wasm, so no need to store anything else.
-    RegList gp_regs;
+struct SaveWasmParamsScope {
+  explicit SaveWasmParamsScope(MacroAssembler* masm) : masm(masm) {
     for (Register gp_param_reg : wasm::kGpParamRegisters) {
       gp_regs.set(gp_param_reg);
     }
-
-    DoubleRegList fp_regs;
     for (DoubleRegister fp_param_reg : wasm::kFpParamRegisters) {
       fp_regs.set(fp_param_reg);
     }
 
-    // List must match register numbers under kFpParamRegisters.
-    constexpr Simd128RegList simd_regs = {v1, v2, v3, v4, v5, v6, v7, v8};
-
     CHECK_EQ(gp_regs.Count(), arraysize(wasm::kGpParamRegisters));
     CHECK_EQ(fp_regs.Count(), arraysize(wasm::kFpParamRegisters));
     CHECK_EQ(simd_regs.Count(), arraysize(wasm::kFpParamRegisters));
-    CHECK_EQ(WasmCompileLazyFrameConstants::kNumberOfSavedGpParamRegs + 1,
+    CHECK_EQ(WasmLiftoffSetupFrameConstants::kNumberOfSavedGpParamRegs + 1,
              gp_regs.Count());
-    CHECK_EQ(WasmCompileLazyFrameConstants::kNumberOfSavedFpParamRegs,
+    CHECK_EQ(WasmLiftoffSetupFrameConstants::kNumberOfSavedFpParamRegs,
              fp_regs.Count());
-    CHECK_EQ(WasmCompileLazyFrameConstants::kNumberOfSavedFpParamRegs,
+    CHECK_EQ(WasmLiftoffSetupFrameConstants::kNumberOfSavedFpParamRegs,
              simd_regs.Count());
 
     __ MultiPush(gp_regs);
     __ MultiPushF64AndV128(fp_regs, simd_regs, ip, r0);
+  }
+  ~SaveWasmParamsScope() {
+    __ MultiPopF64AndV128(fp_regs, simd_regs, ip, r0);
+    __ MultiPop(gp_regs);
+  }
+
+  RegList gp_regs;
+  DoubleRegList fp_regs;
+  // List must match register numbers under kFpParamRegisters.
+  Simd128RegList simd_regs = {v1, v2, v3, v4, v5, v6, v7, v8};
+  MacroAssembler* masm;
+};
+
+void Builtins::Generate_WasmLiftoffFrameSetup(MacroAssembler* masm) {
+  Register func_index = wasm::kLiftoffFrameSetupFunctionReg;
+  Register vector = r11;
+  Register scratch = ip;
+  Label allocate_vector, done;
 
-    // Push the Wasm instance as an explicit argument to the runtime function.
-    __ Push(kWasmInstanceRegister);
-    // Push the function index as second argument.
-    __ Push(kWasmCompileLazyFuncIndexRegister);
-    // Allocate a stack slot for the NativeModule, the pushed value does not
-    // matter.
+  __ LoadTaggedPointerField(
+      vector,
+      FieldMemOperand(kWasmInstanceRegister,
+                      WasmInstanceObject::kFeedbackVectorsOffset),
+      scratch);
+  __ ShiftLeftU64(scratch, func_index, Operand(kTaggedSizeLog2));
+  __ AddS64(vector, vector, scratch);
+  __ LoadTaggedPointerField(
+      vector, FieldMemOperand(vector, FixedArray::kHeaderSize), scratch);
+  __ JumpIfSmi(vector, &allocate_vector);
+  __ bind(&done);
+  __ push(kWasmInstanceRegister);
+  __ push(vector);
+  __ Ret();
+
+  __ bind(&allocate_vector);
+
+  // Feedback vector doesn't exist yet. Call the runtime to allocate it.
+  // We temporarily change the frame type for this, because we need special
+  // handling by the stack walker in case of GC.
+  __ mov(scratch,
+         Operand(StackFrame::TypeToMarker(StackFrame::WASM_LIFTOFF_SETUP)));
+  __ StoreU64(scratch, MemOperand(sp));
+
+  // Save current return address as it will get clobbered during CallRuntime.
+  __ mflr(scratch);
+  __ push(scratch);
+  {
+    SaveWasmParamsScope save_params(masm);  // Will use r0 and ip as scratch.
+    // Arguments to the runtime function: instance, func_index.
+    __ push(kWasmInstanceRegister);
+    __ SmiTag(func_index);
+    __ push(func_index);
+    // Allocate a stack slot where the runtime function can spill a pointer
+    // to the {NativeModule}.
     __ push(r11);
-    // Initialize the JavaScript context with 0. CEntry will use it to
-    // set the current context on the isolate.
     __ LoadSmiLiteral(cp, Smi::zero());
-    __ CallRuntime(Runtime::kWasmCompileLazy, 3);
-    // The runtime function returns the jump table slot offset as a Smi. Use
-    // that to compute the jump target in r11.
-    __ SmiUntag(kReturnRegister0);
-    __ mr(r11, kReturnRegister0);
+    __ CallRuntime(Runtime::kWasmAllocateFeedbackVector, 3);
+    __ mr(vector, kReturnRegister0);
+    // Saved parameters are restored at the end of this block.
+  }
+  __ pop(scratch);
+  __ mtlr(scratch);
+
+  __ mov(scratch, Operand(StackFrame::TypeToMarker(StackFrame::WASM)));
+  __ StoreU64(scratch, MemOperand(sp));
+  __ b(&done);
+}
 
-    // Restore registers.
-    __ MultiPopF64AndV128(fp_regs, simd_regs, ip, r0);
-    __ MultiPop(gp_regs);
+void Builtins::Generate_WasmCompileLazy(MacroAssembler* masm) {
+  // The function index was put in a register by the jump table trampoline.
+  // Convert to Smi for the runtime call.
+  __ SmiTag(kWasmCompileLazyFuncIndexRegister);
+
+  {
+    HardAbortScope hard_abort(masm);  // Avoid calls to Abort.
+    FrameAndConstantPoolScope scope(masm, StackFrame::INTERNAL);
+
+    {
+      SaveWasmParamsScope save_params(masm);  // Will use r0 and ip as scratch.
+
+      // Push the Wasm instance as an explicit argument to the runtime function.
+      __ push(kWasmInstanceRegister);
+      // Push the function index as second argument.
+      __ push(kWasmCompileLazyFuncIndexRegister);
+      // Initialize the JavaScript context with 0. CEntry will use it to
+      // set the current context on the isolate.
+      __ LoadSmiLiteral(cp, Smi::zero());
+      __ CallRuntime(Runtime::kWasmCompileLazy, 2);
+      // The runtime function returns the jump table slot offset as a Smi. Use
+      // that to compute the jump target in r11.
+      __ SmiUntag(kReturnRegister0);
+      __ mr(r11, kReturnRegister0);
+
+      // Saved parameters are restored at the end of this block.
+    }
 
     // After the instance register has been restored, we can add the jump table
-    // start to the jump table offset already stored in r8.
-    __ LoadU64(
-        ip,
-        MemOperand(kWasmInstanceRegister,
-                   WasmInstanceObject::kJumpTableStartOffset - kHeapObjectTag),
-        r0);
+    // start to the jump table offset already stored in r11.
+    __ LoadU64(ip,
+               FieldMemOperand(kWasmInstanceRegister,
+                               WasmInstanceObject::kJumpTableStartOffset),
+               r0);
     __ AddS64(r11, r11, ip);
   }
 
diff -r -u --color up/v8/src/builtins/promise-all-element-closure.tq nw/v8/src/builtins/promise-all-element-closure.tq
--- up/v8/src/builtins/promise-all-element-closure.tq	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/promise-all-element-closure.tq	2023-01-19 16:46:36.016442956 -0500
@@ -174,6 +174,12 @@
     const arrayMap =
         *NativeContextSlot(
         nativeContext, ContextSlot::JS_ARRAY_PACKED_ELEMENTS_MAP_INDEX);
+
+    // If resolve and reject handlers close over values to keep track of whether
+    // an input promise is already settled, mark the values array as COW before
+    // letting it escape to user code.
+    if (hasResolveAndRejectClosures) MakeFixedArrayCOW(values);
+
     const valuesArray = NewJSArray(arrayMap, values);
     Call(promiseContext, resolve, Undefined, valuesArray);
   }
diff -r -u --color up/v8/src/builtins/promise-all.tq nw/v8/src/builtins/promise-all.tq
--- up/v8/src/builtins/promise-all.tq	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/promise-all.tq	2023-01-19 16:46:36.016442956 -0500
@@ -138,7 +138,8 @@
     nativeContext: NativeContext, iter: iterator::IteratorRecord,
     constructor: Constructor, capability: PromiseCapability,
     promiseResolveFunction: JSAny, createResolveElementFunctor: F1,
-    createRejectElementFunctor: F2): JSAny labels
+    createRejectElementFunctor: F2,
+    hasResolveAndRejectClosures: constexpr bool): JSAny labels
 Reject(JSAny) {
   const promise = capability.promise;
   const resolve = capability.resolve;
@@ -308,6 +309,12 @@
       const arrayMap =
           *NativeContextSlot(
           nativeContext, ContextSlot::JS_ARRAY_PACKED_ELEMENTS_MAP_INDEX);
+
+      // If resolve and reject handlers close over values to keep track of
+      // whether an input promise is already settled, mark the values array as
+      // COW before letting it escape to user code.
+      if (hasResolveAndRejectClosures) MakeFixedArrayCOW(values);
+
       const valuesArray = NewJSArray(arrayMap, values);
       Call(nativeContext, UnsafeCast<JSAny>(resolve), Undefined, valuesArray);
     }
@@ -319,7 +326,8 @@
 transitioning macro GeneratePromiseAll<F1: type, F2: type>(
     implicit context: Context)(
     receiver: JSAny, iterable: JSAny, createResolveElementFunctor: F1,
-    createRejectElementFunctor: F2, message: constexpr string): JSAny {
+    createRejectElementFunctor: F2, message: constexpr string,
+    hasResolveAndRejectClosures: constexpr bool): JSAny {
   const nativeContext = LoadNativeContext(context);
   // Let C be the this value.
   // If Type(C) is not Object, throw a TypeError exception.
@@ -352,7 +360,8 @@
     //    IfAbruptRejectPromise(result, promiseCapability).
     return PerformPromiseAll(
         nativeContext, i, constructor, capability, promiseResolveFunction,
-        createResolveElementFunctor, createRejectElementFunctor)
+        createResolveElementFunctor, createRejectElementFunctor,
+        hasResolveAndRejectClosures)
         otherwise Reject;
   } catch (e, _message) deferred {
     goto Reject(e);
@@ -368,7 +377,7 @@
     js-implicit context: Context, receiver: JSAny)(iterable: JSAny): JSAny {
   return GeneratePromiseAll(
       receiver, iterable, PromiseAllResolveElementFunctor{},
-      PromiseAllRejectElementFunctor{}, 'Promise.all');
+      PromiseAllRejectElementFunctor{}, 'Promise.all', false);
 }
 
 // ES#sec-promise.allsettled
@@ -377,7 +386,7 @@
     js-implicit context: Context, receiver: JSAny)(iterable: JSAny): JSAny {
   return GeneratePromiseAll(
       receiver, iterable, PromiseAllSettledResolveElementFunctor{},
-      PromiseAllSettledRejectElementFunctor{}, 'Promise.allSettled');
+      PromiseAllSettledRejectElementFunctor{}, 'Promise.allSettled', true);
 }
 
 extern macro PromiseAllResolveElementSharedFunConstant(): SharedFunctionInfo;
@@ -385,4 +394,6 @@
     SharedFunctionInfo;
 extern macro PromiseAllSettledResolveElementSharedFunConstant():
     SharedFunctionInfo;
+
+extern macro MakeFixedArrayCOW(FixedArray): void;
 }
diff -r -u --color up/v8/src/builtins/riscv/builtins-riscv.cc nw/v8/src/builtins/riscv/builtins-riscv.cc
--- up/v8/src/builtins/riscv/builtins-riscv.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/riscv/builtins-riscv.cc	2023-01-19 16:46:36.016442956 -0500
@@ -21,8 +21,12 @@
 #include "src/objects/objects-inl.h"
 #include "src/objects/smi.h"
 #include "src/runtime/runtime.h"
+
+#if V8_ENABLE_WEBASSEMBLY
+#include "src/wasm/baseline/liftoff-assembler-defs.h"
 #include "src/wasm/wasm-linkage.h"
 #include "src/wasm/wasm-objects.h"
+#endif  // V8_ENABLE_WEBASSEMBLY
 
 namespace v8 {
 namespace internal {
@@ -2625,54 +2629,115 @@
 }
 
 #if V8_ENABLE_WEBASSEMBLY
+// Compute register lists for parameters to be saved. We save all parameter
+// registers (see wasm-linkage.h). They might be overwritten in the runtime
+// call below. We don't have any callee-saved registers in wasm, so no need to
+// store anything else.
+constexpr RegList kSavedGpRegs = ([]() constexpr {
+  RegList saved_gp_regs;
+  for (Register gp_param_reg : wasm::kGpParamRegisters) {
+    saved_gp_regs.set(gp_param_reg);
+  }
+
+  // The instance has already been stored in the fixed part of the frame.
+  saved_gp_regs.clear(kWasmInstanceRegister);
+  // All set registers were unique.
+  CHECK_EQ(saved_gp_regs.Count(), arraysize(wasm::kGpParamRegisters) - 1);
+  CHECK_EQ(WasmLiftoffSetupFrameConstants::kNumberOfSavedGpParamRegs,
+           saved_gp_regs.Count());
+  return saved_gp_regs;
+})();
+
+constexpr DoubleRegList kSavedFpRegs = ([]() constexpr {
+  DoubleRegList saved_fp_regs;
+  for (DoubleRegister fp_param_reg : wasm::kFpParamRegisters) {
+    saved_fp_regs.set(fp_param_reg);
+  }
+
+  CHECK_EQ(saved_fp_regs.Count(), arraysize(wasm::kFpParamRegisters));
+  CHECK_EQ(WasmLiftoffSetupFrameConstants::kNumberOfSavedFpParamRegs,
+           saved_fp_regs.Count());
+  return saved_fp_regs;
+})();
+
+// When entering this builtin, we have just created a Wasm stack frame:
+//
+// [   Wasm instance   ]  <-- sp
+// [ WASM frame marker ]
+// [     saved fp      ]  <-- fp
+//
+// Add the feedback vector to the stack.
+//
+// [  feedback vector  ]  <-- sp
+// [   Wasm instance   ]
+// [ WASM frame marker ]
+// [     saved fp      ]  <-- fp
+void Builtins::Generate_WasmLiftoffFrameSetup(MacroAssembler* masm) {
+  Register func_index = wasm::kLiftoffFrameSetupFunctionReg;
+  Register vector = t1;
+  Register scratch = t2;
+  Label allocate_vector, done;
+
+  __ LoadWord(vector,
+              FieldMemOperand(kWasmInstanceRegister,
+                              WasmInstanceObject::kFeedbackVectorsOffset));
+  __ CalcScaledAddress(vector, vector, func_index, kTaggedSizeLog2);
+  __ LoadWord(vector, FieldMemOperand(vector, FixedArray::kHeaderSize));
+  __ JumpIfSmi(vector, &allocate_vector);
+  __ bind(&done);
+  __ Push(vector);
+  __ Ret();
+
+  __ bind(&allocate_vector);
+  // Feedback vector doesn't exist yet. Call the runtime to allocate it.
+  // We temporarily change the frame type for this, because we need special
+  // handling by the stack walker in case of GC.
+  __ li(scratch, StackFrame::TypeToMarker(StackFrame::WASM_LIFTOFF_SETUP));
+  __ StoreWord(scratch, MemOperand(fp, TypedFrameConstants::kFrameTypeOffset));
+
+  // Save registers.
+  __ MultiPush(kSavedGpRegs);
+  __ MultiPushFPU(kSavedFpRegs);
+  __ Push(ra);
+
+  // Arguments to the runtime function: instance, func_index, and an
+  // additional stack slot for the NativeModule.
+  __ SmiTag(func_index);
+  __ Push(kWasmInstanceRegister, func_index, zero_reg);
+  __ Move(cp, Smi::zero());
+  __ CallRuntime(Runtime::kWasmAllocateFeedbackVector, 3);
+  __ mv(vector, kReturnRegister0);
+
+  // Restore registers and frame type.
+  __ Pop(ra);
+  __ MultiPopFPU(kSavedFpRegs);
+  __ MultiPop(kSavedGpRegs);
+  __ LoadWord(kWasmInstanceRegister,
+              MemOperand(fp, WasmFrameConstants::kWasmInstanceOffset));
+  __ li(scratch, StackFrame::TypeToMarker(StackFrame::WASM));
+  __ StoreWord(scratch, MemOperand(fp, TypedFrameConstants::kFrameTypeOffset));
+  __ Branch(&done);
+}
+
 void Builtins::Generate_WasmCompileLazy(MacroAssembler* masm) {
   // The function index was put in t0 by the jump table trampoline.
   // Convert to Smi for the runtime call
   __ SmiTag(kWasmCompileLazyFuncIndexRegister);
 
-  RegList kSavedGpRegs = ([]() constexpr {
-    RegList saved_gp_regs;
-    for (Register gp_param_reg : wasm::kGpParamRegisters) {
-      saved_gp_regs.set(gp_param_reg);
-    }
-
-    // All set registers were unique.
-    CHECK_EQ(saved_gp_regs.Count(), arraysize(wasm::kGpParamRegisters));
-    // The Wasm instance must be part of the saved registers.
-    CHECK(saved_gp_regs.has(kWasmInstanceRegister));
-    CHECK_EQ(WasmCompileLazyFrameConstants::kNumberOfSavedGpParamRegs + 1,
-             saved_gp_regs.Count());
-    return saved_gp_regs;
-  })();
-
-  DoubleRegList kSavedFpRegs = ([]() constexpr {
-    DoubleRegList saved_fp_regs;
-    for (DoubleRegister fp_param_reg : wasm::kFpParamRegisters) {
-      saved_fp_regs.set(fp_param_reg);
-    }
-
-    CHECK_EQ(saved_fp_regs.Count(), arraysize(wasm::kFpParamRegisters));
-    CHECK_EQ(WasmCompileLazyFrameConstants::kNumberOfSavedFpParamRegs,
-             saved_fp_regs.Count());
-    return saved_fp_regs;
-  })();
-
   {
     HardAbortScope hard_abort(masm);  // Avoid calls to Abort.
-    FrameScope scope(masm, StackFrame::WASM_COMPILE_LAZY);
+    FrameScope scope(masm, StackFrame::INTERNAL);
 
+    // Save registers that we need to keep alive across the runtime call.
+    __ Push(kWasmInstanceRegister);
     __ MultiPush(kSavedGpRegs);
     __ MultiPushFPU(kSavedFpRegs);
 
-    // Pass instance and function index as an explicit arguments to the runtime
-    // function.
-    // Also allocate a stack slot for the NativeModule, the pushed value does
-    // not matter.
-    __ Push(kWasmInstanceRegister, kWasmCompileLazyFuncIndexRegister, a0);
+    __ Push(kWasmInstanceRegister, kWasmCompileLazyFuncIndexRegister);
     // Initialize the JavaScript context with 0. CEntry will use it to
     // set the current context on the isolate.
     __ Move(kContextRegister, Smi::zero());
-    __ CallRuntime(Runtime::kWasmCompileLazy, 3);
+    __ CallRuntime(Runtime::kWasmCompileLazy, 2);
 
     __ SmiUntag(s1, a0);  // move return value to s1 since a0 will be restored
                           // to the value before the call
@@ -2681,14 +2746,14 @@
     // Restore registers.
     __ MultiPopFPU(kSavedFpRegs);
     __ MultiPop(kSavedGpRegs);
+    __ Pop(kWasmInstanceRegister);
   }
 
   // The runtime function returned the jump table slot offset as a Smi (now in
   // x17). Use that to compute the jump target.
-  __ LoadWord(
-      kScratchReg,
-      MemOperand(kWasmInstanceRegister,
-                 WasmInstanceObject::kJumpTableStartOffset - kHeapObjectTag));
+  __ LoadWord(kScratchReg,
+              FieldMemOperand(kWasmInstanceRegister,
+                              WasmInstanceObject::kJumpTableStartOffset));
   __ AddWord(s1, s1, Operand(kScratchReg));
   // Finally, jump to the entrypoint.
   __ Jump(s1);
diff -r -u --color up/v8/src/builtins/s390/builtins-s390.cc nw/v8/src/builtins/s390/builtins-s390.cc
--- up/v8/src/builtins/s390/builtins-s390.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/s390/builtins-s390.cc	2023-01-19 16:46:36.027276288 -0500
@@ -24,6 +24,7 @@
 #include "src/runtime/runtime.h"
 
 #if V8_ENABLE_WEBASSEMBLY
+#include "src/wasm/baseline/liftoff-assembler-defs.h"
 #include "src/wasm/wasm-linkage.h"
 #include "src/wasm/wasm-objects.h"
 #endif  // V8_ENABLE_WEBASSEMBLY
@@ -2767,63 +2768,119 @@
 }
 
 #if V8_ENABLE_WEBASSEMBLY
-void Builtins::Generate_WasmCompileLazy(MacroAssembler* masm) {
-  // The function index was put in a register by the jump table trampoline.
-  // Convert to Smi for the runtime call.
-  __ SmiTag(kWasmCompileLazyFuncIndexRegister);
 
-  {
-    HardAbortScope hard_abort(masm);  // Avoid calls to Abort.
-    FrameAndConstantPoolScope scope(masm, StackFrame::WASM_COMPILE_LAZY);
-
-    // Save all parameter registers (see wasm-linkage.h). They might be
-    // overwritten in the runtime call below. We don't have any callee-saved
-    // registers in wasm, so no need to store anything else.
-    RegList gp_regs;
+struct SaveWasmParamsScope {
+  explicit SaveWasmParamsScope(MacroAssembler* masm) : masm(masm) {
     for (Register gp_param_reg : wasm::kGpParamRegisters) {
       gp_regs.set(gp_param_reg);
     }
-
-    DoubleRegList fp_regs;
     for (DoubleRegister fp_param_reg : wasm::kFpParamRegisters) {
       fp_regs.set(fp_param_reg);
     }
 
     CHECK_EQ(gp_regs.Count(), arraysize(wasm::kGpParamRegisters));
     CHECK_EQ(fp_regs.Count(), arraysize(wasm::kFpParamRegisters));
-    CHECK_EQ(WasmCompileLazyFrameConstants::kNumberOfSavedGpParamRegs + 1,
+    CHECK_EQ(WasmLiftoffSetupFrameConstants::kNumberOfSavedGpParamRegs + 1,
              gp_regs.Count());
-    CHECK_EQ(WasmCompileLazyFrameConstants::kNumberOfSavedFpParamRegs,
+    CHECK_EQ(WasmLiftoffSetupFrameConstants::kNumberOfSavedFpParamRegs,
              fp_regs.Count());
 
     __ MultiPush(gp_regs);
-    __ MultiPushF64OrV128(fp_regs, ip);
+    __ MultiPushF64OrV128(fp_regs, r1);
+  }
+  ~SaveWasmParamsScope() {
+    __ MultiPopF64OrV128(fp_regs, r1);
+    __ MultiPop(gp_regs);
+  }
+
+  RegList gp_regs;
+  DoubleRegList fp_regs;
+  MacroAssembler* masm;
+};
+
+void Builtins::Generate_WasmLiftoffFrameSetup(MacroAssembler* masm) {
+  Register func_index = wasm::kLiftoffFrameSetupFunctionReg;
+  Register vector = ip;
+  Register scratch = r0;
+  Label allocate_vector, done;
+
+  __ LoadTaggedPointerField(
+      vector, FieldMemOperand(kWasmInstanceRegister,
+                              WasmInstanceObject::kFeedbackVectorsOffset));
+  __ ShiftLeftU64(scratch, func_index, Operand(kTaggedSizeLog2));
+  __ AddS64(vector, vector, scratch);
+  __ LoadTaggedPointerField(vector,
+                            FieldMemOperand(vector, FixedArray::kHeaderSize));
+  __ JumpIfSmi(vector, &allocate_vector);
+  __ bind(&done);
+  __ push(kWasmInstanceRegister);
+  __ push(vector);
+  __ Ret();
+
+  __ bind(&allocate_vector);
 
-    // Push the Wasm instance as an explicit argument to the runtime function.
-    __ Push(kWasmInstanceRegister);
-    // Push the function index as second argument.
-    __ Push(kWasmCompileLazyFuncIndexRegister);
-    // Allocate a stack slot for the NativeModule, the pushed value does not
-    // matter.
-    __ push(ip);
-    // Initialize the JavaScript context with 0. CEntry will use it to
-    // set the current context on the isolate.
+  // Feedback vector doesn't exist yet. Call the runtime to allocate it.
+  // We temporarily change the frame type for this, because we need special
+  // handling by the stack walker in case of GC.
+  __ mov(scratch,
+         Operand(StackFrame::TypeToMarker(StackFrame::WASM_LIFTOFF_SETUP)));
+  __ StoreU64(scratch, MemOperand(sp));
+
+  // Save current return address as it will get clobbered during CallRuntime.
+  __ push(r14);
+  {
+    SaveWasmParamsScope save_params(masm);
+    // Arguments to the runtime function: instance, func_index.
+    __ push(kWasmInstanceRegister);
+    __ SmiTag(func_index);
+    __ push(func_index);
+    // Allocate a stack slot where the runtime function can spill a pointer
+    // to the {NativeModule}.
+    __ push(r10);
     __ LoadSmiLiteral(cp, Smi::zero());
-    __ CallRuntime(Runtime::kWasmCompileLazy, 3);
-    // The runtime function returns the jump table slot offset as a Smi. Use
-    // that to compute the jump target in ip.
-    __ SmiUntag(kReturnRegister0);
-    __ mov(ip, kReturnRegister0);
+    __ CallRuntime(Runtime::kWasmAllocateFeedbackVector, 3);
+    __ mov(vector, kReturnRegister0);
+    // Saved parameters are restored at the end of this block.
+  }
+  __ pop(r14);
+
+  __ mov(scratch, Operand(StackFrame::TypeToMarker(StackFrame::WASM)));
+  __ StoreU64(scratch, MemOperand(sp));
+  __ b(&done);
+}
 
-    // Restore registers.
-    __ MultiPopF64OrV128(fp_regs, r1);
-    __ MultiPop(gp_regs);
+void Builtins::Generate_WasmCompileLazy(MacroAssembler* masm) {
+  // The function index was put in a register by the jump table trampoline.
+  // Convert to Smi for the runtime call.
+  __ SmiTag(kWasmCompileLazyFuncIndexRegister);
+
+  {
+    HardAbortScope hard_abort(masm);  // Avoid calls to Abort.
+    FrameAndConstantPoolScope scope(masm, StackFrame::INTERNAL);
+
+    {
+      SaveWasmParamsScope save_params(masm);
+
+      // Push the Wasm instance as an explicit argument to the runtime function.
+      __ push(kWasmInstanceRegister);
+      // Push the function index as second argument.
+      __ push(kWasmCompileLazyFuncIndexRegister);
+      // Initialize the JavaScript context with 0. CEntry will use it to
+      // set the current context on the isolate.
+      __ LoadSmiLiteral(cp, Smi::zero());
+      __ CallRuntime(Runtime::kWasmCompileLazy, 2);
+      // The runtime function returns the jump table slot offset as a Smi. Use
+      // that to compute the jump target in ip.
+      __ SmiUntag(kReturnRegister0);
+      __ mov(ip, kReturnRegister0);
+
+      // Saved parameters are restored at the end of this block.
+    }
 
     // After the instance register has been restored, we can add the jump table
     // start to the jump table offset already stored in r8.
-    __ LoadU64(r0, MemOperand(kWasmInstanceRegister,
-                              WasmInstanceObject::kJumpTableStartOffset -
-                                  kHeapObjectTag));
+    __ LoadU64(r0, FieldMemOperand(kWasmInstanceRegister,
+                                   WasmInstanceObject::kJumpTableStartOffset));
     __ AddS64(ip, ip, r0);
   }
 
diff -r -u --color up/v8/src/builtins/typed-array-createtypedarray.tq nw/v8/src/builtins/typed-array-createtypedarray.tq
--- up/v8/src/builtins/typed-array-createtypedarray.tq	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/typed-array-createtypedarray.tq	2023-01-19 16:46:36.027276288 -0500
@@ -329,11 +329,18 @@
 
   // 2. Perform ? ValidateTypedArray(newTypedArray).
   //    ValidateTypedArray currently returns the array, not the ViewBuffer.
+  const newTypedArrayLength =
+      ValidateTypedArrayAndGetLength(context, newTypedArrayObj, methodName);
   const newTypedArray: JSTypedArray =
-      ValidateTypedArray(context, newTypedArrayObj, methodName);
-  // TODO(v8:11111): bit_field should be initialized to 0.
-  newTypedArray.bit_field.is_length_tracking = false;
-  newTypedArray.bit_field.is_backed_by_rab = false;
+      UnsafeCast<JSTypedArray>(newTypedArrayObj);
+
+  dcheck(
+      newTypedArray.bit_field.is_backed_by_rab ==
+      (IsResizableArrayBuffer(newTypedArray.buffer) &&
+       !IsSharedArrayBuffer(newTypedArray.buffer)));
+  dcheck(
+      !newTypedArray.bit_field.is_length_tracking ||
+      IsResizableArrayBuffer(newTypedArray.buffer));
 
   if (IsDetachedBuffer(newTypedArray.buffer)) deferred {
       ThrowTypeError(MessageTemplate::kDetachedOperation, methodName);
@@ -342,7 +349,7 @@
   // 3. If argumentList is a List of a single Number, then
   //   a. If newTypedArray.[[ArrayLength]] < argumentList[0], throw a
   //      TypeError exception.
-  if (newTypedArray.length < Convert<uintptr>(length)) deferred {
+  if (newTypedArrayLength < Convert<uintptr>(length)) deferred {
       ThrowTypeError(MessageTemplate::kTypedArrayTooShort);
     }
 
diff -r -u --color up/v8/src/builtins/typed-array-from.tq nw/v8/src/builtins/typed-array-from.tq
--- up/v8/src/builtins/typed-array-from.tq	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/typed-array-from.tq	2023-01-19 16:46:36.027276288 -0500
@@ -104,8 +104,9 @@
             finalSource = source;
           }
           case (sourceTypedArray: JSTypedArray): {
-            const sourceBuffer = sourceTypedArray.buffer;
-            if (IsDetachedBuffer(sourceBuffer)) goto UseUserProvidedIterator;
+            finalLength =
+                LoadJSTypedArrayLengthAndCheckDetached(sourceTypedArray)
+                otherwise UseUserProvidedIterator;
 
             // Check that the iterator function is exactly
             // Builtin::kTypedArrayPrototypeValues.
@@ -117,7 +118,6 @@
             // Source is a TypedArray with unmodified iterator behavior. Use the
             // source object directly, taking advantage of the special-case code
             // in TypedArrayCopyElements
-            finalLength = sourceTypedArray.length;
             finalSource = source;
           }
           case (Object): {
diff -r -u --color up/v8/src/builtins/wasm.tq nw/v8/src/builtins/wasm.tq
--- up/v8/src/builtins/wasm.tq	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/wasm.tq	2023-01-19 16:46:36.027276288 -0500
@@ -484,30 +484,22 @@
   return Unsigned(SmiToInt32(result));
 }
 
-builtin WasmI32AtomicWait64(
-    offset: uintptr, expectedValue: int32, timeout: intptr): uint32 {
-  if constexpr (Is64()) {
-    const instance: WasmInstanceObject = LoadInstanceFromFrame();
-    const result: Smi = runtime::WasmI32AtomicWait(
-        LoadContextFromInstance(instance), instance, UintPtr53ToNumber(offset),
-        WasmInt32ToNumber(expectedValue), I64ToBigInt(timeout));
-    return Unsigned(SmiToInt32(result));
-  } else {
-    unreachable;
-  }
+builtin WasmI32AtomicWait(
+    offset: uintptr, expectedValue: int32, timeout: BigInt): uint32 {
+  const instance: WasmInstanceObject = LoadInstanceFromFrame();
+  const result: Smi = runtime::WasmI32AtomicWait(
+      LoadContextFromInstance(instance), instance, UintPtr53ToNumber(offset),
+      WasmInt32ToNumber(expectedValue), timeout);
+  return Unsigned(SmiToInt32(result));
 }
 
-builtin WasmI64AtomicWait64(
-    offset: uintptr, expectedValue: intptr, timeout: intptr): uint32 {
-  if constexpr (Is64()) {
-    const instance: WasmInstanceObject = LoadInstanceFromFrame();
-    const result: Smi = runtime::WasmI64AtomicWait(
-        LoadContextFromInstance(instance), instance, UintPtr53ToNumber(offset),
-        I64ToBigInt(expectedValue), I64ToBigInt(timeout));
-    return Unsigned(SmiToInt32(result));
-  } else {
-    unreachable;
-  }
+builtin WasmI64AtomicWait(
+    offset: uintptr, expectedValue: BigInt, timeout: BigInt): uint32 {
+  const instance: WasmInstanceObject = LoadInstanceFromFrame();
+  const result: Smi = runtime::WasmI64AtomicWait(
+      LoadContextFromInstance(instance), instance, UintPtr53ToNumber(offset),
+      expectedValue, timeout);
+  return Unsigned(SmiToInt32(result));
 }
 
 // Type feedback collection support for `call_ref`.
diff -r -u --color up/v8/src/builtins/x64/builtins-x64.cc nw/v8/src/builtins/x64/builtins-x64.cc
--- up/v8/src/builtins/x64/builtins-x64.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/builtins/x64/builtins-x64.cc	2023-01-19 16:46:36.027276288 -0500
@@ -2909,6 +2909,113 @@
 }
 
 #if V8_ENABLE_WEBASSEMBLY
+
+// Returns the offset beyond the last saved FP register.
+int SaveWasmParams(MacroAssembler* masm) {
+  // Save all parameter registers (see wasm-linkage.h). They might be
+  // overwritten in the subsequent runtime call. We don't have any callee-saved
+  // registers in wasm, so no need to store anything else.
+  static_assert(WasmLiftoffSetupFrameConstants::kNumberOfSavedGpParamRegs + 1 ==
+                    arraysize(wasm::kGpParamRegisters),
+                "frame size mismatch");
+  for (Register reg : wasm::kGpParamRegisters) {
+    __ Push(reg);
+  }
+  static_assert(WasmLiftoffSetupFrameConstants::kNumberOfSavedFpParamRegs ==
+                    arraysize(wasm::kFpParamRegisters),
+                "frame size mismatch");
+  __ AllocateStackSpace(kSimd128Size * arraysize(wasm::kFpParamRegisters));
+  int offset = 0;
+  for (DoubleRegister reg : wasm::kFpParamRegisters) {
+    __ movdqu(Operand(rsp, offset), reg);
+    offset += kSimd128Size;
+  }
+  return offset;
+}
+
+// Consumes the offset beyond the last saved FP register (as returned by
+// {SaveWasmParams}).
+void RestoreWasmParams(MacroAssembler* masm, int offset) {
+  for (DoubleRegister reg : base::Reversed(wasm::kFpParamRegisters)) {
+    offset -= kSimd128Size;
+    __ movdqu(reg, Operand(rsp, offset));
+  }
+  DCHECK_EQ(0, offset);
+  __ addq(rsp, Immediate(kSimd128Size * arraysize(wasm::kFpParamRegisters)));
+  for (Register reg : base::Reversed(wasm::kGpParamRegisters)) {
+    __ Pop(reg);
+  }
+}
+
+// When this builtin is called, the topmost stack entry is the calling pc.
+// This is replaced with the following:
+//
+// [    calling pc     ]  <-- rsp; popped by {ret}.
+// [  feedback vector  ]
+// [   Wasm instance   ]
+// [ WASM frame marker ]
+// [    saved rbp      ]  <-- rbp; this is where "calling pc" used to be.
+void Builtins::Generate_WasmLiftoffFrameSetup(MacroAssembler* masm) {
+  Register func_index = wasm::kLiftoffFrameSetupFunctionReg;
+  Register vector = r15;
+  Register calling_pc = rdi;
+
+  __ Pop(calling_pc);
+  __ Push(rbp);
+  __ Move(rbp, rsp);
+  __ Push(Immediate(StackFrame::TypeToMarker(StackFrame::WASM)));
+  __ LoadTaggedPointerField(
+      vector, FieldOperand(kWasmInstanceRegister,
+                           WasmInstanceObject::kFeedbackVectorsOffset));
+  __ LoadTaggedPointerField(vector,
+                            FieldOperand(vector, func_index, times_tagged_size,
+                                         FixedArray::kHeaderSize));
+  Label allocate_vector, done;
+  __ JumpIfSmi(vector, &allocate_vector);
+  __ bind(&done);
+  __ Push(kWasmInstanceRegister);
+  __ Push(vector);
+  __ Push(calling_pc);
+  __ ret(0);
+
+  __ bind(&allocate_vector);
+  // Feedback vector doesn't exist yet. Call the runtime to allocate it.
+  // We temporarily change the frame type for this, because we need special
+  // handling by the stack walker in case of GC.
+  // For the runtime call, we create the following stack layout:
+  //
+  // [ reserved slot for NativeModule ]  <-- arg[2]
+  // [  ("declared") function index   ]  <-- arg[1] for runtime func.
+  // [         Wasm instance          ]  <-- arg[0]
+  // [ ...spilled Wasm parameters...  ]
+  // [           calling pc           ]
+  // [   WASM_LIFTOFF_SETUP marker    ]
+  // [           saved rbp            ]
+  __ movq(Operand(rbp, TypedFrameConstants::kFrameTypeOffset),
+          Immediate(StackFrame::TypeToMarker(StackFrame::WASM_LIFTOFF_SETUP)));
+  __ set_has_frame(true);
+  __ Push(calling_pc);
+  int offset = SaveWasmParams(masm);
+
+  // Arguments to the runtime function: instance, func_index.
+  __ Push(kWasmInstanceRegister);
+  __ SmiTag(func_index);
+  __ Push(func_index);
+  // Allocate a stack slot where the runtime function can spill a pointer
+  // to the NativeModule.
+  __ Push(rsp);
+  __ Move(kContextRegister, Smi::zero());
+  __ CallRuntime(Runtime::kWasmAllocateFeedbackVector, 3);
+  __ movq(vector, kReturnRegister0);
+
+  RestoreWasmParams(masm, offset);
+  __ Pop(calling_pc);
+  // Restore correct frame type.
+  __ movq(Operand(rbp, TypedFrameConstants::kFrameTypeOffset),
+          Immediate(StackFrame::TypeToMarker(StackFrame::WASM)));
+  __ jmp(&done);
+}
+
 void Builtins::Generate_WasmCompileLazy(MacroAssembler* masm) {
   // The function index was pushed to the stack by the caller as int32.
   __ Pop(r15);
@@ -2917,55 +3024,23 @@
 
   {
     HardAbortScope hard_abort(masm);  // Avoid calls to Abort.
-    FrameScope scope(masm, StackFrame::WASM_COMPILE_LAZY);
+    FrameScope scope(masm, StackFrame::INTERNAL);
 
-    // Save all parameter registers (see wasm-linkage.h). They might be
-    // overwritten in the runtime call below. We don't have any callee-saved
-    // registers in wasm, so no need to store anything else.
-    static_assert(
-        WasmCompileLazyFrameConstants::kNumberOfSavedGpParamRegs + 1 ==
-            arraysize(wasm::kGpParamRegisters),
-        "frame size mismatch");
-    for (Register reg : wasm::kGpParamRegisters) {
-      __ Push(reg);
-    }
-    static_assert(WasmCompileLazyFrameConstants::kNumberOfSavedFpParamRegs ==
-                      arraysize(wasm::kFpParamRegisters),
-                  "frame size mismatch");
-    __ AllocateStackSpace(kSimd128Size * arraysize(wasm::kFpParamRegisters));
-    int offset = 0;
-    for (DoubleRegister reg : wasm::kFpParamRegisters) {
-      __ movdqu(Operand(rsp, offset), reg);
-      offset += kSimd128Size;
-    }
+    int offset = SaveWasmParams(masm);
 
-    // Push the Wasm instance as an explicit argument to the runtime function.
+    // Push arguments for the runtime function.
     __ Push(kWasmInstanceRegister);
-    // Push the function index as second argument.
     __ Push(r15);
-
-    // Allocate a stack slot, where the runtime function can spill a pointer to
-    // the the NativeModule.
-    __ Push(rsp);
     // Initialize the JavaScript context with 0. CEntry will use it to
     // set the current context on the isolate.
     __ Move(kContextRegister, Smi::zero());
-    __ CallRuntime(Runtime::kWasmCompileLazy, 3);
+    __ CallRuntime(Runtime::kWasmCompileLazy, 2);
     // The runtime function returns the jump table slot offset as a Smi. Use
     // that to compute the jump target in r15.
     __ SmiUntagUnsigned(kReturnRegister0);
     __ movq(r15, kReturnRegister0);
 
-    // Restore registers.
-    for (DoubleRegister reg : base::Reversed(wasm::kFpParamRegisters)) {
-      offset -= kSimd128Size;
-      __ movdqu(reg, Operand(rsp, offset));
-    }
-    DCHECK_EQ(0, offset);
-    __ addq(rsp, Immediate(kSimd128Size * arraysize(wasm::kFpParamRegisters)));
-    for (Register reg : base::Reversed(wasm::kGpParamRegisters)) {
-      __ Pop(reg);
-    }
+    RestoreWasmParams(masm, offset);
     // After the instance register has been restored, we can add the jump table
     // start to the jump table offset already stored in r15.
     __ addq(r15, MemOperand(kWasmInstanceRegister,
@@ -3076,7 +3151,7 @@
 void SwitchStackState(MacroAssembler* masm, Register jmpbuf,
                       wasm::JumpBuffer::StackState old_state,
                       wasm::JumpBuffer::StackState new_state) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     __ cmpl(MemOperand(jmpbuf, wasm::kJmpBufStateOffset), Immediate(old_state));
     Label ok;
     __ j(equal, &ok, Label::kNear);
diff -r -u --color up/v8/src/codegen/OWNERS nw/v8/src/codegen/OWNERS
--- up/v8/src/codegen/OWNERS	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/OWNERS	2023-01-19 16:46:36.027276288 -0500
@@ -7,5 +7,6 @@
 leszeks@chromium.org
 mslekova@chromium.org
 nicohartmann@chromium.org
+tebbi@chromium.org
 
 per-file compiler.*=marja@chromium.org
diff -r -u --color up/v8/src/codegen/arm/assembler-arm.cc nw/v8/src/codegen/arm/assembler-arm.cc
--- up/v8/src/codegen/arm/assembler-arm.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/arm/assembler-arm.cc	2023-01-19 16:46:36.027276288 -0500
@@ -1444,10 +1444,6 @@
     L->link_to(pc_offset());
   }
 
-  // Block the emission of the constant pool, since the branch instruction must
-  // be emitted at the pc offset recorded by the label.
-  if (!is_const_pool_blocked()) BlockConstPoolFor(1);
-
   return target_pos - (pc_offset() + Instruction::kPcLoadDelta);
 }
 
@@ -1458,6 +1454,11 @@
   int imm24 = branch_offset >> 2;
   const bool b_imm_check = is_int24(imm24);
   CHECK(b_imm_check);
+
+  // Block the emission of the constant pool before the next instruction.
+  // Otherwise the passed-in branch offset would be off.
+  BlockConstPoolFor(1);
+
   emit(cond | B27 | B25 | (imm24 & kImm24Mask));
 
   if (cond == al) {
@@ -1472,6 +1473,11 @@
   int imm24 = branch_offset >> 2;
   const bool bl_imm_check = is_int24(imm24);
   CHECK(bl_imm_check);
+
+  // Block the emission of the constant pool before the next instruction.
+  // Otherwise the passed-in branch offset would be off.
+  BlockConstPoolFor(1);
+
   emit(cond | B27 | B25 | B24 | (imm24 & kImm24Mask));
 }
 
@@ -1481,6 +1487,11 @@
   int imm24 = branch_offset >> 2;
   const bool blx_imm_check = is_int24(imm24);
   CHECK(blx_imm_check);
+
+  // Block the emission of the constant pool before the next instruction.
+  // Otherwise the passed-in branch offset would be off.
+  BlockConstPoolFor(1);
+
   emit(kSpecialCondition | B27 | B25 | h | (imm24 & kImm24Mask));
 }
 
diff -r -u --color up/v8/src/codegen/arm/macro-assembler-arm.cc nw/v8/src/codegen/arm/macro-assembler-arm.cc
--- up/v8/src/codegen/arm/macro-assembler-arm.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/arm/macro-assembler-arm.cc	2023-01-19 16:46:36.027276288 -0500
@@ -1635,13 +1635,6 @@
   DCHECK_EQ(actual_parameter_count, r0);
   DCHECK_EQ(expected_parameter_count, r2);
 
-  // If the expected parameter count is equal to the adaptor sentinel, no need
-  // to push undefined value as arguments.
-  if (kDontAdaptArgumentsSentinel != 0) {
-    cmp(expected_parameter_count, Operand(kDontAdaptArgumentsSentinel));
-    b(eq, &regular_invoke);
-  }
-
   // If overapplication or if the actual argument count is equal to the
   // formal parameter count, no need to push extra undefined values.
   sub(expected_parameter_count, expected_parameter_count,
diff -r -u --color up/v8/src/codegen/arm64/macro-assembler-arm64.cc nw/v8/src/codegen/arm64/macro-assembler-arm64.cc
--- up/v8/src/codegen/arm64/macro-assembler-arm64.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/arm64/macro-assembler-arm64.cc	2023-01-19 16:46:36.038109619 -0500
@@ -2348,12 +2348,10 @@
     Register destination, Register code_data_container_object) {
   ASM_CODE_COMMENT(this);
   CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
-  // Given the fields layout we can read the Code reference as a full word.
-  static_assert(!V8_EXTERNAL_CODE_SPACE_BOOL ||
-                (CodeDataContainer::kCodeCageBaseUpper32BitsOffset ==
-                 CodeDataContainer::kCodeOffset + kTaggedSize));
+  // Compute the Code object pointer from the code entry point.
   Ldr(destination, FieldMemOperand(code_data_container_object,
-                                   CodeDataContainer::kCodeOffset));
+                                   CodeDataContainer::kCodeEntryPointOffset));
+  Sub(destination, destination, Immediate(Code::kHeaderSize - kHeapObjectTag));
 }
 
 void TurboAssembler::CallCodeDataContainerObject(
@@ -2506,13 +2504,6 @@
   DCHECK_EQ(actual_argument_count, x0);
   DCHECK_EQ(formal_parameter_count, x2);
 
-  // If the formal parameter count is equal to the adaptor sentinel, no need
-  // to push undefined value as arguments.
-  if (kDontAdaptArgumentsSentinel != 0) {
-    Cmp(formal_parameter_count, Operand(kDontAdaptArgumentsSentinel));
-    B(eq, &regular_invoke);
-  }
-
   // If overapplication or if the actual argument count is equal to the
   // formal parameter count, no need to push extra undefined values.
   Register extra_argument_count = x2;
@@ -2860,8 +2851,8 @@
         fourth_reg = cp;
 #if V8_ENABLE_WEBASSEMBLY
       } else if (type == StackFrame::WASM ||
-                type == StackFrame::WASM_COMPILE_LAZY ||
-                type == StackFrame::WASM_EXIT) {
+                 type == StackFrame::WASM_LIFTOFF_SETUP ||
+                 type == StackFrame::WASM_EXIT) {
         fourth_reg = kWasmInstanceRegister;
 #endif  // V8_ENABLE_WEBASSEMBLY
       } else {
diff -r -u --color up/v8/src/codegen/background-merge-task.h nw/v8/src/codegen/background-merge-task.h
--- up/v8/src/codegen/background-merge-task.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/background-merge-task.h	2023-01-19 16:46:36.038109619 -0500
@@ -73,13 +73,13 @@
   std::vector<Handle<SharedFunctionInfo>> used_new_sfis_;
 
   // SharedFunctionInfos from the cached script which were not compiled, with
-  // function_data and feedback_metadata from the corresponding new
-  // SharedFunctionInfo. If the SharedFunctionInfo from the cached script is
-  // still uncompiled when finishing, the main thread must set the two fields.
+  // the corresponding new SharedFunctionInfo. If the SharedFunctionInfo from
+  // the cached script is still uncompiled when finishing, the main thread must
+  // copy all fields from the new SharedFunctionInfo to the SharedFunctionInfo
+  // from the cached script.
   struct NewCompiledDataForCachedSfi {
     Handle<SharedFunctionInfo> cached_sfi;
-    Handle<Object> function_data;
-    Handle<FeedbackMetadata> feedback_metadata;
+    Handle<SharedFunctionInfo> new_sfi;
   };
   std::vector<NewCompiledDataForCachedSfi> new_compiled_data_for_cached_sfis_;
 
diff -r -u --color up/v8/src/codegen/code-stub-assembler.cc nw/v8/src/codegen/code-stub-assembler.cc
--- up/v8/src/codegen/code-stub-assembler.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/code-stub-assembler.cc	2023-01-19 16:46:36.038109619 -0500
@@ -4,6 +4,8 @@
 
 #include "src/codegen/code-stub-assembler.h"
 
+#include <stdio.h>
+
 #include <functional>
 
 #include "include/v8-internal.h"
@@ -845,6 +847,20 @@
   return Projection<0>(pair);
 }
 
+TNode<IntPtrT> CodeStubAssembler::TryIntPtrDiv(TNode<IntPtrT> a,
+                                               TNode<IntPtrT> b,
+                                               Label* if_div_zero) {
+  GotoIf(IntPtrEqual(b, IntPtrConstant(0)), if_div_zero);
+  return IntPtrDiv(a, b);
+}
+
+TNode<IntPtrT> CodeStubAssembler::TryIntPtrMod(TNode<IntPtrT> a,
+                                               TNode<IntPtrT> b,
+                                               Label* if_div_zero) {
+  GotoIf(IntPtrEqual(b, IntPtrConstant(0)), if_div_zero);
+  return IntPtrMod(a, b);
+}
+
 TNode<Int32T> CodeStubAssembler::TryInt32Mul(TNode<Int32T> a, TNode<Int32T> b,
                                              Label* if_overflow) {
   TNode<PairT<Int32T, BoolT>> pair = Int32MulWithOverflow(a, b);
@@ -3025,7 +3041,8 @@
                           Int32Constant(static_cast<int>(CodeKind::BASELINE))));
 #endif  // DEBUG
     TNode<HeapObject> baseline_data = LoadObjectField<HeapObject>(
-        FromCodeT(code), Code::kDeoptimizationDataOrInterpreterDataOffset);
+        FromCodeTNonBuiltin(code),
+        Code::kDeoptimizationDataOrInterpreterDataOffset);
     var_result = baseline_data;
   }
   Goto(&check_for_interpreter_data);
@@ -3090,9 +3107,14 @@
                                           object, offset, value);
 }
 
-void CodeStubAssembler::StoreJSSharedStructInObjectField(
-    TNode<HeapObject> object, TNode<IntPtrT> offset, TNode<Object> value) {
-  CSA_DCHECK(this, IsJSSharedStruct(object));
+void CodeStubAssembler::StoreSharedObjectField(TNode<HeapObject> object,
+                                               TNode<IntPtrT> offset,
+                                               TNode<Object> value) {
+  CSA_DCHECK(
+      this,
+      WordNotEqual(WordAnd(LoadBasicMemoryChunkFlags(object),
+                           IntPtrConstant(BasicMemoryChunk::IN_SHARED_HEAP)),
+                   IntPtrConstant(0)));
   // JSSharedStructs are allocated in the shared old space, which is currently
   // collected by stopping the world, so the incremental write barrier is not
   // needed. They can only store Smis and other HeapObjects in the shared old
@@ -15292,26 +15314,46 @@
 }
 
 void CodeStubAssembler::Print(const char* s) {
+  PrintToStream(s, fileno(stdout));
+}
+
+void CodeStubAssembler::PrintErr(const char* s) {
+  PrintToStream(s, fileno(stderr));
+}
+
+void CodeStubAssembler::PrintToStream(const char* s, int stream) {
   std::string formatted(s);
   formatted += "\n";
   CallRuntime(Runtime::kGlobalPrint, NoContextConstant(),
-              StringConstant(formatted.c_str()));
+              StringConstant(formatted.c_str()), SmiConstant(stream));
 }
 
 void CodeStubAssembler::Print(const char* prefix,
                               TNode<MaybeObject> tagged_value) {
+  PrintToStream(prefix, tagged_value, fileno(stdout));
+}
+
+void CodeStubAssembler::PrintErr(const char* prefix,
+                                 TNode<MaybeObject> tagged_value) {
+  PrintToStream(prefix, tagged_value, fileno(stderr));
+}
+
+void CodeStubAssembler::PrintToStream(const char* prefix,
+                                      TNode<MaybeObject> tagged_value,
+                                      int stream) {
   if (prefix != nullptr) {
     std::string formatted(prefix);
     formatted += ": ";
     Handle<String> string = isolate()->factory()->NewStringFromAsciiChecked(
         formatted.c_str(), AllocationType::kOld);
     CallRuntime(Runtime::kGlobalPrint, NoContextConstant(),
-                HeapConstant(string));
+                HeapConstant(string), SmiConstant(stream));
   }
   // CallRuntime only accepts Objects, so do an UncheckedCast to object.
   // DebugPrint explicitly checks whether the tagged value is a MaybeObject.
   TNode<Object> arg = UncheckedCast<Object>(tagged_value);
-  CallRuntime(Runtime::kDebugPrint, NoContextConstant(), arg);
+  CallRuntime(Runtime::kDebugPrint, NoContextConstant(), arg,
+              SmiConstant(stream));
 }
 
 IntegerLiteral CodeStubAssembler::ConstexprIntegerLiteralAdd(
diff -r -u --color up/v8/src/codegen/code-stub-assembler.h nw/v8/src/codegen/code-stub-assembler.h
--- up/v8/src/codegen/code-stub-assembler.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/code-stub-assembler.h	2023-01-19 16:46:36.038109619 -0500
@@ -630,6 +630,10 @@
                               Label* if_overflow);
   TNode<IntPtrT> TryIntPtrMul(TNode<IntPtrT> a, TNode<IntPtrT> b,
                               Label* if_overflow);
+  TNode<IntPtrT> TryIntPtrDiv(TNode<IntPtrT> a, TNode<IntPtrT> b,
+                              Label* if_div_zero);
+  TNode<IntPtrT> TryIntPtrMod(TNode<IntPtrT> a, TNode<IntPtrT> b,
+                              Label* if_div_zero);
   TNode<Int32T> TryInt32Mul(TNode<Int32T> a, TNode<Int32T> b,
                             Label* if_overflow);
   TNode<Smi> TrySmiAdd(TNode<Smi> a, TNode<Smi> b, Label* if_overflow);
@@ -838,16 +842,14 @@
 
   // TODO(v8:11880): remove once Code::bytecode_or_interpreter_data field
   // is cached in or moved to CodeT.
-  TNode<Code> FromCodeT(TNode<CodeT> code) {
+  TNode<Code> FromCodeTNonBuiltin(TNode<CodeT> code) {
 #ifdef V8_EXTERNAL_CODE_SPACE
-#if V8_TARGET_BIG_ENDIAN
-#error "This code requires updating for big-endian architectures"
-#endif
-    // Given the fields layout we can read the Code reference as a full word.
-    static_assert(CodeDataContainer::kCodeCageBaseUpper32BitsOffset ==
-                  CodeDataContainer::kCodeOffset + kTaggedSize);
-    TNode<Object> o = BitcastWordToTagged(Load<RawPtrT>(
-        code, IntPtrConstant(CodeDataContainer::kCodeOffset - kHeapObjectTag)));
+    // Compute the Code object pointer from the code entry point.
+    TNode<RawPtrT> code_entry = Load<RawPtrT>(
+        code, IntPtrConstant(CodeDataContainer::kCodeEntryPointOffset -
+                             kHeapObjectTag));
+    TNode<Object> o = BitcastWordToTagged(IntPtrSub(
+        code_entry, IntPtrConstant(Code::kHeaderSize - kHeapObjectTag)));
     return CAST(o);
 #else
     return code;
@@ -1856,9 +1858,8 @@
       WriteBarrierMode barrier_mode = UPDATE_WRITE_BARRIER,
       int additional_offset = 0);
 
-  void StoreJSSharedStructInObjectField(TNode<HeapObject> object,
-                                        TNode<IntPtrT> offset,
-                                        TNode<Object> value);
+  void StoreSharedObjectField(TNode<HeapObject> object, TNode<IntPtrT> offset,
+                              TNode<Object> value);
 
   void StoreJSSharedStructPropertyArrayElement(TNode<PropertyArray> array,
                                                TNode<IntPtrT> index,
@@ -3931,6 +3932,14 @@
   void Print(TNode<MaybeObject> tagged_value) {
     return Print(nullptr, tagged_value);
   }
+  void PrintErr(const char* s);
+  void PrintErr(const char* prefix, TNode<MaybeObject> tagged_value);
+  void PrintErr(TNode<MaybeObject> tagged_value) {
+    return PrintErr(nullptr, tagged_value);
+  }
+  void PrintToStream(const char* s, int stream);
+  void PrintToStream(const char* prefix, TNode<MaybeObject> tagged_value,
+                     int stream);
 
   template <class... TArgs>
   TNode<HeapObject> MakeTypeError(MessageTemplate message,
diff -r -u --color up/v8/src/codegen/compiler.cc nw/v8/src/codegen/compiler.cc
--- up/v8/src/codegen/compiler.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/compiler.cc	2023-01-19 16:46:36.048942949 -0500
@@ -1284,12 +1284,6 @@
   // turbo_filter.
   if (!ShouldOptimize(code_kind, shared)) return {};
 
-  // If code was pending optimization for testing, remove the entry from the
-  // table that was preventing the bytecode from being flushed.
-  if (V8_UNLIKELY(v8_flags.testing_d8_test_runner)) {
-    PendingOptimizationTable::FunctionWasOptimized(isolate, function);
-  }
-
   Handle<CodeT> cached_code;
   if (OptimizedCodeCache::Get(isolate, function, osr_offset, code_kind)
           .ToHandle(&cached_code)) {
@@ -2045,12 +2039,9 @@
             old_sfi.GetBytecodeArray(isolate).set_bytecode_age(0);
           } else {
             // The old SFI can use the compiled data from the new SFI.
-            Object function_data = new_sfi.function_data(kAcquireLoad);
-            FeedbackMetadata feedback_metadata = new_sfi.feedback_metadata();
             new_compiled_data_for_cached_sfis_.push_back(
                 {local_heap->NewPersistentHandle(old_sfi),
-                 local_heap->NewPersistentHandle(function_data),
-                 local_heap->NewPersistentHandle(feedback_metadata)});
+                 local_heap->NewPersistentHandle(new_sfi)});
             forwarder.AddBytecodeArray(new_sfi.GetBytecodeArray(isolate));
           }
         }
@@ -2087,11 +2078,19 @@
   Handle<Script> old_script = cached_script_.ToHandleChecked();
 
   for (const auto& new_compiled_data : new_compiled_data_for_cached_sfis_) {
-    if (!new_compiled_data.cached_sfi->is_compiled()) {
-      new_compiled_data.cached_sfi->set_function_data(
-          *new_compiled_data.function_data, kReleaseStore);
-      new_compiled_data.cached_sfi->set_feedback_metadata(
-          *new_compiled_data.feedback_metadata, kReleaseStore);
+    if (!new_compiled_data.cached_sfi->is_compiled() &&
+        new_compiled_data.new_sfi->is_compiled()) {
+      // Updating existing DebugInfos is not supported, but we don't expect
+      // uncompiled SharedFunctionInfos to contain DebugInfos.
+      DCHECK(!new_compiled_data.cached_sfi->HasDebugInfo());
+      // The goal here is to copy every field except script_or_debug_info from
+      // new_sfi to cached_sfi. The safest way to do so (including a DCHECK that
+      // no fields were skipped) is to first copy the script_or_debug_info from
+      // cached_sfi to new_sfi, and then copy every field using CopyFrom.
+      new_compiled_data.new_sfi->set_script_or_debug_info(
+          new_compiled_data.cached_sfi->script_or_debug_info(kAcquireLoad),
+          kReleaseStore);
+      new_compiled_data.cached_sfi->CopyFrom(*new_compiled_data.new_sfi);
     }
   }
   for (Handle<SharedFunctionInfo> new_sfi : used_new_sfis_) {
@@ -3497,7 +3496,9 @@
     maybe_script = lookup_result.script();
     maybe_result = lookup_result.toplevel_sfi();
     is_compiled_scope = lookup_result.is_compiled_scope();
-    if (!maybe_result.is_null()) {
+    //NWJS#5168: will hit previous cache, use 0 source_length trick to
+    //bypass and try to consume cache
+    if (!maybe_result.is_null() && source->length()) {
       compile_timer.set_hit_isolate_cache();
     } else if (can_consume_code_cache) {
       compile_timer.set_consuming_code_cache();
diff -r -u --color up/v8/src/codegen/external-reference.cc nw/v8/src/codegen/external-reference.cc
--- up/v8/src/codegen/external-reference.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/external-reference.cc	2023-01-19 16:46:36.048942949 -0500
@@ -25,6 +25,7 @@
 #include "src/logging/log.h"
 #include "src/numbers/hash-seed-inl.h"
 #include "src/numbers/math-random.h"
+#include "src/objects/elements-kind.h"
 #include "src/objects/elements.h"
 #include "src/objects/object-type.h"
 #include "src/objects/objects-inl.h"
@@ -454,8 +455,8 @@
 IF_WASM(FUNCTION_REFERENCE, wasm_call_trap_callback_for_testing,
         wasm::call_trap_callback_for_testing)
 IF_WASM(FUNCTION_REFERENCE, wasm_array_copy, wasm::array_copy_wrapper)
-IF_WASM(FUNCTION_REFERENCE, wasm_array_fill_with_zeroes,
-        wasm::array_fill_with_zeroes_wrapper)
+IF_WASM(FUNCTION_REFERENCE, wasm_array_fill_with_number_or_null,
+        wasm::array_fill_with_number_or_null_wrapper)
 
 static void f64_acos_wrapper(Address data) {
   double input = ReadUnalignedValue<double>(data);
@@ -587,7 +588,7 @@
 
 ExternalReference
 ExternalReference::address_of_FLAG_harmony_symbol_as_weakmap_key() {
-  return ExternalReference(&FLAG_harmony_symbol_as_weakmap_key);
+  return ExternalReference(&v8_flags.harmony_symbol_as_weakmap_key);
 }
 
 ExternalReference ExternalReference::address_of_builtin_subclassing_flag() {
@@ -950,6 +951,20 @@
   return search_string_raw<const base::uc16, const base::uc16>();
 }
 
+ExternalReference
+ExternalReference::typed_array_and_rab_gsab_typed_array_elements_kind_shifts() {
+  uint8_t* ptr =
+      const_cast<uint8_t*>(TypedArrayAndRabGsabTypedArrayElementsKindShifts());
+  return ExternalReference(reinterpret_cast<Address>(ptr));
+}
+
+ExternalReference
+ExternalReference::typed_array_and_rab_gsab_typed_array_elements_kind_sizes() {
+  uint8_t* ptr =
+      const_cast<uint8_t*>(TypedArrayAndRabGsabTypedArrayElementsKindSizes());
+  return ExternalReference(reinterpret_cast<Address>(ptr));
+}
+
 namespace {
 
 void StringWriteToFlatOneByte(Address source, uint8_t* sink, int32_t start,
@@ -1103,6 +1118,9 @@
 FUNCTION_REFERENCE(mutable_big_int_absolute_div_and_canonicalize_function,
                    MutableBigInt_AbsoluteDivAndCanonicalize)
 
+FUNCTION_REFERENCE(mutable_big_int_absolute_mod_and_canonicalize_function,
+                   MutableBigInt_AbsoluteModAndCanonicalize)
+
 FUNCTION_REFERENCE(mutable_big_int_bitwise_and_pp_and_canonicalize_function,
                    MutableBigInt_BitwiseAndPosPosAndCanonicalize)
 
diff -r -u --color up/v8/src/codegen/external-reference.h nw/v8/src/codegen/external-reference.h
--- up/v8/src/codegen/external-reference.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/external-reference.h	2023-01-19 16:46:36.048942949 -0500
@@ -184,6 +184,8 @@
     "MutableBigInt_AbsoluteMulAndCanonicalize")                                \
   V(mutable_big_int_absolute_div_and_canonicalize_function,                    \
     "MutableBigInt_AbsoluteDivAndCanonicalize")                                \
+  V(mutable_big_int_absolute_mod_and_canonicalize_function,                    \
+    "MutableBigInt_AbsoluteModAndCanonicalize")                                \
   V(mutable_big_int_bitwise_and_pp_and_canonicalize_function,                  \
     "MutableBigInt_BitwiseAndPosPosAndCanonicalize")                           \
   V(mutable_big_int_bitwise_and_nn_and_canonicalize_function,                  \
@@ -272,7 +274,8 @@
   IF_WASM(V, wasm_memory_copy, "wasm::memory_copy")                            \
   IF_WASM(V, wasm_memory_fill, "wasm::memory_fill")                            \
   IF_WASM(V, wasm_array_copy, "wasm::array_copy")                              \
-  IF_WASM(V, wasm_array_fill_with_zeroes, "wasm::array_fill_with_zeroes")      \
+  IF_WASM(V, wasm_array_fill_with_number_or_null,                              \
+          "wasm::array_fill_with_number_or_null")                              \
   V(address_of_wasm_i8x16_swizzle_mask, "wasm_i8x16_swizzle_mask")             \
   V(address_of_wasm_i8x16_popcnt_mask, "wasm_i8x16_popcnt_mask")               \
   V(address_of_wasm_i8x16_splat_0x01, "wasm_i8x16_splat_0x01")                 \
@@ -337,6 +340,10 @@
   V(re_match_for_call_from_js, "IrregexpInterpreter::MatchForCallFromJs")      \
   V(re_experimental_match_for_call_from_js,                                    \
     "ExperimentalRegExp::MatchForCallFromJs")                                  \
+  V(typed_array_and_rab_gsab_typed_array_elements_kind_shifts,                 \
+    "TypedArrayAndRabGsabTypedArrayElementsKindShifts")                        \
+  V(typed_array_and_rab_gsab_typed_array_elements_kind_sizes,                  \
+    "TypedArrayAndRabGsabTypedArrayElementsKindSizes")                         \
   EXTERNAL_REFERENCE_LIST_INTL(V)                                              \
   EXTERNAL_REFERENCE_LIST_SANDBOX(V)
 #ifdef V8_INTL_SUPPORT
diff -r -u --color up/v8/src/codegen/ia32/macro-assembler-ia32.cc nw/v8/src/codegen/ia32/macro-assembler-ia32.cc
--- up/v8/src/codegen/ia32/macro-assembler-ia32.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/ia32/macro-assembler-ia32.cc	2023-01-19 16:46:36.048942949 -0500
@@ -1399,13 +1399,6 @@
   DCHECK_EQ(expected_parameter_count, ecx);
   Label regular_invoke;
 
-  // If the expected parameter count is equal to the adaptor sentinel, no need
-  // to push undefined value as arguments.
-  if (kDontAdaptArgumentsSentinel != 0) {
-    cmp(expected_parameter_count, Immediate(kDontAdaptArgumentsSentinel));
-    j(equal, &regular_invoke, Label::kFar);
-  }
-
   // If overapplication or if the actual argument count is equal to the
   // formal parameter count, no need to push extra undefined values.
   sub(expected_parameter_count, actual_parameter_count);
diff -r -u --color up/v8/src/codegen/interface-descriptors-inl.h nw/v8/src/codegen/interface-descriptors-inl.h
--- up/v8/src/codegen/interface-descriptors-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/interface-descriptors-inl.h	2023-01-19 16:46:36.048942949 -0500
@@ -189,6 +189,14 @@
 
 // static
 template <typename DerivedDescriptor>
+constexpr int
+StaticCallInterfaceDescriptor<DerivedDescriptor>::GetStackParameterIndex(
+    int i) {
+  return i - DerivedDescriptor::GetRegisterParameterCount();
+}
+
+// static
+template <typename DerivedDescriptor>
 constexpr DoubleRegister
 StaticCallInterfaceDescriptor<DerivedDescriptor>::GetDoubleRegisterParameter(
     int i) {
diff -r -u --color up/v8/src/codegen/interface-descriptors.h nw/v8/src/codegen/interface-descriptors.h
--- up/v8/src/codegen/interface-descriptors.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/interface-descriptors.h	2023-01-19 16:46:36.048942949 -0500
@@ -130,8 +130,6 @@
   V(Void)                                            \
   V(WasmFloat32ToNumber)                             \
   V(WasmFloat64ToNumber)                             \
-  V(WasmI32AtomicWait32)                             \
-  V(WasmI64AtomicWait32)                             \
   V(WasmSuspend)                                     \
   V(WriteBarrier)                                    \
   IF_TSAN(V, TSANLoad)                               \
@@ -482,6 +480,7 @@
   static constexpr inline int GetStackParameterCount();
   static constexpr inline Register* GetRegisterData();
   static constexpr inline Register GetRegisterParameter(int i);
+  static constexpr inline int GetStackParameterIndex(int i);
 
   // Interface descriptors don't really support double registers.
   // This reinterprets the i-th register as a double with the same code.
@@ -1982,38 +1981,6 @@
   DECLARE_DESCRIPTOR(BigIntToI32PairDescriptor)
 };
 
-class WasmI32AtomicWait32Descriptor final
-    : public StaticCallInterfaceDescriptor<WasmI32AtomicWait32Descriptor> {
- public:
-  DEFINE_PARAMETERS_NO_CONTEXT(kAddress, kExpectedValue, kTimeoutLow,
-                               kTimeoutHigh)
-  DEFINE_RESULT_AND_PARAMETER_TYPES(MachineType::Uint32(),  // result 1
-                                    MachineType::Uint32(),  // kAddress
-                                    MachineType::Int32(),   // kExpectedValue
-                                    MachineType::Uint32(),  // kTimeoutLow
-                                    MachineType::Uint32())  // kTimeoutHigh
-  DECLARE_DESCRIPTOR(WasmI32AtomicWait32Descriptor)
-};
-
-class WasmI64AtomicWait32Descriptor final
-    : public StaticCallInterfaceDescriptor<WasmI64AtomicWait32Descriptor> {
- public:
-  DEFINE_PARAMETERS_NO_CONTEXT(kAddress, kExpectedValueLow, kExpectedValueHigh,
-                               kTimeoutLow, kTimeoutHigh)
-
-  static constexpr bool kNoStackScan = true;
-
-  DEFINE_RESULT_AND_PARAMETER_TYPES(
-      MachineType::Uint32(),  // result 1
-      MachineType::Uint32(),  // kAddress
-      MachineType::Uint32(),  // kExpectedValueLow
-      MachineType::Uint32(),  // kExpectedValueHigh
-      MachineType::Uint32(),  // kTimeoutLow
-      MachineType::Uint32())  // kTimeoutHigh
-
-  DECLARE_DESCRIPTOR(WasmI64AtomicWait32Descriptor)
-};
-
 class CloneObjectWithVectorDescriptor final
     : public StaticCallInterfaceDescriptor<CloneObjectWithVectorDescriptor> {
  public:
diff -r -u --color up/v8/src/codegen/loong64/constants-loong64.h nw/v8/src/codegen/loong64/constants-loong64.h
--- up/v8/src/codegen/loong64/constants-loong64.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/loong64/constants-loong64.h	2023-01-19 16:46:36.048942949 -0500
@@ -693,7 +693,8 @@
 // registers and other constants.
 
 // Break 0xfffff, reserved for redirected real time call.
-const Instr rtCallRedirInstr = BREAK | call_rt_redirected;
+const Instr rtCallRedirInstr =
+    static_cast<uint32_t>(BREAK) | call_rt_redirected;
 // A nop instruction. (Encoding of addi_w 0 0 0).
 const Instr nopInstr = ADDI_W;
 
diff -r -u --color up/v8/src/codegen/loong64/macro-assembler-loong64.cc nw/v8/src/codegen/loong64/macro-assembler-loong64.cc
--- up/v8/src/codegen/loong64/macro-assembler-loong64.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/loong64/macro-assembler-loong64.cc	2023-01-19 16:46:36.048942949 -0500
@@ -3033,13 +3033,6 @@
   DCHECK_EQ(actual_parameter_count, a0);
   DCHECK_EQ(expected_parameter_count, a2);
 
-  // If the expected parameter count is equal to the adaptor sentinel, no need
-  // to push undefined value as arguments.
-  if (kDontAdaptArgumentsSentinel != 0) {
-    Branch(&regular_invoke, eq, expected_parameter_count,
-           Operand(kDontAdaptArgumentsSentinel));
-  }
-
   // If overapplication or if the actual argument count is equal to the
   // formal parameter count, no need to push extra undefined values.
   sub_d(expected_parameter_count, expected_parameter_count,
@@ -3551,7 +3544,8 @@
     Push(kScratchReg);
   }
 #if V8_ENABLE_WEBASSEMBLY
-  if (type == StackFrame::WASM) Push(kWasmInstanceRegister);
+  if (type == StackFrame::WASM || type == StackFrame::WASM_LIFTOFF_SETUP)
+    Push(kWasmInstanceRegister);
 #endif  // V8_ENABLE_WEBASSEMBLY
 }
 
diff -r -u --color up/v8/src/codegen/mips64/constants-mips64.h nw/v8/src/codegen/mips64/constants-mips64.h
--- up/v8/src/codegen/mips64/constants-mips64.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/mips64/constants-mips64.h	2023-01-19 16:46:36.048942949 -0500
@@ -418,88 +418,88 @@
 // ----- MIPS Opcodes and Function Fields.
 // We use this presentation to stay close to the table representation in
 // MIPS32 Architecture For Programmers, Volume II: The MIPS32 Instruction Set.
-enum Opcode : uint32_t {
-  SPECIAL = 0U << kOpcodeShift,
-  REGIMM = 1U << kOpcodeShift,
-
-  J = ((0U << 3) + 2) << kOpcodeShift,
-  JAL = ((0U << 3) + 3) << kOpcodeShift,
-  BEQ = ((0U << 3) + 4) << kOpcodeShift,
-  BNE = ((0U << 3) + 5) << kOpcodeShift,
-  BLEZ = ((0U << 3) + 6) << kOpcodeShift,
-  BGTZ = ((0U << 3) + 7) << kOpcodeShift,
-
-  ADDI = ((1U << 3) + 0) << kOpcodeShift,
-  ADDIU = ((1U << 3) + 1) << kOpcodeShift,
-  SLTI = ((1U << 3) + 2) << kOpcodeShift,
-  SLTIU = ((1U << 3) + 3) << kOpcodeShift,
-  ANDI = ((1U << 3) + 4) << kOpcodeShift,
-  ORI = ((1U << 3) + 5) << kOpcodeShift,
-  XORI = ((1U << 3) + 6) << kOpcodeShift,
-  LUI = ((1U << 3) + 7) << kOpcodeShift,  // LUI/AUI family.
-  DAUI = ((3U << 3) + 5) << kOpcodeShift,
-
-  BEQC = ((2U << 3) + 0) << kOpcodeShift,
-  COP1 = ((2U << 3) + 1) << kOpcodeShift,  // Coprocessor 1 class.
-  BEQL = ((2U << 3) + 4) << kOpcodeShift,
-  BNEL = ((2U << 3) + 5) << kOpcodeShift,
-  BLEZL = ((2U << 3) + 6) << kOpcodeShift,
-  BGTZL = ((2U << 3) + 7) << kOpcodeShift,
-
-  DADDI = ((3U << 3) + 0) << kOpcodeShift,  // This is also BNEC.
-  DADDIU = ((3U << 3) + 1) << kOpcodeShift,
-  LDL = ((3U << 3) + 2) << kOpcodeShift,
-  LDR = ((3U << 3) + 3) << kOpcodeShift,
-  SPECIAL2 = ((3U << 3) + 4) << kOpcodeShift,
-  MSA = ((3U << 3) + 6) << kOpcodeShift,
-  SPECIAL3 = ((3U << 3) + 7) << kOpcodeShift,
-
-  LB = ((4U << 3) + 0) << kOpcodeShift,
-  LH = ((4U << 3) + 1) << kOpcodeShift,
-  LWL = ((4U << 3) + 2) << kOpcodeShift,
-  LW = ((4U << 3) + 3) << kOpcodeShift,
-  LBU = ((4U << 3) + 4) << kOpcodeShift,
-  LHU = ((4U << 3) + 5) << kOpcodeShift,
-  LWR = ((4U << 3) + 6) << kOpcodeShift,
-  LWU = ((4U << 3) + 7) << kOpcodeShift,
-
-  SB = ((5U << 3) + 0) << kOpcodeShift,
-  SH = ((5U << 3) + 1) << kOpcodeShift,
-  SWL = ((5U << 3) + 2) << kOpcodeShift,
-  SW = ((5U << 3) + 3) << kOpcodeShift,
-  SDL = ((5U << 3) + 4) << kOpcodeShift,
-  SDR = ((5U << 3) + 5) << kOpcodeShift,
-  SWR = ((5U << 3) + 6) << kOpcodeShift,
-
-  LL = ((6U << 3) + 0) << kOpcodeShift,
-  LWC1 = ((6U << 3) + 1) << kOpcodeShift,
-  BC = ((6U << 3) + 2) << kOpcodeShift,
-  LLD = ((6U << 3) + 4) << kOpcodeShift,
-  LDC1 = ((6U << 3) + 5) << kOpcodeShift,
-  POP66 = ((6U << 3) + 6) << kOpcodeShift,
-  LD = ((6U << 3) + 7) << kOpcodeShift,
-
-  PREF = ((6U << 3) + 3) << kOpcodeShift,
-
-  SC = ((7U << 3) + 0) << kOpcodeShift,
-  SWC1 = ((7U << 3) + 1) << kOpcodeShift,
-  BALC = ((7U << 3) + 2) << kOpcodeShift,
-  PCREL = ((7U << 3) + 3) << kOpcodeShift,
-  SCD = ((7U << 3) + 4) << kOpcodeShift,
-  SDC1 = ((7U << 3) + 5) << kOpcodeShift,
-  POP76 = ((7U << 3) + 6) << kOpcodeShift,
-  SD = ((7U << 3) + 7) << kOpcodeShift,
-
-  COP1X = ((1U << 4) + 3) << kOpcodeShift,
-
-  // New r6 instruction.
-  POP06 = BLEZ,   // bgeuc/bleuc, blezalc, bgezalc
-  POP07 = BGTZ,   // bltuc/bgtuc, bgtzalc, bltzalc
-  POP10 = ADDI,   // beqzalc, bovc, beqc
-  POP26 = BLEZL,  // bgezc, blezc, bgec/blec
-  POP27 = BGTZL,  // bgtzc, bltzc, bltc/bgtc
-  POP30 = DADDI,  // bnezalc, bnvc, bnec
-};
+using Opcode = uint32_t;
+constexpr Opcode SPECIAL = 0U << kOpcodeShift;
+constexpr Opcode REGIMM = 1U << kOpcodeShift;
+
+constexpr Opcode J = ((0U << 3) + 2) << kOpcodeShift;
+constexpr Opcode JAL = ((0U << 3) + 3) << kOpcodeShift;
+constexpr Opcode BEQ = ((0U << 3) + 4) << kOpcodeShift;
+constexpr Opcode BNE = ((0U << 3) + 5) << kOpcodeShift;
+constexpr Opcode BLEZ = ((0U << 3) + 6) << kOpcodeShift;
+constexpr Opcode BGTZ = ((0U << 3) + 7) << kOpcodeShift;
+
+constexpr Opcode ADDI = ((1U << 3) + 0) << kOpcodeShift;
+constexpr Opcode ADDIU = ((1U << 3) + 1) << kOpcodeShift;
+constexpr Opcode SLTI = ((1U << 3) + 2) << kOpcodeShift;
+constexpr Opcode SLTIU = ((1U << 3) + 3) << kOpcodeShift;
+constexpr Opcode ANDI = ((1U << 3) + 4) << kOpcodeShift;
+constexpr Opcode ORI = ((1U << 3) + 5) << kOpcodeShift;
+constexpr Opcode XORI = ((1U << 3) + 6) << kOpcodeShift;
+constexpr Opcode LUI = ((1U << 3) + 7) << kOpcodeShift;  // LUI/AUI family.
+constexpr Opcode DAUI = ((3U << 3) + 5) << kOpcodeShift;
+
+constexpr Opcode BEQC = ((2U << 3) + 0) << kOpcodeShift;
+constexpr Opcode COP1 = ((2U << 3) + 1)
+                        << kOpcodeShift;  // Coprocessor 1 class.
+constexpr Opcode BEQL = ((2U << 3) + 4) << kOpcodeShift;
+constexpr Opcode BNEL = ((2U << 3) + 5) << kOpcodeShift;
+constexpr Opcode BLEZL = ((2U << 3) + 6) << kOpcodeShift;
+constexpr Opcode BGTZL = ((2U << 3) + 7) << kOpcodeShift;
+
+constexpr Opcode DADDI = ((3U << 3) + 0) << kOpcodeShift;  // This is also BNEC.
+constexpr Opcode DADDIU = ((3U << 3) + 1) << kOpcodeShift;
+constexpr Opcode LDL = ((3U << 3) + 2) << kOpcodeShift;
+constexpr Opcode LDR = ((3U << 3) + 3) << kOpcodeShift;
+constexpr Opcode SPECIAL2 = ((3U << 3) + 4) << kOpcodeShift;
+constexpr Opcode MSA = ((3U << 3) + 6) << kOpcodeShift;
+constexpr Opcode SPECIAL3 = ((3U << 3) + 7) << kOpcodeShift;
+
+constexpr Opcode LB = ((4U << 3) + 0) << kOpcodeShift;
+constexpr Opcode LH = ((4U << 3) + 1) << kOpcodeShift;
+constexpr Opcode LWL = ((4U << 3) + 2) << kOpcodeShift;
+constexpr Opcode LW = ((4U << 3) + 3) << kOpcodeShift;
+constexpr Opcode LBU = ((4U << 3) + 4) << kOpcodeShift;
+constexpr Opcode LHU = ((4U << 3) + 5) << kOpcodeShift;
+constexpr Opcode LWR = ((4U << 3) + 6) << kOpcodeShift;
+constexpr Opcode LWU = ((4U << 3) + 7) << kOpcodeShift;
+
+constexpr Opcode SB = ((5U << 3) + 0) << kOpcodeShift;
+constexpr Opcode SH = ((5U << 3) + 1) << kOpcodeShift;
+constexpr Opcode SWL = ((5U << 3) + 2) << kOpcodeShift;
+constexpr Opcode SW = ((5U << 3) + 3) << kOpcodeShift;
+constexpr Opcode SDL = ((5U << 3) + 4) << kOpcodeShift;
+constexpr Opcode SDR = ((5U << 3) + 5) << kOpcodeShift;
+constexpr Opcode SWR = ((5U << 3) + 6) << kOpcodeShift;
+
+constexpr Opcode LL = ((6U << 3) + 0) << kOpcodeShift;
+constexpr Opcode LWC1 = ((6U << 3) + 1) << kOpcodeShift;
+constexpr Opcode BC = ((6U << 3) + 2) << kOpcodeShift;
+constexpr Opcode LLD = ((6U << 3) + 4) << kOpcodeShift;
+constexpr Opcode LDC1 = ((6U << 3) + 5) << kOpcodeShift;
+constexpr Opcode POP66 = ((6U << 3) + 6) << kOpcodeShift;
+constexpr Opcode LD = ((6U << 3) + 7) << kOpcodeShift;
+
+constexpr Opcode PREF = ((6U << 3) + 3) << kOpcodeShift;
+
+constexpr Opcode SC = ((7U << 3) + 0) << kOpcodeShift;
+constexpr Opcode SWC1 = ((7U << 3) + 1) << kOpcodeShift;
+constexpr Opcode BALC = ((7U << 3) + 2) << kOpcodeShift;
+constexpr Opcode PCREL = ((7U << 3) + 3) << kOpcodeShift;
+constexpr Opcode SCD = ((7U << 3) + 4) << kOpcodeShift;
+constexpr Opcode SDC1 = ((7U << 3) + 5) << kOpcodeShift;
+constexpr Opcode POP76 = ((7U << 3) + 6) << kOpcodeShift;
+constexpr Opcode SD = ((7U << 3) + 7) << kOpcodeShift;
+
+constexpr Opcode COP1X = ((1U << 4) + 3) << kOpcodeShift;
+
+// New r6 instruction.
+constexpr Opcode POP06 = BLEZ;   // bgeuc/bleuc, blezalc, bgezalc
+constexpr Opcode POP07 = BGTZ;   // bltuc/bgtuc, bgtzalc, bltzalc
+constexpr Opcode POP10 = ADDI;   // beqzalc, bovc, beqc
+constexpr Opcode POP26 = BLEZL;  // bgezc, blezc, bgec/blec
+constexpr Opcode POP27 = BGTZL;  // bgtzc, bltzc, bltc/bgtc
+constexpr Opcode POP30 = DADDI;  // bnezalc, bnvc, bnec
 
 enum SecondaryField : uint32_t {
   // SPECIAL Encoding of Function Field.
diff -r -u --color up/v8/src/codegen/mips64/macro-assembler-mips64.cc nw/v8/src/codegen/mips64/macro-assembler-mips64.cc
--- up/v8/src/codegen/mips64/macro-assembler-mips64.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/mips64/macro-assembler-mips64.cc	2023-01-19 16:46:36.048942949 -0500
@@ -4951,13 +4951,6 @@
   DCHECK_EQ(actual_parameter_count, a0);
   DCHECK_EQ(expected_parameter_count, a2);
 
-  // If the expected parameter count is equal to the adaptor sentinel, no need
-  // to push undefined value as arguments.
-  if (kDontAdaptArgumentsSentinel != 0) {
-    Branch(&regular_invoke, eq, expected_parameter_count,
-           Operand(kDontAdaptArgumentsSentinel));
-  }
-
   // If overapplication or if the actual argument count is equal to the
   // formal parameter count, no need to push extra undefined values.
   Dsubu(expected_parameter_count, expected_parameter_count,
@@ -5460,7 +5453,8 @@
     Push(kScratchReg);
   }
 #if V8_ENABLE_WEBASSEMBLY
-  if (type == StackFrame::WASM) Push(kWasmInstanceRegister);
+  if (type == StackFrame::WASM || type == StackFrame::WASM_LIFTOFF_SETUP)
+    Push(kWasmInstanceRegister);
 #endif  // V8_ENABLE_WEBASSEMBLY
 }
 
diff -r -u --color up/v8/src/codegen/pending-optimization-table.cc nw/v8/src/codegen/pending-optimization-table.cc
--- up/v8/src/codegen/pending-optimization-table.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/pending-optimization-table.cc	2023-01-19 16:46:36.048942949 -0500
@@ -13,123 +13,54 @@
 namespace v8 {
 namespace internal {
 
-enum class FunctionStatus : int {
-  kPrepareForOptimize = 1 << 0,
-  kMarkForOptimize = 1 << 1,
-  kAllowHeuristicOptimization = 1 << 2,
-};
-
-using FunctionStatusFlags = base::Flags<FunctionStatus>;
-
-void PendingOptimizationTable::PreparedForOptimization(
+void ManualOptimizationTable::MarkFunctionForManualOptimization(
     Isolate* isolate, Handle<JSFunction> function,
-    bool allow_heuristic_optimization) {
+    IsCompiledScope* is_compiled_scope) {
   DCHECK(v8_flags.testing_d8_test_runner);
+  DCHECK(is_compiled_scope->is_compiled());
+  DCHECK(function->has_feedback_vector());
 
-  FunctionStatusFlags status = FunctionStatus::kPrepareForOptimize;
-  if (allow_heuristic_optimization) {
-    status |= FunctionStatus::kAllowHeuristicOptimization;
-  }
   Handle<SharedFunctionInfo> shared_info(function->shared(), isolate);
 
-  IsCompiledScope is_compiled_scope;
-  SharedFunctionInfo::EnsureBytecodeArrayAvailable(isolate, shared_info,
-                                                   &is_compiled_scope);
-
   Handle<ObjectHashTable> table =
-      isolate->heap()->pending_optimize_for_test_bytecode().IsUndefined()
+      isolate->heap()->functions_marked_for_manual_optimization().IsUndefined()
           ? ObjectHashTable::New(isolate, 1)
           : handle(ObjectHashTable::cast(
-                       isolate->heap()->pending_optimize_for_test_bytecode()),
+                       isolate->heap()
+                           ->functions_marked_for_manual_optimization()),
                    isolate);
-  Handle<Tuple2> tuple = isolate->factory()->NewTuple2(
-      handle(shared_info->GetBytecodeArray(isolate), isolate),
-      handle(Smi::FromInt(status), isolate), AllocationType::kYoung);
-  table =
-      ObjectHashTable::Put(table, handle(function->shared(), isolate), tuple);
-  isolate->heap()->SetPendingOptimizeForTestBytecode(*table);
+  table = ObjectHashTable::Put(
+      table, shared_info,
+      handle(shared_info->GetBytecodeArray(isolate), isolate));
+  isolate->heap()->SetFunctionsMarkedForManualOptimization(*table);
 }
 
-bool PendingOptimizationTable::IsHeuristicOptimizationAllowed(
+void ManualOptimizationTable::CheckMarkedForManualOptimization(
     Isolate* isolate, JSFunction function) {
-  DCHECK(v8_flags.testing_d8_test_runner);
-
-  Handle<Object> table =
-      handle(isolate->heap()->pending_optimize_for_test_bytecode(), isolate);
-  Handle<Object> entry =
-      table->IsUndefined()
-          ? handle(ReadOnlyRoots(isolate).the_hole_value(), isolate)
-          : handle(Handle<ObjectHashTable>::cast(table)->Lookup(
-                       handle(function.shared(), isolate)),
-                   isolate);
-  if (entry->IsTheHole()) {
-    return true;
-  }
-  DCHECK(entry->IsTuple2());
-  DCHECK(Handle<Tuple2>::cast(entry)->value2().IsSmi());
-  FunctionStatusFlags status(Smi::ToInt(Handle<Tuple2>::cast(entry)->value2()));
-  return status & FunctionStatus::kAllowHeuristicOptimization;
-}
-
-void PendingOptimizationTable::MarkedForOptimization(
-    Isolate* isolate, Handle<JSFunction> function) {
-  DCHECK(v8_flags.testing_d8_test_runner);
-
-  Handle<Object> table =
-      handle(isolate->heap()->pending_optimize_for_test_bytecode(), isolate);
-  Handle<Object> entry =
-      table->IsUndefined()
-          ? handle(ReadOnlyRoots(isolate).the_hole_value(), isolate)
-          : handle(Handle<ObjectHashTable>::cast(table)->Lookup(
-                       handle(function->shared(), isolate)),
-                   isolate);
-  if (entry->IsTheHole()) {
+  if (!IsMarkedForManualOptimization(isolate, function)) {
     PrintF("Error: Function ");
-    function->ShortPrint();
+    function.ShortPrint();
     PrintF(
         " should be prepared for optimization with "
         "%%PrepareFunctionForOptimization before  "
         "%%OptimizeFunctionOnNextCall / %%OptimizeOSR ");
     UNREACHABLE();
   }
-
-  DCHECK(entry->IsTuple2());
-  DCHECK(Handle<Tuple2>::cast(entry)->value2().IsSmi());
-  FunctionStatusFlags status(Smi::ToInt(Handle<Tuple2>::cast(entry)->value2()));
-  status = status.without(FunctionStatus::kPrepareForOptimize) |
-           FunctionStatus::kMarkForOptimize;
-  Handle<Tuple2>::cast(entry)->set_value2(Smi::FromInt(status));
-  table = ObjectHashTable::Put(Handle<ObjectHashTable>::cast(table),
-                               handle(function->shared(), isolate), entry);
-  isolate->heap()->SetPendingOptimizeForTestBytecode(*table);
 }
 
-void PendingOptimizationTable::FunctionWasOptimized(
-    Isolate* isolate, Handle<JSFunction> function) {
+bool ManualOptimizationTable::IsMarkedForManualOptimization(
+    Isolate* isolate, JSFunction function) {
   DCHECK(v8_flags.testing_d8_test_runner);
 
-  if (isolate->heap()->pending_optimize_for_test_bytecode().IsUndefined()) {
-    return;
-  }
-
-  Handle<ObjectHashTable> table =
-      handle(ObjectHashTable::cast(
-                 isolate->heap()->pending_optimize_for_test_bytecode()),
-             isolate);
-  Handle<Object> value(table->Lookup(handle(function->shared(), isolate)),
-                       isolate);
-  // Remove only if we have already seen %OptimizeFunctionOnNextCall. If it is
-  // optimized for other reasons, still keep holding the bytecode since we may
-  // optimize it later.
-  if (!value->IsTheHole() &&
-      Smi::cast(Handle<Tuple2>::cast(value)->value2()).value() ==
-          static_cast<int>(FunctionStatus::kMarkForOptimize)) {
-    bool was_present;
-    table = table->Remove(isolate, table, handle(function->shared(), isolate),
-                          &was_present);
-    DCHECK(was_present);
-    isolate->heap()->SetPendingOptimizeForTestBytecode(*table);
-  }
+  Handle<Object> table = handle(
+      isolate->heap()->functions_marked_for_manual_optimization(), isolate);
+  Handle<Object> entry =
+      table->IsUndefined()
+          ? handle(ReadOnlyRoots(isolate).the_hole_value(), isolate)
+          : handle(Handle<ObjectHashTable>::cast(table)->Lookup(
+                       handle(function.shared(), isolate)),
+                   isolate);
+  return !entry->IsTheHole();
 }
 
 }  // namespace internal
diff -r -u --color up/v8/src/codegen/pending-optimization-table.h nw/v8/src/codegen/pending-optimization-table.h
--- up/v8/src/codegen/pending-optimization-table.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/pending-optimization-table.h	2023-01-19 16:46:36.048942949 -0500
@@ -10,39 +10,33 @@
 namespace v8 {
 namespace internal {
 
+class IsCompiledScope;
+
 // This class adds the functionality to properly test the optimized code. This
 // is only for use in tests. All these functions should only be called when
 // testing_d8_flag_for_tests is set.
-class PendingOptimizationTable {
+class ManualOptimizationTable {
  public:
   // This function should be called before we mark the function for
-  // optimization. Calling this function ensures that |function| is compiled and
-  // has a feedback vector allocated. This also holds on to the bytecode
-  // strongly in pending optimization table preventing the bytecode to be
-  // flushed.
-  static void PreparedForOptimization(Isolate* isolate,
-                                      Handle<JSFunction> function,
-                                      bool allow_heuristic_optimization);
+  // optimization. It should be called when |function| is already compiled and
+  // has a feedback vector allocated, and it blocks heuristic optimization.
+  //
+  // This also holds on to the bytecode strongly, preventing the bytecode from
+  // being flushed.
+  static void MarkFunctionForManualOptimization(
+      Isolate* isolate, Handle<JSFunction> function,
+      IsCompiledScope* is_compiled_scope);
 
   // This function should be called when the function is marked for optimization
-  // via the intrinsics. This will update the state of the bytecode array in the
-  // pending optimization table, so that the entry can be removed once the
-  // function is optimized. If the function is already optimized it removes the
-  // entry from the table.
-  static void MarkedForOptimization(Isolate* isolate,
-                                    Handle<JSFunction> function);
-
-  // This function should be called once the function is optimized. If there is
-  // an entry in the pending optimization table and it is marked for removal
-  // then this function removes the entry from pending optimization table.
-  static void FunctionWasOptimized(Isolate* isolate,
-                                   Handle<JSFunction> function);
+  // via the intrinsics. This will check whether
+  // MarkFunctionForManualOptimization was called with this function.
+  static void CheckMarkedForManualOptimization(Isolate* isolate,
+                                               JSFunction function);
 
-  // This function returns whether a heuristic is allowed to trigger
-  // optimization the function. This mechanism is used in tests to prevent
-  // heuristics from interfering with manually triggered optimization.
-  static bool IsHeuristicOptimizationAllowed(Isolate* isolate,
-                                             JSFunction function);
+  // Returns true if MarkFunctionForManualOptimization was called with this
+  // function.
+  static bool IsMarkedForManualOptimization(Isolate* isolate,
+                                            JSFunction function);
 };
 
 }  // namespace internal
diff -r -u --color up/v8/src/codegen/ppc/assembler-ppc.cc nw/v8/src/codegen/ppc/assembler-ppc.cc
--- up/v8/src/codegen/ppc/assembler-ppc.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/ppc/assembler-ppc.cc	2023-01-19 16:46:36.048942949 -0500
@@ -690,11 +690,11 @@
 // Branch instructions.
 
 void Assembler::bclr(BOfield bo, int condition_bit, LKBit lk) {
-  emit(EXT1 | bo | condition_bit * B16 | BCLRX | lk);
+  emit(EXT1 | static_cast<uint32_t>(bo) | condition_bit * B16 | BCLRX | lk);
 }
 
 void Assembler::bcctr(BOfield bo, int condition_bit, LKBit lk) {
-  emit(EXT1 | bo | condition_bit * B16 | BCCTRX | lk);
+  emit(EXT1 | static_cast<uint32_t>(bo) | condition_bit * B16 | BCCTRX | lk);
 }
 
 // Pseudo op - branch to link register
@@ -708,7 +708,8 @@
 void Assembler::bc(int branch_offset, BOfield bo, int condition_bit, LKBit lk) {
   int imm16 = branch_offset;
   CHECK(is_int16(imm16) && (imm16 & (kAAMask | kLKMask)) == 0);
-  emit(BCX | bo | condition_bit * B16 | (imm16 & kImm16Mask) | lk);
+  emit(BCX | static_cast<uint32_t>(bo) | condition_bit * B16 |
+       (imm16 & kImm16Mask) | lk);
 }
 
 void Assembler::b(int branch_offset, LKBit lk) {
diff -r -u --color up/v8/src/codegen/ppc/macro-assembler-ppc.cc nw/v8/src/codegen/ppc/macro-assembler-ppc.cc
--- up/v8/src/codegen/ppc/macro-assembler-ppc.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/ppc/macro-assembler-ppc.cc	2023-01-19 16:46:36.048942949 -0500
@@ -1524,14 +1524,6 @@
   DCHECK_EQ(actual_parameter_count, r3);
   DCHECK_EQ(expected_parameter_count, r5);
 
-  // If the expected parameter count is equal to the adaptor sentinel, no need
-  // to push undefined value as arguments.
-  if (kDontAdaptArgumentsSentinel != 0) {
-    mov(r0, Operand(kDontAdaptArgumentsSentinel));
-    CmpS64(expected_parameter_count, r0);
-    beq(&regular_invoke);
-  }
-
   // If overapplication or if the actual argument count is equal to the
   // formal parameter count, no need to push extra undefined values.
   sub(expected_parameter_count, expected_parameter_count,
@@ -3730,7 +3722,11 @@
   V(I8x16AddSatS, vaddsbs) \
   V(I8x16SubSatS, vsubsbs) \
   V(I8x16AddSatU, vaddubs) \
-  V(I8x16SubSatU, vsububs)
+  V(I8x16SubSatU, vsububs) \
+  V(S128And, vand)         \
+  V(S128Or, vor)           \
+  V(S128Xor, vxor)         \
+  V(S128AndNot, vandc)
 
 #define EMIT_SIMD_BINOP(name, op)                                      \
   void TurboAssembler::name(Simd128Register dst, Simd128Register src1, \
@@ -3772,21 +3768,27 @@
 #undef EMIT_SIMD_SHIFT
 #undef SIMD_SHIFT_LIST
 
-#define SIMD_UNOP_LIST(V) \
-  V(F64x2Abs, xvabsdp)    \
-  V(F64x2Neg, xvnegdp)    \
-  V(F64x2Sqrt, xvsqrtdp)  \
-  V(F64x2Ceil, xvrdpip)   \
-  V(F64x2Floor, xvrdpim)  \
-  V(F64x2Trunc, xvrdpiz)  \
-  V(F32x4Abs, xvabssp)    \
-  V(F32x4Neg, xvnegsp)    \
-  V(F32x4Sqrt, xvsqrtsp)  \
-  V(F32x4Ceil, xvrspip)   \
-  V(F32x4Floor, xvrspim)  \
-  V(F32x4Trunc, xvrspiz)  \
-  V(I64x2Neg, vnegd)      \
-  V(I32x4Neg, vnegw)      \
+#define SIMD_UNOP_LIST(V)            \
+  V(F64x2Abs, xvabsdp)               \
+  V(F64x2Neg, xvnegdp)               \
+  V(F64x2Sqrt, xvsqrtdp)             \
+  V(F64x2Ceil, xvrdpip)              \
+  V(F64x2Floor, xvrdpim)             \
+  V(F64x2Trunc, xvrdpiz)             \
+  V(F32x4Abs, xvabssp)               \
+  V(F32x4Neg, xvnegsp)               \
+  V(F32x4Sqrt, xvsqrtsp)             \
+  V(F32x4Ceil, xvrspip)              \
+  V(F32x4Floor, xvrspim)             \
+  V(F32x4Trunc, xvrspiz)             \
+  V(I64x2Neg, vnegd)                 \
+  V(I64x2SConvertI32x4Low, vupklsw)  \
+  V(I64x2SConvertI32x4High, vupkhsw) \
+  V(I32x4Neg, vnegw)                 \
+  V(I32x4SConvertI16x8Low, vupklsh)  \
+  V(I32x4SConvertI16x8High, vupkhsh) \
+  V(I16x8SConvertI8x16Low, vupklsb)  \
+  V(I16x8SConvertI8x16High, vupkhsb) \
   V(I8x16Popcnt, vpopcntb)
 
 #define EMIT_SIMD_UNOP(name, op)                                        \
@@ -3797,6 +3799,94 @@
 #undef EMIT_SIMD_UNOP
 #undef SIMD_UNOP_LIST
 
+#define EXT_MUL(dst_even, dst_odd, mul_even, mul_odd) \
+  mul_even(dst_even, src1, src2);                     \
+  mul_odd(dst_odd, src1, src2);
+#define SIMD_EXT_MUL_LIST(V)                         \
+  V(I32x4ExtMulLowI16x8S, vmulesh, vmulosh, vmrglw)  \
+  V(I32x4ExtMulHighI16x8S, vmulesh, vmulosh, vmrghw) \
+  V(I32x4ExtMulLowI16x8U, vmuleuh, vmulouh, vmrglw)  \
+  V(I32x4ExtMulHighI16x8U, vmuleuh, vmulouh, vmrghw) \
+  V(I16x8ExtMulLowI8x16S, vmulesb, vmulosb, vmrglh)  \
+  V(I16x8ExtMulHighI8x16S, vmulesb, vmulosb, vmrghh) \
+  V(I16x8ExtMulLowI8x16U, vmuleub, vmuloub, vmrglh)  \
+  V(I16x8ExtMulHighI8x16U, vmuleub, vmuloub, vmrghh)
+
+#define EMIT_SIMD_EXT_MUL(name, mul_even, mul_odd, merge)                    \
+  void TurboAssembler::name(Simd128Register dst, Simd128Register src1,       \
+                            Simd128Register src2, Simd128Register scratch) { \
+    EXT_MUL(scratch, dst, mul_even, mul_odd)                                 \
+    merge(dst, scratch, dst);                                                \
+  }
+SIMD_EXT_MUL_LIST(EMIT_SIMD_EXT_MUL)
+#undef EMIT_SIMD_EXT_MUL
+#undef SIMD_EXT_MUL_LIST
+
+#define SIMD_ALL_TRUE_LIST(V) \
+  V(I64x2AllTrue, vcmpgtud)   \
+  V(I32x4AllTrue, vcmpgtuw)   \
+  V(I16x8AllTrue, vcmpgtuh)   \
+  V(I8x16AllTrue, vcmpgtub)
+
+#define EMIT_SIMD_ALL_TRUE(name, op)                              \
+  void TurboAssembler::name(Register dst, Simd128Register src,    \
+                            Register scratch1, Register scratch2, \
+                            Simd128Register scratch3) {           \
+    constexpr uint8_t fxm = 0x2; /* field mask. */                \
+    constexpr int bit_number = 24;                                \
+    li(scratch1, Operand(0));                                     \
+    li(scratch2, Operand(1));                                     \
+    /* Check if all lanes > 0, if not then return false.*/        \
+    vxor(scratch3, scratch3, scratch3);                           \
+    mtcrf(scratch1, fxm); /* Clear cr6.*/                         \
+    op(scratch3, src, scratch3, SetRC);                           \
+    isel(dst, scratch2, scratch1, bit_number);                    \
+  }
+SIMD_ALL_TRUE_LIST(EMIT_SIMD_ALL_TRUE)
+#undef EMIT_SIMD_ALL_TRUE
+#undef SIMD_ALL_TRUE_LIST
+
+void TurboAssembler::I64x2ExtMulLowI32x4S(Simd128Register dst,
+                                          Simd128Register src1,
+                                          Simd128Register src2,
+                                          Simd128Register scratch) {
+  constexpr int lane_width_in_bytes = 8;
+  EXT_MUL(scratch, dst, vmulesw, vmulosw)
+  vextractd(scratch, scratch, Operand(1 * lane_width_in_bytes));
+  vinsertd(dst, scratch, Operand(0));
+}
+
+void TurboAssembler::I64x2ExtMulHighI32x4S(Simd128Register dst,
+                                           Simd128Register src1,
+                                           Simd128Register src2,
+                                           Simd128Register scratch) {
+  constexpr int lane_width_in_bytes = 8;
+  EXT_MUL(scratch, dst, vmulesw, vmulosw)
+  vinsertd(scratch, dst, Operand(1 * lane_width_in_bytes));
+  vor(dst, scratch, scratch);
+}
+
+void TurboAssembler::I64x2ExtMulLowI32x4U(Simd128Register dst,
+                                          Simd128Register src1,
+                                          Simd128Register src2,
+                                          Simd128Register scratch) {
+  constexpr int lane_width_in_bytes = 8;
+  EXT_MUL(scratch, dst, vmuleuw, vmulouw)
+  vextractd(scratch, scratch, Operand(1 * lane_width_in_bytes));
+  vinsertd(dst, scratch, Operand(0));
+}
+
+void TurboAssembler::I64x2ExtMulHighI32x4U(Simd128Register dst,
+                                           Simd128Register src1,
+                                           Simd128Register src2,
+                                           Simd128Register scratch) {
+  constexpr int lane_width_in_bytes = 8;
+  EXT_MUL(scratch, dst, vmuleuw, vmulouw)
+  vinsertd(scratch, dst, Operand(1 * lane_width_in_bytes));
+  vor(dst, scratch, scratch);
+}
+#undef EXT_MUL
+
 void TurboAssembler::LoadSimd128(Simd128Register dst, const MemOperand& mem,
                                  Register scratch) {
   GenerateMemoryOperationRR(dst, mem, lxvx);
@@ -4218,6 +4308,166 @@
   vaddubm(dst, scratch, dst);
 }
 
+void TurboAssembler::F64x2Pmin(Simd128Register dst, Simd128Register src1,
+                               Simd128Register src2, Simd128Register scratch) {
+  xvcmpgtdp(kScratchSimd128Reg, src1, src2);
+  vsel(dst, src1, src2, kScratchSimd128Reg);
+}
+
+void TurboAssembler::F64x2Pmax(Simd128Register dst, Simd128Register src1,
+                               Simd128Register src2, Simd128Register scratch) {
+  xvcmpgtdp(kScratchSimd128Reg, src2, src1);
+  vsel(dst, src1, src2, kScratchSimd128Reg);
+}
+
+void TurboAssembler::F32x4Pmin(Simd128Register dst, Simd128Register src1,
+                               Simd128Register src2, Simd128Register scratch) {
+  xvcmpgtsp(kScratchSimd128Reg, src1, src2);
+  vsel(dst, src1, src2, kScratchSimd128Reg);
+}
+
+void TurboAssembler::F32x4Pmax(Simd128Register dst, Simd128Register src1,
+                               Simd128Register src2, Simd128Register scratch) {
+  xvcmpgtsp(kScratchSimd128Reg, src2, src1);
+  vsel(dst, src1, src2, kScratchSimd128Reg);
+}
+
+void TurboAssembler::I16x8SConvertI32x4(Simd128Register dst,
+                                        Simd128Register src1,
+                                        Simd128Register src2) {
+  vpkswss(dst, src2, src1);
+}
+
+void TurboAssembler::I16x8UConvertI32x4(Simd128Register dst,
+                                        Simd128Register src1,
+                                        Simd128Register src2) {
+  vpkswus(dst, src2, src1);
+}
+
+void TurboAssembler::I8x16SConvertI16x8(Simd128Register dst,
+                                        Simd128Register src1,
+                                        Simd128Register src2) {
+  vpkshss(dst, src2, src1);
+}
+
+void TurboAssembler::I8x16UConvertI16x8(Simd128Register dst,
+                                        Simd128Register src1,
+                                        Simd128Register src2) {
+  vpkshus(dst, src2, src1);
+}
+
+void TurboAssembler::F64x2ConvertLowI32x4S(Simd128Register dst,
+                                           Simd128Register src) {
+  vupklsw(dst, src);
+  xvcvsxddp(dst, dst);
+}
+
+void TurboAssembler::F64x2ConvertLowI32x4U(Simd128Register dst,
+                                           Simd128Register src,
+                                           Register scratch1,
+                                           Simd128Register scratch2) {
+  constexpr int lane_width_in_bytes = 8;
+  vupklsw(dst, src);
+  // Zero extend.
+  mov(scratch1, Operand(0xFFFFFFFF));
+  mtvsrd(scratch2, scratch1);
+  vinsertd(scratch2, scratch2, Operand(1 * lane_width_in_bytes));
+  vand(dst, scratch2, dst);
+  xvcvuxddp(dst, dst);
+}
+
+void TurboAssembler::I64x2UConvertI32x4Low(Simd128Register dst,
+                                           Simd128Register src,
+                                           Register scratch1,
+                                           Simd128Register scratch2) {
+  constexpr int lane_width_in_bytes = 8;
+  vupklsw(dst, src);
+  // Zero extend.
+  mov(scratch1, Operand(0xFFFFFFFF));
+  mtvsrd(scratch2, scratch1);
+  vinsertd(scratch2, scratch2, Operand(1 * lane_width_in_bytes));
+  vand(dst, scratch2, dst);
+}
+
+void TurboAssembler::I64x2UConvertI32x4High(Simd128Register dst,
+                                            Simd128Register src,
+                                            Register scratch1,
+                                            Simd128Register scratch2) {
+  constexpr int lane_width_in_bytes = 8;
+  vupkhsw(dst, src);
+  // Zero extend.
+  mov(scratch1, Operand(0xFFFFFFFF));
+  mtvsrd(scratch2, scratch1);
+  vinsertd(scratch2, scratch2, Operand(1 * lane_width_in_bytes));
+  vand(dst, scratch2, dst);
+}
+
+void TurboAssembler::I32x4UConvertI16x8Low(Simd128Register dst,
+                                           Simd128Register src,
+                                           Register scratch1,
+                                           Simd128Register scratch2) {
+  vupklsh(dst, src);
+  // Zero extend.
+  mov(scratch1, Operand(0xFFFF));
+  mtvsrd(scratch2, scratch1);
+  vspltw(scratch2, scratch2, Operand(1));
+  vand(dst, scratch2, dst);
+}
+
+void TurboAssembler::I32x4UConvertI16x8High(Simd128Register dst,
+                                            Simd128Register src,
+                                            Register scratch1,
+                                            Simd128Register scratch2) {
+  vupkhsh(dst, src);
+  // Zero extend.
+  mov(scratch1, Operand(0xFFFF));
+  mtvsrd(scratch2, scratch1);
+  vspltw(scratch2, scratch2, Operand(1));
+  vand(dst, scratch2, dst);
+}
+
+void TurboAssembler::I16x8UConvertI8x16Low(Simd128Register dst,
+                                           Simd128Register src,
+                                           Register scratch1,
+                                           Simd128Register scratch2) {
+  vupklsb(dst, src);
+  // Zero extend.
+  li(scratch1, Operand(0xFF));
+  mtvsrd(scratch2, scratch1);
+  vsplth(scratch2, scratch2, Operand(3));
+  vand(dst, scratch2, dst);
+}
+
+void TurboAssembler::I16x8UConvertI8x16High(Simd128Register dst,
+                                            Simd128Register src,
+                                            Register scratch1,
+                                            Simd128Register scratch2) {
+  vupkhsb(dst, src);
+  // Zero extend.
+  li(scratch1, Operand(0xFF));
+  mtvsrd(scratch2, scratch1);
+  vsplth(scratch2, scratch2, Operand(3));
+  vand(dst, scratch2, dst);
+}
+
+void TurboAssembler::V128AnyTrue(Register dst, Simd128Register src,
+                                 Register scratch1, Register scratch2,
+                                 Simd128Register scratch3) {
+  constexpr uint8_t fxm = 0x2;  // field mask.
+  constexpr int bit_number = 24;
+  li(scratch1, Operand(0));
+  li(scratch2, Operand(1));
+  // Check if both lanes are 0, if so then return false.
+  vxor(scratch3, scratch3, scratch3);
+  mtcrf(scratch1, fxm);  // Clear cr6.
+  vcmpequd(scratch3, src, scratch3, SetRC);
+  isel(dst, scratch1, scratch2, bit_number);
+}
+
+void TurboAssembler::S128Not(Simd128Register dst, Simd128Register src) {
+  vnor(dst, src, src);
+}
+
 Register GetRegisterThatIsNotOneOf(Register reg1, Register reg2, Register reg3,
                                    Register reg4, Register reg5,
                                    Register reg6) {
diff -r -u --color up/v8/src/codegen/ppc/macro-assembler-ppc.h nw/v8/src/codegen/ppc/macro-assembler-ppc.h
--- up/v8/src/codegen/ppc/macro-assembler-ppc.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/ppc/macro-assembler-ppc.h	2023-01-19 16:46:36.048942949 -0500
@@ -1125,6 +1125,8 @@
   V(I16x8SubSatS)          \
   V(I16x8AddSatU)          \
   V(I16x8SubSatU)          \
+  V(I16x8SConvertI32x4)    \
+  V(I16x8UConvertI32x4)    \
   V(I8x16Add)              \
   V(I8x16Sub)              \
   V(I8x16MinS)             \
@@ -1137,7 +1139,13 @@
   V(I8x16AddSatS)          \
   V(I8x16SubSatS)          \
   V(I8x16AddSatU)          \
-  V(I8x16SubSatU)
+  V(I8x16SubSatU)          \
+  V(I8x16SConvertI16x8)    \
+  V(I8x16UConvertI16x8)    \
+  V(S128And)               \
+  V(S128Or)                \
+  V(S128Xor)               \
+  V(S128AndNot)
 
 #define PROTOTYPE_SIMD_BINOP(name) \
   void name(Simd128Register dst, Simd128Register src1, Simd128Register src2);
@@ -1147,15 +1155,31 @@
 
 #define SIMD_BINOP_WITH_SCRATCH_LIST(V) \
   V(F64x2Ne)                            \
+  V(F64x2Pmin)                          \
+  V(F64x2Pmax)                          \
   V(F32x4Ne)                            \
+  V(F32x4Pmin)                          \
+  V(F32x4Pmax)                          \
   V(I64x2Ne)                            \
   V(I64x2GeS)                           \
+  V(I64x2ExtMulLowI32x4S)               \
+  V(I64x2ExtMulHighI32x4S)              \
+  V(I64x2ExtMulLowI32x4U)               \
+  V(I64x2ExtMulHighI32x4U)              \
   V(I32x4Ne)                            \
   V(I32x4GeS)                           \
   V(I32x4GeU)                           \
+  V(I32x4ExtMulLowI16x8S)               \
+  V(I32x4ExtMulHighI16x8S)              \
+  V(I32x4ExtMulLowI16x8U)               \
+  V(I32x4ExtMulHighI16x8U)              \
   V(I16x8Ne)                            \
   V(I16x8GeS)                           \
   V(I16x8GeU)                           \
+  V(I16x8ExtMulLowI8x16S)               \
+  V(I16x8ExtMulHighI8x16S)              \
+  V(I16x8ExtMulLowI8x16U)               \
+  V(I16x8ExtMulHighI8x16U)              \
   V(I8x16Ne)                            \
   V(I8x16GeS)                           \
   V(I8x16GeU)
@@ -1190,22 +1214,30 @@
 #undef PROTOTYPE_SIMD_SHIFT
 #undef SIMD_SHIFT_LIST
 
-#define SIMD_UNOP_LIST(V) \
-  V(F64x2Abs)             \
-  V(F64x2Neg)             \
-  V(F64x2Sqrt)            \
-  V(F64x2Ceil)            \
-  V(F64x2Floor)           \
-  V(F64x2Trunc)           \
-  V(F32x4Abs)             \
-  V(F32x4Neg)             \
-  V(F32x4Sqrt)            \
-  V(F32x4Ceil)            \
-  V(F32x4Floor)           \
-  V(F32x4Trunc)           \
-  V(I64x2Neg)             \
-  V(I32x4Neg)             \
-  V(I8x16Popcnt)
+#define SIMD_UNOP_LIST(V)   \
+  V(F64x2Abs)               \
+  V(F64x2Neg)               \
+  V(F64x2Sqrt)              \
+  V(F64x2Ceil)              \
+  V(F64x2Floor)             \
+  V(F64x2Trunc)             \
+  V(F32x4Abs)               \
+  V(F32x4Neg)               \
+  V(F32x4Sqrt)              \
+  V(F32x4Ceil)              \
+  V(F32x4Floor)             \
+  V(F32x4Trunc)             \
+  V(I64x2Neg)               \
+  V(F64x2ConvertLowI32x4S)  \
+  V(I64x2SConvertI32x4Low)  \
+  V(I64x2SConvertI32x4High) \
+  V(I32x4Neg)               \
+  V(I32x4SConvertI16x8Low)  \
+  V(I32x4SConvertI16x8High) \
+  V(I16x8SConvertI8x16Low)  \
+  V(I16x8SConvertI8x16High) \
+  V(I8x16Popcnt)            \
+  V(S128Not)
 
 #define PROTOTYPE_SIMD_UNOP(name) \
   void name(Simd128Register dst, Simd128Register src);
@@ -1213,6 +1245,33 @@
 #undef PROTOTYPE_SIMD_UNOP
 #undef SIMD_UNOP_LIST
 
+#define SIMD_UNOP_WITH_SCRATCH_LIST(V) \
+  V(I64x2Abs)                          \
+  V(I32x4Abs)                          \
+  V(I16x8Abs)                          \
+  V(I16x8Neg)                          \
+  V(I8x16Abs)                          \
+  V(I8x16Neg)
+
+#define PROTOTYPE_SIMD_UNOP_WITH_SCRATCH(name) \
+  void name(Simd128Register dst, Simd128Register src, Simd128Register scratch);
+  SIMD_UNOP_WITH_SCRATCH_LIST(PROTOTYPE_SIMD_UNOP_WITH_SCRATCH)
+#undef PROTOTYPE_SIMD_UNOP_WITH_SCRATCH
+#undef SIMD_UNOP_WITH_SCRATCH_LIST
+
+#define SIMD_ALL_TRUE_LIST(V) \
+  V(I64x2AllTrue)             \
+  V(I32x4AllTrue)             \
+  V(I16x8AllTrue)             \
+  V(I8x16AllTrue)
+
+#define PROTOTYPE_SIMD_ALL_TRUE(name)                             \
+  void name(Register dst, Simd128Register src, Register scratch1, \
+            Register scratch2, Simd128Register scratch3);
+  SIMD_ALL_TRUE_LIST(PROTOTYPE_SIMD_ALL_TRUE)
+#undef PROTOTYPE_SIMD_ALL_TRUE
+#undef SIMD_ALL_TRUE_LIST
+
   void LoadSimd128(Simd128Register dst, const MemOperand& mem,
                    Register scratch);
   void StoreSimd128(Simd128Register src, const MemOperand& mem,
@@ -1272,18 +1331,22 @@
                 Simd128Register scratch1, Simd128Register scratch2);
   void F64x2Max(Simd128Register dst, Simd128Register src1, Simd128Register src2,
                 Simd128Register scratch1, Simd128Register scratch2);
-  void I64x2Abs(Simd128Register dst, Simd128Register src,
-                Simd128Register scratch);
-  void I32x4Abs(Simd128Register dst, Simd128Register src,
-                Simd128Register scratch);
-  void I16x8Abs(Simd128Register dst, Simd128Register src,
-                Simd128Register scratch);
-  void I16x8Neg(Simd128Register dst, Simd128Register src,
-                Simd128Register scratch);
-  void I8x16Abs(Simd128Register dst, Simd128Register src,
-                Simd128Register scratch);
-  void I8x16Neg(Simd128Register dst, Simd128Register src,
-                Simd128Register scratch);
+  void F64x2ConvertLowI32x4U(Simd128Register dst, Simd128Register src,
+                             Register scratch1, Simd128Register scratch2);
+  void I64x2UConvertI32x4Low(Simd128Register dst, Simd128Register src,
+                             Register scratch1, Simd128Register scratch2);
+  void I64x2UConvertI32x4High(Simd128Register dst, Simd128Register src,
+                              Register scratch1, Simd128Register scratch2);
+  void I32x4UConvertI16x8Low(Simd128Register dst, Simd128Register src,
+                             Register scratch1, Simd128Register scratch2);
+  void I32x4UConvertI16x8High(Simd128Register dst, Simd128Register src,
+                              Register scratch1, Simd128Register scratch2);
+  void I16x8UConvertI8x16Low(Simd128Register dst, Simd128Register src,
+                             Register scratch1, Simd128Register scratch2);
+  void I16x8UConvertI8x16High(Simd128Register dst, Simd128Register src,
+                              Register scratch1, Simd128Register scratch2);
+  void V128AnyTrue(Register dst, Simd128Register src, Register scratch1,
+                   Register scratch2, Simd128Register scratch3);
 
  private:
   static const int kSmiShift = kSmiTagSize + kSmiShiftSize;
diff -r -u --color up/v8/src/codegen/ppc/register-ppc.h nw/v8/src/codegen/ppc/register-ppc.h
--- up/v8/src/codegen/ppc/register-ppc.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/ppc/register-ppc.h	2023-01-19 16:46:36.048942949 -0500
@@ -17,20 +17,29 @@
   V(r16) V(r17) V(r18) V(r19) V(r20) V(r21) V(r22) V(r23) \
   V(r24) V(r25) V(r26) V(r27) V(r28) V(r29) V(r30) V(fp)
 
-#if V8_EMBEDDED_CONSTANT_POOL_BOOL
-#define ALLOCATABLE_GENERAL_REGISTERS(V)                  \
+#define ALWAYS_ALLOCATABLE_GENERAL_REGISTERS(V)                  \
   V(r3)  V(r4)  V(r5)  V(r6)  V(r7)                       \
   V(r8)  V(r9)  V(r10) V(r14) V(r15)                      \
   V(r16) V(r17) V(r18) V(r19) V(r20) V(r21) V(r22) V(r23) \
-  V(r24) V(r25) V(r26) V(r27) V(r30)
+  V(r24) V(r25) V(r26) V(r30)
+
+#if V8_EMBEDDED_CONSTANT_POOL_BOOL
+#define MAYBE_ALLOCATEABLE_CONSTANT_POOL_REGISTER(V)
 #else
-#define ALLOCATABLE_GENERAL_REGISTERS(V)                  \
-  V(r3)  V(r4)  V(r5)  V(r6)  V(r7)                       \
-  V(r8)  V(r9)  V(r10) V(r14) V(r15)                      \
-  V(r16) V(r17) V(r18) V(r19) V(r20) V(r21) V(r22) V(r23) \
-  V(r24) V(r25) V(r26) V(r27) V(r28) V(r30)
+#define MAYBE_ALLOCATEABLE_CONSTANT_POOL_REGISTER(V) V(r28)
 #endif
 
+#ifdef V8_COMPRESS_POINTERS_IN_SHARED_CAGE
+#define MAYBE_ALLOCATABLE_CAGE_REGISTERS(V)
+#else
+#define MAYBE_ALLOCATABLE_CAGE_REGISTERS(V)  V(r27)
+#endif
+
+#define ALLOCATABLE_GENERAL_REGISTERS(V)  \
+  ALWAYS_ALLOCATABLE_GENERAL_REGISTERS(V) \
+  MAYBE_ALLOCATEABLE_CONSTANT_POOL_REGISTER(V) \
+  MAYBE_ALLOCATABLE_CAGE_REGISTERS(V)
+
 #define LOW_DOUBLE_REGISTERS(V)                           \
   V(d0)  V(d1)  V(d2)  V(d3)  V(d4)  V(d5)  V(d6)  V(d7)  \
   V(d8)  V(d9)  V(d10) V(d11) V(d12) V(d13) V(d14) V(d15)
@@ -137,6 +146,11 @@
 constexpr Register kConstantPoolRegister = r28;  // Constant pool.
 constexpr Register kRootRegister = r29;          // Roots array pointer.
 constexpr Register cp = r30;                     // JavaScript context pointer.
+#ifdef V8_COMPRESS_POINTERS_IN_SHARED_CAGE
+constexpr Register kPtrComprCageBaseRegister = r27;  // callee save
+#else
+constexpr Register kPtrComprCageBaseRegister = kRootRegister;
+#endif
 
 // Returns the number of padding slots needed for stack pointer alignment.
 constexpr int ArgumentPaddingSlots(int argument_count) {
diff -r -u --color up/v8/src/codegen/riscv/macro-assembler-riscv.cc nw/v8/src/codegen/riscv/macro-assembler-riscv.cc
--- up/v8/src/codegen/riscv/macro-assembler-riscv.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/riscv/macro-assembler-riscv.cc	2023-01-19 16:46:36.048942949 -0500
@@ -4791,12 +4791,6 @@
   DCHECK_EQ(actual_parameter_count, a0);
   DCHECK_EQ(expected_parameter_count, a2);
 
-  // If the expected parameter count is equal to the adaptor sentinel, no need
-  // to push undefined value as arguments.
-  if (kDontAdaptArgumentsSentinel != 0) {
-    Branch(&regular_invoke, eq, expected_parameter_count,
-           Operand(kDontAdaptArgumentsSentinel));
-  }
   // If overapplication or if the actual argument count is equal to the
   // formal parameter count, no need to push extra undefined values.
   SubWord(expected_parameter_count, expected_parameter_count,
@@ -5540,7 +5534,8 @@
     Push(scratch);
   }
 #if V8_ENABLE_WEBASSEMBLY
-  if (type == StackFrame::WASM) Push(kWasmInstanceRegister);
+  if (type == StackFrame::WASM || type == StackFrame::WASM_LIFTOFF_SETUP)
+    Push(kWasmInstanceRegister);
 #endif  // V8_ENABLE_WEBASSEMBLY
 }
 
diff -r -u --color up/v8/src/codegen/s390/macro-assembler-s390.cc nw/v8/src/codegen/s390/macro-assembler-s390.cc
--- up/v8/src/codegen/s390/macro-assembler-s390.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/s390/macro-assembler-s390.cc	2023-01-19 16:46:36.059776279 -0500
@@ -1707,13 +1707,6 @@
   DCHECK_EQ(actual_parameter_count, r2);
   DCHECK_EQ(expected_parameter_count, r4);
 
-  // If the expected parameter count is equal to the adaptor sentinel, no need
-  // to push undefined value as arguments.
-  if (kDontAdaptArgumentsSentinel != 0) {
-    CmpS64(expected_parameter_count, Operand(kDontAdaptArgumentsSentinel));
-    beq(&regular_invoke);
-  }
-
   // If overapplication or if the actual argument count is equal to the
   // formal parameter count, no need to push extra undefined values.
   SubS64(expected_parameter_count, expected_parameter_count,
diff -r -u --color up/v8/src/codegen/tnode.h nw/v8/src/codegen/tnode.h
--- up/v8/src/codegen/tnode.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/tnode.h	2023-01-19 16:46:36.059776279 -0500
@@ -359,10 +359,10 @@
  public:
   template <class U,
             typename std::enable_if<is_subtype<U, T>::value, int>::type = 0>
-  TNode(const TNode<U>& other) : node_(other) {
+  TNode(const TNode<U>& other) V8_NOEXCEPT : node_(other) {
     LazyTemplateChecks();
   }
-  TNode(const TNode& other) : node_(other) { LazyTemplateChecks(); }
+  TNode(const TNode& other) V8_NOEXCEPT : node_(other) { LazyTemplateChecks(); }
   TNode() : TNode(nullptr) {}
 
   TNode operator=(TNode other) {
@@ -375,7 +375,7 @@
 
   static TNode UncheckedCast(compiler::Node* node) { return TNode(node); }
 
- private:
+ protected:
   explicit TNode(compiler::Node* node) : node_(node) { LazyTemplateChecks(); }
   // These checks shouldn't be checked before TNode is actually used.
   void LazyTemplateChecks() {
@@ -385,6 +385,21 @@
   compiler::Node* node_;
 };
 
+// SloppyTNode<T> is a variant of TNode<T> and allows implicit casts from
+// Node*. It is intended for function arguments as long as some call sites
+// still use untyped Node* arguments.
+// TODO(turbofan): Delete this class once transition is finished.
+template <class T>
+class SloppyTNode : public TNode<T> {
+ public:
+  SloppyTNode(compiler::Node* node)  // NOLINT(runtime/explicit)
+      : TNode<T>(node) {}
+  template <class U, typename std::enable_if<is_subtype<U, T>::value,
+                                             int>::type = 0>
+  SloppyTNode(const TNode<U>& other) V8_NOEXCEPT  // NOLINT(runtime/explicit)
+      : TNode<T>(other) {}
+};
+
 }  // namespace internal
 }  // namespace v8
 
diff -r -u --color up/v8/src/codegen/x64/assembler-x64.cc nw/v8/src/codegen/x64/assembler-x64.cc
--- up/v8/src/codegen/x64/assembler-x64.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/x64/assembler-x64.cc	2023-01-19 16:46:36.059776279 -0500
@@ -1018,6 +1018,16 @@
   emitl(static_cast<int32_t>(disp));
 }
 
+void Assembler::near_j(Condition cc, intptr_t disp, RelocInfo::Mode rmode) {
+  EnsureSpace ensure_space(this);
+  // 0000 1111 1000 tttn #32-bit disp.
+  emit(0x0F);
+  emit(0x80 | cc);
+  DCHECK(is_int32(disp));
+  if (!RelocInfo::IsNoInfo(rmode)) RecordRelocInfo(rmode);
+  emitl(static_cast<int32_t>(disp));
+}
+
 void Assembler::call(Register adr) {
   EnsureSpace ensure_space(this);
   // Opcode: FF /2 r64.
diff -r -u --color up/v8/src/codegen/x64/assembler-x64.h nw/v8/src/codegen/x64/assembler-x64.h
--- up/v8/src/codegen/x64/assembler-x64.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/x64/assembler-x64.h	2023-01-19 16:46:36.059776279 -0500
@@ -821,6 +821,7 @@
   static constexpr int kNearJmpInstrSize = 5;
   void near_call(intptr_t disp, RelocInfo::Mode rmode);
   void near_jmp(intptr_t disp, RelocInfo::Mode rmode);
+  void near_j(Condition cc, intptr_t disp, RelocInfo::Mode rmode);
 
   void call(Handle<CodeT> target,
             RelocInfo::Mode rmode = RelocInfo::CODE_TARGET);
diff -r -u --color up/v8/src/codegen/x64/macro-assembler-x64.cc nw/v8/src/codegen/x64/macro-assembler-x64.cc
--- up/v8/src/codegen/x64/macro-assembler-x64.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/x64/macro-assembler-x64.cc	2023-01-19 16:46:36.059776279 -0500
@@ -1897,10 +1897,23 @@
 void MacroAssembler::Cmp(Operand dst, Handle<Object> source) {
   if (source->IsSmi()) {
     Cmp(dst, Smi::cast(*source));
+  } else if (root_array_available_ && options().isolate_independent_code) {
+    // TODO(jgruber,v8:8887): Also consider a root-relative load when generating
+    // non-isolate-independent code. In many cases it might be cheaper than
+    // embedding the relocatable value.
+    // TODO(v8:9706): Fix-it! This load will always uncompress the value
+    // even when we are loading a compressed embedded object.
+    IndirectLoadConstant(kScratchRegister, Handle<HeapObject>::cast(source));
+    cmp_tagged(dst, kScratchRegister);
+  } else if (COMPRESS_POINTERS_BOOL) {
+    EmbeddedObjectIndex index =
+        AddEmbeddedObject(Handle<HeapObject>::cast(source));
+    DCHECK(is_uint32(index));
+    cmpl(dst, Immediate(static_cast<int>(index),
+                        RelocInfo::COMPRESSED_EMBEDDED_OBJECT));
   } else {
     Move(kScratchRegister, Handle<HeapObject>::cast(source),
-         COMPRESS_POINTERS_BOOL ? RelocInfo::COMPRESSED_EMBEDDED_OBJECT
-                                : RelocInfo::FULL_EMBEDDED_OBJECT);
+         RelocInfo::FULL_EMBEDDED_OBJECT);
     cmp_tagged(dst, kScratchRegister);
   }
 }
@@ -2081,11 +2094,26 @@
 
 void TurboAssembler::Jump(Operand op) { jmp(op); }
 
+void TurboAssembler::Jump(Operand op, Condition cc) {
+  Label skip;
+  j(NegateCondition(cc), &skip, Label::kNear);
+  Jump(op);
+  bind(&skip);
+}
+
 void TurboAssembler::Jump(Address destination, RelocInfo::Mode rmode) {
   Move(kScratchRegister, destination, rmode);
   jmp(kScratchRegister);
 }
 
+void TurboAssembler::Jump(Address destination, RelocInfo::Mode rmode,
+                          Condition cc) {
+  Label skip;
+  j(NegateCondition(cc), &skip, Label::kNear);
+  Jump(destination, rmode);
+  bind(&skip);
+}
+
 void TurboAssembler::Jump(Handle<CodeT> code_object, RelocInfo::Mode rmode) {
   DCHECK_IMPLIES(options().isolate_independent_code,
                  Builtins::IsIsolateIndependentBuiltin(*code_object));
@@ -2104,10 +2132,7 @@
                  Builtins::IsIsolateIndependentBuiltin(*code_object));
   Builtin builtin = Builtin::kNoBuiltinId;
   if (isolate()->builtins()->IsBuiltinHandle(code_object, &builtin)) {
-    Label skip;
-    j(NegateCondition(cc), &skip, Label::kNear);
-    TailCallBuiltin(builtin);
-    bind(&skip);
+    TailCallBuiltin(builtin, cc);
     return;
   }
   DCHECK(RelocInfo::IsCodeTarget(rmode));
@@ -2217,6 +2242,27 @@
   }
 }
 
+void TurboAssembler::TailCallBuiltin(Builtin builtin, Condition cc) {
+  ASM_CODE_COMMENT_STRING(this,
+                          CommentForOffHeapTrampoline("tail call", builtin));
+  switch (options().builtin_call_jump_mode) {
+    case BuiltinCallJumpMode::kAbsolute:
+      Jump(BuiltinEntry(builtin), RelocInfo::OFF_HEAP_TARGET, cc);
+      break;
+    case BuiltinCallJumpMode::kPCRelative:
+      near_j(cc, static_cast<intptr_t>(builtin), RelocInfo::NEAR_BUILTIN_ENTRY);
+      break;
+    case BuiltinCallJumpMode::kIndirect:
+      Jump(EntryFromBuiltinAsOperand(builtin), cc);
+      break;
+    case BuiltinCallJumpMode::kForMksnapshot: {
+      Handle<CodeT> code = isolate()->builtins()->code_handle(builtin);
+      j(cc, code, RelocInfo::CODE_TARGET);
+      break;
+    }
+  }
+}
+
 void TurboAssembler::LoadCodeObjectEntry(Register destination,
                                          Register code_object) {
   ASM_CODE_COMMENT(this);
@@ -2297,12 +2343,10 @@
     Register destination, Register code_data_container_object) {
   ASM_CODE_COMMENT(this);
   CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
-  // Given the fields layout we can read the Code reference as a full word.
-  static_assert(!V8_EXTERNAL_CODE_SPACE_BOOL ||
-                (CodeDataContainer::kCodeCageBaseUpper32BitsOffset ==
-                 CodeDataContainer::kCodeOffset + kTaggedSize));
+  // Compute the Code object pointer from the code entry point.
   movq(destination, FieldOperand(code_data_container_object,
-                                 CodeDataContainer::kCodeOffset));
+                                 CodeDataContainer::kCodeEntryPointOffset));
+  subq(destination, Immediate(Code::kHeaderSize - kHeapObjectTag));
 }
 
 void TurboAssembler::CallCodeDataContainerObject(
@@ -2884,7 +2928,7 @@
   }
 
   Label done;
-  InvokePrologue(expected_parameter_count, actual_parameter_count, &done, type);
+  InvokePrologue(expected_parameter_count, actual_parameter_count, type);
   // We call indirectly through the code field in the function to
   // allow recompilation to take effect without changing any of the
   // call sites.
@@ -2949,19 +2993,13 @@
 
 void MacroAssembler::InvokePrologue(Register expected_parameter_count,
                                     Register actual_parameter_count,
-                                    Label* done, InvokeType type) {
+                                    InvokeType type) {
     ASM_CODE_COMMENT(this);
     if (expected_parameter_count == actual_parameter_count) {
       Move(rax, actual_parameter_count);
       return;
     }
     Label regular_invoke;
-    // If the expected parameter count is equal to the adaptor sentinel, no need
-    // to push undefined value as arguments.
-    if (kDontAdaptArgumentsSentinel != 0) {
-      cmpl(expected_parameter_count, Immediate(kDontAdaptArgumentsSentinel));
-      j(equal, &regular_invoke, Label::kFar);
-    }
 
     // If overapplication or if the actual argument count is equal to the
     // formal parameter count, no need to push extra undefined values.
diff -r -u --color up/v8/src/codegen/x64/macro-assembler-x64.h nw/v8/src/codegen/x64/macro-assembler-x64.h
--- up/v8/src/codegen/x64/macro-assembler-x64.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/codegen/x64/macro-assembler-x64.h	2023-01-19 16:46:36.059776279 -0500
@@ -390,6 +390,7 @@
   void CallBuiltinByIndex(Register builtin_index);
   void CallBuiltin(Builtin builtin);
   void TailCallBuiltin(Builtin builtin);
+  void TailCallBuiltin(Builtin builtin, Condition cc);
 
   void LoadCodeObjectEntry(Register destination, Register code_object);
   void CallCodeObject(Register code_object);
@@ -418,8 +419,10 @@
   void CodeDataContainerFromCodeT(Register destination, Register codet);
 
   void Jump(Address destination, RelocInfo::Mode rmode);
+  void Jump(Address destination, RelocInfo::Mode rmode, Condition cc);
   void Jump(const ExternalReference& reference);
   void Jump(Operand op);
+  void Jump(Operand op, Condition cc);
   void Jump(Handle<CodeT> code_object, RelocInfo::Mode rmode);
   void Jump(Handle<CodeT> code_object, RelocInfo::Mode rmode, Condition cc);
 
@@ -943,8 +946,7 @@
  private:
   // Helper functions for generating invokes.
   void InvokePrologue(Register expected_parameter_count,
-                      Register actual_parameter_count, Label* done,
-                      InvokeType type);
+                      Register actual_parameter_count, InvokeType type);
 
   void EnterExitFramePrologue(Register saved_rax_reg,
                               StackFrame::Type frame_type);
diff -r -u --color up/v8/src/common/checks.h nw/v8/src/common/checks.h
--- up/v8/src/common/checks.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/common/checks.h	2023-01-19 16:46:36.059776279 -0500
@@ -15,7 +15,7 @@
 
 #ifdef ENABLE_SLOW_DCHECKS
 #define SLOW_DCHECK(condition) \
-  CHECK(!v8::internal::FLAG_enable_slow_asserts || (condition))
+  CHECK(!v8::internal::v8_flags.enable_slow_asserts || (condition))
 #define SLOW_DCHECK_IMPLIES(lhs, rhs) SLOW_DCHECK(!(lhs) || (rhs))
 #else
 #define SLOW_DCHECK(condition) ((void)0)
diff -r -u --color up/v8/src/common/code-memory-access-inl.h nw/v8/src/common/code-memory-access-inl.h
--- up/v8/src/common/code-memory-access-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/common/code-memory-access-inl.h	2023-01-19 16:46:36.059776279 -0500
@@ -16,13 +16,13 @@
 
 RwxMemoryWriteScope::RwxMemoryWriteScope(const char* comment) {
   DCHECK(is_key_permissions_initialized_for_current_thread());
-  if (!FLAG_jitless) {
+  if (!v8_flags.jitless) {
     SetWritable();
   }
 }
 
 RwxMemoryWriteScope::~RwxMemoryWriteScope() {
-  if (!FLAG_jitless) {
+  if (!v8_flags.jitless) {
     SetExecutable();
   }
 }
diff -r -u --color up/v8/src/common/globals.h nw/v8/src/common/globals.h
--- up/v8/src/common/globals.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/common/globals.h	2023-01-19 16:46:36.059776279 -0500
@@ -227,13 +227,10 @@
 #define V8_EXTERNAL_CODE_SPACE_BOOL true
 // This flag enables the mode when V8 does not create trampoline Code objects
 // for builtins. It should be enough to have only CodeDataContainer objects.
-// TODO(v8:11880): remove the flag one the Code-less builtins mode works.
-#define V8_REMOVE_BUILTINS_CODE_OBJECTS true
 class CodeDataContainer;
 using CodeT = CodeDataContainer;
 #else
 #define V8_EXTERNAL_CODE_SPACE_BOOL false
-#define V8_REMOVE_BUILTINS_CODE_OBJECTS false
 class Code;
 using CodeT = Code;
 #endif
@@ -882,7 +879,6 @@
 class LocalIsolate;
 class MacroAssembler;
 class Map;
-class MapSpace;
 class MarkCompactCollector;
 template <typename T>
 class MaybeHandle;
@@ -983,25 +979,24 @@
 // consecutive.
 enum AllocationSpace {
   RO_SPACE,         // Immortal, immovable and immutable objects,
+  NEW_SPACE,        // Young generation space for regular objects collected
+                    // with Scavenger/MinorMC.
   OLD_SPACE,        // Old generation regular object space.
   CODE_SPACE,       // Old generation code object space, marked executable.
-  MAP_SPACE,        // Old generation map object space, non-movable.
   SHARED_SPACE,     // Space shared between multiple isolates. Optional.
-  NEW_SPACE,        // Young generation space for regular objects collected
-                    // with Scavenger/MinorMC.
+  NEW_LO_SPACE,     // Young generation large object space.
   LO_SPACE,         // Old generation large object space.
   CODE_LO_SPACE,    // Old generation large code object space.
-  NEW_LO_SPACE,     // Young generation large object space.
   SHARED_LO_SPACE,  // Space shared between multiple isolates. Optional.
 
   FIRST_SPACE = RO_SPACE,
   LAST_SPACE = SHARED_LO_SPACE,
-  FIRST_MUTABLE_SPACE = OLD_SPACE,
+  FIRST_MUTABLE_SPACE = NEW_SPACE,
   LAST_MUTABLE_SPACE = SHARED_LO_SPACE,
   FIRST_GROWABLE_PAGED_SPACE = OLD_SPACE,
   LAST_GROWABLE_PAGED_SPACE = SHARED_SPACE,
-  FIRST_SWEEPABLE_SPACE = OLD_SPACE,
-  LAST_SWEEPABLE_SPACE = NEW_SPACE
+  FIRST_SWEEPABLE_SPACE = NEW_SPACE,
+  LAST_SWEEPABLE_SPACE = SHARED_SPACE
 };
 constexpr int kSpaceTagSize = 4;
 static_assert(FIRST_SPACE == 0);
@@ -1010,10 +1005,10 @@
   kYoung,      // Regular object allocated in NEW_SPACE or NEW_LO_SPACE
   kOld,        // Regular object allocated in OLD_SPACE or LO_SPACE
   kCode,       // Code object allocated in CODE_SPACE or CODE_LO_SPACE
-  kMap,        // Map object allocated in MAP_SPACE
+  kMap,        // Map object allocated in OLD_SPACE
   kReadOnly,   // Object allocated in RO_SPACE
   kSharedOld,  // Regular object allocated in OLD_SPACE in the shared heap
-  kSharedMap,  // Map object in MAP_SPACE in the shared heap
+  kSharedMap,  // Map object in OLD_SPACE in the shared heap
 };
 
 // These values are persisted to logs. Entries should not be renumbered and
diff -r -u --color up/v8/src/common/message-template.h nw/v8/src/common/message-template.h
--- up/v8/src/common/message-template.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/common/message-template.h	2023-01-19 16:46:36.059776279 -0500
@@ -31,6 +31,8 @@
   T(ArgumentsDisallowedInInitializerAndStaticBlock,                            \
     "'arguments' is not allowed in class field initializer or static "         \
     "initialization block")                                                    \
+  T(ArrayBufferDetachKeyDoesntMatch,                                           \
+    "Provided key doesn't match [[ArrayBufferDetachKey]]")                     \
   T(ArrayBufferTooShort,                                                       \
     "Derived ArrayBuffer constructor created a buffer which was too small")    \
   T(ArrayBufferSpeciesThis,                                                    \
diff -r -u --color up/v8/src/compiler/access-builder.cc nw/v8/src/compiler/access-builder.cc
--- up/v8/src/compiler/access-builder.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/access-builder.cc	2023-01-19 16:46:36.059776279 -0500
@@ -365,6 +365,22 @@
 }
 
 // static
+FieldAccess AccessBuilder::ForJSArrayBufferByteLength() {
+  FieldAccess access = {kTaggedBase,
+                        JSArrayBuffer::kRawByteLengthOffset,
+                        MaybeHandle<Name>(),
+                        MaybeHandle<Map>(),
+                        TypeCache::Get()->kJSArrayBufferByteLengthType,
+                        MachineType::UintPtr(),
+                        kNoWriteBarrier,
+                        "JSArrayBufferByteLength"};
+#ifdef V8_ENABLE_SANDBOX
+  access.is_bounded_size_access = true;
+#endif
+  return access;
+}
+
+// static
 FieldAccess AccessBuilder::ForJSArrayBufferViewBuffer() {
   FieldAccess access = {kTaggedBase,           JSArrayBufferView::kBufferOffset,
                         MaybeHandle<Name>(),   MaybeHandle<Map>(),
@@ -405,6 +421,19 @@
   return access;
 }
 
+// static
+FieldAccess AccessBuilder::ForJSArrayBufferViewBitField() {
+  FieldAccess access = {kTaggedBase,
+                        JSArrayBufferView::kBitFieldOffset,
+                        MaybeHandle<Name>(),
+                        MaybeHandle<Map>(),
+                        TypeCache::Get()->kUint32,
+                        MachineType::Uint32(),
+                        kNoWriteBarrier,
+                        "JSArrayBufferViewBitField"};
+  return access;
+}
+
 // static
 FieldAccess AccessBuilder::ForJSTypedArrayLength() {
   FieldAccess access = {kTaggedBase,
diff -r -u --color up/v8/src/compiler/access-builder.h nw/v8/src/compiler/access-builder.h
--- up/v8/src/compiler/access-builder.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/access-builder.h	2023-01-19 16:46:36.059776279 -0500
@@ -134,6 +134,9 @@
   // Provides access to JSArrayBuffer::bit_field() field.
   static FieldAccess ForJSArrayBufferBitField();
 
+  // Provides access to JSArrayBuffer::byteLength() field.
+  static FieldAccess ForJSArrayBufferByteLength();
+
   // Provides access to JSArrayBufferView::buffer() field.
   static FieldAccess ForJSArrayBufferViewBuffer();
 
@@ -143,6 +146,9 @@
   // Provides access to JSArrayBufferView::byteOffset() field.
   static FieldAccess ForJSArrayBufferViewByteOffset();
 
+  // Provides access to JSArrayBufferView::bitfield() field
+  static FieldAccess ForJSArrayBufferViewBitField();
+
   // Provides access to JSTypedArray::length() field.
   static FieldAccess ForJSTypedArrayLength();
 
diff -r -u --color up/v8/src/compiler/backend/arm64/code-generator-arm64.cc nw/v8/src/compiler/backend/arm64/code-generator-arm64.cc
--- up/v8/src/compiler/backend/arm64/code-generator-arm64.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/backend/arm64/code-generator-arm64.cc	2023-01-19 16:46:36.070609607 -0500
@@ -2880,6 +2880,18 @@
       __ I64x2AllTrue(i.OutputRegister32(), i.InputSimd128Register(0));
       break;
     }
+    case kArm64V128AnyTrue: {
+      UseScratchRegisterScope scope(tasm());
+      // For AnyTrue, the format does not matter; also, we would like to avoid
+      // an expensive horizontal reduction.
+      VRegister temp = scope.AcquireV(kFormat4S);
+      __ Umaxp(temp, i.InputSimd128Register(0).V4S(),
+               i.InputSimd128Register(0).V4S());
+      __ Fmov(i.OutputRegister64(), temp.D());
+      __ Cmp(i.OutputRegister64(), 0);
+      __ Cset(i.OutputRegister32(), ne);
+      break;
+    }
 #define SIMD_REDUCE_OP_CASE(Op, Instr, format, FORMAT)     \
   case Op: {                                               \
     UseScratchRegisterScope scope(tasm());                 \
@@ -2890,8 +2902,6 @@
     __ Cset(i.OutputRegister32(), ne);                     \
     break;                                                 \
   }
-      // For AnyTrue, the format does not matter.
-      SIMD_REDUCE_OP_CASE(kArm64V128AnyTrue, Umaxv, kFormatS, 4S);
       SIMD_REDUCE_OP_CASE(kArm64I32x4AllTrue, Uminv, kFormatS, 4S);
       SIMD_REDUCE_OP_CASE(kArm64I16x8AllTrue, Uminv, kFormatH, 8H);
       SIMD_REDUCE_OP_CASE(kArm64I8x16AllTrue, Uminv, kFormatB, 16B);
diff -r -u --color up/v8/src/compiler/backend/arm64/instruction-selector-arm64.cc nw/v8/src/compiler/backend/arm64/instruction-selector-arm64.cc
--- up/v8/src/compiler/backend/arm64/instruction-selector-arm64.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/backend/arm64/instruction-selector-arm64.cc	2023-01-19 16:46:36.070609607 -0500
@@ -286,13 +286,15 @@
   return false;
 }
 
+template <typename Matcher>
 bool TryMatchAnyShift(InstructionSelector* selector, Node* node,
                       Node* input_node, InstructionCode* opcode, bool try_ror) {
   Arm64OperandGenerator g(selector);
 
   if (!selector->CanCover(node, input_node)) return false;
   if (input_node->InputCount() != 2) return false;
-  if (!g.IsIntegerConstant(input_node->InputAt(1))) return false;
+  Matcher shift(input_node);
+  if (!shift.right().HasResolvedValue()) return false;
 
   switch (input_node->opcode()) {
     case IrOpcode::kWord32Shl:
@@ -488,16 +490,16 @@
                                &inputs[0], &inputs[1], &opcode)) {
     if (must_commute_cond) cont->Commute();
     input_count += 2;
-  } else if (TryMatchAnyShift(selector, node, right_node, &opcode,
-                              !is_add_sub)) {
+  } else if (TryMatchAnyShift<Matcher>(selector, node, right_node, &opcode,
+                                       !is_add_sub)) {
     Matcher m_shift(right_node);
     inputs[input_count++] = g.UseRegisterOrImmediateZero(left_node);
     inputs[input_count++] = g.UseRegister(m_shift.left().node());
     // We only need at most the last 6 bits of the shift.
     inputs[input_count++] = g.UseImmediate(
         static_cast<int>(m_shift.right().ResolvedValue() & 0x3F));
-  } else if (can_commute && TryMatchAnyShift(selector, node, left_node, &opcode,
-                                             !is_add_sub)) {
+  } else if (can_commute && TryMatchAnyShift<Matcher>(selector, node, left_node,
+                                                      &opcode, !is_add_sub)) {
     if (must_commute_cond) cont->Commute();
     Matcher m_shift(left_node);
     inputs[input_count++] = g.UseRegisterOrImmediateZero(right_node);
Only in nw/v8/src/compiler/backend: bitcast-elider.cc
Only in nw/v8/src/compiler/backend: bitcast-elider.h
diff -r -u --color up/v8/src/compiler/backend/instruction-selector.cc nw/v8/src/compiler/backend/instruction-selector.cc
--- up/v8/src/compiler/backend/instruction-selector.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/backend/instruction-selector.cc	2023-01-19 16:46:36.081442940 -0500
@@ -12,6 +12,7 @@
 #include "src/codegen/tick-counter.h"
 #include "src/common/globals.h"
 #include "src/compiler/backend/instruction-selector-impl.h"
+#include "src/compiler/common-operator.h"
 #include "src/compiler/compiler-source-position-table.h"
 #include "src/compiler/js-heap-broker.h"
 #include "src/compiler/node-properties.h"
@@ -1288,6 +1289,10 @@
     }
     case BasicBlock::kBranch: {
       DCHECK_EQ(IrOpcode::kBranch, input->opcode());
+      // TODO(nicohartmann@): Once all branches have explicitly specified
+      // semantics, we should allow only BranchSemantics::kMachine here.
+      DCHECK_NE(BranchSemantics::kJS,
+                BranchParametersOf(input->op()).semantics());
       BasicBlock* tbranch = block->SuccessorAt(0);
       BasicBlock* fbranch = block->SuccessorAt(1);
       if (tbranch == fbranch) {
diff -r -u --color up/v8/src/compiler/backend/ppc/code-generator-ppc.cc nw/v8/src/compiler/backend/ppc/code-generator-ppc.cc
--- up/v8/src/compiler/backend/ppc/code-generator-ppc.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/backend/ppc/code-generator-ppc.cc	2023-01-19 16:46:36.081442940 -0500
@@ -2247,6 +2247,8 @@
   V(I16x8SubSatS)          \
   V(I16x8AddSatU)          \
   V(I16x8SubSatU)          \
+  V(I16x8SConvertI32x4)    \
+  V(I16x8UConvertI32x4)    \
   V(I8x16Add)              \
   V(I8x16Sub)              \
   V(I8x16MinS)             \
@@ -2259,7 +2261,13 @@
   V(I8x16AddSatS)          \
   V(I8x16SubSatS)          \
   V(I8x16AddSatU)          \
-  V(I8x16SubSatU)
+  V(I8x16SubSatU)          \
+  V(I8x16SConvertI16x8)    \
+  V(I8x16UConvertI16x8)    \
+  V(S128And)               \
+  V(S128Or)                \
+  V(S128Xor)               \
+  V(S128AndNot)
 
 #define EMIT_SIMD_BINOP(name)                                     \
   case kPPC_##name: {                                             \
@@ -2273,15 +2281,31 @@
 
 #define SIMD_BINOP_WITH_SCRATCH_LIST(V) \
   V(F64x2Ne)                            \
+  V(F64x2Pmin)                          \
+  V(F64x2Pmax)                          \
   V(F32x4Ne)                            \
+  V(F32x4Pmin)                          \
+  V(F32x4Pmax)                          \
   V(I64x2Ne)                            \
   V(I64x2GeS)                           \
+  V(I64x2ExtMulLowI32x4S)               \
+  V(I64x2ExtMulHighI32x4S)              \
+  V(I64x2ExtMulLowI32x4U)               \
+  V(I64x2ExtMulHighI32x4U)              \
   V(I32x4Ne)                            \
   V(I32x4GeS)                           \
   V(I32x4GeU)                           \
+  V(I32x4ExtMulLowI16x8S)               \
+  V(I32x4ExtMulHighI16x8S)              \
+  V(I32x4ExtMulLowI16x8U)               \
+  V(I32x4ExtMulHighI16x8U)              \
   V(I16x8Ne)                            \
   V(I16x8GeS)                           \
   V(I16x8GeU)                           \
+  V(I16x8ExtMulLowI8x16S)               \
+  V(I16x8ExtMulHighI8x16S)              \
+  V(I16x8ExtMulLowI8x16U)               \
+  V(I16x8ExtMulHighI8x16U)              \
   V(I8x16Ne)                            \
   V(I8x16GeS)                           \
   V(I8x16GeU)
@@ -2320,22 +2344,30 @@
 #undef EMIT_SIMD_SHIFT
 #undef SIMD_SHIFT_LIST
 
-#define SIMD_UNOP_LIST(V) \
-  V(F64x2Abs)             \
-  V(F64x2Neg)             \
-  V(F64x2Sqrt)            \
-  V(F64x2Ceil)            \
-  V(F64x2Floor)           \
-  V(F64x2Trunc)           \
-  V(F32x4Abs)             \
-  V(F32x4Neg)             \
-  V(I64x2Neg)             \
-  V(I32x4Neg)             \
-  V(F32x4Sqrt)            \
-  V(F32x4Ceil)            \
-  V(F32x4Floor)           \
-  V(F32x4Trunc)           \
-  V(I8x16Popcnt)
+#define SIMD_UNOP_LIST(V)   \
+  V(F64x2Abs)               \
+  V(F64x2Neg)               \
+  V(F64x2Sqrt)              \
+  V(F64x2Ceil)              \
+  V(F64x2Floor)             \
+  V(F64x2Trunc)             \
+  V(F32x4Abs)               \
+  V(F32x4Neg)               \
+  V(I64x2Neg)               \
+  V(I32x4Neg)               \
+  V(F32x4Sqrt)              \
+  V(F32x4Ceil)              \
+  V(F32x4Floor)             \
+  V(F32x4Trunc)             \
+  V(F64x2ConvertLowI32x4S)  \
+  V(I64x2SConvertI32x4Low)  \
+  V(I64x2SConvertI32x4High) \
+  V(I32x4SConvertI16x8Low)  \
+  V(I32x4SConvertI16x8High) \
+  V(I16x8SConvertI8x16Low)  \
+  V(I16x8SConvertI8x16High) \
+  V(I8x16Popcnt)            \
+  V(S128Not)
 
 #define EMIT_SIMD_UNOP(name)                                       \
   case kPPC_##name: {                                              \
@@ -2346,6 +2378,39 @@
 #undef EMIT_SIMD_UNOP
 #undef SIMD_UNOP_LIST
 
+#define SIMD_UNOP_WITH_SCRATCH_LIST(V) \
+  V(I64x2Abs)                          \
+  V(I32x4Abs)                          \
+  V(I16x8Abs)                          \
+  V(I16x8Neg)                          \
+  V(I8x16Abs)                          \
+  V(I8x16Neg)
+
+#define EMIT_SIMD_UNOP_WITH_SCRATCH(name)                         \
+  case kPPC_##name: {                                             \
+    __ name(i.OutputSimd128Register(), i.InputSimd128Register(0), \
+            kScratchSimd128Reg);                                  \
+    break;                                                        \
+  }
+      SIMD_UNOP_WITH_SCRATCH_LIST(EMIT_SIMD_UNOP_WITH_SCRATCH)
+#undef EMIT_SIMD_UNOP_WITH_SCRATCH
+#undef SIMD_UNOP_WITH_SCRATCH_LIST
+
+#define SIMD_ALL_TRUE_LIST(V) \
+  V(I64x2AllTrue)             \
+  V(I32x4AllTrue)             \
+  V(I16x8AllTrue)             \
+  V(I8x16AllTrue)
+#define EMIT_SIMD_ALL_TRUE(name)                                   \
+  case kPPC_##name: {                                              \
+    __ name(i.OutputRegister(), i.InputSimd128Register(0), r0, ip, \
+            kScratchSimd128Reg);                                   \
+    break;                                                         \
+  }
+      SIMD_ALL_TRUE_LIST(EMIT_SIMD_ALL_TRUE)
+#undef EMIT_SIMD_ALL_TRUE
+#undef SIMD_ALL_TRUE_LIST
+
     case kPPC_F64x2Splat: {
       __ F64x2Splat(i.OutputSimd128Register(), i.InputDoubleRegister(0),
                     kScratchReg);
@@ -2472,54 +2537,6 @@
                   kScratchSimd128Reg2);
       break;
     }
-    case kPPC_I64x2Abs: {
-      __ I64x2Abs(i.OutputSimd128Register(), i.InputSimd128Register(0),
-                  kScratchSimd128Reg);
-      break;
-    }
-    case kPPC_I32x4Abs: {
-      __ I32x4Abs(i.OutputSimd128Register(), i.InputSimd128Register(0),
-                  kScratchSimd128Reg);
-      break;
-    }
-    case kPPC_I16x8Abs: {
-      __ I16x8Abs(i.OutputSimd128Register(), i.InputSimd128Register(0),
-                  kScratchSimd128Reg);
-      break;
-    }
-    case kPPC_I16x8Neg: {
-      __ I16x8Neg(i.OutputSimd128Register(), i.InputSimd128Register(0),
-                  kScratchSimd128Reg);
-      break;
-    }
-    case kPPC_I8x16Abs: {
-      __ I8x16Abs(i.OutputSimd128Register(), i.InputSimd128Register(0),
-                  kScratchSimd128Reg);
-      break;
-    }
-    case kPPC_I8x16Neg: {
-      __ I8x16Neg(i.OutputSimd128Register(), i.InputSimd128Register(0),
-                  kScratchSimd128Reg);
-      break;
-    }
-    case kPPC_S128And: {
-      Simd128Register dst = i.OutputSimd128Register();
-      Simd128Register src = i.InputSimd128Register(1);
-      __ vand(dst, i.InputSimd128Register(0), src);
-      break;
-    }
-    case kPPC_S128Or: {
-      Simd128Register dst = i.OutputSimd128Register();
-      Simd128Register src = i.InputSimd128Register(1);
-      __ vor(dst, i.InputSimd128Register(0), src);
-      break;
-    }
-    case kPPC_S128Xor: {
-      Simd128Register dst = i.OutputSimd128Register();
-      Simd128Register src = i.InputSimd128Register(1);
-      __ vxor(dst, i.InputSimd128Register(0), src);
-      break;
-    }
     case kPPC_S128Const: {
       uint64_t low = make_uint64(i.InputUint32(1), i.InputUint32(0));
       uint64_t high = make_uint64(i.InputUint32(3), i.InputUint32(2));
@@ -2538,12 +2555,6 @@
       __ vcmpequb(dst, dst, dst);
       break;
     }
-    case kPPC_S128Not: {
-      Simd128Register dst = i.OutputSimd128Register();
-      Simd128Register src = i.InputSimd128Register(0);
-      __ vnor(dst, src, src);
-      break;
-    }
     case kPPC_S128Select: {
       Simd128Register dst = i.OutputSimd128Register();
       Simd128Register mask = i.InputSimd128Register(0);
@@ -2553,48 +2564,10 @@
       break;
     }
     case kPPC_V128AnyTrue: {
-      Simd128Register src = i.InputSimd128Register(0);
-      Register dst = i.OutputRegister();
-      constexpr uint8_t fxm = 0x2;  // field mask.
-      constexpr int bit_number = 24;
-      __ li(r0, Operand(0));
-      __ li(ip, Operand(1));
-      // Check if both lanes are 0, if so then return false.
-      __ vxor(kScratchSimd128Reg, kScratchSimd128Reg, kScratchSimd128Reg);
-      __ mtcrf(r0, fxm);  // Clear cr6.
-      __ vcmpequd(kScratchSimd128Reg, src, kScratchSimd128Reg, SetRC);
-      __ isel(dst, r0, ip, bit_number);
-      break;
-    }
-#define SIMD_ALL_TRUE(opcode)                                 \
-  Simd128Register src = i.InputSimd128Register(0);            \
-  Register dst = i.OutputRegister();                          \
-  constexpr uint8_t fxm = 0x2; /* field mask. */              \
-  constexpr int bit_number = 24;                              \
-  __ li(r0, Operand(0));                                      \
-  __ li(ip, Operand(1));                                      \
-  /* Check if all lanes > 0, if not then return false.*/      \
-  __ vxor(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero); \
-  __ mtcrf(r0, fxm); /* Clear cr6.*/                          \
-  __ opcode(kSimd128RegZero, src, kSimd128RegZero, SetRC);    \
-  __ isel(dst, ip, r0, bit_number);
-    case kPPC_I64x2AllTrue: {
-      SIMD_ALL_TRUE(vcmpgtud)
-      break;
-    }
-    case kPPC_I32x4AllTrue: {
-      SIMD_ALL_TRUE(vcmpgtuw)
-      break;
-    }
-    case kPPC_I16x8AllTrue: {
-      SIMD_ALL_TRUE(vcmpgtuh)
-      break;
-    }
-    case kPPC_I8x16AllTrue: {
-      SIMD_ALL_TRUE(vcmpgtub)
+      __ V128AnyTrue(i.OutputRegister(), i.InputSimd128Register(0), r0, ip,
+                     kScratchSimd128Reg);
       break;
     }
-#undef SIMD_ALL_TRUE
     case kPPC_I32x4SConvertF32x4: {
       Simd128Register src = i.InputSimd128Register(0);
       // NaN to 0
@@ -2616,115 +2589,46 @@
       __ xvcvuxwsp(i.OutputSimd128Register(), i.InputSimd128Register(0));
       break;
     }
-
-    case kPPC_I64x2SConvertI32x4Low: {
-      __ vupklsw(i.OutputSimd128Register(), i.InputSimd128Register(0));
-      break;
-    }
-    case kPPC_I64x2SConvertI32x4High: {
-      __ vupkhsw(i.OutputSimd128Register(), i.InputSimd128Register(0));
+    case kPPC_F64x2ConvertLowI32x4U: {
+      __ F64x2ConvertLowI32x4U(i.OutputSimd128Register(),
+                               i.InputSimd128Register(0), kScratchReg,
+                               kScratchSimd128Reg);
       break;
     }
     case kPPC_I64x2UConvertI32x4Low: {
-      constexpr int lane_width_in_bytes = 8;
-      __ vupklsw(i.OutputSimd128Register(), i.InputSimd128Register(0));
-      // Zero extend.
-      __ mov(ip, Operand(0xFFFFFFFF));
-      __ mtvsrd(kScratchSimd128Reg, ip);
-      __ vinsertd(kScratchSimd128Reg, kScratchSimd128Reg,
-                  Operand(1 * lane_width_in_bytes));
-      __ vand(i.OutputSimd128Register(), kScratchSimd128Reg,
-              i.OutputSimd128Register());
+      __ I64x2UConvertI32x4Low(i.OutputSimd128Register(),
+                               i.InputSimd128Register(0), kScratchReg,
+                               kScratchSimd128Reg);
       break;
     }
     case kPPC_I64x2UConvertI32x4High: {
-      constexpr int lane_width_in_bytes = 8;
-      __ vupkhsw(i.OutputSimd128Register(), i.InputSimd128Register(0));
-      // Zero extend.
-      __ mov(ip, Operand(0xFFFFFFFF));
-      __ mtvsrd(kScratchSimd128Reg, ip);
-      __ vinsertd(kScratchSimd128Reg, kScratchSimd128Reg,
-                  Operand(1 * lane_width_in_bytes));
-      __ vand(i.OutputSimd128Register(), kScratchSimd128Reg,
-              i.OutputSimd128Register());
-      break;
-    }
-
-    case kPPC_I32x4SConvertI16x8Low: {
-      __ vupklsh(i.OutputSimd128Register(), i.InputSimd128Register(0));
-      break;
-    }
-    case kPPC_I32x4SConvertI16x8High: {
-      __ vupkhsh(i.OutputSimd128Register(), i.InputSimd128Register(0));
+      __ I64x2UConvertI32x4High(i.OutputSimd128Register(),
+                                i.InputSimd128Register(0), kScratchReg,
+                                kScratchSimd128Reg);
       break;
     }
     case kPPC_I32x4UConvertI16x8Low: {
-      __ vupklsh(i.OutputSimd128Register(), i.InputSimd128Register(0));
-      // Zero extend.
-      __ mov(ip, Operand(0xFFFF));
-      __ mtvsrd(kScratchSimd128Reg, ip);
-      __ vspltw(kScratchSimd128Reg, kScratchSimd128Reg, Operand(1));
-      __ vand(i.OutputSimd128Register(), kScratchSimd128Reg,
-              i.OutputSimd128Register());
+      __ I32x4UConvertI16x8Low(i.OutputSimd128Register(),
+                               i.InputSimd128Register(0), kScratchReg,
+                               kScratchSimd128Reg);
       break;
     }
     case kPPC_I32x4UConvertI16x8High: {
-      __ vupkhsh(i.OutputSimd128Register(), i.InputSimd128Register(0));
-      // Zero extend.
-      __ mov(ip, Operand(0xFFFF));
-      __ mtvsrd(kScratchSimd128Reg, ip);
-      __ vspltw(kScratchSimd128Reg, kScratchSimd128Reg, Operand(1));
-      __ vand(i.OutputSimd128Register(), kScratchSimd128Reg,
-              i.OutputSimd128Register());
-      break;
-    }
-
-    case kPPC_I16x8SConvertI8x16Low: {
-      __ vupklsb(i.OutputSimd128Register(), i.InputSimd128Register(0));
-      break;
-    }
-    case kPPC_I16x8SConvertI8x16High: {
-      __ vupkhsb(i.OutputSimd128Register(), i.InputSimd128Register(0));
+      __ I32x4UConvertI16x8High(i.OutputSimd128Register(),
+                                i.InputSimd128Register(0), kScratchReg,
+                                kScratchSimd128Reg);
       break;
     }
     case kPPC_I16x8UConvertI8x16Low: {
-      __ vupklsb(i.OutputSimd128Register(), i.InputSimd128Register(0));
-      // Zero extend.
-      __ li(ip, Operand(0xFF));
-      __ mtvsrd(kScratchSimd128Reg, ip);
-      __ vsplth(kScratchSimd128Reg, kScratchSimd128Reg, Operand(3));
-      __ vand(i.OutputSimd128Register(), kScratchSimd128Reg,
-              i.OutputSimd128Register());
+      __ I16x8UConvertI8x16Low(i.OutputSimd128Register(),
+                               i.InputSimd128Register(0), kScratchReg,
+                               kScratchSimd128Reg);
       break;
     }
     case kPPC_I16x8UConvertI8x16High: {
-      __ vupkhsb(i.OutputSimd128Register(), i.InputSimd128Register(0));
-      // Zero extend.
-      __ li(ip, Operand(0xFF));
-      __ mtvsrd(kScratchSimd128Reg, ip);
-      __ vsplth(kScratchSimd128Reg, kScratchSimd128Reg, Operand(3));
-      __ vand(i.OutputSimd128Register(), kScratchSimd128Reg,
-              i.OutputSimd128Register());
-      break;
-    }
-    case kPPC_I16x8SConvertI32x4: {
-      __ vpkswss(i.OutputSimd128Register(), i.InputSimd128Register(1),
-                 i.InputSimd128Register(0));
-      break;
-    }
-    case kPPC_I16x8UConvertI32x4: {
-      __ vpkswus(i.OutputSimd128Register(), i.InputSimd128Register(1),
-                 i.InputSimd128Register(0));
-      break;
-    }
-    case kPPC_I8x16SConvertI16x8: {
-      __ vpkshss(i.OutputSimd128Register(), i.InputSimd128Register(1),
-                 i.InputSimd128Register(0));
-      break;
-    }
-    case kPPC_I8x16UConvertI16x8: {
-      __ vpkshus(i.OutputSimd128Register(), i.InputSimd128Register(1),
-                 i.InputSimd128Register(0));
+      __ I16x8UConvertI8x16High(i.OutputSimd128Register(),
+                                i.InputSimd128Register(0), kScratchReg,
+                                kScratchSimd128Reg);
       break;
     }
     case kPPC_I8x16Shuffle: {
@@ -2803,12 +2707,6 @@
                 i.InputSimd128Register(1));
       break;
     }
-    case kPPC_S128AndNot: {
-      Simd128Register dst = i.OutputSimd128Register();
-      Simd128Register src = i.InputSimd128Register(0);
-      __ vandc(dst, src, i.InputSimd128Register(1));
-      break;
-    }
     case kPPC_I64x2BitMask: {
       if (CpuFeatures::IsSupported(PPC_10_PLUS)) {
         __ vextractdm(i.OutputRegister(), i.InputSimd128Register(0));
@@ -2870,38 +2768,6 @@
                   i.InputSimd128Register(1), kScratchSimd128Reg);
       break;
     }
-    case kPPC_F32x4Pmin: {
-      Simd128Register dst = i.OutputSimd128Register(),
-                      src0 = i.InputSimd128Register(0),
-                      src1 = i.InputSimd128Register(1);
-      __ xvcmpgtsp(kScratchSimd128Reg, src0, src1);
-      __ vsel(dst, src0, src1, kScratchSimd128Reg);
-      break;
-    }
-    case kPPC_F32x4Pmax: {
-      Simd128Register dst = i.OutputSimd128Register(),
-                      src0 = i.InputSimd128Register(0),
-                      src1 = i.InputSimd128Register(1);
-      __ xvcmpgtsp(kScratchSimd128Reg, src1, src0);
-      __ vsel(dst, src0, src1, kScratchSimd128Reg);
-      break;
-    }
-    case kPPC_F64x2Pmin: {
-      Simd128Register dst = i.OutputSimd128Register(),
-                      src0 = i.InputSimd128Register(0),
-                      src1 = i.InputSimd128Register(1);
-      __ xvcmpgtdp(kScratchSimd128Reg, src0, src1);
-      __ vsel(dst, src0, src1, kScratchSimd128Reg);
-      break;
-    }
-    case kPPC_F64x2Pmax: {
-      Simd128Register dst = i.OutputSimd128Register(),
-                      src0 = i.InputSimd128Register(0),
-                      src1 = i.InputSimd128Register(1);
-      __ xvcmpgtdp(kScratchSimd128Reg, src1, src0);
-      __ vsel(dst, src0, src1, kScratchSimd128Reg);
-      break;
-    }
 #define ASSEMBLE_LOAD_TRANSFORM(scratch, load_instr) \
   AddressingMode mode = kMode_None;                  \
   MemOperand operand = i.MemoryOperand(&mode);       \
@@ -3158,101 +3024,6 @@
                     i.InputSimd128Register(1), kScratchSimd128Reg);
       break;
     }
-#define EXT_MUL(mul_even, mul_odd)                  \
-  Simd128Register dst = i.OutputSimd128Register(),  \
-                  src0 = i.InputSimd128Register(0), \
-                  src1 = i.InputSimd128Register(1); \
-  __ mul_even(kScratchSimd128Reg, src0, src1);      \
-  __ mul_odd(dst, src0, src1);
-    case kPPC_I64x2ExtMulLowI32x4S: {
-      constexpr int lane_width_in_bytes = 8;
-      EXT_MUL(vmulesw, vmulosw)
-      __ vextractd(kScratchSimd128Reg, kScratchSimd128Reg,
-                   Operand(1 * lane_width_in_bytes));
-      __ vextractd(dst, dst, Operand(1 * lane_width_in_bytes));
-      __ vinsertd(dst, kScratchSimd128Reg, Operand(1 * lane_width_in_bytes));
-      break;
-    }
-    case kPPC_I64x2ExtMulHighI32x4S: {
-      constexpr int lane_width_in_bytes = 8;
-      EXT_MUL(vmulesw, vmulosw)
-      __ vinsertd(dst, kScratchSimd128Reg, Operand(1 * lane_width_in_bytes));
-      break;
-    }
-    case kPPC_I64x2ExtMulLowI32x4U: {
-      constexpr int lane_width_in_bytes = 8;
-      EXT_MUL(vmuleuw, vmulouw)
-      __ vextractd(kScratchSimd128Reg, kScratchSimd128Reg,
-                   Operand(1 * lane_width_in_bytes));
-      __ vextractd(dst, dst, Operand(1 * lane_width_in_bytes));
-      __ vinsertd(dst, kScratchSimd128Reg, Operand(1 * lane_width_in_bytes));
-      break;
-    }
-    case kPPC_I64x2ExtMulHighI32x4U: {
-      constexpr int lane_width_in_bytes = 8;
-      EXT_MUL(vmuleuw, vmulouw)
-      __ vinsertd(dst, kScratchSimd128Reg, Operand(1 * lane_width_in_bytes));
-      break;
-    }
-    case kPPC_I32x4ExtMulLowI16x8S: {
-      EXT_MUL(vmulesh, vmulosh)
-      __ vmrglw(dst, dst, kScratchSimd128Reg);
-      break;
-    }
-    case kPPC_I32x4ExtMulHighI16x8S: {
-      EXT_MUL(vmulesh, vmulosh)
-      __ vmrghw(dst, dst, kScratchSimd128Reg);
-      break;
-    }
-    case kPPC_I32x4ExtMulLowI16x8U: {
-      EXT_MUL(vmuleuh, vmulouh)
-      __ vmrglw(dst, dst, kScratchSimd128Reg);
-      break;
-    }
-    case kPPC_I32x4ExtMulHighI16x8U: {
-      EXT_MUL(vmuleuh, vmulouh)
-      __ vmrghw(dst, dst, kScratchSimd128Reg);
-      break;
-    }
-    case kPPC_I16x8ExtMulLowI8x16S: {
-      EXT_MUL(vmulesb, vmulosb)
-      __ vmrglh(dst, dst, kScratchSimd128Reg);
-      break;
-    }
-    case kPPC_I16x8ExtMulHighI8x16S: {
-      EXT_MUL(vmulesb, vmulosb)
-      __ vmrghh(dst, dst, kScratchSimd128Reg);
-      break;
-    }
-    case kPPC_I16x8ExtMulLowI8x16U: {
-      EXT_MUL(vmuleub, vmuloub)
-      __ vmrglh(dst, dst, kScratchSimd128Reg);
-      break;
-    }
-    case kPPC_I16x8ExtMulHighI8x16U: {
-      EXT_MUL(vmuleub, vmuloub)
-      __ vmrghh(dst, dst, kScratchSimd128Reg);
-      break;
-    }
-#undef EXT_MUL
-    case kPPC_F64x2ConvertLowI32x4S: {
-      __ vupklsw(kScratchSimd128Reg, i.InputSimd128Register(0));
-      __ xvcvsxddp(i.OutputSimd128Register(), kScratchSimd128Reg);
-      break;
-    }
-    case kPPC_F64x2ConvertLowI32x4U: {
-      Simd128Register dst = i.OutputSimd128Register();
-      constexpr int lane_width_in_bytes = 8;
-      __ vupklsw(dst, i.InputSimd128Register(0));
-      // Zero extend.
-      __ mov(ip, Operand(0xFFFFFFFF));
-      __ mtvsrd(kScratchSimd128Reg, ip);
-      __ vinsertd(kScratchSimd128Reg, kScratchSimd128Reg,
-                  Operand(1 * lane_width_in_bytes));
-      __ vand(dst, kScratchSimd128Reg, dst);
-      __ xvcvuxddp(dst, dst);
-      break;
-    }
     case kPPC_F64x2PromoteLowF32x4: {
       constexpr int lane_number = 8;
       Simd128Register src = i.InputSimd128Register(0);
diff -r -u --color up/v8/src/compiler/backend/x64/code-generator-x64.cc nw/v8/src/compiler/backend/x64/code-generator-x64.cc
--- up/v8/src/compiler/backend/x64/code-generator-x64.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/backend/x64/code-generator-x64.cc	2023-01-19 16:46:36.092276270 -0500
@@ -5295,7 +5295,22 @@
     case MoveType::kStackToRegister: {
       Operand src = g.ToOperand(source);
       if (source->IsStackSlot()) {
-        __ movq(g.ToRegister(destination), src);
+        MachineRepresentation mr =
+            LocationOperand::cast(source)->representation();
+        const bool is_32_bit = mr == MachineRepresentation::kWord32 ||
+                               mr == MachineRepresentation::kCompressed ||
+                               mr == MachineRepresentation::kCompressedPointer;
+        // TODO(13581): Fix this for other code kinds (see
+        // https://crbug.com/1356461).
+        if (code_kind() == CodeKind::WASM_FUNCTION && is_32_bit) {
+          // When we need only 32 bits, move only 32 bits. Benefits:
+          // - Save a byte here and there (depending on the destination
+          //   register; "movl eax, ..." is smaller than "movq rax, ...").
+          // - Safeguard against accidental decompression of compressed slots.
+          __ movl(g.ToRegister(destination), src);
+        } else {
+          __ movq(g.ToRegister(destination), src);
+        }
       } else {
         DCHECK(source->IsFPStackSlot());
         XMMRegister dst = g.ToDoubleRegister(destination);
diff -r -u --color up/v8/src/compiler/branch-elimination.cc nw/v8/src/compiler/branch-elimination.cc
--- up/v8/src/compiler/branch-elimination.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/branch-elimination.cc	2023-01-19 16:46:36.092276270 -0500
@@ -5,8 +5,10 @@
 #include "src/compiler/branch-elimination.h"
 
 #include "src/base/small-vector.h"
+#include "src/compiler/common-operator.h"
 #include "src/compiler/js-graph.h"
 #include "src/compiler/node-properties.h"
+#include "src/compiler/opcodes.h"
 
 namespace v8 {
 namespace internal {
@@ -81,11 +83,24 @@
   //                                   second_true  second_false
   //
 
+  auto SemanticsOf = [phase = this->phase_](Node* branch) {
+    BranchSemantics semantics = BranchSemantics::kUnspecified;
+    if (branch->opcode() == IrOpcode::kBranch) {
+      semantics = BranchParametersOf(branch->op()).semantics();
+    }
+    if (semantics == BranchSemantics::kUnspecified) {
+      semantics =
+          (phase == kEARLY ? BranchSemantics::kJS : BranchSemantics::kMachine);
+    }
+    return semantics;
+  };
+
   DCHECK_EQ(IrOpcode::kBranch, branch->opcode());
   Node* merge = NodeProperties::GetControlInput(branch);
   if (merge->opcode() != IrOpcode::kMerge) return;
 
   Node* condition = branch->InputAt(0);
+  BranchSemantics semantics = SemanticsOf(branch);
   Graph* graph = jsgraph()->graph();
   base::SmallVector<Node*, 2> phi_inputs;
 
@@ -97,12 +112,14 @@
 
     BranchCondition branch_condition = from_input.LookupState(condition);
     if (!branch_condition.IsSet()) return;
+    if (SemanticsOf(branch_condition.branch) != semantics) return;
     bool condition_value = branch_condition.is_true;
 
-    if (phase_ == kEARLY) {
+    if (semantics == BranchSemantics::kJS) {
       phi_inputs.emplace_back(condition_value ? jsgraph()->TrueConstant()
                                               : jsgraph()->FalseConstant());
     } else {
+      DCHECK_EQ(semantics, BranchSemantics::kMachine);
       phi_inputs.emplace_back(
           condition_value
               ? graph->NewNode(jsgraph()->common()->Int32Constant(1))
@@ -110,11 +127,12 @@
     }
   }
   phi_inputs.emplace_back(merge);
-  Node* new_phi = graph->NewNode(
-      common()->Phi(phase_ == kEARLY ? MachineRepresentation::kTagged
-                                     : MachineRepresentation::kWord32,
-                    input_count),
-      input_count + 1, &phi_inputs.at(0));
+  Node* new_phi =
+      graph->NewNode(common()->Phi(semantics == BranchSemantics::kJS
+                                       ? MachineRepresentation::kTagged
+                                       : MachineRepresentation::kWord32,
+                                   input_count),
+                     input_count + 1, &phi_inputs.at(0));
 
   // Replace the branch condition with the new phi.
   NodeProperties::ReplaceValueInput(branch, new_phi, 0);
diff -r -u --color up/v8/src/compiler/branch-elimination.h nw/v8/src/compiler/branch-elimination.h
--- up/v8/src/compiler/branch-elimination.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/branch-elimination.h	2023-01-19 16:46:36.103109600 -0500
@@ -43,6 +43,8 @@
     : public NON_EXPORTED_BASE(AdvancedReducerWithControlPathState)<
           BranchCondition, kUniqueInstance> {
  public:
+  // TODO(nicohartmann@): Remove {Phase} once all Branch operators have
+  // specified semantics.
   enum Phase {
     kEARLY,
     kLATE,
diff -r -u --color up/v8/src/compiler/bytecode-graph-builder.cc nw/v8/src/compiler/bytecode-graph-builder.cc
--- up/v8/src/compiler/bytecode-graph-builder.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/bytecode-graph-builder.cc	2023-01-19 16:46:36.103109600 -0500
@@ -3224,6 +3224,14 @@
   Node* node = NewNode(javascript()->FindNonDefaultConstructorOrConstruct(),
                        this_function, new_target);
 
+  // The outputs of the JSFindNonDefaultConstructor node are [boolean_thing,
+  // object_thing]. In some cases we reduce it to JSCreate, which has only one
+  // output, [object_thing], and we also fix the poke location in that case.
+  // Here we hard-wire the FrameState for [boolean_thing] to be `true`, which is
+  // the correct value in the case where JSFindNonDefaultConstructor is reduced
+  // to JSCreate.
+  environment()->BindRegister(bytecode_iterator().GetRegisterOperand(2),
+                              jsgraph()->TrueConstant());
   environment()->BindRegistersToProjections(
       bytecode_iterator().GetRegisterOperand(2), node,
       Environment::kAttachFrameState);
diff -r -u --color up/v8/src/compiler/code-assembler.h nw/v8/src/compiler/code-assembler.h
--- up/v8/src/compiler/code-assembler.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/code-assembler.h	2023-01-19 16:46:36.103109600 -0500
@@ -248,6 +248,7 @@
   V(IntPtrMulHigh, IntPtrT, IntPtrT, IntPtrT)                           \
   V(UintPtrMulHigh, UintPtrT, UintPtrT, UintPtrT)                       \
   V(IntPtrDiv, IntPtrT, IntPtrT, IntPtrT)                               \
+  V(IntPtrMod, IntPtrT, IntPtrT, IntPtrT)                               \
   V(IntPtrAddWithOverflow, PAIR_TYPE(IntPtrT, BoolT), IntPtrT, IntPtrT) \
   V(IntPtrSubWithOverflow, PAIR_TYPE(IntPtrT, BoolT), IntPtrT, IntPtrT) \
   V(IntPtrMulWithOverflow, PAIR_TYPE(IntPtrT, BoolT), IntPtrT, IntPtrT) \
diff -r -u --color up/v8/src/compiler/common-operator.cc nw/v8/src/compiler/common-operator.cc
--- up/v8/src/compiler/common-operator.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/common-operator.cc	2023-01-19 16:46:36.103109600 -0500
@@ -29,6 +29,18 @@
 
 namespace compiler {
 
+std::ostream& operator<<(std::ostream& os, BranchSemantics semantics) {
+  switch (semantics) {
+    case BranchSemantics::kJS:
+      return os << "JS";
+    case BranchSemantics::kMachine:
+      return os << "Machine";
+    case BranchSemantics::kUnspecified:
+      return os << "Unspecified";
+  }
+  UNREACHABLE();
+}
+
 std::ostream& operator<<(std::ostream& os, TrapId trap_id) {
   switch (trap_id) {
 #define TRAP_CASE(Name) \
@@ -48,13 +60,33 @@
   return OpParameter<TrapId>(op);
 }
 
+bool operator==(const BranchParameters& lhs, const BranchParameters& rhs) {
+  return lhs.semantics() == rhs.semantics() && lhs.hint() == rhs.hint();
+}
+
+size_t hash_value(const BranchParameters& p) {
+  return base::hash_combine(p.semantics(), p.hint());
+}
+
+std::ostream& operator<<(std::ostream& os, const BranchParameters& p) {
+  return os << p.semantics() << ", " << p.hint();
+}
+
+const BranchParameters& BranchParametersOf(const Operator* const op) {
+  DCHECK_EQ(op->opcode(), IrOpcode::kBranch);
+  return OpParameter<BranchParameters>(op);
+}
+
 BranchHint BranchHintOf(const Operator* const op) {
   switch (op->opcode()) {
     case IrOpcode::kIfValue:
       return IfValueParametersOf(op).hint();
     case IrOpcode::kIfDefault:
-    case IrOpcode::kBranch:
       return OpParameter<BranchHint>(op);
+    // TODO(nicohartmann@): Should remove all uses of BranchHintOf for branches
+    // and replace with BranchParametersOf.
+    case IrOpcode::kBranch:
+      return BranchParametersOf(op).hint();
     default:
       UNREACHABLE();
   }
@@ -434,6 +466,27 @@
   return OpParameter<SLVerifierHintParameters>(op);
 }
 
+V8_EXPORT_PRIVATE bool operator==(const ExitMachineGraphParameters& lhs,
+                                  const ExitMachineGraphParameters& rhs) {
+  return lhs.output_representation() == rhs.output_representation() &&
+         lhs.output_type().Equals(rhs.output_type());
+}
+
+size_t hash_value(const ExitMachineGraphParameters& p) {
+  return base::hash_combine(p.output_representation(), p.output_type());
+}
+
+V8_EXPORT_PRIVATE std::ostream& operator<<(
+    std::ostream& os, const ExitMachineGraphParameters& p) {
+  return os << p.output_representation() << ", " << p.output_type();
+}
+
+const ExitMachineGraphParameters& ExitMachineGraphParametersOf(
+    const Operator* op) {
+  DCHECK_EQ(op->opcode(), IrOpcode::kExitMachineGraph);
+  return OpParameter<ExitMachineGraphParameters>(op);
+}
+
 #define COMMON_CACHED_OP_LIST(V)                          \
   V(Plug, Operator::kNoProperties, 0, 0, 0, 1, 0, 0)      \
   V(Dead, Operator::kFoldable, 0, 0, 0, 1, 1, 1)          \
@@ -453,9 +506,15 @@
 #define CACHED_LOOP_EXIT_VALUE_LIST(V) V(kTagged)
 
 #define CACHED_BRANCH_LIST(V) \
-  V(None)                     \
-  V(True)                     \
-  V(False)
+  V(JS, None)                 \
+  V(JS, True)                 \
+  V(JS, False)                \
+  V(Machine, None)            \
+  V(Machine, True)            \
+  V(Machine, False)           \
+  V(Unspecified, None)        \
+  V(Unspecified, True)        \
+  V(Unspecified, False)
 
 #define CACHED_RETURN_LIST(V) \
   V(1)                        \
@@ -626,17 +685,18 @@
   CACHED_RETURN_LIST(CACHED_RETURN)
 #undef CACHED_RETURN
 
-  template <BranchHint hint>
-  struct BranchOperator final : public Operator1<BranchHint> {
+  template <BranchSemantics semantics, BranchHint hint>
+  struct BranchOperator final : public Operator1<BranchParameters> {
     BranchOperator()
-        : Operator1<BranchHint>(                      // --
+        : Operator1<BranchParameters>(                // --
               IrOpcode::kBranch, Operator::kKontrol,  // opcode
               "Branch",                               // name
               1, 0, 1, 0, 0, 2,                       // counts
-              hint) {}                                // parameter
+              {semantics, hint}) {}                   // parameter
   };
-#define CACHED_BRANCH(Hint) \
-  BranchOperator<BranchHint::k##Hint> kBranch##Hint##Operator;
+#define CACHED_BRANCH(Semantics, Hint)                               \
+  BranchOperator<BranchSemantics::k##Semantics, BranchHint::k##Hint> \
+      kBranch##Semantics##Hint##Operator;
   CACHED_BRANCH_LIST(CACHED_BRANCH)
 #undef CACHED_BRANCH
 
@@ -924,10 +984,12 @@
       0, 0, 1, 0, 0, SLVerifierHintParameters(semantics, override_output_type));
 }
 
-const Operator* CommonOperatorBuilder::Branch(BranchHint hint) {
-#define CACHED_BRANCH(Hint)                 \
-  if (hint == BranchHint::k##Hint) {        \
-    return &cache_.kBranch##Hint##Operator; \
+const Operator* CommonOperatorBuilder::Branch(BranchHint hint,
+                                              BranchSemantics semantics) {
+#define CACHED_BRANCH(Semantics, Hint)                 \
+  if (semantics == BranchSemantics::k##Semantics &&    \
+      hint == BranchHint::k##Hint) {                   \
+    return &cache_.kBranch##Semantics##Hint##Operator; \
   }
   CACHED_BRANCH_LIST(CACHED_BRANCH)
 #undef CACHED_BRANCH
@@ -1309,6 +1371,19 @@
       2, 0, 0, 1, 0, 0);                         // counts
 }
 
+const Operator* CommonOperatorBuilder::EnterMachineGraph(UseInfo use_info) {
+  return zone()->New<Operator1<UseInfo>>(IrOpcode::kEnterMachineGraph,
+                                         Operator::kPure, "EnterMachineGraph",
+                                         1, 0, 0, 1, 0, 0, use_info);
+}
+
+const Operator* CommonOperatorBuilder::ExitMachineGraph(
+    MachineRepresentation output_representation, Type output_type) {
+  return zone()->New<Operator1<ExitMachineGraphParameters>>(
+      IrOpcode::kExitMachineGraph, Operator::kPure, "ExitMachineGraph", 1, 0, 0,
+      1, 0, 0, ExitMachineGraphParameters{output_representation, output_type});
+}
+
 const Operator* CommonOperatorBuilder::EffectPhi(int effect_input_count) {
   DCHECK_LT(0, effect_input_count);  // Disallow empty effect phis.
   switch (effect_input_count) {
diff -r -u --color up/v8/src/compiler/common-operator.h nw/v8/src/compiler/common-operator.h
--- up/v8/src/compiler/common-operator.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/common-operator.h	2023-01-19 16:46:36.103109600 -0500
@@ -13,6 +13,7 @@
 #include "src/compiler/frame-states.h"
 #include "src/compiler/linkage.h"
 #include "src/compiler/node-properties.h"
+#include "src/compiler/use-info.h"
 #include "src/deoptimizer/deoptimize-reason.h"
 #include "src/zone/zone-containers.h"
 
@@ -35,7 +36,11 @@
 // (machine branch semantics). Some passes are applied both before and after
 // SimplifiedLowering, and use the BranchSemantics enum to know how branches
 // should be treated.
-enum class BranchSemantics { kJS, kMachine };
+// TODO(nicohartmann@): Need to remove BranchSemantics::kUnspecified once all
+// branch uses have been updated.
+enum class BranchSemantics { kJS, kMachine, kUnspecified };
+
+V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream&, BranchSemantics);
 
 inline BranchHint NegateBranchHint(BranchHint hint) {
   switch (hint) {
@@ -62,6 +67,32 @@
 
 TrapId TrapIdOf(const Operator* const op);
 
+class BranchParameters final {
+ public:
+  BranchParameters(BranchSemantics semantics, BranchHint hint)
+      : semantics_(semantics), hint_(hint) {}
+
+  BranchSemantics semantics() const { return semantics_; }
+  BranchHint hint() const { return hint_; }
+
+ private:
+  const BranchSemantics semantics_;
+  const BranchHint hint_;
+};
+
+bool operator==(const BranchParameters& lhs, const BranchParameters& rhs);
+inline bool operator!=(const BranchParameters& lhs,
+                       const BranchParameters& rhs) {
+  return !(lhs == rhs);
+}
+
+size_t hash_value(const BranchParameters& p);
+
+std::ostream& operator<<(std::ostream&, const BranchParameters& p);
+
+V8_EXPORT_PRIVATE const BranchParameters& BranchParametersOf(
+    const Operator* const) V8_WARN_UNUSED_RESULT;
+
 V8_EXPORT_PRIVATE BranchHint BranchHintOf(const Operator* const)
     V8_WARN_UNUSED_RESULT;
 
@@ -439,6 +470,35 @@
 V8_EXPORT_PRIVATE const SLVerifierHintParameters& SLVerifierHintParametersOf(
     const Operator* op) V8_WARN_UNUSED_RESULT;
 
+class ExitMachineGraphParameters final {
+ public:
+  ExitMachineGraphParameters(MachineRepresentation output_representation,
+                             Type output_type)
+      : output_representation_(output_representation),
+        output_type_(output_type) {}
+
+  MachineRepresentation output_representation() const {
+    return output_representation_;
+  }
+
+  const Type& output_type() const { return output_type_; }
+
+ private:
+  const MachineRepresentation output_representation_;
+  const Type output_type_;
+};
+
+V8_EXPORT_PRIVATE bool operator==(const ExitMachineGraphParameters& lhs,
+                                  const ExitMachineGraphParameters& rhs);
+
+size_t hash_value(const ExitMachineGraphParameters& p);
+
+V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
+                                           const ExitMachineGraphParameters& p);
+
+V8_EXPORT_PRIVATE const ExitMachineGraphParameters&
+ExitMachineGraphParametersOf(const Operator* op) V8_WARN_UNUSED_RESULT;
+
 // Interface for building common operators that can be used at any level of IR,
 // including JavaScript, mid-level, and low-level.
 class V8_EXPORT_PRIVATE CommonOperatorBuilder final
@@ -464,7 +524,11 @@
       const Operator* semantics,
       const base::Optional<Type>& override_output_type);
   const Operator* End(size_t control_input_count);
-  const Operator* Branch(BranchHint = BranchHint::kNone);
+  // TODO(nicohartmann@): Remove the default argument for {semantics} once all
+  // uses are updated.
+  const Operator* Branch(
+      BranchHint = BranchHint::kNone,
+      BranchSemantics semantics = BranchSemantics::kUnspecified);
   const Operator* IfTrue();
   const Operator* IfFalse();
   const Operator* IfSuccess();
@@ -537,6 +601,9 @@
   const Operator* Retain();
   const Operator* TypeGuard(Type type);
   const Operator* FoldConstant();
+  const Operator* EnterMachineGraph(UseInfo use_info);
+  const Operator* ExitMachineGraph(MachineRepresentation output_representation,
+                                   Type output_type);
 
   // Constructs a new merge or phi operator with the same opcode as {op}, but
   // with {size} inputs.
diff -r -u --color up/v8/src/compiler/diamond.h nw/v8/src/compiler/diamond.h
--- up/v8/src/compiler/diamond.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/diamond.h	2023-01-19 16:46:36.103109600 -0500
@@ -23,10 +23,12 @@
   Node* merge;
 
   Diamond(Graph* g, CommonOperatorBuilder* b, Node* cond,
-          BranchHint hint = BranchHint::kNone) {
+          BranchHint hint = BranchHint::kNone,
+          BranchSemantics semantics = BranchSemantics::kUnspecified) {
     graph = g;
     common = b;
-    branch = graph->NewNode(common->Branch(hint), cond, graph->start());
+    branch =
+        graph->NewNode(common->Branch(hint, semantics), cond, graph->start());
     if_true = graph->NewNode(common->IfTrue(), branch);
     if_false = graph->NewNode(common->IfFalse(), branch);
     merge = graph->NewNode(common->Merge(2), if_true, if_false);
diff -r -u --color up/v8/src/compiler/effect-control-linearizer.cc nw/v8/src/compiler/effect-control-linearizer.cc
--- up/v8/src/compiler/effect-control-linearizer.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/effect-control-linearizer.cc	2023-01-19 16:46:36.103109600 -0500
@@ -84,7 +84,7 @@
   Node* LowerCheckReceiverOrNullOrUndefined(Node* node, Node* frame_state);
   Node* LowerCheckString(Node* node, Node* frame_state);
   Node* LowerCheckBigInt(Node* node, Node* frame_state);
-  Node* LowerCheckBigInt64(Node* node, Node* frame_state);
+  Node* LowerCheckedBigIntToBigInt64(Node* node, Node* frame_state);
   Node* LowerCheckSymbol(Node* node, Node* frame_state);
   void LowerCheckIf(Node* node, Node* frame_state);
   Node* LowerCheckedInt32Add(Node* node, Node* frame_state);
@@ -94,7 +94,11 @@
   Node* LowerCheckedUint32Div(Node* node, Node* frame_state);
   Node* LowerCheckedUint32Mod(Node* node, Node* frame_state);
   Node* LowerCheckedInt32Mul(Node* node, Node* frame_state);
-  Node* LowerCheckedBigInt64Add(Node* node, Node* frame_state);
+  Node* LowerCheckedInt64Add(Node* node, Node* frame_state);
+  Node* LowerCheckedInt64Sub(Node* node, Node* frame_state);
+  Node* LowerCheckedInt64Mul(Node* node, Node* frame_state);
+  Node* LowerCheckedInt64Div(Node* node, Node* frame_state);
+  Node* LowerCheckedInt64Mod(Node* node, Node* frame_state);
   Node* LowerCheckedInt32ToTaggedSigned(Node* node, Node* frame_state);
   Node* LowerCheckedInt64ToInt32(Node* node, Node* frame_state);
   Node* LowerCheckedInt64ToTaggedSigned(Node* node, Node* frame_state);
@@ -179,6 +183,7 @@
   Node* LowerBigIntSubtract(Node* node, Node* frame_state);
   Node* LowerBigIntMultiply(Node* node, Node* frame_state);
   Node* LowerBigIntDivide(Node* node, Node* frame_state);
+  Node* LowerBigIntModulus(Node* node, Node* frame_state);
   Node* LowerBigIntBitwiseAnd(Node* node, Node* frame_state);
   Node* LowerBigIntNegate(Node* node);
   Node* LowerCheckFloat64Hole(Node* node, Node* frame_state);
@@ -237,6 +242,7 @@
   Node* LowerFoldConstant(Node* node);
   Node* LowerConvertReceiver(Node* node);
   Node* LowerDateNow(Node* node);
+  Node* LowerDoubleArrayMinMax(Node* node);
 
   // Lowering of optional operators.
   Maybe<Node*> LowerFloat64RoundUp(Node* node);
@@ -1009,8 +1015,8 @@
     case IrOpcode::kCheckBigInt:
       result = LowerCheckBigInt(node, frame_state);
       break;
-    case IrOpcode::kCheckBigInt64:
-      result = LowerCheckBigInt64(node, frame_state);
+    case IrOpcode::kCheckedBigIntToBigInt64:
+      result = LowerCheckedBigIntToBigInt64(node, frame_state);
       break;
     case IrOpcode::kCheckInternalizedString:
       result = LowerCheckInternalizedString(node, frame_state);
@@ -1039,8 +1045,20 @@
     case IrOpcode::kCheckedInt32Mul:
       result = LowerCheckedInt32Mul(node, frame_state);
       break;
-    case IrOpcode::kCheckedBigInt64Add:
-      result = LowerCheckedBigInt64Add(node, frame_state);
+    case IrOpcode::kCheckedInt64Add:
+      result = LowerCheckedInt64Add(node, frame_state);
+      break;
+    case IrOpcode::kCheckedInt64Sub:
+      result = LowerCheckedInt64Sub(node, frame_state);
+      break;
+    case IrOpcode::kCheckedInt64Mul:
+      result = LowerCheckedInt64Mul(node, frame_state);
+      break;
+    case IrOpcode::kCheckedInt64Div:
+      result = LowerCheckedInt64Div(node, frame_state);
+      break;
+    case IrOpcode::kCheckedInt64Mod:
+      result = LowerCheckedInt64Mod(node, frame_state);
       break;
     case IrOpcode::kCheckedInt32ToTaggedSigned:
       result = LowerCheckedInt32ToTaggedSigned(node, frame_state);
@@ -1259,6 +1277,9 @@
     case IrOpcode::kBigIntDivide:
       result = LowerBigIntDivide(node, frame_state);
       break;
+    case IrOpcode::kBigIntModulus:
+      result = LowerBigIntModulus(node, frame_state);
+      break;
     case IrOpcode::kBigIntBitwiseAnd:
       result = LowerBigIntBitwiseAnd(node, frame_state);
       break;
@@ -1405,6 +1426,10 @@
     case IrOpcode::kFoldConstant:
       result = LowerFoldConstant(node);
       break;
+    case IrOpcode::kDoubleArrayMax:
+    case IrOpcode::kDoubleArrayMin:
+      result = LowerDoubleArrayMinMax(node);
+      break;
     default:
       return false;
   }
@@ -2287,19 +2312,13 @@
   //   if rhs <= 0 then
   //     rhs = -rhs
   //     deopt if rhs == 0
-  //   let msk = rhs - 1 in
   //   if lhs < 0 then
   //     let lhs_abs = -lsh in
-  //     let res = if rhs & msk == 0 then
-  //                 lhs_abs & msk
-  //               else
-  //                 lhs_abs % rhs in
-  //     if lhs < 0 then
-  //       deopt if res == 0
-  //       -res
-  //     else
-  //       res
+  //     let res = lhs_abs % rhs in
+  //     deopt if res == 0
+  //     -res
   //   else
+  //     let msk = rhs - 1 in
   //     if rhs & msk == 0 then
   //       lhs & msk
   //     else
@@ -2937,28 +2956,17 @@
   return value;
 }
 
-Node* EffectControlLinearizer::LowerCheckBigInt64(Node* node,
-                                                  Node* frame_state) {
+Node* EffectControlLinearizer::LowerCheckedBigIntToBigInt64(Node* node,
+                                                            Node* frame_state) {
   DCHECK(machine()->Is64());
 
   auto done = __ MakeLabel();
   auto if_not_zero = __ MakeLabel();
+  auto if_may_be_out_of_range = __ MakeDeferredLabel();
 
   Node* value = node->InputAt(0);
   const CheckParameters& params = CheckParametersOf(node->op());
 
-  // Check for Smi.
-  Node* smi_check = ObjectIsSmi(value);
-  __ DeoptimizeIf(DeoptimizeReason::kSmi, params.feedback(), smi_check,
-                  frame_state);
-
-  // Check for BigInt.
-  Node* value_map = __ LoadField(AccessBuilder::ForMap(), value);
-  Node* bi_check = __ TaggedEqual(value_map, __ BigIntMapConstant());
-  __ DeoptimizeIfNot(DeoptimizeReason::kWrongInstanceType, params.feedback(),
-                     bi_check, frame_state);
-
-  // Check for BigInt64.
   Node* bitfield = __ LoadField(AccessBuilder::ForBigIntBitfield(), value);
   __ GotoIfNot(__ Word32Equal(bitfield, __ Int32Constant(0)), &if_not_zero);
   __ Goto(&done);
@@ -2969,19 +2977,32 @@
     Node* length =
         __ Word32And(bitfield, __ Int32Constant(BigInt::LengthBits::kMask));
     __ DeoptimizeIfNot(
-        DeoptimizeReason::kWrongInstanceType, params.feedback(),
+        DeoptimizeReason::kNotABigInt64, params.feedback(),
         __ Word32Equal(length, __ Int32Constant(uint32_t{1}
                                                 << BigInt::LengthBits::kShift)),
         frame_state);
 
     Node* lsd =
         __ LoadField(AccessBuilder::ForBigIntLeastSignificantDigit64(), value);
-    // Accepted small BigInts are in the range [-2^63 + 1, 2^63 - 1].
-    // Excluding -2^63 from the range makes the check simpler and faster.
-    Node* bi64_check = __ Uint64LessThanOrEqual(
+
+    Node* magnitude_check = __ Uint64LessThanOrEqual(
         lsd, __ Int64Constant(std::numeric_limits<int64_t>::max()));
-    __ DeoptimizeIfNot(DeoptimizeReason::kWrongInstanceType, params.feedback(),
-                       bi64_check, frame_state);
+    __ Branch(magnitude_check, &done, &if_may_be_out_of_range);
+
+    __ Bind(&if_may_be_out_of_range);
+    Node* sign =
+        __ Word32And(bitfield, __ Int32Constant(BigInt::SignBits::kMask));
+
+    Node* sign_check =
+        __ Word32Equal(sign, __ Int32Constant(BigInt::SignBits::kMask));
+    __ DeoptimizeIfNot(DeoptimizeReason::kNotABigInt64, params.feedback(),
+                       sign_check, frame_state);
+
+    Node* min_check = __ Word64Equal(
+        lsd, __ Int64Constant(std::numeric_limits<int64_t>::min()));
+    __ DeoptimizeIfNot(DeoptimizeReason::kNotABigInt64, params.feedback(),
+                       min_check, frame_state);
+
     __ Goto(&done);
   }
 
@@ -2989,8 +3010,8 @@
   return value;
 }
 
-Node* EffectControlLinearizer::LowerCheckedBigInt64Add(Node* node,
-                                                       Node* frame_state) {
+Node* EffectControlLinearizer::LowerCheckedInt64Add(Node* node,
+                                                    Node* frame_state) {
   DCHECK(machine()->Is64());
 
   Node* lhs = node->InputAt(0);
@@ -3004,6 +3025,90 @@
   return __ Projection(0, value);
 }
 
+Node* EffectControlLinearizer::LowerCheckedInt64Sub(Node* node,
+                                                    Node* frame_state) {
+  DCHECK(machine()->Is64());
+
+  Node* lhs = node->InputAt(0);
+  Node* rhs = node->InputAt(1);
+
+  Node* value = __ Int64SubWithOverflow(lhs, rhs);
+
+  Node* check = __ Projection(1, value);
+  __ DeoptimizeIf(DeoptimizeReason::kOverflow, FeedbackSource(), check,
+                  frame_state);
+  return __ Projection(0, value);
+}
+
+Node* EffectControlLinearizer::LowerCheckedInt64Mul(Node* node,
+                                                    Node* frame_state) {
+  DCHECK(machine()->Is64());
+
+  Node* lhs = node->InputAt(0);
+  Node* rhs = node->InputAt(1);
+
+  Node* value = __ Int64MulWithOverflow(lhs, rhs);
+
+  Node* check = __ Projection(1, value);
+  __ DeoptimizeIf(DeoptimizeReason::kOverflow, FeedbackSource(), check,
+                  frame_state);
+  return __ Projection(0, value);
+}
+
+Node* EffectControlLinearizer::LowerCheckedInt64Div(Node* node,
+                                                    Node* frame_state) {
+  DCHECK(machine()->Is64());
+
+  auto division = __ MakeLabel();
+
+  Node* lhs = node->InputAt(0);
+  Node* rhs = node->InputAt(1);
+
+  Node* check_rhs_zero = __ Word64Equal(rhs, __ Int64Constant(0));
+  __ DeoptimizeIf(DeoptimizeReason::kDivisionByZero, FeedbackSource(),
+                  check_rhs_zero, frame_state);
+
+  __ GotoIfNot(__ Word64Equal(
+                   lhs, __ Int64Constant(std::numeric_limits<int64_t>::min())),
+               &division);
+  Node* check_overflow = __ Word64Equal(rhs, __ Int64Constant(-1));
+  __ DeoptimizeIf(DeoptimizeReason::kOverflow, FeedbackSource(), check_overflow,
+                  frame_state);
+  __ Goto(&division);
+
+  __ Bind(&division);
+  Node* value = __ Int64Div(lhs, rhs);
+  return value;
+}
+
+Node* EffectControlLinearizer::LowerCheckedInt64Mod(Node* node,
+                                                    Node* frame_state) {
+  DCHECK(machine()->Is64());
+
+  auto modulo_op = __ MakeLabel();
+
+  Node* lhs = node->InputAt(0);
+  Node* rhs = node->InputAt(1);
+
+  Node* check_rhs_zero = __ Word64Equal(rhs, __ Int64Constant(0));
+  __ DeoptimizeIf(DeoptimizeReason::kDivisionByZero, FeedbackSource(),
+                  check_rhs_zero, frame_state);
+
+  // While the mod-result cannot overflow, the underlying instruction is
+  // `idiv` and will trap when the accompanying div-result overflows.
+  __ GotoIfNot(__ Word64Equal(
+                   lhs, __ Int64Constant(std::numeric_limits<int64_t>::min())),
+               &modulo_op);
+  Node* check_overflow = __ Word64Equal(rhs, __ Int64Constant(-1));
+  __ DeoptimizeIf(DeoptimizeReason::kOverflow, FeedbackSource(), check_overflow,
+                  frame_state);
+  __ Goto(&modulo_op);
+
+  __ Bind(&modulo_op);
+  Node* value = __ Int64Mod(lhs, rhs);
+  return value;
+}
+
 Node* EffectControlLinearizer::LowerChangeInt64ToBigInt(Node* node) {
   DCHECK(machine()->Is64());
 
@@ -4533,6 +4638,50 @@
   return value;
 }
 
+Node* EffectControlLinearizer::LowerBigIntModulus(Node* node,
+                                                  Node* frame_state) {
+  Node* lhs = node->InputAt(0);
+  Node* rhs = node->InputAt(1);
+
+  Callable const callable =
+      Builtins::CallableFor(isolate(), Builtin::kBigIntModulusNoThrow);
+  auto call_descriptor = Linkage::GetStubCallDescriptor(
+      graph()->zone(), callable.descriptor(),
+      callable.descriptor().GetStackParameterCount(), CallDescriptor::kNoFlags,
+      Operator::kFoldable | Operator::kNoThrow);
+  Node* value = __ Call(call_descriptor, __ HeapConstant(callable.code()), lhs,
+                        rhs, __ NoContextConstant());
+
+  auto if_termreq = __ MakeDeferredLabel();
+  auto done = __ MakeLabel();
+
+  // Check for exception sentinel
+  // - Smi 0 is returned to signal BigIntDivZero
+  // - Smi 1 is returned to signal TerminationRequested
+  __ GotoIf(__ TaggedEqual(value, __ SmiConstant(1)), &if_termreq);
+
+  __ DeoptimizeIf(DeoptimizeReason::kDivisionByZero, FeedbackSource{},
+                  ObjectIsSmi(value), frame_state);
+
+  __ Goto(&done);
+
+  __ Bind(&if_termreq);
+  {
+    Runtime::FunctionId id = Runtime::kTerminateExecution;
+    auto call_descriptor = Linkage::GetRuntimeCallDescriptor(
+        graph()->zone(), id, 0, Operator::kNoDeopt,
+        CallDescriptor::kNeedsFrameState);
+    __ Call(call_descriptor, __ CEntryStubConstant(1),
+            __ ExternalConstant(ExternalReference::Create(id)),
+            __ Int32Constant(0), __ NoContextConstant(), frame_state);
+    __ Goto(&done);
+  }
+
+  __ Bind(&done);
+
+  return value;
+}
+
 Node* EffectControlLinearizer::LowerBigIntBitwiseAnd(Node* node,
                                                      Node* frame_state) {
   Node* lhs = node->InputAt(0);
@@ -5044,14 +5193,14 @@
   Node* buffer_is_not_detached = __ Word32Equal(
       __ Word32And(buffer_bit_field,
                    __ Int32Constant(JSArrayBuffer::WasDetachedBit::kMask)),
-      __ ZeroConstant());
+      __ Int32Constant(0));
   __ GotoIfNot(buffer_is_not_detached, bailout);
 
   // Go to the slow path if the {buffer} is shared.
   Node* buffer_is_not_shared = __ Word32Equal(
       __ Word32And(buffer_bit_field,
                    __ Int32Constant(JSArrayBuffer::IsSharedBit::kMask)),
-      __ ZeroConstant());
+      __ Int32Constant(0));
   __ GotoIfNot(buffer_is_not_shared, bailout);
 
   // Unpack the store and length, and store them to a struct
@@ -6197,6 +6346,48 @@
   return constant;
 }
 
+Node* EffectControlLinearizer::LowerDoubleArrayMinMax(Node* node) {
+  DCHECK(node->opcode() == IrOpcode::kDoubleArrayMin ||
+         node->opcode() == IrOpcode::kDoubleArrayMax);
+
+  bool is_max = node->opcode() == IrOpcode::kDoubleArrayMax;
+  Node* arguments_list = node->InputAt(0);
+
+  // Iterate the elements and find the result.
+  Node* empty_value = is_max ? __ Float64Constant(-V8_INFINITY)
+                             : __ Float64Constant(V8_INFINITY);
+  Node* array_length = __ LoadField(
+      AccessBuilder::ForJSArrayLength(ElementsKind::PACKED_DOUBLE_ELEMENTS),
+      arguments_list);
+  array_length = ChangeSmiToIntPtr(array_length);
+  Node* elements =
+      __ LoadField(AccessBuilder::ForJSObjectElements(), arguments_list);
+
+  auto loop = __ MakeLoopLabel(MachineType::PointerRepresentation(),
+                               MachineRepresentation::kFloat64);
+  auto done = __ MakeLabel(MachineRepresentation::kFloat64);
+
+  __ Goto(&loop, __ IntPtrConstant(0), empty_value);
+  __ Bind(&loop);
+  {
+    Node* index = loop.PhiAt(0);
+    Node* accumulator = loop.PhiAt(1);
+
+    Node* check = __ UintLessThan(index, array_length);
+    __ GotoIfNot(check, &done, accumulator);
+
+    Node* element = __ LoadElement(AccessBuilder::ForFixedDoubleArrayElement(),
+                                   elements, index);
+    __ Goto(&loop, __ IntAdd(index, __ IntPtrConstant(1)),
+            is_max ? __ Float64Max(accumulator, element)
+                   : __ Float64Min(accumulator, element));
+  }
+
+  __ Bind(&done);
+  return ChangeFloat64ToTagged(done.PhiAt(0),
+                               CheckForMinusZeroMode::kCheckForMinusZero);
+}
+
 Node* EffectControlLinearizer::LowerConvertReceiver(Node* node) {
   ConvertReceiverMode const mode = ConvertReceiverModeOf(node->op());
   Node* value = node->InputAt(0);
@@ -6829,7 +7020,8 @@
                             SourcePositionTable* source_positions,
                             NodeOriginTable* node_origins,
                             JSHeapBroker* broker) {
-  JSGraphAssembler graph_assembler_(graph, temp_zone);
+  JSGraphAssembler graph_assembler_(graph, temp_zone,
+                                    BranchSemantics::kMachine);
   EffectControlLinearizer linearizer(graph, schedule, &graph_assembler_,
                                      temp_zone, source_positions, node_origins,
                                      MaintainSchedule::kDiscard, broker);
diff -r -u --color up/v8/src/compiler/fast-api-calls.cc nw/v8/src/compiler/fast-api-calls.cc
--- up/v8/src/compiler/fast-api-calls.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/fast-api-calls.cc	2023-01-19 16:46:36.103109600 -0500
@@ -4,6 +4,7 @@
 
 #include "src/compiler/fast-api-calls.h"
 
+#include "src/codegen/cpu-features.h"
 #include "src/compiler/globals.h"
 
 namespace v8 {
@@ -84,6 +85,13 @@
 bool CanOptimizeFastSignature(const CFunctionInfo* c_signature) {
   USE(c_signature);
 
+#if defined(V8_OS_MACOS) && defined(V8_TARGET_ARCH_ARM64)
+  // On MacArm64 hardware we don't support passing of arguments on the stack.
+  if (c_signature->ArgumentCount() > 8) {
+    return false;
+  }
+#endif  // defined(V8_OS_MACOS) && defined(V8_TARGET_ARCH_ARM64)
+
 #ifndef V8_ENABLE_FP_PARAMS_IN_C_LINKAGE
   if (c_signature->ReturnInfo().GetType() == CTypeInfo::Type::kFloat32 ||
       c_signature->ReturnInfo().GetType() == CTypeInfo::Type::kFloat64) {
@@ -101,6 +109,14 @@
   for (unsigned int i = 0; i < c_signature->ArgumentCount(); ++i) {
     USE(i);
 
+#ifdef V8_TARGET_ARCH_X64
+    // Clamp lowering in EffectControlLinearizer uses rounding.
+    uint8_t flags = uint8_t(c_signature->ArgumentInfo(i).GetFlags());
+    if (flags & uint8_t(CTypeInfo::Flags::kClampBit)) {
+      return CpuFeatures::IsSupported(SSE4_2);
+    }
+#endif  // V8_TARGET_ARCH_X64
+
 #ifndef V8_ENABLE_FP_PARAMS_IN_C_LINKAGE
     if (c_signature->ArgumentInfo(i).GetType() == CTypeInfo::Type::kFloat32 ||
         c_signature->ArgumentInfo(i).GetType() == CTypeInfo::Type::kFloat64) {
diff -r -u --color up/v8/src/compiler/frame-states.cc nw/v8/src/compiler/frame-states.cc
--- up/v8/src/compiler/frame-states.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/frame-states.cc	2023-01-19 16:46:36.113942930 -0500
@@ -257,6 +257,22 @@
       ContinuationFrameStateMode::LAZY);
 }
 
+FrameState CloneFrameState(JSGraph* jsgraph, FrameState frame_state,
+                           OutputFrameStateCombine changed_state_combine) {
+  Graph* graph = jsgraph->graph();
+  CommonOperatorBuilder* common = jsgraph->common();
+
+  DCHECK_EQ(IrOpcode::kFrameState, frame_state->op()->opcode());
+
+  const Operator* op = common->FrameState(
+      frame_state.frame_state_info().bailout_id(), changed_state_combine,
+      frame_state.frame_state_info().function_info());
+  return FrameState(
+      graph->NewNode(op, frame_state.parameters(), frame_state.locals(),
+                     frame_state.stack(), frame_state.context(),
+                     frame_state.function(), frame_state.outer_frame_state()));
+}
+
 }  // namespace compiler
 }  // namespace internal
 }  // namespace v8
diff -r -u --color up/v8/src/compiler/frame-states.h nw/v8/src/compiler/frame-states.h
--- up/v8/src/compiler/frame-states.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/frame-states.h	2023-01-19 16:46:36.113942930 -0500
@@ -198,6 +198,11 @@
     JSGraph* graph, const SharedFunctionInfoRef& shared, Node* target,
     Node* context, Node* receiver, Node* outer_frame_state);
 
+// Creates a FrameState otherwise identical to `frame_state` except the
+// OutputFrameStateCombine is changed.
+FrameState CloneFrameState(JSGraph* jsgraph, FrameState frame_state,
+                           OutputFrameStateCombine changed_state_combine);
+
 }  // namespace compiler
 }  // namespace internal
 }  // namespace v8
diff -r -u --color up/v8/src/compiler/graph-assembler.cc nw/v8/src/compiler/graph-assembler.cc
--- up/v8/src/compiler/graph-assembler.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/graph-assembler.cc	2023-01-19 16:46:36.113942930 -0500
@@ -4,12 +4,21 @@
 
 #include "src/compiler/graph-assembler.h"
 
+#include "src/base/container-utils.h"
 #include "src/codegen/callable.h"
+#include "src/codegen/machine-type.h"
+#include "src/codegen/tnode.h"
+#include "src/common/globals.h"
 #include "src/compiler/access-builder.h"
+#include "src/compiler/common-operator.h"
 #include "src/compiler/graph-reducer.h"
 #include "src/compiler/linkage.h"
+#include "src/compiler/type-cache.h"
 // For TNode types.
+#include "src/objects/elements-kind.h"
 #include "src/objects/heap-number.h"
+#include "src/objects/instance-type.h"
+#include "src/objects/js-array-buffer.h"
 #include "src/objects/oddball.h"
 #include "src/objects/string.h"
 
@@ -33,18 +42,21 @@
 };
 
 GraphAssembler::GraphAssembler(
-    MachineGraph* mcgraph, Zone* zone,
+    MachineGraph* mcgraph, Zone* zone, BranchSemantics default_branch_semantics,
     base::Optional<NodeChangedCallback> node_changed_callback,
     bool mark_loop_exits)
     : temp_zone_(zone),
       mcgraph_(mcgraph),
+      default_branch_semantics_(default_branch_semantics),
       effect_(nullptr),
       control_(nullptr),
       node_changed_callback_(node_changed_callback),
       inline_reducers_(zone),
       inline_reductions_blocked_(false),
       loop_headers_(zone),
-      mark_loop_exits_(mark_loop_exits) {}
+      mark_loop_exits_(mark_loop_exits) {
+  DCHECK_NE(default_branch_semantics_, BranchSemantics::kUnspecified);
+}
 
 GraphAssembler::~GraphAssembler() { DCHECK_EQ(loop_nesting_level_, 0); }
 
@@ -52,16 +64,16 @@
   return AddClonedNode(mcgraph()->IntPtrConstant(value));
 }
 
-Node* GraphAssembler::UintPtrConstant(uintptr_t value) {
-  return AddClonedNode(mcgraph()->UintPtrConstant(value));
+TNode<UintPtrT> GraphAssembler::UintPtrConstant(uintptr_t value) {
+  return TNode<UintPtrT>::UncheckedCast(mcgraph()->UintPtrConstant(value));
 }
 
 Node* GraphAssembler::Int32Constant(int32_t value) {
   return AddClonedNode(mcgraph()->Int32Constant(value));
 }
 
-Node* GraphAssembler::Uint32Constant(uint32_t value) {
-  return AddClonedNode(mcgraph()->Uint32Constant(value));
+TNode<Uint32T> GraphAssembler::Uint32Constant(uint32_t value) {
+  return TNode<Uint32T>::UncheckedCast(mcgraph()->Uint32Constant(value));
 }
 
 Node* GraphAssembler::Int64Constant(int64_t value) {
@@ -150,8 +162,43 @@
   Node* GraphAssembler::Name(Node* left, Node* right) {               \
     return AddNode(graph()->NewNode(machine()->Name(), left, right)); \
   }
-PURE_ASSEMBLER_MACH_BINOP_LIST(PURE_BINOP_DEF)
+#define PURE_BINOP_DEF_TNODE(Name, Result, Left, Right)                       \
+  TNode<Result> GraphAssembler::Name(SloppyTNode<Left> left,                  \
+                                     SloppyTNode<Right> right) {              \
+    return AddNode<Result>(graph()->NewNode(machine()->Name(), left, right)); \
+  }
+PURE_ASSEMBLER_MACH_BINOP_LIST(PURE_BINOP_DEF, PURE_BINOP_DEF_TNODE)
 #undef PURE_BINOP_DEF
+#undef PURE_BINOP_DEF_TNODE
+
+TNode<BoolT> GraphAssembler::UintPtrLessThanOrEqual(TNode<UintPtrT> left,
+                                                    TNode<UintPtrT> right) {
+  return kSystemPointerSize == 8
+             ? Uint64LessThanOrEqual(TNode<Uint64T>::UncheckedCast(left),
+                                     TNode<Uint64T>::UncheckedCast(right))
+             : Uint32LessThanOrEqual(TNode<Uint32T>::UncheckedCast(left),
+                                     TNode<Uint32T>::UncheckedCast(right));
+}
+
+TNode<UintPtrT> GraphAssembler::UintPtrAdd(TNode<UintPtrT> left,
+                                           TNode<UintPtrT> right) {
+  return kSystemPointerSize == 8
+             ? TNode<UintPtrT>::UncheckedCast(Int64Add(left, right))
+             : TNode<UintPtrT>::UncheckedCast(Int32Add(left, right));
+}
+TNode<UintPtrT> GraphAssembler::UintPtrSub(TNode<UintPtrT> left,
+                                           TNode<UintPtrT> right) {
+  return kSystemPointerSize == 8
+             ? TNode<UintPtrT>::UncheckedCast(Int64Sub(left, right))
+             : TNode<UintPtrT>::UncheckedCast(Int32Sub(left, right));
+}
+
+TNode<UintPtrT> GraphAssembler::UintPtrDiv(TNode<UintPtrT> left,
+                                           TNode<UintPtrT> right) {
+  return kSystemPointerSize == 8
+             ? TNode<UintPtrT>::UncheckedCast(Uint64Div(left, right))
+             : TNode<UintPtrT>::UncheckedCast(Uint32Div(left, right));
+}
 
 #define CHECKED_BINOP_DEF(Name)                                       \
   Node* GraphAssembler::Name(Node* left, Node* right) {               \
@@ -226,6 +273,15 @@
   return value;
 }
 
+TNode<Uint32T> JSGraphAssembler::LoadElementsKind(TNode<Map> map) {
+  TNode<Uint8T> bit_field2 = EnterMachineGraph<Uint8T>(
+      LoadField<Uint8T>(AccessBuilder::ForMapBitField2(), map),
+      UseInfo::TruncatingWord32());
+  return TNode<Uint32T>::UncheckedCast(
+      Word32Shr(TNode<Word32T>::UncheckedCast(bit_field2),
+                Uint32Constant(Map::Bits2::ElementsKindBits::kShift)));
+}
+
 Node* JSGraphAssembler::LoadElement(ElementAccess const& access, Node* object,
                                     Node* index) {
   Node* value = AddNode(graph()->NewNode(simplified()->LoadElement(access),
@@ -330,6 +386,24 @@
       graph()->NewNode(simplified()->NumberLessThanOrEqual(), lhs, rhs));
 }
 
+TNode<Number> JSGraphAssembler::NumberShiftRightLogical(TNode<Number> lhs,
+                                                        TNode<Number> rhs) {
+  return AddNode<Number>(
+      graph()->NewNode(simplified()->NumberShiftRightLogical(), lhs, rhs));
+}
+
+TNode<Number> JSGraphAssembler::NumberBitwiseAnd(TNode<Number> lhs,
+                                                 TNode<Number> rhs) {
+  return AddNode<Number>(
+      graph()->NewNode(simplified()->NumberBitwiseAnd(), lhs, rhs));
+}
+
+TNode<Number> JSGraphAssembler::NumberBitwiseOr(TNode<Number> lhs,
+                                                TNode<Number> rhs) {
+  return AddNode<Number>(
+      graph()->NewNode(simplified()->NumberBitwiseOr(), lhs, rhs));
+}
+
 TNode<String> JSGraphAssembler::StringSubstring(TNode<String> string,
                                                 TNode<Number> from,
                                                 TNode<Number> to) {
@@ -342,6 +416,10 @@
       graph()->NewNode(simplified()->ObjectIsCallable(), value));
 }
 
+TNode<Boolean> JSGraphAssembler::ObjectIsSmi(TNode<Object> value) {
+  return AddNode<Boolean>(graph()->NewNode(simplified()->ObjectIsSmi(), value));
+}
+
 TNode<Boolean> JSGraphAssembler::ObjectIsUndetectable(TNode<Object> value) {
   return AddNode<Boolean>(
       graph()->NewNode(simplified()->ObjectIsUndetectable(), value));
@@ -379,12 +457,390 @@
       index_needed, old_length, effect(), control()));
 }
 
+TNode<Object> JSGraphAssembler::DoubleArrayMax(TNode<JSArray> array) {
+  return AddNode<Object>(graph()->NewNode(simplified()->DoubleArrayMax(), array,
+                                          effect(), control()));
+}
+
+TNode<Object> JSGraphAssembler::DoubleArrayMin(TNode<JSArray> array) {
+  return AddNode<Object>(graph()->NewNode(simplified()->DoubleArrayMin(), array,
+                                          effect(), control()));
+}
+
 Node* JSGraphAssembler::StringCharCodeAt(TNode<String> string,
                                          TNode<Number> position) {
   return AddNode(graph()->NewNode(simplified()->StringCharCodeAt(), string,
                                   position, effect(), control()));
 }
 
+class ArrayBufferViewAccessBuilder {
+ public:
+  explicit ArrayBufferViewAccessBuilder(JSGraphAssembler* assembler,
+                                        InstanceType instance_type,
+                                        std::set<ElementsKind> candidates)
+      : assembler_(assembler),
+        instance_type_(instance_type),
+        candidates_(std::move(candidates)) {
+    DCHECK_NOT_NULL(assembler_);
+    DCHECK(instance_type_ == JS_DATA_VIEW_TYPE ||
+           instance_type_ == JS_TYPED_ARRAY_TYPE);
+  }
+
+  bool maybe_rab_gsab() const {
+    if (candidates_.empty()) return true;
+    return !base::all_of(candidates_, [](auto e) {
+      return !IsRabGsabTypedArrayElementsKind(e);
+    });
+  }
+
+  base::Optional<int> TryComputeStaticElementShift() {
+    if (instance_type_ == JS_DATA_VIEW_TYPE) return 0;
+    if (candidates_.empty()) return base::nullopt;
+    int shift = ElementsKindToShiftSize(*candidates_.begin());
+    if (!base::all_of(candidates_, [shift](auto e) {
+          return ElementsKindToShiftSize(e) == shift;
+        })) {
+      return base::nullopt;
+    }
+    return shift;
+  }
+
+  base::Optional<int> TryComputeStaticElementSize() {
+    if (instance_type_ == JS_DATA_VIEW_TYPE) return 1;
+    if (candidates_.empty()) return base::nullopt;
+    int size = ElementsKindToByteSize(*candidates_.begin());
+    if (!base::all_of(candidates_, [size](auto e) {
+          return ElementsKindToByteSize(e) == size;
+        })) {
+      return base::nullopt;
+    }
+    return size;
+  }
+
+  TNode<UintPtrT> BuildLength(TNode<JSArrayBufferView> view,
+                              TNode<Context> context) {
+    auto& a = *assembler_;
+
+    // Case 1: Normal (backed by AB/SAB) or non-length tracking backed by GSAB
+    // (can't go oob once constructed)
+    auto GsabFixedOrNormal = [&]() {
+      return MachineLoadField<UintPtrT>(AccessBuilder::ForJSTypedArrayLength(),
+                                        view, UseInfo::Word());
+    };
+
+    // If we statically know we cannot have rab/gsab backed, we can simply
+    // load from the view.
+    if (!maybe_rab_gsab()) {
+      return GsabFixedOrNormal();
+    }
+
+    // Otherwise, we need to generate the checks for the view's bitfield.
+    TNode<Word32T> bitfield = a.EnterMachineGraph<Word32T>(
+        a.LoadField<Word32T>(AccessBuilder::ForJSArrayBufferViewBitField(),
+                             view),
+        UseInfo::TruncatingWord32());
+    TNode<Word32T> length_tracking_bit = a.Word32And(
+        bitfield, a.Uint32Constant(JSArrayBufferView::kIsLengthTracking));
+    TNode<Word32T> backed_by_rab_bit = a.Word32And(
+        bitfield, a.Uint32Constant(JSArrayBufferView::kIsBackedByRab));
+
+    // Load the underlying buffer.
+    TNode<HeapObject> buffer = a.LoadField<HeapObject>(
+        AccessBuilder::ForJSArrayBufferViewBuffer(), view);
+
+    // Compute the element size.
+    TNode<Uint32T> element_size;
+    if (auto size_opt = TryComputeStaticElementSize()) {
+      element_size = a.Uint32Constant(*size_opt);
+    } else {
+      DCHECK_EQ(instance_type_, JS_TYPED_ARRAY_TYPE);
+      TNode<Map> typed_array_map = a.LoadField<Map>(
+          AccessBuilder::ForMap(WriteBarrierKind::kNoWriteBarrier), view);
+      TNode<Uint32T> elements_kind = a.LoadElementsKind(typed_array_map);
+      element_size = a.LookupByteSizeForElementsKind(elements_kind);
+    }
+
+    // 2) Fixed length backed by RAB (can go oob once constructed)
+    auto RabFixed = [&]() {
+      TNode<UintPtrT> unchecked_byte_length = MachineLoadField<UintPtrT>(
+          AccessBuilder::ForJSArrayBufferViewByteLength(), view,
+          UseInfo::Word());
+      TNode<UintPtrT> underlying_byte_length = MachineLoadField<UintPtrT>(
+          AccessBuilder::ForJSArrayBufferByteLength(), buffer, UseInfo::Word());
+      TNode<UintPtrT> byte_offset = MachineLoadField<UintPtrT>(
+          AccessBuilder::ForJSArrayBufferViewByteOffset(), view,
+          UseInfo::Word());
+
+      TNode<UintPtrT> byte_length =
+          a
+              .MachineSelectIf<UintPtrT>(a.UintPtrLessThanOrEqual(
+                  a.UintPtrAdd(byte_offset, unchecked_byte_length),
+                  underlying_byte_length))
+              .Then([&]() { return unchecked_byte_length; })
+              .Else([&]() { return a.UintPtrConstant(0); })
+              .Value();
+      return a.UintPtrDiv(byte_length,
+                          TNode<UintPtrT>::UncheckedCast(element_size));
+    };
+
+    // 3) Length-tracking backed by RAB (JSArrayBuffer stores the length)
+    auto RabTracking = [&]() {
+      TNode<UintPtrT> byte_length = MachineLoadField<UintPtrT>(
+          AccessBuilder::ForJSArrayBufferByteLength(), buffer, UseInfo::Word());
+      TNode<UintPtrT> byte_offset = MachineLoadField<UintPtrT>(
+          AccessBuilder::ForJSArrayBufferViewByteOffset(), view,
+          UseInfo::Word());
+
+      return a
+          .MachineSelectIf<UintPtrT>(
+              a.UintPtrLessThanOrEqual(byte_offset, byte_length))
+          .Then([&]() {
+            // length = floor((byte_length - byte_offset) / element_size)
+            return a.UintPtrDiv(a.UintPtrSub(byte_length, byte_offset),
+                                TNode<UintPtrT>::UncheckedCast(element_size));
+          })
+          .Else([&]() { return a.UintPtrConstant(0); })
+          .ExpectTrue()
+          .Value();
+    };
+
+    // 4) Length-tracking backed by GSAB (BackingStore stores the length)
+    auto GsabTracking = [&]() {
+      TNode<Number> temp = TNode<Number>::UncheckedCast(a.TypeGuard(
+          TypeCache::Get()->kJSArrayBufferViewByteLengthType,
+          a.JSCallRuntime1(Runtime::kGrowableSharedArrayBufferByteLength,
+                           buffer, context, base::nullopt,
+                           Operator::kNoWrite)));
+      TNode<UintPtrT> byte_length =
+          a.EnterMachineGraph<UintPtrT>(temp, UseInfo::Word());
+      TNode<UintPtrT> byte_offset = MachineLoadField<UintPtrT>(
+          AccessBuilder::ForJSArrayBufferViewByteOffset(), view,
+          UseInfo::Word());
+
+      return a.UintPtrDiv(a.UintPtrSub(byte_length, byte_offset),
+                          TNode<UintPtrT>::UncheckedCast(element_size));
+    };
+
+    return a.MachineSelectIf<UintPtrT>(length_tracking_bit)
+        .Then([&]() {
+          return a.MachineSelectIf<UintPtrT>(backed_by_rab_bit)
+              .Then(RabTracking)
+              .Else(GsabTracking)
+              .Value();
+        })
+        .Else([&]() {
+          return a.MachineSelectIf<UintPtrT>(backed_by_rab_bit)
+              .Then(RabFixed)
+              .Else(GsabFixedOrNormal)
+              .Value();
+        })
+        .Value();
+  }
+
+  TNode<UintPtrT> BuildByteLength(TNode<JSArrayBufferView> view,
+                                  TNode<Context> context) {
+    auto& a = *assembler_;
+
+    // Case 1: Normal (backed by AB/SAB) or non-length tracking backed by GSAB
+    // (can't go oob once constructed)
+    auto GsabFixedOrNormal = [&]() {
+      return MachineLoadField<UintPtrT>(
+          AccessBuilder::ForJSArrayBufferViewByteLength(), view,
+          UseInfo::Word());
+    };
+
+    // If we statically know we cannot have rab/gsab backed, we can simply
+    // use load from the view.
+    if (!maybe_rab_gsab()) {
+      return GsabFixedOrNormal();
+    }
+
+    // Otherwise, we need to generate the checks for the view's bitfield.
+    TNode<Word32T> bitfield = a.EnterMachineGraph<Word32T>(
+        a.LoadField<Word32T>(AccessBuilder::ForJSArrayBufferViewBitField(),
+                             view),
+        UseInfo::TruncatingWord32());
+    TNode<Word32T> length_tracking_bit = a.Word32And(
+        bitfield, a.Uint32Constant(JSArrayBufferView::kIsLengthTracking));
+    TNode<Word32T> backed_by_rab_bit = a.Word32And(
+        bitfield, a.Uint32Constant(JSArrayBufferView::kIsBackedByRab));
+
+    // Load the underlying buffer.
+    TNode<HeapObject> buffer = a.LoadField<HeapObject>(
+        AccessBuilder::ForJSArrayBufferViewBuffer(), view);
+
+    // Case 2: Fixed length backed by RAB (can go oob once constructed)
+    auto RabFixed = [&]() {
+      TNode<UintPtrT> unchecked_byte_length = MachineLoadField<UintPtrT>(
+          AccessBuilder::ForJSArrayBufferViewByteLength(), view,
+          UseInfo::Word());
+      TNode<UintPtrT> underlying_byte_length = MachineLoadField<UintPtrT>(
+          AccessBuilder::ForJSArrayBufferByteLength(), buffer, UseInfo::Word());
+      TNode<UintPtrT> byte_offset = MachineLoadField<UintPtrT>(
+          AccessBuilder::ForJSArrayBufferViewByteOffset(), view,
+          UseInfo::Word());
+
+      return a
+          .MachineSelectIf<UintPtrT>(a.UintPtrLessThanOrEqual(
+              a.UintPtrAdd(byte_offset, unchecked_byte_length),
+              underlying_byte_length))
+          .Then([&]() { return unchecked_byte_length; })
+          .Else([&]() { return a.UintPtrConstant(0); })
+          .Value();
+    };
+
+    auto RoundDownToElementSize = [&](TNode<UintPtrT> byte_size) {
+      if (auto shift_opt = TryComputeStaticElementShift()) {
+        constexpr uintptr_t all_bits = static_cast<uintptr_t>(-1);
+        if (*shift_opt == 0) return byte_size;
+        return TNode<UintPtrT>::UncheckedCast(
+            a.WordAnd(byte_size, a.UintPtrConstant(all_bits << (*shift_opt))));
+      }
+      DCHECK_EQ(instance_type_, JS_TYPED_ARRAY_TYPE);
+      TNode<Map> typed_array_map = a.LoadField<Map>(
+          AccessBuilder::ForMap(WriteBarrierKind::kNoWriteBarrier), view);
+      TNode<Uint32T> elements_kind = a.LoadElementsKind(typed_array_map);
+      TNode<Uint32T> element_shift =
+          a.LookupByteShiftForElementsKind(elements_kind);
+      return TNode<UintPtrT>::UncheckedCast(
+          a.WordShl(a.WordShr(byte_size, element_shift), element_shift));
+    };
+
+    // Case 3: Length-tracking backed by RAB (JSArrayBuffer stores the length)
+    auto RabTracking = [&]() {
+      TNode<UintPtrT> byte_length = MachineLoadField<UintPtrT>(
+          AccessBuilder::ForJSArrayBufferByteLength(), buffer, UseInfo::Word());
+      TNode<UintPtrT> byte_offset = MachineLoadField<UintPtrT>(
+          AccessBuilder::ForJSArrayBufferViewByteOffset(), view,
+          UseInfo::Word());
+
+      return a
+          .MachineSelectIf<UintPtrT>(
+              a.UintPtrLessThanOrEqual(byte_offset, byte_length))
+          .Then([&]() {
+            return RoundDownToElementSize(
+                a.UintPtrSub(byte_length, byte_offset));
+          })
+          .Else([&]() { return a.UintPtrConstant(0); })
+          .ExpectTrue()
+          .Value();
+    };
+
+    // Case 4: Length-tracking backed by GSAB (BackingStore stores the length)
+    auto GsabTracking = [&]() {
+      TNode<Number> temp = TNode<Number>::UncheckedCast(a.TypeGuard(
+          TypeCache::Get()->kJSArrayBufferViewByteLengthType,
+          a.JSCallRuntime1(Runtime::kGrowableSharedArrayBufferByteLength,
+                           buffer, context, base::nullopt,
+                           Operator::kNoWrite)));
+      TNode<UintPtrT> byte_length =
+          a.EnterMachineGraph<UintPtrT>(temp, UseInfo::Word());
+      TNode<UintPtrT> byte_offset = MachineLoadField<UintPtrT>(
+          AccessBuilder::ForJSArrayBufferViewByteOffset(), view,
+          UseInfo::Word());
+      return RoundDownToElementSize(a.UintPtrSub(byte_length, byte_offset));
+    };
+
+    return a.MachineSelectIf<UintPtrT>(length_tracking_bit)
+        .Then([&]() {
+          return a.MachineSelectIf<UintPtrT>(backed_by_rab_bit)
+              .Then(RabTracking)
+              .Else(GsabTracking)
+              .Value();
+        })
+        .Else([&]() {
+          return a.MachineSelectIf<UintPtrT>(backed_by_rab_bit)
+              .Then(RabFixed)
+              .Else(GsabFixedOrNormal)
+              .Value();
+        })
+        .Value();
+  }
+
+ private:
+  template <typename T>
+  TNode<T> MachineLoadField(FieldAccess const& access, TNode<HeapObject> object,
+                            const UseInfo& use_info) {
+    return assembler_->EnterMachineGraph<T>(
+        assembler_->LoadField<T>(access, object), use_info);
+  }
+
+  JSGraphAssembler* assembler_;
+  InstanceType instance_type_;
+  std::set<ElementsKind> candidates_;
+};
+
+TNode<Number> JSGraphAssembler::ArrayBufferViewByteLength(
+    TNode<JSArrayBufferView> array_buffer_view, InstanceType instance_type,
+    std::set<ElementsKind> elements_kinds_candidates, TNode<Context> context) {
+  ArrayBufferViewAccessBuilder builder(this, instance_type,
+                                       std::move(elements_kinds_candidates));
+  return ExitMachineGraph<Number>(
+      builder.BuildByteLength(array_buffer_view, context),
+      MachineType::PointerRepresentation(),
+      TypeCache::Get()->kJSArrayBufferByteLengthType);
+}
+
+TNode<Number> JSGraphAssembler::TypedArrayLength(
+    TNode<JSTypedArray> typed_array,
+    std::set<ElementsKind> elements_kinds_candidates, TNode<Context> context) {
+  ArrayBufferViewAccessBuilder builder(this, JS_TYPED_ARRAY_TYPE,
+                                       elements_kinds_candidates);
+  return ExitMachineGraph<Number>(builder.BuildLength(typed_array, context),
+                                  MachineType::PointerRepresentation(),
+                                  TypeCache::Get()->kJSTypedArrayLengthType);
+}
+
+TNode<Uint32T> JSGraphAssembler::LookupByteShiftForElementsKind(
+    TNode<Uint32T> elements_kind) {
+  TNode<Uint32T> index = TNode<Uint32T>::UncheckedCast(Int32Sub(
+      elements_kind, Uint32Constant(FIRST_FIXED_TYPED_ARRAY_ELEMENTS_KIND)));
+  TNode<RawPtrT> shift_table = TNode<RawPtrT>::UncheckedCast(ExternalConstant(
+      ExternalReference::
+          typed_array_and_rab_gsab_typed_array_elements_kind_shifts()));
+  return TNode<Uint8T>::UncheckedCast(
+      Load(MachineType::Uint8(), shift_table, index));
+}
+
+TNode<Uint32T> JSGraphAssembler::LookupByteSizeForElementsKind(
+    TNode<Uint32T> elements_kind) {
+  TNode<Uint32T> index = TNode<Uint32T>::UncheckedCast(Int32Sub(
+      elements_kind, Uint32Constant(FIRST_FIXED_TYPED_ARRAY_ELEMENTS_KIND)));
+  TNode<RawPtrT> size_table = TNode<RawPtrT>::UncheckedCast(ExternalConstant(
+      ExternalReference::
+          typed_array_and_rab_gsab_typed_array_elements_kind_sizes()));
+  return TNode<Uint8T>::UncheckedCast(
+      Load(MachineType::Uint8(), size_table, index));
+}
+
+TNode<Object> JSGraphAssembler::JSCallRuntime1(
+    Runtime::FunctionId function_id, TNode<Object> arg0, TNode<Context> context,
+    base::Optional<FrameState> frame_state, Operator::Properties properties) {
+  return MayThrow([&]() {
+    if (frame_state.has_value()) {
+      return AddNode<Object>(graph()->NewNode(
+          javascript()->CallRuntime(function_id, 1, properties), arg0, context,
+          static_cast<Node*>(*frame_state), effect(), control()));
+    } else {
+      return AddNode<Object>(graph()->NewNode(
+          javascript()->CallRuntime(function_id, 1, properties), arg0, context,
+          effect(), control()));
+    }
+  });
+}
+
+TNode<Object> JSGraphAssembler::JSCallRuntime2(Runtime::FunctionId function_id,
+                                               TNode<Object> arg0,
+                                               TNode<Object> arg1,
+                                               TNode<Context> context,
+                                               FrameState frame_state) {
+  return MayThrow([&]() {
+    return AddNode<Object>(
+        graph()->NewNode(javascript()->CallRuntime(function_id, 2), arg0, arg1,
+                         context, frame_state, effect(), control()));
+  });
+}
+
 Node* GraphAssembler::TypeGuard(Type type, Node* value) {
   return AddNode(
       graph()->NewNode(common()->TypeGuard(type), value, effect(), control()));
@@ -564,7 +1020,7 @@
     hint = if_false->IsDeferred() ? BranchHint::kTrue : BranchHint::kFalse;
   }
 
-  BranchImpl(condition, if_true, if_false, hint);
+  BranchImpl(default_branch_semantics_, condition, if_true, if_false, hint);
 }
 
 void GraphAssembler::ConnectUnreachableToEnd() {
diff -r -u --color up/v8/src/compiler/graph-assembler.h nw/v8/src/compiler/graph-assembler.h
--- up/v8/src/compiler/graph-assembler.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/graph-assembler.h	2023-01-19 16:46:36.113942930 -0500
@@ -5,14 +5,17 @@
 #ifndef V8_COMPILER_GRAPH_ASSEMBLER_H_
 #define V8_COMPILER_GRAPH_ASSEMBLER_H_
 
+#include <optional>
 #include <type_traits>
 
 #include "src/base/small-vector.h"
 #include "src/codegen/tnode.h"
+#include "src/common/globals.h"
 #include "src/compiler/feedback-source.h"
 #include "src/compiler/js-graph.h"
 #include "src/compiler/node.h"
 #include "src/compiler/simplified-operator.h"
+#include "src/objects/oddball.h"
 
 namespace v8 {
 namespace internal {
@@ -62,54 +65,57 @@
   V(Word32ReverseBytes)                  \
   V(Word64ReverseBytes)
 
-#define PURE_ASSEMBLER_MACH_BINOP_LIST(V) \
-  V(Float64Add)                           \
-  V(Float64Div)                           \
-  V(Float64Equal)                         \
-  V(Float64InsertHighWord32)              \
-  V(Float64InsertLowWord32)               \
-  V(Float64LessThan)                      \
-  V(Float64LessThanOrEqual)               \
-  V(Float64Mod)                           \
-  V(Float64Sub)                           \
-  V(Int32Add)                             \
-  V(Int32LessThan)                        \
-  V(Int32LessThanOrEqual)                 \
-  V(Int32Mul)                             \
-  V(Int32Sub)                             \
-  V(Int64Sub)                             \
-  V(IntAdd)                               \
-  V(IntLessThan)                          \
-  V(IntMul)                               \
-  V(IntSub)                               \
-  V(Uint32LessThan)                       \
-  V(Uint32LessThanOrEqual)                \
-  V(Uint64LessThan)                       \
-  V(Uint64LessThanOrEqual)                \
-  V(UintLessThan)                         \
-  V(Word32And)                            \
-  V(Word32Equal)                          \
-  V(Word32Or)                             \
-  V(Word32Sar)                            \
-  V(Word32SarShiftOutZeros)               \
-  V(Word32Shl)                            \
-  V(Word32Shr)                            \
-  V(Word32Xor)                            \
-  V(Word64And)                            \
-  V(Word64Equal)                          \
-  V(Word64Or)                             \
-  V(Word64Sar)                            \
-  V(Word64SarShiftOutZeros)               \
-  V(Word64Shl)                            \
-  V(Word64Shr)                            \
-  V(Word64Xor)                            \
-  V(WordAnd)                              \
-  V(WordEqual)                            \
-  V(WordOr)                               \
-  V(WordSar)                              \
-  V(WordSarShiftOutZeros)                 \
-  V(WordShl)                              \
-  V(WordShr)                              \
+#define PURE_ASSEMBLER_MACH_BINOP_LIST(V, T)        \
+  V(Float64Add)                                     \
+  V(Float64Div)                                     \
+  V(Float64Equal)                                   \
+  V(Float64InsertHighWord32)                        \
+  V(Float64InsertLowWord32)                         \
+  V(Float64LessThan)                                \
+  V(Float64LessThanOrEqual)                         \
+  V(Float64Max)                                     \
+  V(Float64Min)                                     \
+  V(Float64Mod)                                     \
+  V(Float64Sub)                                     \
+  V(Int32Add)                                       \
+  V(Int32LessThan)                                  \
+  V(Int32LessThanOrEqual)                           \
+  V(Int32Mul)                                       \
+  V(Int32Sub)                                       \
+  V(Int64Add)                                       \
+  V(Int64Sub)                                       \
+  V(IntAdd)                                         \
+  V(IntLessThan)                                    \
+  V(IntMul)                                         \
+  V(IntSub)                                         \
+  V(Uint32LessThan)                                 \
+  T(Uint32LessThanOrEqual, BoolT, Uint32T, Uint32T) \
+  V(Uint64LessThan)                                 \
+  T(Uint64LessThanOrEqual, BoolT, Uint64T, Uint64T) \
+  V(UintLessThan)                                   \
+  T(Word32And, Word32T, Word32T, Word32T)           \
+  T(Word32Equal, BoolT, Word32T, Word32T)           \
+  V(Word32Or)                                       \
+  V(Word32Sar)                                      \
+  V(Word32SarShiftOutZeros)                         \
+  V(Word32Shl)                                      \
+  T(Word32Shr, Word32T, Word32T, Word32T)           \
+  V(Word32Xor)                                      \
+  V(Word64And)                                      \
+  V(Word64Equal)                                    \
+  V(Word64Or)                                       \
+  V(Word64Sar)                                      \
+  V(Word64SarShiftOutZeros)                         \
+  V(Word64Shl)                                      \
+  V(Word64Shr)                                      \
+  V(Word64Xor)                                      \
+  V(WordAnd)                                        \
+  V(WordEqual)                                      \
+  V(WordOr)                                         \
+  V(WordSar)                                        \
+  V(WordSarShiftOutZeros)                           \
+  V(WordShl)                                        \
+  V(WordShr)                                        \
   V(WordXor)
 
 #define CHECKED_ASSEMBLER_MACH_BINOP_LIST(V) \
@@ -118,7 +124,9 @@
   V(Int32Div)                                \
   V(Int32Mod)                                \
   V(Int32MulWithOverflow)                    \
+  V(Int64MulWithOverflow)                    \
   V(Int32SubWithOverflow)                    \
+  V(Int64SubWithOverflow)                    \
   V(Int64Div)                                \
   V(Int64Mod)                                \
   V(Uint32Div)                               \
@@ -276,9 +284,11 @@
   // will maintain the schedule as it updates blocks.
   GraphAssembler(
       MachineGraph* jsgraph, Zone* zone,
+      BranchSemantics default_branch_semantics,
       base::Optional<NodeChangedCallback> node_changed_callback = base::nullopt,
       bool mark_loop_exits = false);
   virtual ~GraphAssembler();
+  virtual SimplifiedOperatorBuilder* simplified() { UNREACHABLE(); }
 
   void Reset();
   void InitializeEffectControl(Node* effect, Node* control);
@@ -318,9 +328,9 @@
 
   // Value creation.
   Node* IntPtrConstant(intptr_t value);
-  Node* UintPtrConstant(uintptr_t value);
+  TNode<UintPtrT> UintPtrConstant(uintptr_t value);
   Node* Int32Constant(int32_t value);
-  Node* Uint32Constant(uint32_t value);
+  TNode<Uint32T> Uint32Constant(uint32_t value);
   Node* Int64Constant(int64_t value);
   Node* Uint64Constant(uint64_t value);
   Node* UniqueIntPtrConstant(intptr_t value);
@@ -339,9 +349,17 @@
 #undef PURE_UNOP_DECL
 
 #define BINOP_DECL(Name) Node* Name(Node* left, Node* right);
-  PURE_ASSEMBLER_MACH_BINOP_LIST(BINOP_DECL)
+#define BINOP_DECL_TNODE(Name, Result, Left, Right) \
+  TNode<Result> Name(SloppyTNode<Left> left, SloppyTNode<Right> right);
+  PURE_ASSEMBLER_MACH_BINOP_LIST(BINOP_DECL, BINOP_DECL_TNODE)
   CHECKED_ASSEMBLER_MACH_BINOP_LIST(BINOP_DECL)
 #undef BINOP_DECL
+#undef BINOP_DECL_TNODE
+  TNode<BoolT> UintPtrLessThanOrEqual(TNode<UintPtrT> left,
+                                      TNode<UintPtrT> right);
+  TNode<UintPtrT> UintPtrAdd(TNode<UintPtrT> left, TNode<UintPtrT> right);
+  TNode<UintPtrT> UintPtrSub(TNode<UintPtrT> left, TNode<UintPtrT> right);
+  TNode<UintPtrT> UintPtrDiv(TNode<UintPtrT> left, TNode<UintPtrT> right);
 
 #ifdef V8_MAP_PACKING
   Node* PackMapWord(TNode<Map> map);
@@ -381,6 +399,10 @@
   Node* BitcastMaybeObjectToWord(Node* value);
 
   Node* TypeGuard(Type type, Node* value);
+  template <typename T>
+  TNode<T> TypeGuard(Type type, TNode<T> value) {
+    return TNode<T>::UncheckedCast(TypeGuard(type, static_cast<Node*>(value)));
+  }
   Node* Checkpoint(FrameState frame_state);
 
   TNode<RawPtrT> StackSlot(int size, int alignment);
@@ -439,6 +461,16 @@
                       detail::GraphAssemblerLabelForVars<Vars...>* if_true,
                       detail::GraphAssemblerLabelForVars<Vars...>* if_false,
                       BranchHint hint, Vars...);
+  template <typename... Vars>
+  void MachineBranch(TNode<Word32T> condition,
+                     GraphAssemblerLabel<sizeof...(Vars)>* if_true,
+                     GraphAssemblerLabel<sizeof...(Vars)>* if_false,
+                     BranchHint hint, Vars...);
+  template <typename... Vars>
+  void JSBranch(TNode<Boolean> condition,
+                GraphAssemblerLabel<sizeof...(Vars)>* if_true,
+                GraphAssemblerLabel<sizeof...(Vars)>* if_false, BranchHint hint,
+                Vars...);
 
   // Control helpers.
 
@@ -511,6 +543,8 @@
   Effect effect() const { return Effect(effect_); }
 
  protected:
+  constexpr bool Is64() const { return kSystemPointerSize == 8; }
+
   template <typename... Vars>
   void MergeState(detail::GraphAssemblerLabelForVars<Vars...>* label,
                   Vars... vars);
@@ -600,13 +634,14 @@
   class BlockInlineReduction;
 
   template <typename... Vars>
-  void BranchImpl(Node* condition,
-                  detail::GraphAssemblerLabelForVars<Vars...>* if_true,
-                  detail::GraphAssemblerLabelForVars<Vars...>* if_false,
+  void BranchImpl(BranchSemantics semantics, Node* condition,
+                  GraphAssemblerLabel<sizeof...(Vars)>* if_true,
+                  GraphAssemblerLabel<sizeof...(Vars)>* if_false,
                   BranchHint hint, Vars...);
 
   Zone* temp_zone_;
   MachineGraph* mcgraph_;
+  BranchSemantics default_branch_semantics_;
   Node* effect_;
   Node* control_;
   // {node_changed_callback_} should be called when a node outside the
@@ -784,7 +819,8 @@
     hint = if_false->IsDeferred() ? BranchHint::kTrue : BranchHint::kFalse;
   }
 
-  BranchImpl(condition, if_true, if_false, hint, vars...);
+  BranchImpl(default_branch_semantics_, condition, if_true, if_false, hint,
+             vars...);
 }
 
 template <typename... Vars>
@@ -792,17 +828,36 @@
     Node* condition, detail::GraphAssemblerLabelForVars<Vars...>* if_true,
     detail::GraphAssemblerLabelForVars<Vars...>* if_false, BranchHint hint,
     Vars... vars) {
-  BranchImpl(condition, if_true, if_false, hint, vars...);
+  BranchImpl(default_branch_semantics_, condition, if_true, if_false, hint,
+             vars...);
 }
 
 template <typename... Vars>
-void GraphAssembler::BranchImpl(
-    Node* condition, detail::GraphAssemblerLabelForVars<Vars...>* if_true,
-    detail::GraphAssemblerLabelForVars<Vars...>* if_false, BranchHint hint,
+void GraphAssembler::MachineBranch(
+    TNode<Word32T> condition, GraphAssemblerLabel<sizeof...(Vars)>* if_true,
+    GraphAssemblerLabel<sizeof...(Vars)>* if_false, BranchHint hint,
     Vars... vars) {
+  BranchImpl(BranchSemantics::kMachine, condition, if_true, if_false, hint,
+             vars...);
+}
+
+template <typename... Vars>
+void GraphAssembler::JSBranch(TNode<Boolean> condition,
+                              GraphAssemblerLabel<sizeof...(Vars)>* if_true,
+                              GraphAssemblerLabel<sizeof...(Vars)>* if_false,
+                              BranchHint hint, Vars... vars) {
+  BranchImpl(BranchSemantics::kJS, condition, if_true, if_false, hint, vars...);
+}
+
+template <typename... Vars>
+void GraphAssembler::BranchImpl(BranchSemantics semantics, Node* condition,
+                                GraphAssemblerLabel<sizeof...(Vars)>* if_true,
+                                GraphAssemblerLabel<sizeof...(Vars)>* if_false,
+                                BranchHint hint, Vars... vars) {
   DCHECK_NOT_NULL(control());
 
-  Node* branch = graph()->NewNode(common()->Branch(hint), condition, control());
+  Node* branch =
+      graph()->NewNode(common()->Branch(hint, semantics), condition, control());
 
   control_ = graph()->NewNode(common()->IfTrue(), branch);
   MergeState(if_true, vars...);
@@ -829,7 +884,8 @@
 void GraphAssembler::GotoIf(Node* condition,
                             detail::GraphAssemblerLabelForVars<Vars...>* label,
                             BranchHint hint, Vars... vars) {
-  Node* branch = graph()->NewNode(common()->Branch(hint), condition, control());
+  Node* branch = graph()->NewNode(
+      common()->Branch(hint, default_branch_semantics_), condition, control());
 
   control_ = graph()->NewNode(common()->IfTrue(), branch);
   MergeState(label, vars...);
@@ -841,7 +897,8 @@
 void GraphAssembler::GotoIfNot(
     Node* condition, detail::GraphAssemblerLabelForVars<Vars...>* label,
     BranchHint hint, Vars... vars) {
-  Node* branch = graph()->NewNode(common()->Branch(hint), condition, control());
+  Node* branch = graph()->NewNode(
+      common()->Branch(hint, default_branch_semantics_), condition, control());
 
   control_ = graph()->NewNode(common()->IfFalse(), branch);
   MergeState(label, vars...);
@@ -887,11 +944,16 @@
   // Constructs a JSGraphAssembler. If {schedule} is not null, the graph
   // assembler will maintain the schedule as it updates blocks.
   JSGraphAssembler(
-      JSGraph* jsgraph, Zone* zone,
+      JSGraph* jsgraph, Zone* zone, BranchSemantics branch_semantics,
       base::Optional<NodeChangedCallback> node_changed_callback = base::nullopt,
       bool mark_loop_exits = false)
-      : GraphAssembler(jsgraph, zone, node_changed_callback, mark_loop_exits),
-        jsgraph_(jsgraph) {}
+      : GraphAssembler(jsgraph, zone, branch_semantics, node_changed_callback,
+                       mark_loop_exits),
+        jsgraph_(jsgraph),
+        outermost_catch_scope_(CatchScope::Outermost(zone)),
+        catch_scope_(&outermost_catch_scope_) {
+    outermost_catch_scope_.set_gasm(this);
+  }
 
   Node* SmiConstant(int32_t value);
   TNode<HeapObject> HeapConstant(Handle<HeapObject> object);
@@ -918,6 +980,7 @@
     //     access.machine_type.representation()));
     return TNode<T>::UncheckedCast(LoadField(access, object));
   }
+  TNode<Uint32T> LoadElementsKind(TNode<Map> map);
   Node* LoadElement(ElementAccess const&, Node* object, Node* index);
   template <typename T>
   TNode<T> LoadElement(ElementAccess const& access, TNode<HeapObject> object,
@@ -943,9 +1006,15 @@
   TNode<Boolean> NumberLessThanOrEqual(TNode<Number> lhs, TNode<Number> rhs);
   TNode<Number> NumberAdd(TNode<Number> lhs, TNode<Number> rhs);
   TNode<Number> NumberSubtract(TNode<Number> lhs, TNode<Number> rhs);
+  TNode<Number> NumberShiftRightLogical(TNode<Number> lhs, TNode<Number> rhs);
+  TNode<Number> NumberBitwiseAnd(TNode<Number> lhs, TNode<Number> rhs);
+  TNode<Number> NumberBitwiseOr(TNode<Number> lhs, TNode<Number> rhs);
+  TNode<Number> NumberDivide(TNode<Number> lhs, TNode<Number> rhs);
+  TNode<Number> NumberFloor(TNode<Number> value);
   TNode<String> StringSubstring(TNode<String> string, TNode<Number> from,
                                 TNode<Number> to);
   TNode<Boolean> ObjectIsCallable(TNode<Object> value);
+  TNode<Boolean> ObjectIsSmi(TNode<Object> value);
   TNode<Boolean> ObjectIsUndetectable(TNode<Object> value);
   Node* CheckIf(Node* cond, DeoptimizeReason reason);
   TNode<Boolean> NumberIsFloat64Hole(TNode<Number> value);
@@ -958,12 +1027,351 @@
                                               TNode<Number> new_length,
                                               TNode<Number> old_length);
   Node* StringCharCodeAt(TNode<String> string, TNode<Number> position);
+  TNode<Object> DoubleArrayMax(TNode<JSArray> array);
+  TNode<Object> DoubleArrayMin(TNode<JSArray> array);
+  // Computes the byte length for a given {array_buffer_view}. If the set of
+  // possible ElementsKinds is known statically pass as
+  // {elements_kinds_candidates} to allow the assembler to generate more
+  // efficient code. Pass an empty {elements_kinds_candidates} to generate code
+  // that is generic enough to handle all ElementsKinds.
+  TNode<Number> ArrayBufferViewByteLength(
+      TNode<JSArrayBufferView> array_buffer_view, InstanceType instance_type,
+      std::set<ElementsKind> elements_kinds_candidates, TNode<Context> context);
+  // Computes the length for a given {typed_array}. If the set of possible
+  // ElementsKinds is known statically pass as {elements_kinds_candidates} to
+  // allow the assembler to generate more efficient code. Pass an empty
+  // {elements_kinds_candidates} to generate code that is generic enough to
+  // handle all ElementsKinds.
+  TNode<Number> TypedArrayLength(
+      TNode<JSTypedArray> typed_array,
+      std::set<ElementsKind> elements_kinds_candidates, TNode<Context> context);
+  TNode<Uint32T> LookupByteShiftForElementsKind(TNode<Uint32T> elements_kind);
+  TNode<Uint32T> LookupByteSizeForElementsKind(TNode<Uint32T> elements_kind);
+
+  TNode<Object> JSCallRuntime1(
+      Runtime::FunctionId function_id, TNode<Object> arg0,
+      TNode<Context> context, base::Optional<FrameState> frame_state,
+      Operator::Properties properties = Operator::kNoProperties);
+  TNode<Object> JSCallRuntime2(Runtime::FunctionId function_id,
+                               TNode<Object> arg0, TNode<Object> arg1,
+                               TNode<Context> context, FrameState frame_state);
 
   JSGraph* jsgraph() const { return jsgraph_; }
   Isolate* isolate() const { return jsgraph()->isolate(); }
-  SimplifiedOperatorBuilder* simplified() const {
+  SimplifiedOperatorBuilder* simplified() override {
     return jsgraph()->simplified();
   }
+  JSOperatorBuilder* javascript() const { return jsgraph()->javascript(); }
+
+  template <typename T, typename U>
+  TNode<T> EnterMachineGraph(TNode<U> input, UseInfo use_info) {
+    DCHECK_EQ(use_info.type_check(), TypeCheckKind::kNone);
+    return AddNode<T>(
+        graph()->NewNode(common()->EnterMachineGraph(use_info), input));
+  }
+
+  template <typename T, typename U>
+  TNode<T> ExitMachineGraph(TNode<U> input,
+                            MachineRepresentation output_representation,
+                            Type output_type) {
+    return AddNode<T>(graph()->NewNode(
+        common()->ExitMachineGraph(output_representation, output_type), input));
+  }
+
+  // A catch scope represents a single catch handler. The handler can be
+  // custom catch logic within the reduction itself; or a catch handler in the
+  // outside graph into which the reduction will be integrated (in this case
+  // the scope is called 'outermost').
+  class V8_NODISCARD CatchScope {
+   private:
+    // Only used to partially construct the outermost scope.
+    explicit CatchScope(Zone* zone) : if_exception_nodes_(zone) {}
+
+    // For all inner scopes.
+    CatchScope(Zone* zone, JSGraphAssembler* gasm)
+        : gasm_(gasm),
+          parent_(gasm->catch_scope_),
+          has_handler_(true),
+          if_exception_nodes_(zone) {
+      DCHECK_NOT_NULL(gasm_);
+      gasm_->catch_scope_ = this;
+    }
+
+   public:
+    ~CatchScope() { gasm_->catch_scope_ = parent_; }
+
+    static CatchScope Outermost(Zone* zone) { return CatchScope{zone}; }
+    static CatchScope Inner(Zone* zone, JSGraphAssembler* gasm) {
+      return {zone, gasm};
+    }
+
+    bool has_handler() const { return has_handler_; }
+    bool is_outermost() const { return parent_ == nullptr; }
+    CatchScope* parent() const { return parent_; }
+
+    // Should only be used to initialize the outermost scope (inner scopes
+    // always have a handler and are passed the gasm pointer at construction).
+    void set_has_handler(bool v) {
+      DCHECK(is_outermost());
+      has_handler_ = v;
+    }
+    void set_gasm(JSGraphAssembler* v) {
+      DCHECK(is_outermost());
+      DCHECK_NOT_NULL(v);
+      gasm_ = v;
+    }
+
+    bool has_exceptional_control_flow() const {
+      return !if_exception_nodes_.empty();
+    }
+
+    void RegisterIfExceptionNode(Node* if_exception) {
+      DCHECK(has_handler());
+      if_exception_nodes_.push_back(if_exception);
+    }
+
+    void MergeExceptionalPaths(TNode<Object>* exception_out, Effect* effect_out,
+                               Control* control_out) {
+      DCHECK(has_handler());
+      DCHECK(has_exceptional_control_flow());
+
+      const int size = static_cast<int>(if_exception_nodes_.size());
+
+      if (size == 1) {
+        // No merge needed.
+        Node* e = if_exception_nodes_.at(0);
+        *exception_out = TNode<Object>::UncheckedCast(e);
+        *effect_out = Effect(e);
+        *control_out = Control(e);
+      } else {
+        DCHECK_GT(size, 1);
+
+        Node* merge = gasm_->graph()->NewNode(gasm_->common()->Merge(size),
+                                              size, if_exception_nodes_.data());
+
+        // These phis additionally take {merge} as an input. Temporarily add
+        // it to the list.
+        if_exception_nodes_.push_back(merge);
+        const int size_with_merge =
+            static_cast<int>(if_exception_nodes_.size());
+
+        Node* ephi = gasm_->graph()->NewNode(gasm_->common()->EffectPhi(size),
+                                             size_with_merge,
+                                             if_exception_nodes_.data());
+        Node* phi = gasm_->graph()->NewNode(
+            gasm_->common()->Phi(MachineRepresentation::kTagged, size),
+            size_with_merge, if_exception_nodes_.data());
+        if_exception_nodes_.pop_back();
+
+        *exception_out = TNode<Object>::UncheckedCast(phi);
+        *effect_out = Effect(ephi);
+        *control_out = Control(merge);
+      }
+    }
+
+   private:
+    JSGraphAssembler* gasm_ = nullptr;
+    CatchScope* const parent_ = nullptr;
+    bool has_handler_ = false;
+    NodeVector if_exception_nodes_;
+  };
+
+  CatchScope* catch_scope() const { return catch_scope_; }
+  Node* outermost_handler() const { return outermost_handler_; }
+
+  using NodeGenerator0 = std::function<TNode<Object>()>;
+  // TODO(jgruber): Currently, it's the responsibility of the developer to note
+  // which operations may throw and appropriately wrap these in a call to
+  // MayThrow (see e.g. JSCall3 and CallRuntime2). A more methodical approach
+  // would be good.
+  TNode<Object> MayThrow(const NodeGenerator0& body) {
+    TNode<Object> result = body();
+
+    if (catch_scope()->has_handler()) {
+      // The IfException node is later merged into the outer graph.
+      // Note: AddNode is intentionally not called since effect and control
+      // should not be updated.
+      Node* if_exception =
+          graph()->NewNode(common()->IfException(), effect(), control());
+      catch_scope()->RegisterIfExceptionNode(if_exception);
+
+      // Control resumes here.
+      AddNode(graph()->NewNode(common()->IfSuccess(), control()));
+    }
+
+    return result;
+  }
+
+  using VoidGenerator0 = std::function<void()>;
+  // TODO(jgruber): Currently IfBuilder0 and IfBuilder1 are implemented as
+  // separate classes. If, in the future, we encounter additional use cases that
+  // return more than 1 value, we should merge these back into a single variadic
+  // implementation.
+  class IfBuilder0 final {
+   public:
+    IfBuilder0(JSGraphAssembler* gasm, TNode<Boolean> cond, bool negate_cond)
+        : gasm_(gasm),
+          cond_(cond),
+          negate_cond_(negate_cond),
+          initial_effect_(gasm->effect()),
+          initial_control_(gasm->control()) {}
+
+    IfBuilder0& ExpectTrue() {
+      DCHECK_EQ(hint_, BranchHint::kNone);
+      hint_ = BranchHint::kTrue;
+      return *this;
+    }
+    IfBuilder0& ExpectFalse() {
+      DCHECK_EQ(hint_, BranchHint::kNone);
+      hint_ = BranchHint::kFalse;
+      return *this;
+    }
+
+    IfBuilder0& Then(const VoidGenerator0& body) {
+      then_body_ = body;
+      return *this;
+    }
+    IfBuilder0& Else(const VoidGenerator0& body) {
+      else_body_ = body;
+      return *this;
+    }
+
+    ~IfBuilder0() {
+      // Ensure correct usage: effect/control must not have been modified while
+      // the IfBuilder0 instance is alive.
+      DCHECK_EQ(gasm_->effect(), initial_effect_);
+      DCHECK_EQ(gasm_->control(), initial_control_);
+
+      // Unlike IfBuilder1, this supports an empty then or else body. This is
+      // possible since the merge does not take any value inputs.
+      DCHECK(then_body_ || else_body_);
+
+      if (negate_cond_) std::swap(then_body_, else_body_);
+
+      auto if_true = (hint_ == BranchHint::kFalse) ? gasm_->MakeDeferredLabel()
+                                                   : gasm_->MakeLabel();
+      auto if_false = (hint_ == BranchHint::kTrue) ? gasm_->MakeDeferredLabel()
+                                                   : gasm_->MakeLabel();
+      auto merge = gasm_->MakeLabel();
+      gasm_->Branch(cond_, &if_true, &if_false);
+
+      gasm_->Bind(&if_true);
+      if (then_body_) then_body_();
+      if (gasm_->HasActiveBlock()) gasm_->Goto(&merge);
+
+      gasm_->Bind(&if_false);
+      if (else_body_) else_body_();
+      if (gasm_->HasActiveBlock()) gasm_->Goto(&merge);
+
+      gasm_->Bind(&merge);
+    }
+
+    IfBuilder0(const IfBuilder0&) = delete;
+    IfBuilder0& operator=(const IfBuilder0&) = delete;
+
+   private:
+    JSGraphAssembler* const gasm_;
+    const TNode<Boolean> cond_;
+    const bool negate_cond_;
+    const Effect initial_effect_;
+    const Control initial_control_;
+    BranchHint hint_ = BranchHint::kNone;
+    VoidGenerator0 then_body_;
+    VoidGenerator0 else_body_;
+  };
+
+  IfBuilder0 If(TNode<Boolean> cond) { return {this, cond, false}; }
+  IfBuilder0 IfNot(TNode<Boolean> cond) { return {this, cond, true}; }
+
+  template <typename T, typename Cond>
+  class IfBuilder1 {
+    using If1BodyFunction = std::function<TNode<T>()>;
+
+   public:
+    IfBuilder1(JSGraphAssembler* gasm, TNode<Cond> cond, bool negate_cond)
+        : gasm_(gasm), cond_(cond), negate_cond_(negate_cond) {}
+
+    V8_WARN_UNUSED_RESULT IfBuilder1& ExpectTrue() {
+      DCHECK_EQ(hint_, BranchHint::kNone);
+      hint_ = BranchHint::kTrue;
+      return *this;
+    }
+
+    V8_WARN_UNUSED_RESULT IfBuilder1& ExpectFalse() {
+      DCHECK_EQ(hint_, BranchHint::kNone);
+      hint_ = BranchHint::kFalse;
+      return *this;
+    }
+
+    V8_WARN_UNUSED_RESULT IfBuilder1& Then(const If1BodyFunction& body) {
+      then_body_ = body;
+      return *this;
+    }
+    V8_WARN_UNUSED_RESULT IfBuilder1& Else(const If1BodyFunction& body) {
+      else_body_ = body;
+      return *this;
+    }
+
+    V8_WARN_UNUSED_RESULT TNode<T> Value() {
+      DCHECK(then_body_);
+      DCHECK(else_body_);
+
+      if (negate_cond_) std::swap(then_body_, else_body_);
+
+      auto if_true = (hint_ == BranchHint::kFalse) ? gasm_->MakeDeferredLabel()
+                                                   : gasm_->MakeLabel();
+      auto if_false = (hint_ == BranchHint::kTrue) ? gasm_->MakeDeferredLabel()
+                                                   : gasm_->MakeLabel();
+      auto merge = gasm_->MakeLabel(PhiMachineRepresentationOf<T>);
+      if constexpr (std::is_same_v<Cond, Word32T>) {
+        gasm_->MachineBranch(cond_, &if_true, &if_false, hint_);
+      } else {
+        static_assert(std::is_same_v<Cond, Boolean>);
+        if (hint_ != BranchHint::kNone) {
+          gasm_->BranchWithHint(cond_, &if_true, &if_false, hint_);
+        } else {
+          gasm_->Branch(cond_, &if_true, &if_false);
+        }
+      }
+
+      gasm_->Bind(&if_true);
+      TNode<T> then_result = then_body_();
+      if (gasm_->HasActiveBlock()) gasm_->Goto(&merge, then_result);
+
+      gasm_->Bind(&if_false);
+      TNode<T> else_result = else_body_();
+      if (gasm_->HasActiveBlock()) {
+        gasm_->Goto(&merge, else_result);
+      }
+
+      gasm_->Bind(&merge);
+      return merge.template PhiAt<T>(0);
+    }
+
+   private:
+    static constexpr MachineRepresentation kPhiRepresentation =
+        MachineRepresentation::kTagged;
+
+    JSGraphAssembler* const gasm_;
+    const TNode<Cond> cond_;
+    const bool negate_cond_;
+    BranchHint hint_ = BranchHint::kNone;
+    If1BodyFunction then_body_;
+    If1BodyFunction else_body_;
+  };
+
+  template <typename T>
+  IfBuilder1<T, Boolean> SelectIf(TNode<Boolean> cond) {
+    return {this, cond, false};
+  }
+  template <typename T>
+  IfBuilder1<T, Boolean> SelectIfNot(TNode<Boolean> cond) {
+    return {this, cond, true};
+  }
+  template <typename T>
+  IfBuilder1<T, Word32T> MachineSelectIf(TNode<Word32T> cond) {
+    return {this, cond, false};
+  }
 
  protected:
   Operator const* PlainPrimitiveToNumberOperator();
@@ -971,6 +1379,12 @@
  private:
   JSGraph* jsgraph_;
   SetOncePointer<Operator const> to_number_operator_;
+
+ protected:
+  CatchScope outermost_catch_scope_;
+  Node* outermost_handler_;
+  CatchScope* catch_scope_;
+  friend class CatchScope;
 };
 
 }  // namespace compiler
diff -r -u --color up/v8/src/compiler/heap-refs.cc nw/v8/src/compiler/heap-refs.cc
--- up/v8/src/compiler/heap-refs.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/heap-refs.cc	2023-01-19 16:46:36.113942930 -0500
@@ -4,6 +4,8 @@
 
 #include "src/compiler/heap-refs.h"
 
+#include "src/objects/elements-kind.h"
+
 #ifdef ENABLE_SLOW_DCHECKS
 #include <algorithm>
 #endif
@@ -1079,6 +1081,11 @@
       kind != BIGINT64_ELEMENTS) {
     return true;
   }
+  if (v8_flags.turbo_rab_gsab && IsRabGsabTypedArrayElementsKind(kind) &&
+      kind != RAB_GSAB_BIGUINT64_ELEMENTS &&
+      kind != RAB_GSAB_BIGINT64_ELEMENTS) {
+    return true;
+  }
   return false;
 }
 
diff -r -u --color up/v8/src/compiler/heap-refs.h nw/v8/src/compiler/heap-refs.h
--- up/v8/src/compiler/heap-refs.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/heap-refs.h	2023-01-19 16:46:36.113942930 -0500
@@ -262,16 +262,6 @@
       return base::hash_combine(ref.object().address());
     }
   };
-  struct Equal {
-    bool operator()(const ObjectRef& lhs, const ObjectRef& rhs) const {
-      return lhs.equals(rhs);
-    }
-  };
-  struct Less {
-    bool operator()(const ObjectRef& lhs, const ObjectRef& rhs) const {
-      return lhs.data_ < rhs.data_;
-    }
-  };
 
  protected:
   JSHeapBroker* broker() const;
@@ -291,16 +281,28 @@
   friend class TinyRef;
 
   friend std::ostream& operator<<(std::ostream& os, const ObjectRef& ref);
+  friend bool operator<(const ObjectRef& lhs, const ObjectRef& rhs);
 
   JSHeapBroker* broker_;
 };
 
+inline bool operator==(const ObjectRef& lhs, const ObjectRef& rhs) {
+  return lhs.equals(rhs);
+}
+
+inline bool operator!=(const ObjectRef& lhs, const ObjectRef& rhs) {
+  return !lhs.equals(rhs);
+}
+
+inline bool operator<(const ObjectRef& lhs, const ObjectRef& rhs) {
+  return lhs.data_ < rhs.data_;
+}
+
 template <class T>
-using ZoneRefUnorderedSet =
-    ZoneUnorderedSet<T, ObjectRef::Hash, ObjectRef::Equal>;
+using ZoneRefUnorderedSet = ZoneUnorderedSet<T, ObjectRef::Hash>;
 
 template <class K, class V>
-using ZoneRefMap = ZoneMap<K, V, ObjectRef::Less>;
+using ZoneRefMap = ZoneMap<K, V>;
 
 // Temporary class that carries information from a Map. We'd like to remove
 // this class and use MapRef instead, but we can't as long as we support the
@@ -887,6 +889,7 @@
 };
 
 #define BROKER_SFI_FIELDS(V)                               \
+  V(int, internal_formal_parameter_count_with_receiver)    \
   V(int, internal_formal_parameter_count_without_receiver) \
   V(bool, IsDontAdaptArguments)                            \
   V(bool, has_simple_parameters)                           \
diff -r -u --color up/v8/src/compiler/js-call-reducer.cc nw/v8/src/compiler/js-call-reducer.cc
--- up/v8/src/compiler/js-call-reducer.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/js-call-reducer.cc	2023-01-19 16:46:36.113942930 -0500
@@ -6,6 +6,7 @@
 
 #include <functional>
 
+#include "src/base/container-utils.h"
 #include "src/base/small-vector.h"
 #include "src/builtins/builtins-promise.h"
 #include "src/builtins/builtins-utils.h"
@@ -28,8 +29,11 @@
 #include "src/compiler/simplified-operator.h"
 #include "src/compiler/state-values-utils.h"
 #include "src/compiler/type-cache.h"
+#include "src/compiler/use-info.h"
+#include "src/flags/flags.h"
 #include "src/ic/call-optimization.h"
 #include "src/objects/elements-kind.h"
+#include "src/objects/instance-type.h"
 #include "src/objects/js-function.h"
 #include "src/objects/objects-inl.h"
 #include "src/objects/ordered-hash-table.h"
@@ -46,32 +50,26 @@
 #define _ [&]()
 
 class JSCallReducerAssembler : public JSGraphAssembler {
- protected:
-  class CatchScope;
-
- private:
   static constexpr bool kMarkLoopExits = true;
 
  public:
-  JSCallReducerAssembler(JSCallReducer* reducer, Node* node)
+  JSCallReducerAssembler(JSCallReducer* reducer, Node* node,
+                         Node* effect = nullptr, Node* control = nullptr)
       : JSGraphAssembler(
             reducer->JSGraphForGraphAssembler(),
-            reducer->ZoneForGraphAssembler(),
+            reducer->ZoneForGraphAssembler(), BranchSemantics::kJS,
             [reducer](Node* n) { reducer->RevisitForGraphAssembler(n); },
             kMarkLoopExits),
         dependencies_(reducer->dependencies()),
-        node_(node),
-        outermost_catch_scope_(
-            CatchScope::Outermost(reducer->ZoneForGraphAssembler())),
-        catch_scope_(&outermost_catch_scope_) {
-    InitializeEffectControl(NodeProperties::GetEffectInput(node),
-                            NodeProperties::GetControlInput(node));
+        node_(node) {
+    InitializeEffectControl(
+        effect ? effect : NodeProperties::GetEffectInput(node),
+        control ? control : NodeProperties::GetControlInput(node));
 
     // Finish initializing the outermost catch scope.
     bool has_handler =
         NodeProperties::IsExceptionalCall(node, &outermost_handler_);
     outermost_catch_scope_.set_has_handler(has_handler);
-    outermost_catch_scope_.set_gasm(this);
   }
 
   TNode<Object> ReduceJSCallWithArrayLikeOrSpreadOfEmpty(
@@ -83,6 +81,7 @@
   TNode<Boolean> ReduceStringPrototypeStartsWith(
       const StringRef& search_element_string);
   TNode<String> ReduceStringPrototypeSlice();
+  TNode<Object> ReduceJSCallMathMinMaxWithArrayLike(Builtin builtin);
 
   TNode<Object> TargetInput() const { return JSCallNode{node_ptr()}.target(); }
 
@@ -93,164 +92,8 @@
 
   TNode<Object> ReceiverInput() const { return ReceiverInputAs<Object>(); }
 
-  CatchScope* catch_scope() const { return catch_scope_; }
-  Node* outermost_handler() const { return outermost_handler_; }
-
   Node* node_ptr() const { return node_; }
 
- protected:
-  using NodeGenerator0 = std::function<TNode<Object>()>;
-  using VoidGenerator0 = std::function<void()>;
-
-  // TODO(jgruber): Currently IfBuilder0 and IfBuilder1 are implemented as
-  // separate classes. If, in the future, we encounter additional use cases that
-  // return more than 1 value, we should merge these back into a single variadic
-  // implementation.
-  class IfBuilder0 final {
-   public:
-    IfBuilder0(JSGraphAssembler* gasm, TNode<Boolean> cond, bool negate_cond)
-        : gasm_(gasm),
-          cond_(cond),
-          negate_cond_(negate_cond),
-          initial_effect_(gasm->effect()),
-          initial_control_(gasm->control()) {}
-
-    IfBuilder0& ExpectTrue() {
-      DCHECK_EQ(hint_, BranchHint::kNone);
-      hint_ = BranchHint::kTrue;
-      return *this;
-    }
-    IfBuilder0& ExpectFalse() {
-      DCHECK_EQ(hint_, BranchHint::kNone);
-      hint_ = BranchHint::kFalse;
-      return *this;
-    }
-
-    IfBuilder0& Then(const VoidGenerator0& body) {
-      then_body_ = body;
-      return *this;
-    }
-    IfBuilder0& Else(const VoidGenerator0& body) {
-      else_body_ = body;
-      return *this;
-    }
-
-    ~IfBuilder0() {
-      // Ensure correct usage: effect/control must not have been modified while
-      // the IfBuilder0 instance is alive.
-      DCHECK_EQ(gasm_->effect(), initial_effect_);
-      DCHECK_EQ(gasm_->control(), initial_control_);
-
-      // Unlike IfBuilder1, this supports an empty then or else body. This is
-      // possible since the merge does not take any value inputs.
-      DCHECK(then_body_ || else_body_);
-
-      if (negate_cond_) std::swap(then_body_, else_body_);
-
-      auto if_true = (hint_ == BranchHint::kFalse) ? gasm_->MakeDeferredLabel()
-                                                   : gasm_->MakeLabel();
-      auto if_false = (hint_ == BranchHint::kTrue) ? gasm_->MakeDeferredLabel()
-                                                   : gasm_->MakeLabel();
-      auto merge = gasm_->MakeLabel();
-      gasm_->Branch(cond_, &if_true, &if_false);
-
-      gasm_->Bind(&if_true);
-      if (then_body_) then_body_();
-      if (gasm_->HasActiveBlock()) gasm_->Goto(&merge);
-
-      gasm_->Bind(&if_false);
-      if (else_body_) else_body_();
-      if (gasm_->HasActiveBlock()) gasm_->Goto(&merge);
-
-      gasm_->Bind(&merge);
-    }
-
-    IfBuilder0(const IfBuilder0&) = delete;
-    IfBuilder0& operator=(const IfBuilder0&) = delete;
-
-   private:
-    JSGraphAssembler* const gasm_;
-    const TNode<Boolean> cond_;
-    const bool negate_cond_;
-    const Effect initial_effect_;
-    const Control initial_control_;
-    BranchHint hint_ = BranchHint::kNone;
-    VoidGenerator0 then_body_;
-    VoidGenerator0 else_body_;
-  };
-
-  IfBuilder0 If(TNode<Boolean> cond) { return {this, cond, false}; }
-  IfBuilder0 IfNot(TNode<Boolean> cond) { return {this, cond, true}; }
-
-  template <typename T>
-  class IfBuilder1 {
-    using If1BodyFunction = std::function<TNode<T>()>;
-
-   public:
-    IfBuilder1(JSGraphAssembler* gasm, TNode<Boolean> cond)
-        : gasm_(gasm), cond_(cond) {}
-
-    V8_WARN_UNUSED_RESULT IfBuilder1& ExpectTrue() {
-      DCHECK_EQ(hint_, BranchHint::kNone);
-      hint_ = BranchHint::kTrue;
-      return *this;
-    }
-
-    V8_WARN_UNUSED_RESULT IfBuilder1& ExpectFalse() {
-      DCHECK_EQ(hint_, BranchHint::kNone);
-      hint_ = BranchHint::kFalse;
-      return *this;
-    }
-
-    V8_WARN_UNUSED_RESULT IfBuilder1& Then(const If1BodyFunction& body) {
-      then_body_ = body;
-      return *this;
-    }
-    V8_WARN_UNUSED_RESULT IfBuilder1& Else(const If1BodyFunction& body) {
-      else_body_ = body;
-      return *this;
-    }
-
-    V8_WARN_UNUSED_RESULT TNode<T> Value() {
-      DCHECK(then_body_);
-      DCHECK(else_body_);
-      auto if_true = (hint_ == BranchHint::kFalse) ? gasm_->MakeDeferredLabel()
-                                                   : gasm_->MakeLabel();
-      auto if_false = (hint_ == BranchHint::kTrue) ? gasm_->MakeDeferredLabel()
-                                                   : gasm_->MakeLabel();
-      auto merge = gasm_->MakeLabel(kPhiRepresentation);
-      gasm_->Branch(cond_, &if_true, &if_false);
-
-      gasm_->Bind(&if_true);
-      TNode<T> then_result = then_body_();
-      if (gasm_->HasActiveBlock()) gasm_->Goto(&merge, then_result);
-
-      gasm_->Bind(&if_false);
-      TNode<T> else_result = else_body_();
-      if (gasm_->HasActiveBlock()) {
-        gasm_->Goto(&merge, else_result);
-      }
-
-      gasm_->Bind(&merge);
-      return merge.PhiAt<T>(0);
-    }
-
-   private:
-    static constexpr MachineRepresentation kPhiRepresentation =
-        MachineRepresentation::kTagged;
-
-    JSGraphAssembler* const gasm_;
-    const TNode<Boolean> cond_;
-    BranchHint hint_ = BranchHint::kNone;
-    If1BodyFunction then_body_;
-    If1BodyFunction else_body_;
-  };
-
-  template <typename T>
-  IfBuilder1<T> SelectIf(TNode<Boolean> cond) {
-    return {this, cond};
-  }
-
   // Simplified operators.
   TNode<Number> SpeculativeToNumber(
       TNode<Object> value,
@@ -276,9 +119,6 @@
                         TNode<Object> arg0, TNode<Object> arg1,
                         TNode<Object> arg2, TNode<Object> arg3,
                         FrameState frame_state);
-  TNode<Object> JSCallRuntime2(Runtime::FunctionId function_id,
-                               TNode<Object> arg0, TNode<Object> arg1,
-                               FrameState frame_state);
 
   // Emplace a copy of the call node into the graph at current effect/control.
   TNode<Object> CopyNode();
@@ -294,6 +134,22 @@
     return NumberAdd(value, OneConstant());
   }
 
+  TNode<Number> LoadMapElementsKind(TNode<Map> map);
+
+  template <typename T, typename U>
+  TNode<T> EnterMachineGraph(TNode<U> input, UseInfo use_info) {
+    return AddNode<T>(
+        graph()->NewNode(common()->EnterMachineGraph(use_info), input));
+  }
+
+  template <typename T, typename U>
+  TNode<T> ExitMachineGraph(TNode<U> input,
+                            MachineRepresentation output_representation,
+                            Type output_type) {
+    return AddNode<T>(graph()->NewNode(
+        common()->ExitMachineGraph(output_representation, output_type), input));
+  }
+
   void MaybeInsertMapChecks(MapInference* inference,
                             bool has_stability_dependency) {
     // TODO(jgruber): Implement MapInference::InsertMapChecks in graph
@@ -305,124 +161,6 @@
     }
   }
 
-  // TODO(jgruber): Currently, it's the responsibility of the developer to note
-  // which operations may throw and appropriately wrap these in a call to
-  // MayThrow (see e.g. JSCall3 and CallRuntime2). A more methodical approach
-  // would be good.
-  TNode<Object> MayThrow(const NodeGenerator0& body) {
-    TNode<Object> result = body();
-
-    if (catch_scope()->has_handler()) {
-      // The IfException node is later merged into the outer graph.
-      // Note: AddNode is intentionally not called since effect and control
-      // should not be updated.
-      Node* if_exception =
-          graph()->NewNode(common()->IfException(), effect(), control());
-      catch_scope()->RegisterIfExceptionNode(if_exception);
-
-      // Control resumes here.
-      AddNode(graph()->NewNode(common()->IfSuccess(), control()));
-    }
-
-    return result;
-  }
-
-  // A catch scope represents a single catch handler. The handler can be
-  // custom catch logic within the reduction itself; or a catch handler in the
-  // outside graph into which the reduction will be integrated (in this case
-  // the scope is called 'outermost').
-  class V8_NODISCARD CatchScope {
-   private:
-    // Only used to partially construct the outermost scope.
-    explicit CatchScope(Zone* zone) : if_exception_nodes_(zone) {}
-
-    // For all inner scopes.
-    CatchScope(Zone* zone, JSCallReducerAssembler* gasm)
-        : gasm_(gasm),
-          parent_(gasm->catch_scope_),
-          has_handler_(true),
-          if_exception_nodes_(zone) {
-      gasm_->catch_scope_ = this;
-    }
-
-   public:
-    ~CatchScope() { gasm_->catch_scope_ = parent_; }
-
-    static CatchScope Outermost(Zone* zone) { return CatchScope{zone}; }
-    static CatchScope Inner(Zone* zone, JSCallReducerAssembler* gasm) {
-      return {zone, gasm};
-    }
-
-    bool has_handler() const { return has_handler_; }
-    bool is_outermost() const { return parent_ == nullptr; }
-    CatchScope* parent() const { return parent_; }
-
-    // Should only be used to initialize the outermost scope (inner scopes
-    // always have a handler and are passed the gasm pointer at construction).
-    void set_has_handler(bool v) {
-      DCHECK(is_outermost());
-      has_handler_ = v;
-    }
-    void set_gasm(JSCallReducerAssembler* v) {
-      DCHECK(is_outermost());
-      gasm_ = v;
-    }
-
-    bool has_exceptional_control_flow() const {
-      return !if_exception_nodes_.empty();
-    }
-
-    void RegisterIfExceptionNode(Node* if_exception) {
-      DCHECK(has_handler());
-      if_exception_nodes_.push_back(if_exception);
-    }
-
-    void MergeExceptionalPaths(TNode<Object>* exception_out, Effect* effect_out,
-                               Control* control_out) {
-      DCHECK(has_handler());
-      DCHECK(has_exceptional_control_flow());
-
-      const int size = static_cast<int>(if_exception_nodes_.size());
-
-      if (size == 1) {
-        // No merge needed.
-        Node* e = if_exception_nodes_.at(0);
-        *exception_out = TNode<Object>::UncheckedCast(e);
-        *effect_out = Effect(e);
-        *control_out = Control(e);
-      } else {
-        DCHECK_GT(size, 1);
-
-        Node* merge = gasm_->graph()->NewNode(gasm_->common()->Merge(size),
-                                              size, if_exception_nodes_.data());
-
-        // These phis additionally take {merge} as an input. Temporarily add
-        // it to the list.
-        if_exception_nodes_.push_back(merge);
-        const int size_with_merge =
-            static_cast<int>(if_exception_nodes_.size());
-
-        Node* ephi = gasm_->graph()->NewNode(gasm_->common()->EffectPhi(size),
-                                             size_with_merge,
-                                             if_exception_nodes_.data());
-        Node* phi = gasm_->graph()->NewNode(
-            gasm_->common()->Phi(MachineRepresentation::kTagged, size),
-            size_with_merge, if_exception_nodes_.data());
-        if_exception_nodes_.pop_back();
-
-        *exception_out = TNode<Object>::UncheckedCast(phi);
-        *effect_out = Effect(ephi);
-        *control_out = Control(merge);
-      }
-    }
-
-   private:
-    JSCallReducerAssembler* gasm_ = nullptr;
-    CatchScope* const parent_ = nullptr;
-    bool has_handler_ = false;
-    NodeVector if_exception_nodes_;
-  };
-
   class TryCatchBuilder0 {
    public:
     using TryFunction = VoidGenerator0;
@@ -613,7 +351,7 @@
           JSCallRuntime2(Runtime::kThrowTypeError,
                          NumberConstant(static_cast<double>(
                              MessageTemplate::kCalledNonCallable)),
-                         maybe_callable, frame_state);
+                         maybe_callable, ContextInput(), frame_state);
           Unreachable();  // The runtime call throws unconditionally.
         })
         .ExpectTrue();
@@ -659,17 +397,11 @@
     return FrameState(NodeProperties::GetFrameStateInput(node_));
   }
 
-  JSOperatorBuilder* javascript() const { return jsgraph()->javascript(); }
-
   CompilationDependencies* dependencies() const { return dependencies_; }
 
  private:
   CompilationDependencies* const dependencies_;
   Node* const node_;
-  CatchScope outermost_catch_scope_;
-  Node* outermost_handler_;
-  CatchScope* catch_scope_;
-  friend class CatchScope;
 };
 
 enum class ArrayReduceDirection { kLeft, kRight };
@@ -1109,16 +841,6 @@
   });
 }
 
-TNode<Object> JSCallReducerAssembler::JSCallRuntime2(
-    Runtime::FunctionId function_id, TNode<Object> arg0, TNode<Object> arg1,
-    FrameState frame_state) {
-  return MayThrow(_ {
-    return AddNode<Object>(
-        graph()->NewNode(javascript()->CallRuntime(function_id, 2), arg0, arg1,
-                         ContextInput(), frame_state, effect(), control()));
-  });
-}
-
 TNode<Object> JSCallReducerAssembler::CopyNode() {
   return MayThrow(_ {
     Node* copy = graph()->CloneNode(node_ptr());
@@ -1134,6 +856,7 @@
       graph()->NewNode(javascript()->CreateArray(1, base::nullopt), ctor, ctor,
                        size, ContextInput(), frame_state, effect(), control()));
 }
+
 TNode<JSArray> JSCallReducerAssembler::AllocateEmptyJSArray(
     ElementsKind kind, const NativeContextRef& native_context) {
   // TODO(jgruber): Port AllocationBuilder to JSGraphAssembler.
@@ -1156,6 +879,15 @@
   return TNode<JSArray>::UncheckedCast(result);
 }
 
+TNode<Number> JSCallReducerAssembler::LoadMapElementsKind(TNode<Map> map) {
+  TNode<Number> bit_field2 =
+      LoadField<Number>(AccessBuilder::ForMapBitField2(), map);
+  return NumberShiftRightLogical(
+      NumberBitwiseAnd(bit_field2,
+                       NumberConstant(Map::Bits2::ElementsKindBits::kMask)),
+      NumberConstant(Map::Bits2::ElementsKindBits::kShift));
+}
+
 TNode<Object> JSCallReducerAssembler::ReduceMathUnary(const Operator* op) {
   TNode<Object> input = Argument(0);
   TNode<Number> input_as_number = SpeculativeToNumber(input);
@@ -1326,6 +1058,62 @@
       .Value();
 }
 
+TNode<Object> JSCallReducerAssembler::ReduceJSCallMathMinMaxWithArrayLike(
+    Builtin builtin) {
+  JSCallWithArrayLikeNode n(node_ptr());
+  TNode<Object> arguments_list = n.Argument(0);
+
+  auto call_builtin = MakeLabel();
+  auto done = MakeLabel(MachineRepresentation::kTagged);
+
+  // Check if {arguments_list} is a JSArray.
+  GotoIf(ObjectIsSmi(arguments_list), &call_builtin);
+  TNode<Map> arguments_list_map =
+      LoadField<Map>(AccessBuilder::ForMap(),
+                     TNode<HeapObject>::UncheckedCast(arguments_list));
+  TNode<Number> arguments_list_instance_type = LoadField<Number>(
+      AccessBuilder::ForMapInstanceType(), arguments_list_map);
+  auto check_instance_type =
+      NumberEqual(arguments_list_instance_type, NumberConstant(JS_ARRAY_TYPE));
+  GotoIfNot(check_instance_type, &call_builtin);
+
+  // Check if {arguments_list} has PACKED_DOUBLE_ELEMENTS or
+  // HOLEY_DOUBLE_ELEMENTS.
+  TNode<Number> arguments_list_elements_kind =
+      LoadMapElementsKind(arguments_list_map);
+
+  static_assert(PACKED_DOUBLE_ELEMENTS == 4);
+  static_assert(HOLEY_DOUBLE_ELEMENTS == 5);
+  auto check_elements_kind = NumberEqual(
+      NumberBitwiseOr(arguments_list_elements_kind, NumberConstant(1)),
+      NumberConstant(HOLEY_DOUBLE_ELEMENTS));
+  GotoIfNot(check_elements_kind, &call_builtin);
+
+  // If {arguments_list} is a JSArray with PACKED_DOUBLE_ELEMENTS, calculate the
+  // result with inlined loop.
+  TNode<JSArray> array_arguments_list =
+      TNode<JSArray>::UncheckedCast(arguments_list);
+  Goto(&done, builtin == Builtin::kMathMax
+                  ? DoubleArrayMax(array_arguments_list)
+                  : DoubleArrayMin(array_arguments_list));
+
+  // Otherwise, call BuiltinMathMin/Max as usual.
+  Bind(&call_builtin);
+  TNode<Object> call = CopyNode();
+  CallParameters const& p = n.Parameters();
+
+  // Set SpeculationMode to kDisallowSpeculation to avoid infinite
+  // recursion.
+  NodeProperties::ChangeOp(
+      call, javascript()->CallWithArrayLike(
+                p.frequency(), p.feedback(),
+                SpeculationMode::kDisallowSpeculation, p.feedback_relation()));
+  Goto(&done, call);
+
+  Bind(&done);
+  return done.PhiAt<Object>(0);
+}
+
 TNode<Object> IteratingArrayBuiltinReducerAssembler::ReduceArrayPrototypeAt(
     ZoneVector<const MapRef*> maps, bool needs_fallback_builtin_call) {
   TNode<JSArray> receiver = ReceiverInputAs<JSArray>();
@@ -1334,7 +1122,8 @@
   TNode<Number> index_num = CheckSmi(index);
   TNode<FixedArrayBase> elements = LoadElements(receiver);
 
-  TNode<Map> receiver_map = LoadMap(receiver);
+  TNode<Map> receiver_map =
+      TNode<Map>::UncheckedCast(LoadField(AccessBuilder::ForMap(), receiver));
 
   auto out = MakeLabel(MachineRepresentation::kTagged);
 
@@ -2516,6 +2305,26 @@
 
 #undef _
 
+std::pair<Node*, Node*> JSCallReducer::ReleaseEffectAndControlFromAssembler(
+    JSCallReducerAssembler* gasm) {
+  auto catch_scope = gasm->catch_scope();
+  DCHECK(catch_scope->is_outermost());
+
+  if (catch_scope->has_handler() &&
+      catch_scope->has_exceptional_control_flow()) {
+    TNode<Object> handler_exception;
+    Effect handler_effect{nullptr};
+    Control handler_control{nullptr};
+    gasm->catch_scope()->MergeExceptionalPaths(
+        &handler_exception, &handler_effect, &handler_control);
+
+    ReplaceWithValue(gasm->outermost_handler(), handler_exception,
+                     handler_effect, handler_control);
+  }
+
+  return {gasm->effect(), gasm->control()};
+}
+
 Reduction JSCallReducer::ReplaceWithSubgraph(JSCallReducerAssembler* gasm,
                                              Node* subgraph) {
   // TODO(jgruber): Consider a less fiddly way of integrating the new subgraph
@@ -4836,13 +4645,11 @@
     case Builtin::kArrayBufferIsView:
       return ReduceArrayBufferIsView(node);
     case Builtin::kDataViewPrototypeGetByteLength:
-      return ReduceArrayBufferViewAccessor(
-          node, JS_DATA_VIEW_TYPE,
-          AccessBuilder::ForJSArrayBufferViewByteLength());
+      return ReduceArrayBufferViewByteLengthAccessor(node, JS_DATA_VIEW_TYPE);
     case Builtin::kDataViewPrototypeGetByteOffset:
       return ReduceArrayBufferViewAccessor(
           node, JS_DATA_VIEW_TYPE,
-          AccessBuilder::ForJSArrayBufferViewByteOffset());
+          AccessBuilder::ForJSArrayBufferViewByteOffset(), builtin);
     case Builtin::kDataViewPrototypeGetUint8:
       return ReduceDataViewAccess(node, DataViewAccess::kGet,
                                   ExternalArrayType::kExternalUint8Array);
@@ -4892,16 +4699,13 @@
       return ReduceDataViewAccess(node, DataViewAccess::kSet,
                                   ExternalArrayType::kExternalFloat64Array);
     case Builtin::kTypedArrayPrototypeByteLength:
-      return ReduceArrayBufferViewAccessor(
-          node, JS_TYPED_ARRAY_TYPE,
-          AccessBuilder::ForJSArrayBufferViewByteLength());
+      return ReduceArrayBufferViewByteLengthAccessor(node, JS_TYPED_ARRAY_TYPE);
     case Builtin::kTypedArrayPrototypeByteOffset:
       return ReduceArrayBufferViewAccessor(
           node, JS_TYPED_ARRAY_TYPE,
-          AccessBuilder::ForJSArrayBufferViewByteOffset());
+          AccessBuilder::ForJSArrayBufferViewByteOffset(), builtin);
     case Builtin::kTypedArrayPrototypeLength:
-      return ReduceArrayBufferViewAccessor(
-          node, JS_TYPED_ARRAY_TYPE, AccessBuilder::ForJSTypedArrayLength());
+      return ReduceTypedArrayPrototypeLength(node);
     case Builtin::kTypedArrayPrototypeToStringTag:
       return ReduceTypedArrayPrototypeToStringTag(node);
     case Builtin::kMathAbs:
@@ -5193,6 +4997,13 @@
   if (TargetIsClassConstructor(node, broker())) {
     return NoChange();
   }
+
+  base::Optional<Reduction> maybe_result =
+      TryReduceJSCallMathMinMaxWithArrayLike(node);
+  if (maybe_result.has_value()) {
+    return maybe_result.value();
+  }
+
   return ReduceCallOrConstructWithArrayLikeOrSpread(
       node, n.ArgumentCount(), n.LastArgumentIndex(), p.frequency(),
       p.feedback(), p.speculation_mode(), p.feedback_relation(), n.target(),
@@ -7423,6 +7234,113 @@
   return Replace(value);
 }
 
+Reduction JSCallReducer::ReduceArrayBufferViewByteLengthAccessor(
+    Node* node, InstanceType instance_type) {
+  DCHECK(instance_type == JS_TYPED_ARRAY_TYPE ||
+         instance_type == JS_DATA_VIEW_TYPE);
+  Node* receiver = NodeProperties::GetValueInput(node, 1);
+  Effect effect{NodeProperties::GetEffectInput(node)};
+  Control control{NodeProperties::GetControlInput(node)};
+
+  MapInference inference(broker(), receiver, effect);
+  if (!inference.HaveMaps() ||
+      !inference.AllOfInstanceTypesAre(instance_type)) {
+    return inference.NoChange();
+  }
+
+  std::set<ElementsKind> elements_kinds;
+  bool maybe_rab_gsab = false;
+  if (instance_type == JS_DATA_VIEW_TYPE) {
+    maybe_rab_gsab = true;
+  } else {
+    for (const auto& map : inference.GetMaps()) {
+      ElementsKind kind = map.elements_kind();
+      elements_kinds.insert(kind);
+      if (IsRabGsabTypedArrayElementsKind(kind)) maybe_rab_gsab = true;
+    }
+  }
+
+  if (!v8_flags.harmony_rab_gsab || !maybe_rab_gsab) {
+    // We do not perform any change depending on this inference.
+    Reduction unused_reduction = inference.NoChange();
+    USE(unused_reduction);
+    // Call default implementation for non-rab/gsab TAs.
+    return ReduceArrayBufferViewAccessor(
+        node, JS_TYPED_ARRAY_TYPE,
+        AccessBuilder::ForJSArrayBufferViewByteLength(),
+        Builtin::kTypedArrayPrototypeByteLength);
+  } else if (!v8_flags.turbo_rab_gsab) {
+    return inference.NoChange();
+  }
+
+  inference.RelyOnMapsPreferStability(dependencies(), jsgraph(), &effect,
+                                      control,
+                                      CallParametersOf(node->op()).feedback());
+
+  const bool depended_on_detaching_protector =
+      dependencies()->DependOnArrayBufferDetachingProtector();
+  if (!depended_on_detaching_protector && instance_type == JS_DATA_VIEW_TYPE) {
+    // DataView prototype accessors throw on detached ArrayBuffers instead of
+    // return 0, so skip the optimization.
+    //
+    // TODO(turbofan): Ideally we would bail out if the buffer is actually
+    // detached.
+    return inference.NoChange();
+  }
+
+  JSCallReducerAssembler a(this, node);
+  TNode<JSTypedArray> typed_array =
+      TNode<JSTypedArray>::UncheckedCast(receiver);
+  TNode<Number> length = a.ArrayBufferViewByteLength(
+      typed_array, instance_type, std::move(elements_kinds), a.ContextInput());
+
+  return ReplaceWithSubgraph(&a, length);
+}
+
+Reduction JSCallReducer::ReduceTypedArrayPrototypeLength(Node* node) {
+  Node* receiver = NodeProperties::GetValueInput(node, 1);
+  Effect effect{NodeProperties::GetEffectInput(node)};
+  Control control{NodeProperties::GetControlInput(node)};
+
+  MapInference inference(broker(), receiver, effect);
+  if (!inference.HaveMaps() ||
+      !inference.AllOfInstanceTypesAre(JS_TYPED_ARRAY_TYPE)) {
+    return inference.NoChange();
+  }
+
+  std::set<ElementsKind> elements_kinds;
+  bool maybe_rab_gsab = false;
+  for (const auto& map : inference.GetMaps()) {
+    ElementsKind kind = map.elements_kind();
+    elements_kinds.insert(kind);
+    if (IsRabGsabTypedArrayElementsKind(kind)) maybe_rab_gsab = true;
+  }
+
+  if (!v8_flags.harmony_rab_gsab || !maybe_rab_gsab) {
+    // We do not perform any change depending on this inference.
+    Reduction unused_reduction = inference.NoChange();
+    USE(unused_reduction);
+    // Call default implementation for non-rab/gsab TAs.
+    return ReduceArrayBufferViewAccessor(node, JS_TYPED_ARRAY_TYPE,
+                                         AccessBuilder::ForJSTypedArrayLength(),
+                                         Builtin::kTypedArrayPrototypeLength);
+  } else if (!v8_flags.turbo_rab_gsab) {
+    return inference.NoChange();
+  }
+
+  inference.RelyOnMapsPreferStability(dependencies(), jsgraph(), &effect,
+                                      control,
+                                      CallParametersOf(node->op()).feedback());
+
+  JSCallReducerAssembler a(this, node);
+  TNode<JSTypedArray> typed_array =
+      TNode<JSTypedArray>::UncheckedCast(receiver);
+  TNode<Number> length = a.TypedArrayLength(
+      typed_array, std::move(elements_kinds), a.ContextInput());
+
+  return ReplaceWithSubgraph(&a, length);
+}
+
 // ES #sec-number.isfinite
 Reduction JSCallReducer::ReduceNumberIsFinite(Node* node) {
   JSCallNode n(node);
@@ -7931,7 +7849,8 @@
 }
 
 Reduction JSCallReducer::ReduceArrayBufferViewAccessor(
-    Node* node, InstanceType instance_type, FieldAccess const& access) {
+    Node* node, InstanceType instance_type, FieldAccess const& access,
+    Builtin builtin) {
   Node* receiver = NodeProperties::GetValueInput(node, 1);
   Effect effect{NodeProperties::GetEffectInput(node)};
   Control control{NodeProperties::GetControlInput(node)};
@@ -7942,15 +7861,15 @@
     return inference.NoChange();
   }
 
-  // TODO(v8:11111): We skip this optimization for RAB/GSAB for now. Should
-  // have some optimization here eventually.
-  for (const auto& map : inference.GetMaps()) {
-    if (IsRabGsabTypedArrayElementsKind(map.elements_kind())) {
-      return inference.NoChange();
-    }
-  }
+  DCHECK_IMPLIES((builtin == Builtin::kTypedArrayPrototypeLength ||
+                  builtin == Builtin::kTypedArrayPrototypeByteLength),
+                 base::none_of(inference.GetMaps(), [](const auto& map) {
+                   return IsRabGsabTypedArrayElementsKind(map.elements_kind());
+                 }));
 
-  CHECK(inference.RelyOnMapsViaStability(dependencies()));
+  if (!inference.RelyOnMapsViaStability(dependencies())) {
+    return inference.NoChange();
+  }
 
   const bool depended_on_detaching_protector =
       dependencies()->DependOnArrayBufferDetachingProtector();
@@ -8054,11 +7973,20 @@
     offset = effect = graph()->NewNode(simplified()->CheckBounds(p.feedback()),
                                        offset, byte_length, effect, control);
   } else {
-    // We only deal with DataViews here that have Smi [[ByteLength]]s.
-    Node* byte_length = effect =
-        graph()->NewNode(simplified()->LoadField(
-                             AccessBuilder::ForJSArrayBufferViewByteLength()),
-                         receiver, effect, control);
+    Node* byte_length;
+    if (!v8_flags.harmony_rab_gsab) {
+      // We only deal with DataViews here that have Smi [[ByteLength]]s.
+      byte_length = effect =
+          graph()->NewNode(simplified()->LoadField(
+                               AccessBuilder::ForJSArrayBufferViewByteLength()),
+                           receiver, effect, control);
+    } else {
+      JSCallReducerAssembler a(this, node);
+      byte_length = a.ArrayBufferViewByteLength(
+          TNode<JSArrayBufferView>::UncheckedCast(receiver), JS_DATA_VIEW_TYPE,
+          {}, a.ContextInput());
+      std::tie(effect, control) = ReleaseEffectAndControlFromAssembler(&a);
+    }
 
     if (element_size > 1) {
       // For non-byte accesses we also need to check that the {offset}
@@ -8406,6 +8334,113 @@
   return NoChange();
 }
 
+base::Optional<Reduction> JSCallReducer::TryReduceJSCallMathMinMaxWithArrayLike(
+    Node* node) {
+  if (!v8_flags.turbo_optimize_math_minmax) return base::nullopt;
+
+  JSCallWithArrayLikeNode n(node);
+  CallParameters const& p = n.Parameters();
+  Node* target = n.target();
+  Effect effect = n.effect();
+  Control control = n.control();
+
+  if (p.speculation_mode() == SpeculationMode::kDisallowSpeculation) {
+    return base::nullopt;
+  }
+
+  if (n.ArgumentCount() != 1) {
+    return base::nullopt;
+  }
+
+  if (!dependencies()->DependOnNoElementsProtector()) {
+    return base::nullopt;
+  }
+
+  // These ops are handled by ReduceCallOrConstructWithArrayLikeOrSpread.
+  // IrOpcode::kJSCreateEmptyLiteralArray is not included, since arguments_list
+  // for Math.min/min is not likely to keep empty.
+  Node* arguments_list = n.Argument(0);
+  if (arguments_list->opcode() == IrOpcode::kJSCreateLiteralArray ||
+      arguments_list->opcode() == IrOpcode::kJSCreateArguments) {
+    return base::nullopt;
+  }
+
+  HeapObjectMatcher m(target);
+  if (m.HasResolvedValue()) {
+    ObjectRef target_ref = m.Ref(broker());
+    if (target_ref.IsJSFunction()) {
+      JSFunctionRef function = target_ref.AsJSFunction();
+
+      // Don't inline cross native context.
+      if (!function.native_context().equals(native_context())) {
+        return base::nullopt;
+      }
+
+      SharedFunctionInfoRef shared = function.shared();
+      Builtin builtin =
+          shared.HasBuiltinId() ? shared.builtin_id() : Builtin::kNoBuiltinId;
+      if (builtin == Builtin::kMathMax || builtin == Builtin::kMathMin) {
+        return ReduceJSCallMathMinMaxWithArrayLike(node, builtin);
+      } else {
+        return base::nullopt;
+      }
+    }
+  }
+
+  // Try specialize the JSCallWithArrayLike node with feedback target.
+  if (ShouldUseCallICFeedback(target) &&
+      p.feedback_relation() == CallFeedbackRelation::kTarget &&
+      p.feedback().IsValid()) {
+    ProcessedFeedback const& feedback =
+        broker()->GetFeedbackForCall(p.feedback());
+    if (feedback.IsInsufficient()) {
+      return base::nullopt;
+    }
+    base::Optional<HeapObjectRef> feedback_target = feedback.AsCall().target();
+    if (feedback_target.has_value() && feedback_target->map().is_callable()) {
+      Node* target_function = jsgraph()->Constant(*feedback_target);
+      ObjectRef target_ref = feedback_target.value();
+      if (!target_ref.IsJSFunction()) {
+        return base::nullopt;
+      }
+      JSFunctionRef function = target_ref.AsJSFunction();
+      SharedFunctionInfoRef shared = function.shared();
+      Builtin builtin =
+          shared.HasBuiltinId() ? shared.builtin_id() : Builtin::kNoBuiltinId;
+      if (builtin == Builtin::kMathMax || builtin == Builtin::kMathMin) {
+        // Check that the {target} is still the {target_function}.
+        Node* check = graph()->NewNode(simplified()->ReferenceEqual(), target,
+                                       target_function);
+        effect = graph()->NewNode(
+            simplified()->CheckIf(DeoptimizeReason::kWrongCallTarget), check,
+            effect, control);
+
+        // Specialize the JSCallWithArrayLike node to the {target_function}.
+        NodeProperties::ReplaceValueInput(node, target_function,
+                                          n.TargetIndex());
+        NodeProperties::ReplaceEffectInput(node, effect);
+        // Try to further reduce the Call MathMin/Max with double array.
+        return Changed(node).FollowedBy(
+            ReduceJSCallMathMinMaxWithArrayLike(node, builtin));
+      }
+    }
+  }
+
+  return base::nullopt;
+}
+
+Reduction JSCallReducer::ReduceJSCallMathMinMaxWithArrayLike(Node* node,
+                                                             Builtin builtin) {
+  JSCallWithArrayLikeNode n(node);
+  DCHECK_NE(n.Parameters().speculation_mode(),
+            SpeculationMode::kDisallowSpeculation);
+  DCHECK_EQ(n.ArgumentCount(), 1);
+
+  JSCallReducerAssembler a(this, node);
+  Node* subgraph = a.ReduceJSCallMathMinMaxWithArrayLike(builtin);
+  return ReplaceWithSubgraph(&a, subgraph);
+}
+
 CompilationDependencies* JSCallReducer::dependencies() const {
   return broker()->dependencies();
 }
diff -r -u --color up/v8/src/compiler/js-call-reducer.h nw/v8/src/compiler/js-call-reducer.h
--- up/v8/src/compiler/js-call-reducer.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/js-call-reducer.h	2023-01-19 16:46:36.113942930 -0500
@@ -181,6 +181,9 @@
   Reduction ReduceTypedArrayConstructor(Node* node,
                                         const SharedFunctionInfoRef& shared);
   Reduction ReduceTypedArrayPrototypeToStringTag(Node* node);
+  Reduction ReduceArrayBufferViewByteLengthAccessor(Node* node,
+                                                    InstanceType instance_type);
+  Reduction ReduceTypedArrayPrototypeLength(Node* node);
 
   Reduction ReduceForInsufficientFeedback(Node* node, DeoptimizeReason reason);
 
@@ -216,7 +219,8 @@
   Reduction ReduceArrayBufferIsView(Node* node);
   Reduction ReduceArrayBufferViewAccessor(Node* node,
                                           InstanceType instance_type,
-                                          FieldAccess const& access);
+                                          FieldAccess const& access,
+                                          Builtin builtin);
 
   enum class DataViewAccess { kGet, kSet };
   Reduction ReduceDataViewAccess(Node* node, DataViewAccess access,
@@ -229,8 +233,13 @@
   Reduction ReduceNumberConstructor(Node* node);
   Reduction ReduceBigIntAsN(Node* node, Builtin builtin);
 
+  base::Optional<Reduction> TryReduceJSCallMathMinMaxWithArrayLike(Node* node);
+  Reduction ReduceJSCallMathMinMaxWithArrayLike(Node* node, Builtin builtin);
+
   // The pendant to ReplaceWithValue when using GraphAssembler-based reductions.
   Reduction ReplaceWithSubgraph(JSCallReducerAssembler* gasm, Node* subgraph);
+  std::pair<Node*, Node*> ReleaseEffectAndControlFromAssembler(
+      JSCallReducerAssembler* gasm);
 
   // Helper to verify promise receiver maps are as expected.
   // On bailout from a reduction, be sure to return inference.NoChange().
diff -r -u --color up/v8/src/compiler/js-native-context-specialization.cc nw/v8/src/compiler/js-native-context-specialization.cc
--- up/v8/src/compiler/js-native-context-specialization.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/js-native-context-specialization.cc	2023-01-19 16:46:36.113942930 -0500
@@ -14,6 +14,8 @@
 #include "src/compiler/allocation-builder-inl.h"
 #include "src/compiler/allocation-builder.h"
 #include "src/compiler/compilation-dependencies.h"
+#include "src/compiler/frame-states.h"
+#include "src/compiler/graph-assembler.h"
 #include "src/compiler/js-graph.h"
 #include "src/compiler/js-heap-broker.h"
 #include "src/compiler/js-operator.h"
@@ -24,9 +26,11 @@
 #include "src/compiler/property-access-builder.h"
 #include "src/compiler/simplified-operator.h"
 #include "src/compiler/type-cache.h"
+#include "src/flags/flags.h"
 #include "src/handles/handles.h"
 #include "src/heap/factory.h"
 #include "src/heap/heap-write-barrier-inl.h"
+#include "src/objects/elements-kind.h"
 #include "src/objects/feedback-vector.h"
 #include "src/objects/heap-number.h"
 #include "src/objects/string.h"
@@ -577,6 +581,9 @@
   JSFunctionRef this_function_ref = m.Ref(broker()).AsJSFunction();
   MapRef function_map = this_function_ref.map();
   HeapObjectRef current = function_map.prototype();
+  // The uppermost JSFunction on the class hierarchy (above it, there can be
+  // other JSObjects, e.g., Proxies).
+  base::Optional<JSObjectRef> last_function;
 
   Node* return_value;
   Node* ctor_or_instance;
@@ -607,6 +614,7 @@
       if (!dependencies()->DependOnArrayIteratorProtector()) {
         return NoChange();
       }
+      last_function = current_function;
 
       if (kind == FunctionKind::kDefaultBaseConstructor) {
         return_value = jsgraph()->BooleanConstant(true);
@@ -614,9 +622,32 @@
         // Generate a builtin call for creating the instance.
         Node* constructor = jsgraph()->Constant(current_function);
 
+        // In the current FrameState setup, the two outputs of this bytecode are
+        // poked at indices slot(index(reg_2)) (boolean_output) and
+        // slot(index(reg_2) + 1) (object_output). Now we're reducing this
+        // bytecode to a builtin call which only has one output (object_output).
+        // Change where in the FrameState the output is poked at.
+
+        // The current poke location points to the location for boolean_ouput.
+        // We move the poke location by -1, since the poke location decreases
+        // when the register index increases (see
+        // BytecodeGraphBuilder::Environment::BindRegistersToProjections).
+
+        // The location for boolean_output is already hard-wired to true (which
+        // is the correct value here) in
+        // BytecodeGraphBuilder::VisitFindNonDefaultConstructorOrConstruct.
+
+        FrameState old_frame_state = n.frame_state();
+        auto old_poke_offset = old_frame_state.frame_state_info()
+                                   .state_combine()
+                                   .GetOffsetToPokeAt();
+        FrameState new_frame_state = CloneFrameState(
+            jsgraph(), old_frame_state,
+            OutputFrameStateCombine::PokeAt(old_poke_offset - 1));
+
         effect = ctor_or_instance = graph()->NewNode(
             jsgraph()->javascript()->Create(), constructor, new_target,
-            n.context(), n.frame_state(), effect, control);
+            n.context(), new_frame_state, effect, control);
       } else {
         return_value = jsgraph()->BooleanConstant(false);
         ctor_or_instance = jsgraph()->Constant(current_function);
@@ -628,8 +659,8 @@
     current = current_function.map().prototype();
   }
 
-  dependencies()->DependOnStablePrototypeChain(function_map,
-                                               WhereToStart::kStartAtReceiver);
+  dependencies()->DependOnStablePrototypeChain(
+      function_map, WhereToStart::kStartAtReceiver, last_function);
 
   // Update the uses of {node}.
   for (Edge edge : node->use_edges()) {
@@ -2092,6 +2123,7 @@
   Node* receiver = NodeProperties::GetValueInput(node, 0);
   Effect effect{NodeProperties::GetEffectInput(node)};
   Control control{NodeProperties::GetControlInput(node)};
+  Node* context = NodeProperties::GetContextInput(node);
 
   // TODO(neis): It's odd that we do optimizations below that don't really care
   // about the feedback, but we don't do them when the feedback is megamorphic.
@@ -2201,8 +2233,8 @@
 
     // Access the actual element.
     ValueEffectControl continuation =
-        BuildElementAccess(receiver, index, value, effect, control, access_info,
-                           feedback.keyed_mode());
+        BuildElementAccess(receiver, index, value, effect, control, context,
+                           access_info, feedback.keyed_mode());
     value = continuation.value();
     effect = continuation.effect();
     control = continuation.control();
@@ -2266,9 +2298,9 @@
       }
 
       // Access the actual element.
-      ValueEffectControl continuation =
-          BuildElementAccess(this_receiver, this_index, this_value, this_effect,
-                             this_control, access_info, feedback.keyed_mode());
+      ValueEffectControl continuation = BuildElementAccess(
+          this_receiver, this_index, this_value, this_effect, this_control,
+          context, access_info, feedback.keyed_mode());
       values.push_back(continuation.value());
       effects.push_back(continuation.effect());
       controls.push_back(continuation.control());
@@ -3086,6 +3118,7 @@
   switch (kind) {
 #define TYPED_ARRAY_CASE(Type, type, TYPE, ctype) \
   case TYPE##_ELEMENTS:                           \
+  case RAB_GSAB_##TYPE##_ELEMENTS:                \
     return kExternal##Type##Array;
     TYPED_ARRAYS(TYPED_ARRAY_CASE)
 #undef TYPED_ARRAY_CASE
@@ -3100,14 +3133,18 @@
 JSNativeContextSpecialization::ValueEffectControl
 JSNativeContextSpecialization::BuildElementAccess(
     Node* receiver, Node* index, Node* value, Node* effect, Node* control,
-    ElementAccessInfo const& access_info, KeyedAccessMode const& keyed_mode) {
+    Node* context, ElementAccessInfo const& access_info,
+    KeyedAccessMode const& keyed_mode) {
   // TODO(bmeurer): We currently specialize based on elements kind. We should
   // also be able to properly support strings and other JSObjects here.
   ElementsKind elements_kind = access_info.elements_kind();
+  DCHECK_IMPLIES(IsRabGsabTypedArrayElementsKind(elements_kind),
+                 v8_flags.turbo_rab_gsab);
   ZoneVector<MapRef> const& receiver_maps =
       access_info.lookup_start_object_maps();
 
-  if (IsTypedArrayElementsKind(elements_kind)) {
+  if (IsTypedArrayElementsKind(elements_kind) ||
+      IsRabGsabTypedArrayElementsKind(elements_kind)) {
     Node* buffer_or_receiver = receiver;
     Node* length;
     Node* base_pointer;
@@ -3117,7 +3154,9 @@
     // for asm.js-like code patterns).
     base::Optional<JSTypedArrayRef> typed_array =
         GetTypedArrayConstant(broker(), receiver);
-    if (typed_array.has_value()) {
+    if (typed_array.has_value() &&
+        !IsRabGsabTypedArrayElementsKind(elements_kind)) {
+      // TODO(v8:11111): Add support for rab/gsab here.
       length = jsgraph()->Constant(static_cast<double>(typed_array->length()));
 
       DCHECK(!typed_array->is_on_heap());
@@ -3130,9 +3169,14 @@
       external_pointer = jsgraph()->PointerConstant(typed_array->data_ptr());
     } else {
       // Load the {receiver}s length.
-      length = effect = graph()->NewNode(
-          simplified()->LoadField(AccessBuilder::ForJSTypedArrayLength()),
-          receiver, effect, control);
+      JSGraphAssembler assembler(jsgraph_, zone(), BranchSemantics::kJS,
+                                 [this](Node* n) { this->Revisit(n); });
+      assembler.InitializeEffectControl(effect, control);
+      length = assembler.TypedArrayLength(
+          TNode<JSTypedArray>::UncheckedCast(receiver), {elements_kind},
+          TNode<Context>::UncheckedCast(context));
+      std::tie(effect, control) =
+          ReleaseEffectAndControlFromAssembler(&assembler);
 
       // Load the base pointer for the {receiver}. This will always be Smi
       // zero unless we allow on-heap TypedArrays, which is only the case
@@ -3903,6 +3947,27 @@
       control);
 }
 
+std::pair<Node*, Node*>
+JSNativeContextSpecialization::ReleaseEffectAndControlFromAssembler(
+    JSGraphAssembler* gasm) {
+  auto catch_scope = gasm->catch_scope();
+  DCHECK(catch_scope->is_outermost());
+
+  if (catch_scope->has_handler() &&
+      catch_scope->has_exceptional_control_flow()) {
+    TNode<Object> handler_exception;
+    Effect handler_effect{nullptr};
+    Control handler_control{nullptr};
+    gasm->catch_scope()->MergeExceptionalPaths(
+        &handler_exception, &handler_effect, &handler_control);
+
+    ReplaceWithValue(gasm->outermost_handler(), handler_exception,
+                     handler_effect, handler_control);
+  }
+
+  return {gasm->effect(), gasm->control()};
+}
+
 Graph* JSNativeContextSpecialization::graph() const {
   return jsgraph()->graph();
 }
diff -r -u --color up/v8/src/compiler/js-native-context-specialization.h nw/v8/src/compiler/js-native-context-specialization.h
--- up/v8/src/compiler/js-native-context-specialization.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/js-native-context-specialization.h	2023-01-19 16:46:36.113942930 -0500
@@ -7,6 +7,7 @@
 
 #include "src/base/flags.h"
 #include "src/base/optional.h"
+#include "src/compiler/graph-assembler.h"
 #include "src/compiler/graph-reducer.h"
 #include "src/compiler/js-heap-broker.h"
 #include "src/deoptimizer/deoptimize-reason.h"
@@ -189,7 +190,7 @@
   // Construct the appropriate subgraph for element access.
   ValueEffectControl BuildElementAccess(Node* receiver, Node* index,
                                         Node* value, Node* effect,
-                                        Node* control,
+                                        Node* control, Node* context,
                                         ElementAccessInfo const& access_info,
                                         KeyedAccessMode const& keyed_mode);
 
@@ -249,6 +250,9 @@
 
   Node* BuildLoadPrototypeFromObject(Node* object, Node* effect, Node* control);
 
+  std::pair<Node*, Node*> ReleaseEffectAndControlFromAssembler(
+      JSGraphAssembler* assembler);
+
   Graph* graph() const;
   JSGraph* jsgraph() const { return jsgraph_; }
 
diff -r -u --color up/v8/src/compiler/js-operator.cc nw/v8/src/compiler/js-operator.cc
--- up/v8/src/compiler/js-operator.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/js-operator.cc	2023-01-19 16:46:36.113942930 -0500
@@ -916,23 +916,21 @@
   return CallRuntime(f, f->nargs);
 }
 
-
-const Operator* JSOperatorBuilder::CallRuntime(Runtime::FunctionId id,
-                                               size_t arity) {
+const Operator* JSOperatorBuilder::CallRuntime(
+    Runtime::FunctionId id, size_t arity, Operator::Properties properties) {
   const Runtime::Function* f = Runtime::FunctionForId(id);
-  return CallRuntime(f, arity);
+  return CallRuntime(f, arity, properties);
 }
 
-
-const Operator* JSOperatorBuilder::CallRuntime(const Runtime::Function* f,
-                                               size_t arity) {
+const Operator* JSOperatorBuilder::CallRuntime(
+    const Runtime::Function* f, size_t arity, Operator::Properties properties) {
   CallRuntimeParameters parameters(f->function_id, arity);
   DCHECK(f->nargs == -1 || f->nargs == static_cast<int>(parameters.arity()));
-  return zone()->New<Operator1<CallRuntimeParameters>>(   // --
-      IrOpcode::kJSCallRuntime, Operator::kNoProperties,  // opcode
-      "JSCallRuntime",                                    // name
-      parameters.arity(), 1, 1, f->result_size, 1, 2,     // inputs/outputs
-      parameters);                                        // parameter
+  return zone()->New<Operator1<CallRuntimeParameters>>(  // --
+      IrOpcode::kJSCallRuntime, properties,              // opcode
+      "JSCallRuntime",                                   // name
+      parameters.arity(), 1, 1, f->result_size, 1, 2,    // inputs/outputs
+      parameters);                                       // parameter
 }
 
 #if V8_ENABLE_WEBASSEMBLY
diff -r -u --color up/v8/src/compiler/js-operator.h nw/v8/src/compiler/js-operator.h
--- up/v8/src/compiler/js-operator.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/js-operator.h	2023-01-19 16:46:36.113942930 -0500
@@ -13,6 +13,7 @@
 #include "src/compiler/node-properties.h"
 #include "src/compiler/node.h"
 #include "src/compiler/opcodes.h"
+#include "src/compiler/operator-properties.h"
 #include "src/objects/feedback-cell.h"
 #include "src/runtime/runtime.h"
 
@@ -995,8 +996,12 @@
       SpeculationMode speculation_mode = SpeculationMode::kDisallowSpeculation,
       CallFeedbackRelation feedback_relation = CallFeedbackRelation::kTarget);
   const Operator* CallRuntime(Runtime::FunctionId id);
-  const Operator* CallRuntime(Runtime::FunctionId id, size_t arity);
-  const Operator* CallRuntime(const Runtime::Function* function, size_t arity);
+  const Operator* CallRuntime(
+      Runtime::FunctionId id, size_t arity,
+      Operator::Properties properties = Operator::kNoProperties);
+  const Operator* CallRuntime(
+      const Runtime::Function* function, size_t arity,
+      Operator::Properties properties = Operator::kNoProperties);
 
 #if V8_ENABLE_WEBASSEMBLY
   const Operator* CallWasm(const wasm::WasmModule* wasm_module,
diff -r -u --color up/v8/src/compiler/js-type-hint-lowering.cc nw/v8/src/compiler/js-type-hint-lowering.cc
--- up/v8/src/compiler/js-type-hint-lowering.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/js-type-hint-lowering.cc	2023-01-19 16:46:36.113942930 -0500
@@ -165,6 +165,8 @@
         return simplified()->SpeculativeBigIntMultiply(hint);
       case IrOpcode::kJSDivide:
         return simplified()->SpeculativeBigIntDivide(hint);
+      case IrOpcode::kJSModulus:
+        return simplified()->SpeculativeBigIntModulus(hint);
       case IrOpcode::kJSBitwiseAnd:
         return simplified()->SpeculativeBigIntBitwiseAnd(hint);
       default:
@@ -416,6 +418,7 @@
           op->opcode() == IrOpcode::kJSSubtract ||
           op->opcode() == IrOpcode::kJSMultiply ||
           op->opcode() == IrOpcode::kJSDivide ||
+          op->opcode() == IrOpcode::kJSModulus ||
           op->opcode() == IrOpcode::kJSBitwiseAnd) {
         if (Node* node = b.TryBuildBigIntBinop()) {
           return LoweringResult::SideEffectFree(node, node, control);
diff -r -u --color up/v8/src/compiler/js-typed-lowering.cc nw/v8/src/compiler/js-typed-lowering.cc
--- up/v8/src/compiler/js-typed-lowering.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/js-typed-lowering.cc	2023-01-19 16:46:36.113942930 -0500
@@ -9,6 +9,7 @@
 #include "src/codegen/code-factory.h"
 #include "src/codegen/interface-descriptors-inl.h"
 #include "src/compiler/access-builder.h"
+#include "src/compiler/common-operator.h"
 #include "src/compiler/compilation-dependencies.h"
 #include "src/compiler/graph-assembler.h"
 #include "src/compiler/js-graph.h"
@@ -841,7 +842,7 @@
     //    then ObjectIsUndetectable(left)
     // else ReferenceEqual(left, right)
 #define __ gasm.
-    JSGraphAssembler gasm(jsgraph(), jsgraph()->zone());
+    JSGraphAssembler gasm(jsgraph(), jsgraph()->zone(), BranchSemantics::kJS);
     gasm.InitializeEffectControl(r.effect(), r.control());
 
     auto lhs = TNode<Object>::UncheckedCast(r.left());
diff -r -u --color up/v8/src/compiler/linkage.cc nw/v8/src/compiler/linkage.cc
--- up/v8/src/compiler/linkage.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/linkage.cc	2023-01-19 16:46:36.124776260 -0500
@@ -268,6 +268,7 @@
     case Runtime::kAbort:
     case Runtime::kAllocateInOldGeneration:
     case Runtime::kCreateIterResultObject:
+    case Runtime::kGrowableSharedArrayBufferByteLength:
     case Runtime::kIncBlockCounter:
     case Runtime::kIsFunction:
     case Runtime::kNewClosure:
diff -r -u --color up/v8/src/compiler/loop-analysis.cc nw/v8/src/compiler/loop-analysis.cc
--- up/v8/src/compiler/loop-analysis.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/loop-analysis.cc	2023-01-19 16:46:36.124776260 -0500
@@ -625,9 +625,8 @@
             WasmCode::kWasmTableGetFuncRef, WasmCode::kWasmTableSetFuncRef,
             WasmCode::kWasmTableGrow,
             // Atomics.
-            WasmCode::kWasmAtomicNotify, WasmCode::kWasmI32AtomicWait32,
-            WasmCode::kWasmI32AtomicWait64, WasmCode::kWasmI64AtomicWait32,
-            WasmCode::kWasmI64AtomicWait64,
+            WasmCode::kWasmAtomicNotify, WasmCode::kWasmI32AtomicWait,
+            WasmCode::kWasmI64AtomicWait,
             // Exceptions.
             WasmCode::kWasmAllocateFixedArray, WasmCode::kWasmThrow,
             WasmCode::kWasmRethrow, WasmCode::kWasmRethrowExplicitContext,
diff -r -u --color up/v8/src/compiler/machine-operator-reducer.cc nw/v8/src/compiler/machine-operator-reducer.cc
--- up/v8/src/compiler/machine-operator-reducer.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/machine-operator-reducer.cc	2023-01-19 16:46:36.124776260 -0500
@@ -34,6 +34,7 @@
   using IntNBinopMatcher = Int32BinopMatcher;
   using UintNBinopMatcher = Uint32BinopMatcher;
   using intN_t = int32_t;
+  using uintN_t = uint32_t;
   // WORD_SIZE refers to the N for which this adapter specializes.
   static constexpr std::size_t WORD_SIZE = 32;
 
@@ -75,6 +76,9 @@
   const Operator* IntNAdd(MachineOperatorBuilder* machine) {
     return machine->Int32Add();
   }
+  static const Operator* WordNEqual(MachineOperatorBuilder* machine) {
+    return machine->Word32Equal();
+  }
 
   Reduction ReplaceIntN(int32_t value) { return r_->ReplaceInt32(value); }
   Reduction ReduceWordNAnd(Node* node) { return r_->ReduceWord32And(node); }
@@ -85,6 +89,10 @@
   Node* UintNConstant(uint32_t value) { return r_->Uint32Constant(value); }
   Node* WordNAnd(Node* lhs, Node* rhs) { return r_->Word32And(lhs, rhs); }
 
+  Reduction ReduceWordNComparisons(Node* node) {
+    return r_->ReduceWord32Comparisons(node);
+  }
+
  private:
   MachineOperatorReducer* r_;
 };
@@ -98,6 +106,7 @@
   using IntNBinopMatcher = Int64BinopMatcher;
   using UintNBinopMatcher = Uint64BinopMatcher;
   using intN_t = int64_t;
+  using uintN_t = uint64_t;
   // WORD_SIZE refers to the N for which this adapter specializes.
   static constexpr std::size_t WORD_SIZE = 64;
 
@@ -139,6 +148,9 @@
   static const Operator* IntNAdd(MachineOperatorBuilder* machine) {
     return machine->Int64Add();
   }
+  static const Operator* WordNEqual(MachineOperatorBuilder* machine) {
+    return machine->Word64Equal();
+  }
 
   Reduction ReplaceIntN(int64_t value) { return r_->ReplaceInt64(value); }
   Reduction ReduceWordNAnd(Node* node) { return r_->ReduceWord64And(node); }
@@ -152,6 +164,10 @@
   Node* UintNConstant(uint64_t value) { return r_->Uint64Constant(value); }
   Node* WordNAnd(Node* lhs, Node* rhs) { return r_->Word64And(lhs, rhs); }
 
+  Reduction ReduceWordNComparisons(Node* node) {
+    return r_->ReduceWord64Comparisons(node);
+  }
+
  private:
   MachineOperatorReducer* r_;
 };
@@ -457,15 +473,7 @@
       return ReduceWord32Comparisons(node);
     }
     case IrOpcode::kUint32LessThanOrEqual: {
-      Uint32BinopMatcher m(node);
-      if (m.left().Is(0)) return ReplaceBool(true);            // 0 <= x => true
-      if (m.right().Is(kMaxUInt32)) return ReplaceBool(true);  // x <= M => true
-      if (m.IsFoldable()) {  // K <= K => K  (K stands for arbitrary constants)
-        return ReplaceBool(m.left().ResolvedValue() <=
-                           m.right().ResolvedValue());
-      }
-      if (m.LeftEqualsRight()) return ReplaceBool(true);  // x <= x => true
-      return ReduceWord32Comparisons(node);
+      return ReduceUintNLessThanOrEqual<Word32Adapter>(node);
     }
     case IrOpcode::kFloat32Sub: {
       Float32BinopMatcher m(node);
@@ -915,12 +923,7 @@
       return ReduceWord64Comparisons(node);
     }
     case IrOpcode::kUint64LessThanOrEqual: {
-      Uint64BinopMatcher m(node);
-      if (m.IsFoldable()) {  // K <= K => K  (K stands for arbitrary constants)
-        return ReplaceBool(m.left().ResolvedValue() <=
-                           m.right().ResolvedValue());
-      }
-      return ReduceWord64Comparisons(node);
+      return ReduceUintNLessThanOrEqual<Word64Adapter>(node);
     }
     case IrOpcode::kFloat32Select:
     case IrOpcode::kFloat64Select:
@@ -1858,6 +1861,27 @@
   return NoChange();
 }
 
+template <typename WordNAdapter>
+Reduction MachineOperatorReducer::ReduceUintNLessThanOrEqual(Node* node) {
+  using A = WordNAdapter;
+  A a(this);
+
+  typename A::UintNBinopMatcher m(node);
+  typename A::uintN_t kMaxUIntN =
+      std::numeric_limits<typename A::uintN_t>::max();
+  if (m.left().Is(0)) return ReplaceBool(true);           // 0 <= x  =>  true
+  if (m.right().Is(kMaxUIntN)) return ReplaceBool(true);  // x <= M  =>  true
+  if (m.IsFoldable()) {  // K <= K  =>  K  (K stands for arbitrary constants)
+    return ReplaceBool(m.left().ResolvedValue() <= m.right().ResolvedValue());
+  }
+  if (m.LeftEqualsRight()) return ReplaceBool(true);  // x <= x  =>  true
+  if (m.right().Is(0)) {                              // x <= 0  =>  x == 0
+    NodeProperties::ChangeOp(node, a.WordNEqual(machine()));
+    return Changed(node);
+  }
+  return a.ReduceWordNComparisons(node);
+}
+
 namespace {
 
 // Represents an operation of the form `(source & mask) == masked_value`.
diff -r -u --color up/v8/src/compiler/machine-operator-reducer.h nw/v8/src/compiler/machine-operator-reducer.h
--- up/v8/src/compiler/machine-operator-reducer.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/machine-operator-reducer.h	2023-01-19 16:46:36.124776260 -0500
@@ -134,6 +134,8 @@
   Reduction ReduceWordNOr(Node* node);
   template <typename WordNAdapter>
   Reduction ReduceWordNXor(Node* node);
+  template <typename WordNAdapter>
+  Reduction ReduceUintNLessThanOrEqual(Node* node);
 
   // Tries to simplify "if(x == 0)" by removing the "== 0" and inverting
   // branches.
diff -r -u --color up/v8/src/compiler/memory-optimizer.cc nw/v8/src/compiler/memory-optimizer.cc
--- up/v8/src/compiler/memory-optimizer.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/memory-optimizer.cc	2023-01-19 16:46:36.124776260 -0500
@@ -6,6 +6,7 @@
 
 #include "src/base/logging.h"
 #include "src/codegen/tick-counter.h"
+#include "src/compiler/common-operator.h"
 #include "src/compiler/js-graph.h"
 #include "src/compiler/linkage.h"
 #include "src/compiler/node-properties.h"
@@ -183,7 +184,7 @@
     JSGraph* jsgraph, Zone* zone,
     MemoryLowering::AllocationFolding allocation_folding,
     const char* function_debug_name, TickCounter* tick_counter)
-    : graph_assembler_(jsgraph, zone),
+    : graph_assembler_(jsgraph, zone, BranchSemantics::kMachine),
       memory_lowering_(jsgraph, zone, &graph_assembler_, allocation_folding,
                        WriteBarrierAssertFailed, function_debug_name),
       jsgraph_(jsgraph),
diff -r -u --color up/v8/src/compiler/node.cc nw/v8/src/compiler/node.cc
--- up/v8/src/compiler/node.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/node.cc	2023-01-19 16:46:36.124776260 -0500
@@ -406,7 +406,6 @@
   DCHECK_LE(inline_capacity, kMaxInlineCapacity);
 }
 
-
 void Node::AppendUse(Use* use) {
   DCHECK(first_use_ == nullptr || first_use_->prev == nullptr);
   DCHECK_EQ(this, *use->input_ptr());
diff -r -u --color up/v8/src/compiler/node.h nw/v8/src/compiler/node.h
--- up/v8/src/compiler/node.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/node.h	2023-01-19 16:46:36.124776260 -0500
@@ -695,6 +695,13 @@
 
 Node::Uses::const_iterator Node::Uses::end() const { return const_iterator(); }
 
+inline Node::Uses::const_iterator begin(const Node::Uses& uses) {
+  return uses.begin();
+}
+inline Node::Uses::const_iterator end(const Node::Uses& uses) {
+  return uses.end();
+}
+
 }  // namespace compiler
 }  // namespace internal
 }  // namespace v8
diff -r -u --color up/v8/src/compiler/opcodes.h nw/v8/src/compiler/opcodes.h
--- up/v8/src/compiler/opcodes.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/opcodes.h	2023-01-19 16:46:36.124776260 -0500
@@ -33,21 +33,27 @@
   V(Throw)                 \
   V(End)
 
-// Opcodes for constant operators.
-#define CONSTANT_OP_LIST(V)   \
-  V(Int32Constant)            \
-  V(Int64Constant)            \
-  V(TaggedIndexConstant)      \
-  V(Float32Constant)          \
-  V(Float64Constant)          \
-  V(ExternalConstant)         \
-  V(NumberConstant)           \
-  V(PointerConstant)          \
-  V(HeapConstant)             \
-  V(CompressedHeapConstant)   \
-  V(RelocatableInt32Constant) \
+#define MACHINE_LEVEL_CONSTANT_OP_LIST(V) \
+  V(Int32Constant)                        \
+  V(Int64Constant)                        \
+  V(TaggedIndexConstant)                  \
+  V(Float32Constant)                      \
+  V(Float64Constant)                      \
+  V(CompressedHeapConstant)               \
+  V(RelocatableInt32Constant)             \
   V(RelocatableInt64Constant)
 
+#define JS_LEVEL_CONSTANT_OP_LIST(V) \
+  V(ExternalConstant)                \
+  V(NumberConstant)                  \
+  V(PointerConstant)                 \
+  V(HeapConstant)
+
+// Opcodes for constant operators.
+#define CONSTANT_OP_LIST(V)    \
+  JS_LEVEL_CONSTANT_OP_LIST(V) \
+  MACHINE_LEVEL_CONSTANT_OP_LIST(V)
+
 #define INNER_OP_LIST(V)    \
   V(Select)                 \
   V(Phi)                    \
@@ -74,7 +80,9 @@
   V(Retain)                 \
   V(MapGuard)               \
   V(FoldConstant)           \
-  V(TypeGuard)
+  V(TypeGuard)              \
+  V(EnterMachineGraph)      \
+  V(ExitMachineGraph)
 
 #define COMMON_OP_LIST(V) \
   CONSTANT_OP_LIST(V)     \
@@ -280,6 +288,11 @@
   V(CheckedUint32Div)                 \
   V(CheckedUint32Mod)                 \
   V(CheckedInt32Mul)                  \
+  V(CheckedInt64Add)                  \
+  V(CheckedInt64Sub)                  \
+  V(CheckedInt64Mul)                  \
+  V(CheckedInt64Div)                  \
+  V(CheckedInt64Mod)                  \
   V(CheckedInt32ToTaggedSigned)       \
   V(CheckedInt64ToInt32)              \
   V(CheckedInt64ToTaggedSigned)       \
@@ -299,8 +312,7 @@
   V(CheckedTaggedToFloat64)           \
   V(CheckedTaggedToInt64)             \
   V(CheckedTaggedToTaggedSigned)      \
-  V(CheckedTaggedToTaggedPointer)     \
-  V(CheckedBigInt64Add)
+  V(CheckedTaggedToTaggedPointer)
 
 #define SIMPLIFIED_COMPARE_BINOP_LIST(V) \
   V(NumberEqual)                         \
@@ -340,6 +352,7 @@
   V(BigIntSubtract)                     \
   V(BigIntMultiply)                     \
   V(BigIntDivide)                       \
+  V(BigIntModulus)                      \
   V(BigIntBitwiseAnd)
 
 #define SIMPLIFIED_SPECULATIVE_NUMBER_BINOP_LIST(V) \
@@ -397,7 +410,7 @@
 #define SIMPLIFIED_BIGINT_UNOP_LIST(V) \
   V(BigIntNegate)                      \
   V(CheckBigInt)                       \
-  V(CheckBigInt64)
+  V(CheckedBigIntToBigInt64)
 
 #define SIMPLIFIED_SPECULATIVE_NUMBER_UNOP_LIST(V) V(SpeculativeToNumber)
 
@@ -427,6 +440,8 @@
   V(ConvertReceiver)                    \
   V(ConvertTaggedHoleToUndefined)       \
   V(DateNow)                            \
+  V(DoubleArrayMax)                     \
+  V(DoubleArrayMin)                     \
   V(EnsureWritableFastElements)         \
   V(FastApiCall)                        \
   V(FindOrderedHashMapEntry)            \
@@ -508,6 +523,7 @@
   V(SpeculativeBigIntSubtract)                      \
   V(SpeculativeBigIntMultiply)                      \
   V(SpeculativeBigIntDivide)                        \
+  V(SpeculativeBigIntModulus)                       \
   V(SpeculativeBigIntBitwiseAnd)
 
 #define SIMPLIFIED_SPECULATIVE_BIGINT_UNOP_LIST(V) \
@@ -542,6 +558,13 @@
   SIMPLIFIED_OTHER_OP_LIST(V)
 
 // Opcodes for Machine-level operators.
+#define MACHINE_UNOP_32_LIST(V) \
+  V(Word32Clz)                  \
+  V(Word32Ctz)                  \
+  V(Int32AbsWithOverflow)       \
+  V(Word32ReverseBits)          \
+  V(Word32ReverseBytes)
+
 #define MACHINE_COMPARE_BINOP_LIST(V) \
   V(Word32Equal)                      \
   V(Word64Equal)                      \
@@ -560,13 +583,6 @@
   V(Float64LessThan)                  \
   V(Float64LessThanOrEqual)
 
-#define MACHINE_UNOP_32_LIST(V) \
-  V(Word32Clz)                  \
-  V(Word32Ctz)                  \
-  V(Int32AbsWithOverflow)       \
-  V(Word32ReverseBits)          \
-  V(Word32ReverseBytes)
-
 #define MACHINE_BINOP_32_LIST(V) \
   V(Word32And)                   \
   V(Word32Or)                    \
@@ -1081,6 +1097,24 @@
     return kJSEqual <= value && value <= kJSDebugger;
   }
 
+  // Returns true if opcode for machine operator.
+  static bool IsMachineOpcode(Value value) {
+    return kWord32Clz <= value && value <= kTraceInstruction;
+  }
+
+  // Returns true iff opcode is a machine-level constant.
+  static bool IsMachineConstantOpcode(Value value) {
+    switch (value) {
+#define CASE(name) \
+  case k##name:    \
+    return true;
+      MACHINE_LEVEL_CONSTANT_OP_LIST(CASE)
+#undef CASE
+      default:
+        return false;
+    }
+  }
+
   // Returns true if opcode for constant operator.
   static bool IsConstantOpcode(Value value) {
 #define CASE(Name) \
diff -r -u --color up/v8/src/compiler/operation-typer.cc nw/v8/src/compiler/operation-typer.cc
--- up/v8/src/compiler/operation-typer.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/operation-typer.cc	2023-01-19 16:46:36.124776260 -0500
@@ -585,7 +585,9 @@
 
 Type OperationTyper::CheckBigInt(Type type) { return Type::BigInt(); }
 
-Type OperationTyper::CheckBigInt64(Type type) { return Type::SignedBigInt64(); }
+Type OperationTyper::CheckedBigIntToBigInt64(Type type) {
+  return Type::SignedBigInt64();
+}
 
 Type OperationTyper::NumberAdd(Type lhs, Type rhs) {
   DCHECK(lhs.Is(Type::Number()));
@@ -1130,6 +1132,11 @@
 SPECULATIVE_NUMBER_BINOP(NumberShiftRightLogical)
 #undef SPECULATIVE_NUMBER_BINOP
 
+#define MACHINE_BINOP(Name) \
+  Type OperationTyper::Name(Type, Type) { return Type::Machine(); }
+TYPER_SUPPORTED_MACHINE_BINOP_LIST(MACHINE_BINOP)
+#undef MACHINE_BINOP
+
 Type OperationTyper::BigIntAdd(Type lhs, Type rhs) {
   DCHECK(lhs.Is(Type::BigInt()));
   DCHECK(rhs.Is(Type::BigInt()));
@@ -1162,6 +1169,14 @@
   return Type::BigInt();
 }
 
+Type OperationTyper::BigIntModulus(Type lhs, Type rhs) {
+  DCHECK(lhs.Is(Type::BigInt()));
+  DCHECK(rhs.Is(Type::BigInt()));
+
+  if (lhs.IsNone() || rhs.IsNone()) return Type::None();
+  return Type::BigInt();
+}
+
 Type OperationTyper::BigIntBitwiseAnd(Type lhs, Type rhs) {
   DCHECK(lhs.Is(Type::BigInt()));
   DCHECK(rhs.Is(Type::BigInt()));
@@ -1196,6 +1211,11 @@
   if (lhs.IsNone() || rhs.IsNone()) return Type::None();
   return Type::BigInt();
 }
+
+Type OperationTyper::SpeculativeBigIntModulus(Type lhs, Type rhs) {
+  if (lhs.IsNone() || rhs.IsNone()) return Type::None();
+  return Type::BigInt();
+}
 
 Type OperationTyper::SpeculativeBigIntBitwiseAnd(Type lhs, Type rhs) {
   if (lhs.IsNone() || rhs.IsNone()) return Type::None();
diff -r -u --color up/v8/src/compiler/operation-typer.h nw/v8/src/compiler/operation-typer.h
--- up/v8/src/compiler/operation-typer.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/operation-typer.h	2023-01-19 16:46:36.124776260 -0500
@@ -9,6 +9,26 @@
 #include "src/compiler/opcodes.h"
 #include "src/compiler/types.h"
 
+#define TYPER_SUPPORTED_MACHINE_BINOP_LIST(V) \
+  V(Int32Add)                                 \
+  V(Int64Add)                                 \
+  V(Int32Sub)                                 \
+  V(Int64Sub)                                 \
+  V(Load)                                     \
+  V(Uint32Div)                                \
+  V(Uint64Div)                                \
+  V(Uint32LessThan)                           \
+  V(Uint32LessThanOrEqual)                    \
+  V(Uint64LessThanOrEqual)                    \
+  V(Word32And)                                \
+  V(Word32Equal)                              \
+  V(Word32Or)                                 \
+  V(Word32Shl)                                \
+  V(Word32Shr)                                \
+  V(Word64And)                                \
+  V(Word64Shl)                                \
+  V(Word64Shr)
+
 namespace v8 {
 namespace internal {
 
@@ -54,6 +74,7 @@
   SIMPLIFIED_BIGINT_BINOP_LIST(DECLARE_METHOD)
   SIMPLIFIED_SPECULATIVE_NUMBER_BINOP_LIST(DECLARE_METHOD)
   SIMPLIFIED_SPECULATIVE_BIGINT_BINOP_LIST(DECLARE_METHOD)
+  TYPER_SUPPORTED_MACHINE_BINOP_LIST(DECLARE_METHOD)
 #undef DECLARE_METHOD
 
   // Comparison operators.
diff -r -u --color up/v8/src/compiler/pipeline.cc nw/v8/src/compiler/pipeline.cc
--- up/v8/src/compiler/pipeline.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/pipeline.cc	2023-01-19 16:46:36.124776260 -0500
@@ -20,6 +20,7 @@
 #include "src/common/high-allocation-throughput-scope.h"
 #include "src/compiler/add-type-assertions-reducer.h"
 #include "src/compiler/all-nodes.h"
+#include "src/compiler/backend/bitcast-elider.h"
 #include "src/compiler/backend/code-generator.h"
 #include "src/compiler/backend/frame-elider.h"
 #include "src/compiler/backend/instruction-selector.h"
@@ -83,11 +84,12 @@
 #include "src/compiler/turboshaft/graph-builder.h"
 #include "src/compiler/turboshaft/graph-visualizer.h"
 #include "src/compiler/turboshaft/graph.h"
-#include "src/compiler/turboshaft/machine-optimization-assembler.h"
+#include "src/compiler/turboshaft/machine-optimization-reducer.h"
 #include "src/compiler/turboshaft/optimization-phase.h"
 #include "src/compiler/turboshaft/recreate-schedule.h"
+#include "src/compiler/turboshaft/select-lowering-reducer.h"
 #include "src/compiler/turboshaft/simplify-tf-loops.h"
-#include "src/compiler/turboshaft/value-numbering-assembler.h"
+#include "src/compiler/turboshaft/value-numbering-reducer.h"
 #include "src/compiler/type-narrowing-reducer.h"
 #include "src/compiler/typed-optimization.h"
 #include "src/compiler/typer.h"
@@ -1960,30 +1962,44 @@
   DECL_PIPELINE_PHASE_CONSTANTS(LateOptimization)
 
   void Run(PipelineData* data, Zone* temp_zone) {
-    GraphReducer graph_reducer(
-        temp_zone, data->graph(), &data->info()->tick_counter(), data->broker(),
-        data->jsgraph()->Dead(), data->observe_node_manager());
-    LateEscapeAnalysis escape_analysis(&graph_reducer, data->graph(),
-                                       data->common(), temp_zone);
-    BranchElimination branch_condition_elimination(&graph_reducer,
-                                                   data->jsgraph(), temp_zone);
-    DeadCodeElimination dead_code_elimination(&graph_reducer, data->graph(),
-                                              data->common(), temp_zone);
-    ValueNumberingReducer value_numbering(temp_zone, data->graph()->zone());
-    MachineOperatorReducer machine_reducer(&graph_reducer, data->jsgraph());
-    CommonOperatorReducer common_reducer(
-        &graph_reducer, data->graph(), data->broker(), data->common(),
-        data->machine(), temp_zone, BranchSemantics::kMachine);
-    JSGraphAssembler graph_assembler(data->jsgraph(), temp_zone);
-    SelectLowering select_lowering(&graph_assembler, data->graph());
-    AddReducer(data, &graph_reducer, &escape_analysis);
-    AddReducer(data, &graph_reducer, &branch_condition_elimination);
-    AddReducer(data, &graph_reducer, &dead_code_elimination);
-    AddReducer(data, &graph_reducer, &machine_reducer);
-    AddReducer(data, &graph_reducer, &common_reducer);
-    AddReducer(data, &graph_reducer, &select_lowering);
-    AddReducer(data, &graph_reducer, &value_numbering);
-    graph_reducer.ReduceGraph();
+    if (data->HasTurboshaftGraph()) {
+      // TODO(dmercadier,tebbi): add missing reducers (LateEscapeAnalysis,
+      // BranchElimination, MachineOperatorReducer and CommonOperatorReducer).
+      turboshaft::OptimizationPhase<
+          turboshaft::SelectLoweringReducer,
+          turboshaft::ValueNumberingReducer>::Run(&data->turboshaft_graph(),
+                                                  temp_zone,
+                                                  data->node_origins());
+    } else {
+      GraphReducer graph_reducer(temp_zone, data->graph(),
+                                 &data->info()->tick_counter(), data->broker(),
+                                 data->jsgraph()->Dead(),
+                                 data->observe_node_manager());
+      LateEscapeAnalysis escape_analysis(&graph_reducer, data->graph(),
+                                         data->common(), temp_zone);
+      BranchElimination branch_condition_elimination(
+          &graph_reducer, data->jsgraph(), temp_zone);
+      DeadCodeElimination dead_code_elimination(&graph_reducer, data->graph(),
+                                                data->common(), temp_zone);
+      ValueNumberingReducer value_numbering(temp_zone, data->graph()->zone());
+      MachineOperatorReducer machine_reducer(&graph_reducer, data->jsgraph());
+      CommonOperatorReducer common_reducer(
+          &graph_reducer, data->graph(), data->broker(), data->common(),
+          data->machine(), temp_zone, BranchSemantics::kMachine);
+      JSGraphAssembler graph_assembler(data->jsgraph(), temp_zone,
+                                       BranchSemantics::kMachine);
+      SelectLowering select_lowering(&graph_assembler, data->graph());
+      AddReducer(data, &graph_reducer, &escape_analysis);
+      AddReducer(data, &graph_reducer, &branch_condition_elimination);
+      AddReducer(data, &graph_reducer, &dead_code_elimination);
+      AddReducer(data, &graph_reducer, &machine_reducer);
+      AddReducer(data, &graph_reducer, &common_reducer);
+      if (!v8_flags.turboshaft) {
+        AddReducer(data, &graph_reducer, &select_lowering);
+        AddReducer(data, &graph_reducer, &value_numbering);
+      }
+      graph_reducer.ReduceGraph();
+    }
   }
 };
 
@@ -2063,13 +2079,12 @@
 
   void Run(PipelineData* data, Zone* temp_zone) {
     UnparkedScopeIfNeeded scope(data->broker(),
-                                FLAG_turboshaft_trace_reduction);
+                                v8_flags.turboshaft_trace_reduction);
     turboshaft::OptimizationPhase<
-        turboshaft::AnalyzerBase,
-        turboshaft::MachineOptimizationAssembler<
-            turboshaft::ValueNumberingAssembler, false>>::
-        Run(&data->turboshaft_graph(), temp_zone, data->node_origins(),
-            turboshaft::VisitOrder::kDominator);
+        turboshaft::MachineOptimizationReducerSignallingNanImpossible,
+        turboshaft::ValueNumberingReducer>::Run(&data->turboshaft_graph(),
+                                                temp_zone,
+                                                data->node_origins());
   }
 };
 
@@ -2131,11 +2146,12 @@
 struct WasmGCLoweringPhase {
   DECL_PIPELINE_PHASE_CONSTANTS(WasmGCLowering)
 
-  void Run(PipelineData* data, Zone* temp_zone) {
+  void Run(PipelineData* data, Zone* temp_zone,
+           const wasm::WasmModule* module) {
     GraphReducer graph_reducer(
         temp_zone, data->graph(), &data->info()->tick_counter(), data->broker(),
         data->jsgraph()->Dead(), data->observe_node_manager());
-    WasmGCLowering lowering(&graph_reducer, data->mcgraph());
+    WasmGCLowering lowering(&graph_reducer, data->mcgraph(), module);
     DeadCodeElimination dead_code_elimination(&graph_reducer, data->graph(),
                                               data->common(), temp_zone);
     AddReducer(data, &graph_reducer, &lowering);
@@ -2369,6 +2385,14 @@
   }
 };
 
+struct BitcastElisionPhase {
+  DECL_PIPELINE_PHASE_CONSTANTS(BitcastElision)
+
+  void Run(PipelineData* data, Zone* temp_zone) {
+    BitcastElider bitcast_optimizer(temp_zone, data->graph());
+    bitcast_optimizer.Reduce();
+  }
+};
 
 struct MeetRegisterConstraintsPhase {
   DECL_PIPELINE_PHASE_CONSTANTS(MeetRegisterConstraints)
@@ -3006,6 +3030,9 @@
     }
     Run<PrintTurboshaftGraphPhase>(BuildTurboshaftPhase::phase_name());
 
+    Run<LateOptimizationPhase>();
+    Run<PrintTurboshaftGraphPhase>(LateOptimizationPhase::phase_name());
+
     Run<OptimizeTurboshaftPhase>();
     Run<PrintTurboshaftGraphPhase>(OptimizeTurboshaftPhase::phase_name());
 
@@ -3386,7 +3413,7 @@
   if (v8_flags.experimental_wasm_gc ||
       v8_flags.experimental_wasm_typed_funcref ||
       v8_flags.experimental_wasm_stringref) {
-    pipeline.Run<WasmGCLoweringPhase>();
+    pipeline.Run<WasmGCLoweringPhase>(module);
     pipeline.RunPrintAndVerify(WasmGCLoweringPhase::phase_name(), true);
   }
 
@@ -3712,6 +3739,10 @@
                               data->debug_name(), &temp_zone);
   }
 
+  if (Builtins::IsBuiltinId(data->info()->builtin())) {
+    Run<BitcastElisionPhase>();
+  }
+
   data->InitializeInstructionSequence(call_descriptor);
 
   // Depending on which code path led us to this function, the frame may or
diff -r -u --color up/v8/src/compiler/raw-machine-assembler.h nw/v8/src/compiler/raw-machine-assembler.h
--- up/v8/src/compiler/raw-machine-assembler.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/raw-machine-assembler.h	2023-01-19 16:46:36.124776260 -0500
@@ -611,6 +611,7 @@
   INTPTR_BINOP(Int, MulHigh)
   INTPTR_BINOP(Int, MulWithOverflow)
   INTPTR_BINOP(Int, Div)
+  INTPTR_BINOP(Int, Mod)
   INTPTR_BINOP(Int, LessThan)
   INTPTR_BINOP(Int, LessThanOrEqual)
   INTPTR_BINOP(Word, Equal)
diff -r -u --color up/v8/src/compiler/redundancy-elimination.cc nw/v8/src/compiler/redundancy-elimination.cc
--- up/v8/src/compiler/redundancy-elimination.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/redundancy-elimination.cc	2023-01-19 16:46:36.124776260 -0500
@@ -20,7 +20,7 @@
   if (node_checks_.Get(node)) return NoChange();
   switch (node->opcode()) {
     case IrOpcode::kCheckBigInt:
-    case IrOpcode::kCheckBigInt64:
+    case IrOpcode::kCheckedBigIntToBigInt64:
     case IrOpcode::kCheckBounds:
     case IrOpcode::kCheckClosure:
     case IrOpcode::kCheckEqualsInternalizedString:
@@ -38,15 +38,14 @@
     case IrOpcode::kCheckSymbol:
     // These are not really check nodes, but behave the same in that they can be
     // folded together if repeated with identical inputs.
-    case IrOpcode::kBigIntAdd:
-    case IrOpcode::kBigIntSubtract:
     case IrOpcode::kStringCharCodeAt:
     case IrOpcode::kStringCodePointAt:
     case IrOpcode::kStringFromCodePointAt:
     case IrOpcode::kStringSubstring:
-#define SIMPLIFIED_CHECKED_OP(Opcode) case IrOpcode::k##Opcode:
-      SIMPLIFIED_CHECKED_OP_LIST(SIMPLIFIED_CHECKED_OP)
-#undef SIMPLIFIED_CHECKED_OP
+#define SIMPLIFIED_OP(Opcode) case IrOpcode::k##Opcode:
+      SIMPLIFIED_CHECKED_OP_LIST(SIMPLIFIED_OP)
+      SIMPLIFIED_BIGINT_BINOP_LIST(SIMPLIFIED_OP)
+#undef SIMPLIFIED_OP
       return ReduceCheckNode(node);
     case IrOpcode::kSpeculativeNumberEqual:
     case IrOpcode::kSpeculativeNumberLessThan:
@@ -165,7 +164,7 @@
         case IrOpcode::kCheckString:
         case IrOpcode::kCheckNumber:
         case IrOpcode::kCheckBigInt:
-        case IrOpcode::kCheckBigInt64:
+        case IrOpcode::kCheckedBigIntToBigInt64:
           break;
         case IrOpcode::kCheckedInt32ToTaggedSigned:
         case IrOpcode::kCheckedInt64ToInt32:
diff -r -u --color up/v8/src/compiler/representation-change.cc nw/v8/src/compiler/representation-change.cc
--- up/v8/src/compiler/representation-change.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/representation-change.cc	2023-01-19 16:46:36.135609592 -0500
@@ -160,6 +160,9 @@
 Node* RepresentationChanger::GetRepresentationFor(
     Node* node, MachineRepresentation output_rep, Type output_type,
     Node* use_node, UseInfo use_info) {
+  // We are currently not inserting conversions in machine graphs.
+  // We might add that, though.
+  DCHECK_IMPLIES(!output_type.IsNone(), !output_type.Is(Type::Machine()));
   if (output_rep == MachineRepresentation::kNone && !output_type.IsNone()) {
     // The output representation should be set if the type is inhabited (i.e.,
     // if the value is possible).
@@ -487,28 +490,29 @@
       return TypeError(node, output_rep, output_type,
                        MachineRepresentation::kTaggedPointer);
     }
-  } else if (CanBeTaggedSigned(output_rep) &&
-             use_info.type_check() == TypeCheckKind::kHeapObject) {
-    if (!output_type.Maybe(Type::SignedSmall())) {
-      return node;
-    }
-    // TODO(turbofan): Consider adding a Bailout operator that just deopts
-    // for TaggedSigned output representation.
-    op = simplified()->CheckedTaggedToTaggedPointer(use_info.feedback());
   } else if (IsAnyTagged(output_rep)) {
     if (use_info.type_check() == TypeCheckKind::kBigInt) {
       if (output_type.Is(Type::BigInt())) {
+        DCHECK_NE(output_rep, MachineRepresentation::kTaggedSigned);
         return node;
       }
       op = simplified()->CheckBigInt(use_info.feedback());
     } else if (use_info.type_check() == TypeCheckKind::kBigInt64) {
       if (output_type.Is(Type::SignedBigInt64())) {
+        DCHECK_NE(output_rep, MachineRepresentation::kTaggedSigned);
         return node;
       }
-      op = simplified()->CheckBigInt64(use_info.feedback());
+      if (!output_type.Is(Type::BigInt())) {
+        node = InsertConversion(
+            node, simplified()->CheckBigInt(use_info.feedback()), use_node);
+      }
+      op = simplified()->CheckedBigIntToBigInt64(use_info.feedback());
+    } else if (output_rep == MachineRepresentation::kTaggedPointer ||
+               !output_type.Maybe(Type::SignedSmall())) {
+      DCHECK_NE(output_rep, MachineRepresentation::kTaggedSigned);
+      return node;
     } else {
-      return TypeError(node, output_rep, output_type,
-                       MachineRepresentation::kTaggedPointer);
+      op = simplified()->CheckedTaggedToTaggedPointer(use_info.feedback());
     }
   } else {
     return TypeError(node, output_rep, output_type,
@@ -1039,9 +1043,14 @@
     case IrOpcode::kHeapConstant: {
       HeapObjectMatcher m(node);
       if (m.Is(factory()->false_value())) {
-        return jsgraph()->Int32Constant(0);
+        return InsertTypeOverrideForVerifier(
+            Type::Constant(broker_, factory()->false_value(),
+                           jsgraph()->zone()),
+            jsgraph()->Int32Constant(0));
       } else if (m.Is(factory()->true_value())) {
-        return jsgraph()->Int32Constant(1);
+        return InsertTypeOverrideForVerifier(
+            Type::Constant(broker_, factory()->true_value(), jsgraph()->zone()),
+            jsgraph()->Int32Constant(1));
       }
       break;
     }
@@ -1353,11 +1362,51 @@
     case IrOpcode::kSpeculativeNumberAdd:  // Fall through.
     case IrOpcode::kSpeculativeSafeIntegerAdd:
     case IrOpcode::kNumberAdd:
+    case IrOpcode::kSpeculativeBigIntAdd:
       return machine()->Int64Add();
     case IrOpcode::kSpeculativeNumberSubtract:  // Fall through.
     case IrOpcode::kSpeculativeSafeIntegerSubtract:
     case IrOpcode::kNumberSubtract:
+    case IrOpcode::kSpeculativeBigIntSubtract:
       return machine()->Int64Sub();
+    case IrOpcode::kSpeculativeBigIntMultiply:
+      return machine()->Int64Mul();
+    default:
+      UNREACHABLE();
+  }
+}
+
+const Operator* RepresentationChanger::Int64OverflowOperatorFor(
+    IrOpcode::Value opcode) {
+  switch (opcode) {
+    case IrOpcode::kSpeculativeBigIntAdd:
+      return simplified()->CheckedInt64Add();
+    case IrOpcode::kSpeculativeBigIntSubtract:
+      return simplified()->CheckedInt64Sub();
+    case IrOpcode::kSpeculativeBigIntMultiply:
+      return simplified()->CheckedInt64Mul();
+    case IrOpcode::kSpeculativeBigIntDivide:
+      return simplified()->CheckedInt64Div();
+    case IrOpcode::kSpeculativeBigIntModulus:
+      return simplified()->CheckedInt64Mod();
+    default:
+      UNREACHABLE();
+  }
+}
+
+const Operator* RepresentationChanger::BigIntOperatorFor(
+    IrOpcode::Value opcode) {
+  switch (opcode) {
+    case IrOpcode::kSpeculativeBigIntAdd:
+      return simplified()->BigIntAdd();
+    case IrOpcode::kSpeculativeBigIntSubtract:
+      return simplified()->BigIntSubtract();
+    case IrOpcode::kSpeculativeBigIntMultiply:
+      return simplified()->BigIntMultiply();
+    case IrOpcode::kSpeculativeBigIntDivide:
+      return simplified()->BigIntDivide();
+    case IrOpcode::kSpeculativeBigIntModulus:
+      return simplified()->BigIntModulus();
     default:
       UNREACHABLE();
   }
diff -r -u --color up/v8/src/compiler/representation-change.h nw/v8/src/compiler/representation-change.h
--- up/v8/src/compiler/representation-change.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/representation-change.h	2023-01-19 16:46:36.135609592 -0500
@@ -36,6 +36,8 @@
   const Operator* Int32OperatorFor(IrOpcode::Value opcode);
   const Operator* Int32OverflowOperatorFor(IrOpcode::Value opcode);
   const Operator* Int64OperatorFor(IrOpcode::Value opcode);
+  const Operator* Int64OverflowOperatorFor(IrOpcode::Value opcode);
+  const Operator* BigIntOperatorFor(IrOpcode::Value opcode);
   const Operator* TaggedSignedOperatorFor(IrOpcode::Value opcode);
   const Operator* Uint32OperatorFor(IrOpcode::Value opcode);
   const Operator* Uint32OverflowOperatorFor(IrOpcode::Value opcode);
diff -r -u --color up/v8/src/compiler/simplified-lowering-verifier.cc nw/v8/src/compiler/simplified-lowering-verifier.cc
--- up/v8/src/compiler/simplified-lowering-verifier.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/simplified-lowering-verifier.cc	2023-01-19 16:46:36.135609592 -0500
@@ -4,6 +4,8 @@
 
 #include "src/compiler/simplified-lowering-verifier.h"
 
+#include "src/compiler/backend/instruction-codes.h"
+#include "src/compiler/common-operator.h"
 #include "src/compiler/operation-typer.h"
 #include "src/compiler/type-cache.h"
 
@@ -22,7 +24,8 @@
   return LeastGeneralTruncation(LeastGeneralTruncation(t1, t2), t3);
 }
 
-bool IsNonTruncatingMachineTypeFor(const MachineType& mt, const Type& type) {
+bool IsNonTruncatingMachineTypeFor(const MachineType& mt, const Type& type,
+                                   Zone* graph_zone) {
   if (type.IsNone()) return true;
   // TODO(nicohartmann@): Add more cases here.
   if (type.Is(Type::BigInt())) {
@@ -37,7 +40,7 @@
     case MachineRepresentation::kBit:
       CHECK(mt.semantic() == MachineSemantic::kBool ||
             mt.semantic() == MachineSemantic::kAny);
-      return type.Is(Type::Boolean());
+      return type.Is(Type::Boolean()) || type.Is(Type::Range(0, 1, graph_zone));
     default:
       return true;
   }
@@ -75,6 +78,22 @@
   SetTruncation(node, GeneralizeTruncation(trunc, type));
 }
 
+void SimplifiedLoweringVerifier::ReportInvalidTypeCombination(
+    Node* node, const std::vector<Type>& types) {
+  std::ostringstream types_str;
+  for (size_t i = 0; i < types.size(); ++i) {
+    if (i != 0) types_str << ", ";
+    types[i].PrintTo(types_str);
+  }
+  std::ostringstream graph_str;
+  node->Print(graph_str, 2);
+  FATAL(
+      "SimplifiedLoweringVerifierError: invalid combination of input types %s "
+      " for node #%d:%s.\n\nGraph is: %s",
+      types_str.str().c_str(), node->id(), node->op()->mnemonic(),
+      graph_str.str().c_str());
+}
+
 bool IsModuloTruncation(const Truncation& truncation) {
   return truncation.IsUsedAsWord32() || truncation.IsUsedAsWord64() ||
          Truncation::Any().IsLessGeneralThan(truncation);
@@ -134,7 +153,21 @@
     case IrOpcode::kFrameState:
     case IrOpcode::kJSStackCheck:
       break;
-    case IrOpcode::kInt32Constant:
+    case IrOpcode::kInt32Constant: {
+      // NOTE: Constants require special handling as they are shared between
+      // machine graphs and non-machine graphs lowered during SL. The former
+      // might have assigned Type::Machine() to the constant, but to be able
+      // to provide a different type for uses of constants that don't come
+      // from machine graphs, the machine-uses of Int32Constants have been
+      // put behind additional SLVerifierHints to provide the required
+      // Type::Machine() to them, such that we can treat constants here as
+      // having JS types to satisfy their non-machine uses.
+      int32_t value = OpParameter<int32_t>(node->op());
+      Type type = Type::Constant(value, graph_zone());
+      SetType(node, type);
+      SetTruncation(node, GeneralizeTruncation(Truncation::Word32(), type));
+      break;
+    }
     case IrOpcode::kInt64Constant:
     case IrOpcode::kFloat64Constant: {
       // Constants might be untyped, because they are cached in the graph and
@@ -183,8 +216,22 @@
       break;
     }
     case IrOpcode::kInt32Add: {
-      Type output_type =
-          op_typer.NumberAdd(InputType(node, 0), InputType(node, 1));
+      Type left_type = InputType(node, 0);
+      Type right_type = InputType(node, 1);
+      Type output_type;
+      if (left_type.IsNone() && right_type.IsNone()) {
+        output_type = Type::None();
+      } else if (left_type.Is(Type::Machine()) &&
+                 right_type.Is(Type::Machine())) {
+        output_type = Type::Machine();
+      } else if (left_type.Is(Type::NumberOrOddball()) &&
+                 right_type.Is(Type::NumberOrOddball())) {
+        left_type = op_typer.ToNumber(left_type);
+        right_type = op_typer.ToNumber(right_type);
+        output_type = op_typer.NumberAdd(left_type, right_type);
+      } else {
+        ReportInvalidTypeCombination(node, {left_type, right_type});
+      }
       Truncation output_trunc = LeastGeneralTruncation(InputTruncation(node, 0),
                                                        InputTruncation(node, 1),
                                                        Truncation::Word32());
@@ -193,8 +240,22 @@
       break;
     }
     case IrOpcode::kInt32Sub: {
-      Type output_type =
-          op_typer.NumberSubtract(InputType(node, 0), InputType(node, 1));
+      Type left_type = InputType(node, 0);
+      Type right_type = InputType(node, 1);
+      Type output_type;
+      if (left_type.IsNone() && right_type.IsNone()) {
+        output_type = Type::None();
+      } else if (left_type.Is(Type::Machine()) &&
+                 right_type.Is(Type::Machine())) {
+        output_type = Type::Machine();
+      } else if (left_type.Is(Type::NumberOrOddball()) &&
+                 right_type.Is(Type::NumberOrOddball())) {
+        left_type = op_typer.ToNumber(left_type);
+        right_type = op_typer.ToNumber(right_type);
+        output_type = op_typer.NumberSubtract(left_type, right_type);
+      } else {
+        ReportInvalidTypeCombination(node, {left_type, right_type});
+      }
       Truncation output_trunc = LeastGeneralTruncation(InputTruncation(node, 0),
                                                        InputTruncation(node, 1),
                                                        Truncation::Word32());
@@ -222,28 +283,60 @@
     case IrOpcode::kInt64Add: {
       Type left_type = InputType(node, 0);
       Type right_type = InputType(node, 1);
-
       Type output_type;
-      if (left_type.Is(Type::BigInt()) && right_type.Is(Type::BigInt())) {
+      if (left_type.IsNone() && right_type.IsNone()) {
+        // None x None -> None
+        output_type = Type::None();
+      } else if (left_type.Is(Type::Machine()) &&
+                 right_type.Is(Type::Machine())) {
+        // Machine x Machine -> Machine
+        output_type = Type::Machine();
+      } else if (left_type.Is(Type::BigInt()) &&
+                 right_type.Is(Type::BigInt())) {
         // BigInt x BigInt -> BigInt
         output_type = op_typer.BigIntAdd(left_type, right_type);
-      } else if (left_type.Is(Type::Number()) &&
-                 right_type.Is(Type::Number())) {
+      } else if (left_type.Is(Type::NumberOrOddball()) &&
+                 right_type.Is(Type::NumberOrOddball())) {
         // Number x Number -> Number
+        left_type = op_typer.ToNumber(left_type);
+        right_type = op_typer.ToNumber(right_type);
         output_type = op_typer.NumberAdd(left_type, right_type);
       } else {
         // Invalid type combination.
-        std::ostringstream left_str, right_str;
-        left_type.PrintTo(left_str);
-        right_type.PrintTo(right_str);
-        FATAL(
-            "SimplifiedLoweringVerifierError: invalid combination of input "
-            "types "
-            "%s and %s for node #%d:%s",
-            left_str.str().c_str(), right_str.str().c_str(), node->id(),
-            node->op()->mnemonic());
+        ReportInvalidTypeCombination(node, {left_type, right_type});
+      }
+      Truncation output_trunc = LeastGeneralTruncation(InputTruncation(node, 0),
+                                                       InputTruncation(node, 1),
+                                                       Truncation::Word64());
+      CHECK(IsModuloTruncation(output_trunc));
+      CheckAndSet(node, output_type, output_trunc);
+      break;
+    }
+    case IrOpcode::kInt64Sub: {
+      Type left_type = InputType(node, 0);
+      Type right_type = InputType(node, 1);
+      Type output_type;
+      if (left_type.IsNone() && right_type.IsNone()) {
+        // None x None -> None
+        output_type = Type::None();
+      } else if (left_type.Is(Type::Machine()) &&
+                 right_type.Is(Type::Machine())) {
+        // Machine x Machine -> Machine
+        output_type = Type::Machine();
+      } else if (left_type.Is(Type::BigInt()) &&
+                 right_type.Is(Type::BigInt())) {
+        // BigInt x BigInt -> BigInt
+        output_type = op_typer.BigIntSubtract(left_type, right_type);
+      } else if (left_type.Is(Type::NumberOrOddball()) &&
+                 right_type.Is(Type::NumberOrOddball())) {
+        // Number x Number -> Number
+        left_type = op_typer.ToNumber(left_type);
+        right_type = op_typer.ToNumber(right_type);
+        output_type = op_typer.NumberSubtract(left_type, right_type);
+      } else {
+        // Invalid type combination.
+        ReportInvalidTypeCombination(node, {left_type, right_type});
       }
-
       Truncation output_trunc = LeastGeneralTruncation(InputTruncation(node, 0),
                                                        InputTruncation(node, 1),
                                                        Truncation::Word64());
@@ -283,8 +376,9 @@
       CheckAndSet(node, input_type, InputTruncation(node, 0));
       break;
     }
-    case IrOpcode::kCheckBigInt64: {
+    case IrOpcode::kCheckedBigIntToBigInt64: {
       Type input_type = InputType(node, 0);
+      CHECK(input_type.Is(Type::BigInt()));
       input_type =
           Type::Intersect(input_type, Type::SignedBigInt64(), graph_zone());
       CheckAndSet(node, input_type, InputTruncation(node, 0));
@@ -326,8 +420,10 @@
       break;
     }
     case IrOpcode::kBranch: {
-      CHECK(InputType(node, 0).Is(Type::Boolean()));
-      CHECK_EQ(InputTruncation(node, 0), Truncation::Any());
+      CHECK_EQ(BranchParametersOf(node->op()).semantics(),
+               BranchSemantics::kMachine);
+      Type input_type = InputType(node, 0);
+      CHECK(input_type.Is(Type::Boolean()) || input_type.Is(Type::Machine()));
       break;
     }
     case IrOpcode::kTypedStateValues: {
@@ -336,7 +432,7 @@
         // Inputs must not be truncated.
         CHECK_EQ(InputTruncation(node, i), Truncation::Any());
         CHECK(IsNonTruncatingMachineTypeFor(machine_types->at(i),
-                                            InputType(node, i)));
+                                            InputType(node, i), graph_zone()));
       }
       break;
     }
@@ -345,6 +441,11 @@
       SetTruncation(node, Truncation::Any());
       break;
     }
+    case IrOpcode::kEnterMachineGraph:
+    case IrOpcode::kExitMachineGraph: {
+      // Eliminated during lowering.
+      UNREACHABLE();
+    }
 
 #define CASE(code, ...) case IrOpcode::k##code:
       // Control operators
@@ -423,7 +524,11 @@
       CASE(CheckedUint32Div)
       CASE(CheckedUint32Mod)
       CASE(CheckedInt32Mul)
-      CASE(CheckedBigInt64Add)
+      CASE(CheckedInt64Add)
+      CASE(CheckedInt64Sub)
+      CASE(CheckedInt64Mul)
+      CASE(CheckedInt64Div)
+      CASE(CheckedInt64Mod)
       CASE(CheckedInt32ToTaggedSigned)
       CASE(CheckedInt64ToInt32)
       CASE(CheckedInt64ToTaggedSigned)
@@ -485,7 +590,6 @@
       CASE(Word64RolLowerable)
       CASE(Word64RorLowerable)
       CASE(Int64AddWithOverflow)
-      CASE(Int64Sub)
       CASE(Int64SubWithOverflow)
       CASE(Int64Mul)
       CASE(Int64MulHigh)
diff -r -u --color up/v8/src/compiler/simplified-lowering-verifier.h nw/v8/src/compiler/simplified-lowering-verifier.h
--- up/v8/src/compiler/simplified-lowering-verifier.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/simplified-lowering-verifier.h	2023-01-19 16:46:36.135609592 -0500
@@ -5,6 +5,8 @@
 #ifndef V8_COMPILER_SIMPLIFIED_LOWERING_VERIFIER_H_
 #define V8_COMPILER_SIMPLIFIED_LOWERING_VERIFIER_H_
 
+#include "src/base/container-utils.h"
+#include "src/compiler/opcodes.h"
 #include "src/compiler/representation-change.h"
 
 namespace v8 {
@@ -21,7 +23,11 @@
   };
 
   SimplifiedLoweringVerifier(Zone* zone, Graph* graph)
-      : hints_(zone), data_(zone), graph_(graph) {}
+      : hints_(zone),
+        machine_uses_of_constants_(zone),
+        data_(zone),
+        graph_(graph),
+        zone_(zone) {}
 
   void VisitNode(Node* node, OperationTyper& op_typer);
 
@@ -30,10 +36,33 @@
     hints_.push_back(node);
   }
   const ZoneVector<Node*>& inserted_hints() const { return hints_; }
+  void RecordMachineUsesOfConstant(Node* constant, Node::Uses uses) {
+    DCHECK(IrOpcode::IsMachineConstantOpcode(constant->opcode()));
+    auto it = machine_uses_of_constants_.find(constant);
+    if (it == machine_uses_of_constants_.end()) {
+      it =
+          machine_uses_of_constants_.emplace(constant, ZoneVector<Node*>(zone_))
+              .first;
+    }
+    base::vector_append(it->second, uses);
+  }
+  const ZoneUnorderedMap<Node*, ZoneVector<Node*>>& machine_uses_of_constants()
+      const {
+    return machine_uses_of_constants_;
+  }
 
   base::Optional<Type> GetType(Node* node) const {
     if (NodeProperties::IsTyped(node)) {
-      return NodeProperties::GetType(node);
+      Type type = NodeProperties::GetType(node);
+      // We do not use the static type for constants, even if we have one,
+      // because those are cached in the graph and shared between machine
+      // and non-machine subgraphs. The former might have assigned
+      // Type::Machine() to them.
+      if (IrOpcode::IsMachineConstantOpcode(node->opcode())) {
+        DCHECK(type.Is(Type::Machine()));
+      } else {
+        return type;
+      }
     }
     // For nodes that have not been typed before SL, we use the type that has
     // been inferred by the verifier.
@@ -60,16 +89,7 @@
   Type InputType(Node* node, int input_index) const {
     // TODO(nicohartmann): Check that inputs are typed, once all operators are
     // supported.
-    Node* input = node->InputAt(input_index);
-    if (NodeProperties::IsTyped(input)) {
-      return NodeProperties::GetType(input);
-    }
-    // For nodes that have not been typed before SL, we use the type that has
-    // been inferred by the verifier.
-    base::Optional<Type> type_opt;
-    if (input->id() < data_.size()) {
-      type_opt = data_[input->id()].type;
-    }
+    auto type_opt = GetType(node->InputAt(input_index));
     return type_opt.has_value() ? *type_opt : Type::None();
   }
 
@@ -91,6 +111,7 @@
 
   void CheckType(Node* node, const Type& type);
   void CheckAndSet(Node* node, const Type& type, const Truncation& trunc);
+  void ReportInvalidTypeCombination(Node* node, const std::vector<Type>& types);
 
   // Generalize to a less strict truncation in the context of a given type. For
   // example, a Truncation::kWord32[kIdentifyZeros] does not have any effect on
@@ -104,8 +125,10 @@
   Zone* graph_zone() const { return graph_->zone(); }
 
   ZoneVector<Node*> hints_;
+  ZoneUnorderedMap<Node*, ZoneVector<Node*>> machine_uses_of_constants_;
   ZoneVector<PerNodeData> data_;
   Graph* graph_;
+  Zone* zone_;
 };
 
 }  // namespace compiler
diff -r -u --color up/v8/src/compiler/simplified-lowering.cc nw/v8/src/compiler/simplified-lowering.cc
--- up/v8/src/compiler/simplified-lowering.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/simplified-lowering.cc	2023-01-19 16:46:36.135609592 -0500
@@ -359,6 +359,11 @@
         linkage_(linkage),
         observe_node_manager_(observe_node_manager),
         verifier_(verifier) {
+    Factory* factory = broker_->isolate()->factory();
+    singleton_true_ =
+        Type::Constant(broker, factory->true_value(), graph_zone());
+    singleton_false_ =
+        Type::Constant(broker, factory->false_value(), graph_zone());
   }
 
   bool verification_enabled() const { return verifier_ != nullptr; }
@@ -396,6 +401,11 @@
 
   bool UpdateFeedbackType(Node* node) {
     if (node->op()->ValueOutputCount() == 0) return false;
+    if ((IrOpcode::IsMachineOpcode(node->opcode()) ||
+         IrOpcode::IsMachineConstantOpcode(node->opcode())) &&
+        node->opcode() != IrOpcode::kLoadFramePointer) {
+      DCHECK(NodeProperties::GetType(node).Is(Type::Machine()));
+    }
 
     // For any non-phi node just wait until we get all inputs typed. We only
     // allow untyped inputs for phi nodes because phis are the only places
@@ -595,7 +605,6 @@
     while (!stack.empty()) {
       NodeState& current = stack.top();
       Node* node = current.node;
-
       // If there is an unvisited input, push it and continue with that node.
       bool pushed_unvisited = false;
       while (current.input_index < node->InputCount()) {
@@ -750,6 +759,19 @@
 
     TRACE("--{Verify Phase}--\n");
 
+    // Patch pending type overrides.
+    for (auto [constant, uses] : verifier_->machine_uses_of_constants()) {
+      Node* typed_constant =
+          InsertTypeOverrideForVerifier(Type::Machine(), constant);
+      for (auto use : uses) {
+        for (int i = 0; i < use->InputCount(); ++i) {
+          if (use->InputAt(i) == constant) {
+            use->ReplaceInput(i, typed_constant);
+          }
+        }
+      }
+    }
+
     // Generate a new traversal containing all the new nodes created during
     // lowering.
     GenerateTraversal();
@@ -774,7 +796,9 @@
     }
 
     // Verify all nodes.
-    for (Node* node : traversal_nodes_) verifier_->VisitNode(node, op_typer_);
+    for (Node* node : traversal_nodes_) {
+      verifier_->VisitNode(node, op_typer_);
+    }
 
     // Print graph.
     if (info != nullptr && info->trace_turbo_json()) {
@@ -1065,7 +1089,7 @@
   void VisitNoop(Node* node, Truncation truncation) {
     if (truncation.IsUnused()) return VisitUnused<T>(node);
     MachineRepresentation representation =
-        GetOutputInfoForPhi(node, TypeOf(node), truncation);
+        GetOutputInfoForPhi(TypeOf(node), truncation);
     VisitUnop<T>(node, UseInfo(representation, truncation), representation);
     if (lower<T>()) DeferReplacement(node, node->InputAt(0));
   }
@@ -1141,11 +1165,7 @@
   }
 
   // Infer representation for phi-like nodes.
-  // The {node} parameter is only used to decide on the int64 representation.
-  // Once the type system supports an external pointer type, the {node}
-  // parameter can be removed.
-  MachineRepresentation GetOutputInfoForPhi(Node* node, Type type,
-                                            Truncation use) {
+  MachineRepresentation GetOutputInfoForPhi(Type type, Truncation use) {
     // Compute the representation.
     if (type.Is(Type::None())) {
       return MachineRepresentation::kNone;
@@ -1183,7 +1203,7 @@
     ProcessInput<T>(node, 0, UseInfo::Bool());
 
     MachineRepresentation output =
-        GetOutputInfoForPhi(node, TypeOf(node), truncation);
+        GetOutputInfoForPhi(TypeOf(node), truncation);
     SetOutput<T>(node, output);
 
     if (lower<T>()) {
@@ -1204,8 +1224,13 @@
   template <Phase T>
   void VisitPhi(Node* node, Truncation truncation,
                 SimplifiedLowering* lowering) {
-    MachineRepresentation output =
-        GetOutputInfoForPhi(node, TypeOf(node), truncation);
+    // If we already have a non-tagged representation set in the Phi node, it
+    // does come from subgraphs using machine operators we introduced early in
+    // the pipeline. In this case, we just keep the representation.
+    MachineRepresentation output = PhiRepresentationOf(node->op());
+    if (output == MachineRepresentation::kTagged) {
+      output = GetOutputInfoForPhi(TypeOf(node), truncation);
+    }
     // Only set the output representation if not running with type
     // feedback. (Feedback typing will set the representation.)
     SetOutput<T>(node, output);
@@ -1232,12 +1257,16 @@
     if (input_type.Is(type)) {
       VisitUnop<T>(node, UseInfo::None(), MachineRepresentation::kBit);
       if (lower<T>()) {
-        DeferReplacement(node, lowering->jsgraph()->Int32Constant(1));
+        DeferReplacement(
+            node, InsertTypeOverrideForVerifier(
+                      true_type(), lowering->jsgraph()->Int32Constant(1)));
       }
     } else {
       VisitUnop<T>(node, UseInfo::AnyTagged(), MachineRepresentation::kBit);
       if (lower<T>() && !input_type.Maybe(type)) {
-        DeferReplacement(node, lowering->jsgraph()->Int32Constant(0));
+        DeferReplacement(
+            node, InsertTypeOverrideForVerifier(
+                      false_type(), lowering->jsgraph()->Int32Constant(0)));
       }
     }
   }
@@ -1471,6 +1500,14 @@
     return changer_->Int64OperatorFor(node->opcode());
   }
 
+  const Operator* Int64OverflowOp(Node* node) {
+    return changer_->Int64OverflowOperatorFor(node->opcode());
+  }
+
+  const Operator* BigIntOp(Node* node) {
+    return changer_->BigIntOperatorFor(node->opcode());
+  }
+
   const Operator* Uint32Op(Node* node) {
     return changer_->Uint32OperatorFor(node->opcode());
   }
@@ -2140,7 +2177,18 @@
                                 ->GetParameterType(ParameterIndexOf(node->op()))
                                 .representation());
       case IrOpcode::kInt32Constant:
-        return VisitLeaf<T>(node, MachineRepresentation::kWord32);
+        DCHECK_EQ(0, node->InputCount());
+        SetOutput<T>(node, MachineRepresentation::kWord32);
+        DCHECK(NodeProperties::GetType(node).Is(Type::Machine()));
+        if (verification_enabled()) {
+          // During lowering, SimplifiedLowering generates Int32Constants which
+          // need to be treated differently by the verifier than the
+          // Int32Constants introduced explicitly in machine graphs. To be able
+          // to distinguish them, we record those that are being visited here
+          // because they were generated before SimplifiedLowering.
+          verifier_->RecordMachineUsesOfConstant(node, node->uses());
+        }
+        return;
       case IrOpcode::kInt64Constant:
         return VisitLeaf<T>(node, MachineRepresentation::kWord64);
       case IrOpcode::kExternalConstant:
@@ -2174,8 +2222,19 @@
       }
 
       case IrOpcode::kBranch: {
-        DCHECK(TypeOf(node->InputAt(0)).Is(Type::Boolean()));
-        ProcessInput<T>(node, 0, UseInfo::Bool());
+        const auto& p = BranchParametersOf(node->op());
+        if (p.semantics() == BranchSemantics::kMachine) {
+          // If this is a machine branch, the condition is a machine operator,
+          // so we enter machine branch here.
+          ProcessInput<T>(node, 0, UseInfo::Any());
+        } else {
+          DCHECK(TypeOf(node->InputAt(0)).Is(Type::Boolean()));
+          ProcessInput<T>(node, 0, UseInfo::Bool());
+          if (lower<T>()) {
+            ChangeOp(node,
+                     common()->Branch(p.hint(), BranchSemantics::kMachine));
+          }
+        }
         EnqueueInput<T>(node, NodeProperties::FirstControlIndex(node));
         return;
       }
@@ -2963,24 +3022,6 @@
         }
         return;
       }
-      case IrOpcode::kCheckBigInt: {
-        if (InputIs(node, Type::BigInt())) {
-          VisitNoop<T>(node, truncation);
-        } else {
-          VisitUnop<T>(node, UseInfo::AnyTagged(),
-                       MachineRepresentation::kTaggedPointer);
-        }
-        return;
-      }
-      case IrOpcode::kCheckBigInt64: {
-        if (InputIs(node, Type::BigInt())) {
-          VisitNoop<T>(node, truncation);
-        } else {
-          VisitUnop<T>(node, UseInfo::AnyTagged(),
-                       MachineRepresentation::kTaggedPointer);
-        }
-        return;
-      }
       case IrOpcode::kSpeculativeBigIntAsIntN:
       case IrOpcode::kSpeculativeBigIntAsUintN: {
         const bool is_asuintn =
@@ -3197,21 +3238,19 @@
         SetOutput<T>(node, MachineRepresentation::kTaggedPointer);
         return;
       }
-      case IrOpcode::kSpeculativeBigIntAdd: {
-        if (truncation.IsUnused()) {
-          Type left_type = GetUpperBound(node->InputAt(0));
-          Type right_type = GetUpperBound(node->InputAt(1));
-          if (left_type.Is(Type::BigInt()) && right_type.Is(Type::BigInt())) {
-            VisitUnused<T>(node);
-            return;
-          }
+      case IrOpcode::kSpeculativeBigIntAdd:
+      case IrOpcode::kSpeculativeBigIntSubtract:
+      case IrOpcode::kSpeculativeBigIntMultiply: {
+        if (truncation.IsUnused() && BothInputsAre(node, Type::BigInt())) {
+          VisitUnused<T>(node);
+          return;
         }
         if (truncation.IsUsedAsWord64()) {
           VisitBinop<T>(
               node, UseInfo::CheckedBigIntTruncatingWord64(FeedbackSource{}),
               MachineRepresentation::kWord64);
           if (lower<T>()) {
-            ChangeToPureOp(node, lowering->machine()->Int64Add());
+            ChangeToPureOp(node, Int64Op(node));
           }
           return;
         }
@@ -3222,92 +3261,48 @@
                 node, UseInfo::CheckedBigInt64AsWord64(FeedbackSource{}),
                 MachineRepresentation::kWord64, Type::SignedBigInt64());
             if (lower<T>()) {
-              ChangeOp(node, lowering->simplified()->CheckedBigInt64Add());
+              ChangeOp(node, Int64OverflowOp(node));
             }
-            break;
+            return;
           }
           case BigIntOperationHint::kBigInt: {
             VisitBinop<T>(
                 node, UseInfo::CheckedBigIntAsTaggedPointer(FeedbackSource{}),
                 MachineRepresentation::kTaggedPointer);
             if (lower<T>()) {
-              ChangeOp(node, lowering->simplified()->BigIntAdd());
+              ChangeOp(node, BigIntOp(node));
             }
-            break;
-          }
-          default:
-            UNREACHABLE();
-        }
-        return;
-      }
-      case IrOpcode::kSpeculativeBigIntSubtract: {
-        if (truncation.IsUnused()) {
-          Type left_type = GetUpperBound(node->InputAt(0));
-          Type right_type = GetUpperBound(node->InputAt(1));
-          if (left_type.Is(Type::BigInt()) && right_type.Is(Type::BigInt())) {
-            VisitUnused<T>(node);
             return;
           }
         }
-        if (truncation.IsUsedAsWord64()) {
-          VisitBinop<T>(
-              node, UseInfo::CheckedBigIntTruncatingWord64(FeedbackSource{}),
-              MachineRepresentation::kWord64);
-          if (lower<T>()) {
-            ChangeToPureOp(node, lowering->machine()->Int64Sub());
-          }
-        } else {
-          VisitBinop<T>(node,
-                        UseInfo::CheckedBigIntAsTaggedPointer(FeedbackSource{}),
-                        MachineRepresentation::kTaggedPointer);
-          if (lower<T>()) {
-            ChangeOp(node, lowering->simplified()->BigIntSubtract());
-          }
-        }
-        return;
       }
-      case IrOpcode::kSpeculativeBigIntMultiply: {
-        if (truncation.IsUnused()) {
-          Type left_type = GetUpperBound(node->InputAt(0));
-          Type right_type = GetUpperBound(node->InputAt(1));
-          if (left_type.Is(Type::BigInt()) && right_type.Is(Type::BigInt())) {
-            VisitUnused<T>(node);
-            return;
-          }
+      case IrOpcode::kSpeculativeBigIntDivide:
+      case IrOpcode::kSpeculativeBigIntModulus: {
+        if (truncation.IsUnused() && BothInputsAre(node, Type::BigInt())) {
+          VisitUnused<T>(node);
+          return;
         }
-        if (truncation.IsUsedAsWord64()) {
-          VisitBinop<T>(
-              node, UseInfo::CheckedBigIntTruncatingWord64(FeedbackSource{}),
-              MachineRepresentation::kWord64);
-          if (lower<T>()) {
-            ChangeToPureOp(node, lowering->machine()->Int64Mul());
-          }
-        } else {
-          VisitBinop<T>(node,
-                        UseInfo::CheckedBigIntAsTaggedPointer(FeedbackSource{}),
-                        MachineRepresentation::kTaggedPointer);
-          if (lower<T>()) {
-            ChangeOp(node, lowering->simplified()->BigIntMultiply());
+        BigIntOperationHint hint = BigIntOperationHintOf(node->op());
+        switch (hint) {
+          case BigIntOperationHint::kBigInt64: {
+            VisitBinop<T>(
+                node, UseInfo::CheckedBigInt64AsWord64(FeedbackSource{}),
+                MachineRepresentation::kWord64, Type::SignedBigInt64());
+            if (lower<T>()) {
+              ChangeOp(node, Int64OverflowOp(node));
+            }
+            return;
           }
-        }
-        return;
-      }
-      case IrOpcode::kSpeculativeBigIntDivide: {
-        if (truncation.IsUnused()) {
-          Type left_type = GetUpperBound(node->InputAt(0));
-          Type right_type = GetUpperBound(node->InputAt(1));
-          if (left_type.Is(Type::BigInt()) && right_type.Is(Type::BigInt())) {
-            VisitUnused<T>(node);
+          case BigIntOperationHint::kBigInt: {
+            VisitBinop<T>(
+                node, UseInfo::CheckedBigIntAsTaggedPointer(FeedbackSource{}),
+                MachineRepresentation::kTaggedPointer);
+            if (lower<T>()) {
+              ChangeOp(node, BigIntOp(node));
+            }
             return;
           }
         }
-        VisitBinop<T>(node,
-                      UseInfo::CheckedBigIntAsTaggedPointer(FeedbackSource{}),
-                      MachineRepresentation::kTaggedPointer);
-        if (lower<T>()) {
-          ChangeOp(node, lowering->simplified()->BigIntDivide());
-        }
-        return;
       }
       case IrOpcode::kSpeculativeBigIntBitwiseAnd: {
         if (truncation.IsUnused()) {
@@ -3716,7 +3711,7 @@
         if (InputIs(node, Type::Boolean())) {
           VisitUnop<T>(node, UseInfo::Bool(), MachineRepresentation::kWord32);
           if (lower<T>()) {
-            ChangeToSemanticsHintForVerifier(node, node->op());
+            DeferReplacement(node, node->InputAt(0));
           }
         } else if (InputIs(node, Type::String())) {
           VisitUnop<T>(node, UseInfo::AnyTagged(),
@@ -3729,7 +3724,7 @@
             VisitUnop<T>(node, UseInfo::TruncatingWord32(),
                          MachineRepresentation::kWord32);
             if (lower<T>()) {
-              ChangeToSemanticsHintForVerifier(node, node->op());
+              DeferReplacement(node, node->InputAt(0));
             }
           } else {
             VisitUnop<T>(node, UseInfo::AnyTagged(),
@@ -3743,7 +3738,7 @@
             VisitUnop<T>(node, UseInfo::TruncatingFloat64(),
                          MachineRepresentation::kFloat64);
             if (lower<T>()) {
-              ChangeToSemanticsHintForVerifier(node, node->op());
+              DeferReplacement(node, node->InputAt(0));
             }
           } else {
             VisitUnop<T>(node, UseInfo::AnyTagged(),
@@ -3807,12 +3802,16 @@
         if (input_type.Is(type_cache_->kSafeInteger)) {
           VisitUnop<T>(node, UseInfo::None(), MachineRepresentation::kBit);
           if (lower<T>()) {
-            DeferReplacement(node, lowering->jsgraph()->Int32Constant(1));
+            DeferReplacement(
+                node, InsertTypeOverrideForVerifier(
+                          true_type(), lowering->jsgraph()->Int32Constant(1)));
           }
         } else if (!input_type.Maybe(Type::Number())) {
           VisitUnop<T>(node, UseInfo::Any(), MachineRepresentation::kBit);
           if (lower<T>()) {
-            DeferReplacement(node, lowering->jsgraph()->Int32Constant(0));
+            DeferReplacement(
+                node, InsertTypeOverrideForVerifier(
+                          false_type(), lowering->jsgraph()->Int32Constant(0)));
           }
         } else if (input_type.Is(Type::Number())) {
           VisitUnop<T>(node, UseInfo::TruncatingFloat64(),
@@ -3835,12 +3834,16 @@
         if (input_type.Is(type_cache_->kSafeInteger)) {
           VisitUnop<T>(node, UseInfo::None(), MachineRepresentation::kBit);
           if (lower<T>()) {
-            DeferReplacement(node, lowering->jsgraph()->Int32Constant(1));
+            DeferReplacement(
+                node, InsertTypeOverrideForVerifier(
+                          true_type(), lowering->jsgraph()->Int32Constant(1)));
           }
         } else if (!input_type.Maybe(Type::Number())) {
           VisitUnop<T>(node, UseInfo::Any(), MachineRepresentation::kBit);
           if (lower<T>()) {
-            DeferReplacement(node, lowering->jsgraph()->Int32Constant(0));
+            DeferReplacement(
+                node, InsertTypeOverrideForVerifier(
+                          false_type(), lowering->jsgraph()->Int32Constant(0)));
           }
         } else if (input_type.Is(Type::Number())) {
           VisitUnop<T>(node, UseInfo::TruncatingFloat64(),
@@ -3861,12 +3864,16 @@
         if (input_type.Is(type_cache_->kSafeInteger)) {
           VisitUnop<T>(node, UseInfo::None(), MachineRepresentation::kBit);
           if (lower<T>()) {
-            DeferReplacement(node, lowering->jsgraph()->Int32Constant(1));
+            DeferReplacement(
+                node, InsertTypeOverrideForVerifier(
+                          true_type(), lowering->jsgraph()->Int32Constant(1)));
           }
         } else if (!input_type.Maybe(Type::Number())) {
           VisitUnop<T>(node, UseInfo::Any(), MachineRepresentation::kBit);
           if (lower<T>()) {
-            DeferReplacement(node, lowering->jsgraph()->Int32Constant(0));
+            DeferReplacement(
+                node, InsertTypeOverrideForVerifier(
+                          false_type(), lowering->jsgraph()->Int32Constant(0)));
           }
         } else if (input_type.Is(Type::Number())) {
           VisitUnop<T>(node, UseInfo::TruncatingFloat64(),
@@ -3889,12 +3896,16 @@
         if (input_type.Is(Type::MinusZero())) {
           VisitUnop<T>(node, UseInfo::None(), MachineRepresentation::kBit);
           if (lower<T>()) {
-            DeferReplacement(node, lowering->jsgraph()->Int32Constant(1));
+            DeferReplacement(
+                node, InsertTypeOverrideForVerifier(
+                          true_type(), lowering->jsgraph()->Int32Constant(1)));
           }
         } else if (!input_type.Maybe(Type::MinusZero())) {
           VisitUnop<T>(node, UseInfo::Any(), MachineRepresentation::kBit);
           if (lower<T>()) {
-            DeferReplacement(node, lowering->jsgraph()->Int32Constant(0));
+            DeferReplacement(
+                node, InsertTypeOverrideForVerifier(
+                          false_type(), lowering->jsgraph()->Int32Constant(0)));
           }
         } else if (input_type.Is(Type::Number())) {
           VisitUnop<T>(node, UseInfo::TruncatingFloat64(),
@@ -3912,12 +3923,16 @@
         if (input_type.Is(Type::NaN())) {
           VisitUnop<T>(node, UseInfo::None(), MachineRepresentation::kBit);
           if (lower<T>()) {
-            DeferReplacement(node, lowering->jsgraph()->Int32Constant(1));
+            DeferReplacement(
+                node, InsertTypeOverrideForVerifier(
+                          true_type(), lowering->jsgraph()->Int32Constant(1)));
           }
         } else if (!input_type.Maybe(Type::NaN())) {
           VisitUnop<T>(node, UseInfo::Any(), MachineRepresentation::kBit);
           if (lower<T>()) {
-            DeferReplacement(node, lowering->jsgraph()->Int32Constant(0));
+            DeferReplacement(
+                node, InsertTypeOverrideForVerifier(
+                          false_type(), lowering->jsgraph()->Int32Constant(0)));
           }
         } else if (input_type.Is(Type::Number())) {
           VisitUnop<T>(node, UseInfo::TruncatingFloat64(),
@@ -4078,6 +4093,14 @@
       case IrOpcode::kDateNow:
         VisitInputs<T>(node);
         return SetOutput<T>(node, MachineRepresentation::kTagged);
+      case IrOpcode::kDoubleArrayMax: {
+        return VisitUnop<T>(node, UseInfo::AnyTagged(),
+                            MachineRepresentation::kTagged);
+      }
+      case IrOpcode::kDoubleArrayMin: {
+        return VisitUnop<T>(node, UseInfo::AnyTagged(),
+                            MachineRepresentation::kTagged);
+      }
       case IrOpcode::kFrameState:
         return VisitFrameState<T>(FrameState{node});
       case IrOpcode::kStateValues:
@@ -4094,7 +4117,7 @@
         // for the sigma's type.
         Type type = TypeOf(node);
         MachineRepresentation representation =
-            GetOutputInfoForPhi(node, type, truncation);
+            GetOutputInfoForPhi(type, truncation);
 
         // Here we pretend that the input has the sigma's type for the
         // conversion.
@@ -4227,6 +4250,65 @@
         }
         return;
       }
+      case IrOpcode::kDebugBreak:
+        return;
+
+      // Nodes from machine graphs.
+      case IrOpcode::kEnterMachineGraph: {
+        DCHECK_EQ(1, node->op()->ValueInputCount());
+        UseInfo use_info = OpParameter<UseInfo>(node->op());
+        ProcessInput<T>(node, 0, use_info);
+        SetOutput<T>(node, use_info.representation());
+        if (lower<T>()) {
+          DeferReplacement(node, InsertTypeOverrideForVerifier(
+                                     Type::Machine(), node->InputAt(0)));
+        }
+        return;
+      }
+      case IrOpcode::kExitMachineGraph: {
+        DCHECK_EQ(1, node->op()->ValueInputCount());
+        ProcessInput<T>(node, 0, UseInfo::Any());
+        const auto& p = ExitMachineGraphParametersOf(node->op());
+        SetOutput<T>(node, p.output_representation(), p.output_type());
+        if (lower<T>()) {
+          DeferReplacement(node, InsertTypeOverrideForVerifier(
+                                     p.output_type(), node->InputAt(0)));
+        }
+        return;
+      }
+      case IrOpcode::kInt32Add:
+      case IrOpcode::kInt32Sub:
+      case IrOpcode::kUint32LessThan:
+      case IrOpcode::kUint32LessThanOrEqual:
+      case IrOpcode::kUint64LessThanOrEqual:
+      case IrOpcode::kUint32Div:
+      case IrOpcode::kWord32And:
+      case IrOpcode::kWord32Equal:
+      case IrOpcode::kWord32Or:
+      case IrOpcode::kWord32Shl:
+      case IrOpcode::kWord32Shr:
+        for (int i = 0; i < node->InputCount(); ++i) {
+          ProcessInput<T>(node, i, UseInfo::Any());
+        }
+        SetOutput<T>(node, MachineRepresentation::kWord32);
+        return;
+      case IrOpcode::kInt64Add:
+      case IrOpcode::kInt64Sub:
+      case IrOpcode::kUint64Div:
+      case IrOpcode::kWord64And:
+      case IrOpcode::kWord64Shl:
+      case IrOpcode::kWord64Shr:
+        for (int i = 0; i < node->InputCount(); ++i) {
+          ProcessInput<T>(node, i, UseInfo::Any());
+        }
+        SetOutput<T>(node, MachineRepresentation::kWord64);
+        return;
+      case IrOpcode::kLoad:
+        for (int i = 0; i < node->InputCount(); ++i) {
+          ProcessInput<T>(node, i, UseInfo::Any());
+        }
+        SetOutput<T>(node, LoadRepresentationOf(node->op()).representation());
+        return;
 
       default:
         FATAL(
@@ -4272,18 +4354,6 @@
     return node;
   }
 
-  void ChangeToSemanticsHintForVerifier(Node* node, const Operator* semantics) {
-    DCHECK_EQ(node->op()->ValueInputCount(), 1);
-    DCHECK_EQ(node->op()->EffectInputCount(), 0);
-    DCHECK_EQ(node->op()->ControlInputCount(), 0);
-    if (verification_enabled()) {
-      ChangeOp(node, common()->SLVerifierHint(semantics, base::nullopt));
-      verifier_->RecordHint(node);
-    } else {
-      DeferReplacement(node, node->InputAt(0));
-    }
-  }
-
  private:
   void ChangeOp(Node* node, const Operator* new_op) {
     compiler::NodeProperties::ChangeOp(node, new_op);
@@ -4299,6 +4369,9 @@
                                            replacement);
   }
 
+  Type true_type() const { return singleton_true_; }
+  Type false_type() const { return singleton_false_; }
+
   JSGraph* jsgraph_;
   JSHeapBroker* broker_;
   Zone* zone_;                      // Temporary zone.
@@ -4328,6 +4401,8 @@
   NodeOriginTable* node_origins_;
   TypeCache const* type_cache_;
   OperationTyper op_typer_;  // helper for the feedback typer
+  Type singleton_true_;
+  Type singleton_false_;
   TickCounter* const tick_counter_;
   Linkage* const linkage_;
   ObserveNodeManager* const observe_node_manager_;
@@ -4536,8 +4611,9 @@
   Node* control = node->InputAt(4);
 
   Node* check0 = graph()->NewNode(simplified()->ObjectIsSmi(), value);
-  Node* branch0 =
-      graph()->NewNode(common()->Branch(BranchHint::kTrue), check0, control);
+  Node* branch0 = graph()->NewNode(
+      common()->Branch(BranchHint::kTrue, BranchSemantics::kMachine), check0,
+      control);
 
   Node* if_true0 = graph()->NewNode(common()->IfTrue(), branch0);
   Node* etrue0 = effect;
@@ -4575,7 +4651,9 @@
     }
 
     Node* check1 = graph()->NewNode(simplified()->ObjectIsSmi(), vfalse0);
-    Node* branch1 = graph()->NewNode(common()->Branch(), check1, if_false0);
+    Node* branch1 = graph()->NewNode(
+        common()->Branch(BranchHint::kNone, BranchSemantics::kMachine), check1,
+        if_false0);
 
     Node* if_true1 = graph()->NewNode(common()->IfTrue(), branch1);
     Node* etrue1 = efalse0;
@@ -4638,8 +4716,9 @@
   Node* control = node->InputAt(4);
 
   Node* check0 = graph()->NewNode(simplified()->ObjectIsSmi(), value);
-  Node* branch0 =
-      graph()->NewNode(common()->Branch(BranchHint::kTrue), check0, control);
+  Node* branch0 = graph()->NewNode(
+      common()->Branch(BranchHint::kTrue, BranchSemantics::kMachine), check0,
+      control);
 
   Node* if_true0 = graph()->NewNode(common()->IfTrue(), branch0);
   Node* etrue0 = effect;
@@ -4674,7 +4753,9 @@
     }
 
     Node* check1 = graph()->NewNode(simplified()->ObjectIsSmi(), vfalse0);
-    Node* branch1 = graph()->NewNode(common()->Branch(), check1, if_false0);
+    Node* branch1 = graph()->NewNode(
+        common()->Branch(BranchHint::kNone, BranchSemantics::kMachine), check1,
+        if_false0);
 
     Node* if_true1 = graph()->NewNode(common()->IfTrue(), branch1);
     Node* etrue1 = efalse0;
@@ -4802,8 +4883,9 @@
       common()->Phi(MachineRepresentation::kWord32, 2);
 
   Node* check0 = graph()->NewNode(machine()->Int32LessThan(), zero, rhs);
-  Node* branch0 = graph()->NewNode(common()->Branch(BranchHint::kTrue), check0,
-                                   graph()->start());
+  Node* branch0 = graph()->NewNode(
+      common()->Branch(BranchHint::kTrue, BranchSemantics::kMachine), check0,
+      graph()->start());
 
   Node* if_true0 = graph()->NewNode(common()->IfTrue(), branch0);
   Node* true0 = graph()->NewNode(machine()->Int32Div(), lhs, rhs, if_true0);
@@ -4812,7 +4894,9 @@
   Node* false0;
   {
     Node* check1 = graph()->NewNode(machine()->Int32LessThan(), rhs, minus_one);
-    Node* branch1 = graph()->NewNode(common()->Branch(), check1, if_false0);
+    Node* branch1 = graph()->NewNode(
+        common()->Branch(BranchHint::kNone, BranchSemantics::kMachine), check1,
+        if_false0);
 
     Node* if_true1 = graph()->NewNode(common()->IfTrue(), branch1);
     Node* true1 = graph()->NewNode(machine()->Int32Div(), lhs, rhs, if_true1);
@@ -4821,7 +4905,9 @@
     Node* false1;
     {
       Node* check2 = graph()->NewNode(machine()->Word32Equal(), rhs, zero);
-      Node* branch2 = graph()->NewNode(common()->Branch(), check2, if_false1);
+      Node* branch2 = graph()->NewNode(
+          common()->Branch(BranchHint::kNone, BranchSemantics::kMachine),
+          check2, if_false1);
 
       Node* if_true2 = graph()->NewNode(common()->IfTrue(), branch2);
       Node* true2 = zero;
@@ -4879,8 +4965,9 @@
       common()->Phi(MachineRepresentation::kWord32, 2);
 
   Node* check0 = graph()->NewNode(machine()->Int32LessThan(), zero, rhs);
-  Node* branch0 = graph()->NewNode(common()->Branch(BranchHint::kTrue), check0,
-                                   graph()->start());
+  Node* branch0 = graph()->NewNode(
+      common()->Branch(BranchHint::kTrue, BranchSemantics::kMachine), check0,
+      graph()->start());
 
   Node* if_true0 = graph()->NewNode(common()->IfTrue(), branch0);
   Node* true0;
@@ -4888,7 +4975,9 @@
     Node* msk = graph()->NewNode(machine()->Int32Add(), rhs, minus_one);
 
     Node* check1 = graph()->NewNode(machine()->Word32And(), rhs, msk);
-    Node* branch1 = graph()->NewNode(common()->Branch(), check1, if_true0);
+    Node* branch1 = graph()->NewNode(
+        common()->Branch(BranchHint::kNone, BranchSemantics::kMachine), check1,
+        if_true0);
 
     Node* if_true1 = graph()->NewNode(common()->IfTrue(), branch1);
     Node* true1 = graph()->NewNode(machine()->Int32Mod(), lhs, rhs, if_true1);
@@ -4897,8 +4986,9 @@
     Node* false1;
     {
       Node* check2 = graph()->NewNode(machine()->Int32LessThan(), lhs, zero);
-      Node* branch2 = graph()->NewNode(common()->Branch(BranchHint::kFalse),
-                                       check2, if_false1);
+      Node* branch2 = graph()->NewNode(
+          common()->Branch(BranchHint::kFalse, BranchSemantics::kMachine),
+          check2, if_false1);
 
       Node* if_true2 = graph()->NewNode(common()->IfTrue(), branch2);
       Node* true2 = graph()->NewNode(
@@ -4922,8 +5012,9 @@
   Node* false0;
   {
     Node* check1 = graph()->NewNode(machine()->Int32LessThan(), rhs, minus_one);
-    Node* branch1 = graph()->NewNode(common()->Branch(BranchHint::kTrue),
-                                     check1, if_false0);
+    Node* branch1 = graph()->NewNode(
+        common()->Branch(BranchHint::kTrue, BranchSemantics::kMachine), check1,
+        if_false0);
 
     Node* if_true1 = graph()->NewNode(common()->IfTrue(), branch1);
     Node* true1 = graph()->NewNode(machine()->Int32Mod(), lhs, rhs, if_true1);
@@ -4968,7 +5059,8 @@
   }
 
   Node* check = graph()->NewNode(machine()->Word32Equal(), rhs, zero);
-  Diamond d(graph(), common(), check, BranchHint::kFalse);
+  Diamond d(graph(), common(), check, BranchHint::kFalse,
+            BranchSemantics::kMachine);
   Node* div = graph()->NewNode(machine()->Uint32Div(), lhs, rhs, d.if_false);
   return d.Phi(MachineRepresentation::kWord32, zero, div);
 }
@@ -5005,8 +5097,9 @@
       common()->Phi(MachineRepresentation::kWord32, 2);
 
   Node* check0 = graph()->NewNode(machine()->Word32Equal(), rhs, zero);
-  Node* branch0 = graph()->NewNode(common()->Branch(BranchHint::kFalse), check0,
-                                   graph()->start());
+  Node* branch0 = graph()->NewNode(
+      common()->Branch(BranchHint::kFalse, BranchSemantics::kMachine), check0,
+      graph()->start());
 
   Node* if_true0 = graph()->NewNode(common()->IfTrue(), branch0);
   Node* true0 = zero;
@@ -5017,7 +5110,9 @@
     Node* msk = graph()->NewNode(machine()->Int32Add(), rhs, minus_one);
 
     Node* check1 = graph()->NewNode(machine()->Word32And(), rhs, msk);
-    Node* branch1 = graph()->NewNode(common()->Branch(), check1, if_false0);
+    Node* branch1 = graph()->NewNode(
+        common()->Branch(BranchHint::kNone, BranchSemantics::kMachine), check1,
+        if_false0);
 
     Node* if_true1 = graph()->NewNode(common()->IfTrue(), branch1);
     Node* true1 = graph()->NewNode(machine()->Uint32Mod(), lhs, rhs, if_true1);
diff -r -u --color up/v8/src/compiler/simplified-operator.cc nw/v8/src/compiler/simplified-operator.cc
--- up/v8/src/compiler/simplified-operator.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/simplified-operator.cc	2023-01-19 16:46:36.135609592 -0500
@@ -544,7 +544,11 @@
 
 BigIntOperationHint BigIntOperationHintOf(const Operator* op) {
   // TODO(panq): Expand the DCHECK when more BigInt operations are supported.
-  DCHECK(op->opcode() == IrOpcode::kSpeculativeBigIntAdd);
+  DCHECK(op->opcode() == IrOpcode::kSpeculativeBigIntAdd ||
+         op->opcode() == IrOpcode::kSpeculativeBigIntSubtract ||
+         op->opcode() == IrOpcode::kSpeculativeBigIntMultiply ||
+         op->opcode() == IrOpcode::kSpeculativeBigIntDivide ||
+         op->opcode() == IrOpcode::kSpeculativeBigIntModulus);
   return OpParameter<BigIntOperationHint>(op);
 }
 
@@ -804,12 +808,15 @@
   V(BigIntSubtract, Operator::kNoProperties, 2, 1)        \
   V(BigIntMultiply, Operator::kNoProperties, 2, 1)        \
   V(BigIntDivide, Operator::kNoProperties, 2, 1)          \
+  V(BigIntModulus, Operator::kNoProperties, 2, 1)         \
   V(BigIntBitwiseAnd, Operator::kNoProperties, 2, 1)      \
   V(StringCharCodeAt, Operator::kNoProperties, 2, 1)      \
   V(StringCodePointAt, Operator::kNoProperties, 2, 1)     \
   V(StringFromCodePointAt, Operator::kNoProperties, 2, 1) \
   V(StringSubstring, Operator::kNoProperties, 3, 1)       \
-  V(DateNow, Operator::kNoProperties, 0, 1)
+  V(DateNow, Operator::kNoProperties, 0, 1)               \
+  V(DoubleArrayMax, Operator::kNoProperties, 1, 1)        \
+  V(DoubleArrayMin, Operator::kNoProperties, 1, 1)
 
 #define SPECULATIVE_NUMBER_BINOP_LIST(V)      \
   SIMPLIFIED_SPECULATIVE_NUMBER_BINOP_LIST(V) \
@@ -832,14 +839,18 @@
   V(CheckedInt32Sub, 2, 1)                \
   V(CheckedUint32Div, 2, 1)               \
   V(CheckedUint32Mod, 2, 1)               \
-  V(CheckedBigInt64Add, 2, 1)
+  V(CheckedInt64Add, 2, 1)                \
+  V(CheckedInt64Sub, 2, 1)                \
+  V(CheckedInt64Mul, 2, 1)                \
+  V(CheckedInt64Div, 2, 1)                \
+  V(CheckedInt64Mod, 2, 1)
 
 #define CHECKED_WITH_FEEDBACK_OP_LIST(V) \
   V(CheckNumber, 1, 1)                   \
   V(CheckSmi, 1, 1)                      \
   V(CheckString, 1, 1)                   \
   V(CheckBigInt, 1, 1)                   \
-  V(CheckBigInt64, 1, 1)                 \
+  V(CheckedBigIntToBigInt64, 1, 1)       \
   V(CheckedInt32ToTaggedSigned, 1, 1)    \
   V(CheckedInt64ToInt32, 1, 1)           \
   V(CheckedInt64ToTaggedSigned, 1, 1)    \
@@ -1638,6 +1649,14 @@
       1, 1, 1, 0, hint);
 }
 
+const Operator* SimplifiedOperatorBuilder::SpeculativeBigIntModulus(
+    BigIntOperationHint hint) {
+  return zone()->New<Operator1<BigIntOperationHint>>(
+      IrOpcode::kSpeculativeBigIntModulus,
+      Operator::kFoldable | Operator::kNoThrow, "SpeculativeBigIntModulus", 2,
+      1, 1, 1, 1, 0, hint);
+}
+
 const Operator* SimplifiedOperatorBuilder::SpeculativeBigIntBitwiseAnd(
     BigIntOperationHint hint) {
   return zone()->New<Operator1<BigIntOperationHint>>(
diff -r -u --color up/v8/src/compiler/simplified-operator.h nw/v8/src/compiler/simplified-operator.h
--- up/v8/src/compiler/simplified-operator.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/simplified-operator.h	2023-01-19 16:46:36.135609592 -0500
@@ -788,6 +788,7 @@
   const Operator* BigIntSubtract();
   const Operator* BigIntMultiply();
   const Operator* BigIntDivide();
+  const Operator* BigIntModulus();
   const Operator* BigIntBitwiseAnd();
   const Operator* BigIntNegate();
 
@@ -815,6 +816,7 @@
   const Operator* SpeculativeBigIntSubtract(BigIntOperationHint hint);
   const Operator* SpeculativeBigIntMultiply(BigIntOperationHint hint);
   const Operator* SpeculativeBigIntDivide(BigIntOperationHint hint);
+  const Operator* SpeculativeBigIntModulus(BigIntOperationHint hint);
   const Operator* SpeculativeBigIntBitwiseAnd(BigIntOperationHint hint);
   const Operator* SpeculativeBigIntNegate(BigIntOperationHint hint);
   const Operator* SpeculativeBigIntAsIntN(int bits,
@@ -917,7 +919,11 @@
   const Operator* CheckedInt32Mod();
   const Operator* CheckedInt32Mul(CheckForMinusZeroMode);
   const Operator* CheckedInt32Sub();
-  const Operator* CheckedBigInt64Add();
+  const Operator* CheckedInt64Add();
+  const Operator* CheckedInt64Sub();
+  const Operator* CheckedInt64Mul();
+  const Operator* CheckedInt64Div();
+  const Operator* CheckedInt64Mod();
   const Operator* CheckedInt32ToTaggedSigned(const FeedbackSource& feedback);
   const Operator* CheckedInt64ToInt32(const FeedbackSource& feedback);
   const Operator* CheckedInt64ToTaggedSigned(const FeedbackSource& feedback);
@@ -932,7 +938,7 @@
   const Operator* CheckedTaggedToTaggedPointer(const FeedbackSource& feedback);
   const Operator* CheckedTaggedToTaggedSigned(const FeedbackSource& feedback);
   const Operator* CheckBigInt(const FeedbackSource& feedback);
-  const Operator* CheckBigInt64(const FeedbackSource& feedback);
+  const Operator* CheckedBigIntToBigInt64(const FeedbackSource& feedback);
   const Operator* CheckedTruncateTaggedToWord32(CheckTaggedInputMode,
                                                 const FeedbackSource& feedback);
   const Operator* CheckedUint32Div();
@@ -1084,6 +1090,11 @@
 #endif
 
   const Operator* DateNow();
+
+  // Math.min/max for JSArray with PACKED_DOUBLE_ELEMENTS.
+  const Operator* DoubleArrayMin();
+  const Operator* DoubleArrayMax();
+
   // Unsigned32Divide is a special operator to express the division of two
   // Unsigned32 inputs and truncating the result to Unsigned32. It's semantics
   // is equivalent to NumberFloor(NumberDivide(x:Unsigned32, y:Unsigned32)) but
diff -r -u --color up/v8/src/compiler/turboshaft/assembler.h nw/v8/src/compiler/turboshaft/assembler.h
--- up/v8/src/compiler/turboshaft/assembler.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/turboshaft/assembler.h	2023-01-19 16:46:36.135609592 -0500
@@ -19,30 +19,110 @@
 #include "src/compiler/turboshaft/graph.h"
 #include "src/compiler/turboshaft/operation-matching.h"
 #include "src/compiler/turboshaft/operations.h"
+#include "src/compiler/turboshaft/optimization-phase.h"
+#include "src/compiler/turboshaft/representations.h"
 
 namespace v8::internal::compiler::turboshaft {
 
-// This class is used to extend an assembler with useful short-hands that still
-// forward to the regular operations of the deriving assembler.
-template <class Subclass, class Superclass>
-class AssemblerInterface : public Superclass {
+template <class Assembler, template <class> class... Reducers>
+class ReducerStack {};
+
+template <class Assembler, template <class> class FirstReducer,
+          template <class> class... Reducers>
+class ReducerStack<Assembler, FirstReducer, Reducers...>
+    : public FirstReducer<ReducerStack<Assembler, Reducers...>> {};
+
+template <class Assembler>
+class ReducerStack<Assembler> {
+ public:
+  Assembler& Asm() { return *static_cast<Assembler*>(this); }
+};
+
+// This empty base-class is used to provide default-implementations of plain
+// methods emitting operations.
+template <class Next>
+class ReducerBaseForwarder : public Next {
+ public:
+#define EMIT_OP(Name)                                    \
+  template <class... Args>                               \
+  OpIndex Reduce##Name(Args... args) {                   \
+    return this->Asm().template Emit<Name##Op>(args...); \
+  }
+  TURBOSHAFT_OPERATION_LIST(EMIT_OP)
+#undef EMIT_OP
+};
+
+// ReducerBase provides default implementations of Branch-related Operations
+// (Goto, Branch, Switch, CatchException), and takes care of updating Block
+// predecessors (and calls the Assembler to maintain split-edge form).
+// ReducerBase is always added by Assembler at the bottom of the reducer stack.
+template <class Next>
+class ReducerBase : public ReducerBaseForwarder<Next> {
  public:
-  using Superclass::Superclass;
-  using Base = Superclass;
+  using Next::Asm;
+  using Base = ReducerBaseForwarder<Next>;
+
+  void Bind(Block*, const Block*) {}
+
+  OpIndex ReducePhi(base::Vector<const OpIndex> inputs,
+                    RegisterRepresentation rep) {
+    DCHECK(Asm().current_block()->IsMerge() &&
+           inputs.size() == Asm().current_block()->Predecessors().size());
+    return Base::ReducePhi(inputs, rep);
+  }
+
+  template <class... Args>
+  OpIndex ReducePendingLoopPhi(Args... args) {
+    DCHECK(Asm().current_block()->IsLoop());
+    return Base::ReducePendingLoopPhi(args...);
+  }
+
+  OpIndex ReduceGoto(Block* destination) {
+    destination->AddPredecessor(Asm().current_block());
+    return Base::ReduceGoto(destination);
+  }
+
+  OpIndex ReduceBranch(OpIndex condition, Block* if_true, Block* if_false) {
+    if_true->AddPredecessor(Asm().current_block());
+    if_false->AddPredecessor(Asm().current_block());
+    return Base::ReduceBranch(condition, if_true, if_false);
+  }
+
+  OpIndex ReduceCatchException(OpIndex call, Block* if_success,
+                               Block* if_exception) {
+    if_success->AddPredecessor(Asm().current_block());
+    if_exception->AddPredecessor(Asm().current_block());
+    return Base::ReduceCatchException(call, if_success, if_exception);
+  }
+
+  OpIndex ReduceSwitch(OpIndex input, base::Vector<const SwitchOp::Case> cases,
+                       Block* default_case) {
+    for (SwitchOp::Case c : cases) {
+      c.destination->AddPredecessor(Asm().current_block());
+    }
+    default_case->AddPredecessor(Asm().current_block());
+    return Base::ReduceSwitch(input, cases, default_case);
+  }
+};
 
-#define DECL_MULTI_REP_BINOP(name, operation, rep_type, kind)              \
-  OpIndex name(OpIndex left, OpIndex right, rep_type rep) {                \
-    return subclass().operation(left, right, operation##Op::Kind::k##kind, \
-                                rep);                                      \
-  }
-#define DECL_SINGLE_REP_BINOP(name, operation, kind, rep)                  \
-  OpIndex name(OpIndex left, OpIndex right) {                              \
-    return subclass().operation(left, right, operation##Op::Kind::k##kind, \
-                                rep);                                      \
+template <class Assembler>
+class AssemblerOpInterface {
+ public:
+// Methods to be used by the reducers to reducer operations with the whole
+// reducer stack.
+#define DECL_MULTI_REP_BINOP(name, operation, rep_type, kind)            \
+  OpIndex name(OpIndex left, OpIndex right, rep_type rep) {              \
+    return stack().Reduce##operation(left, right,                        \
+                                     operation##Op::Kind::k##kind, rep); \
+  }
+#define DECL_SINGLE_REP_BINOP(name, operation, kind, rep)                \
+  OpIndex name(OpIndex left, OpIndex right) {                            \
+    return stack().Reduce##operation(left, right,                        \
+                                     operation##Op::Kind::k##kind, rep); \
   }
 #define DECL_SINGLE_REP_BINOP_NO_KIND(name, operation, rep) \
   OpIndex name(OpIndex left, OpIndex right) {               \
-    return subclass().operation(left, right, rep);          \
+    return stack().Reduce##operation(left, right, rep);     \
   }
   DECL_MULTI_REP_BINOP(WordAdd, WordBinop, WordRepresentation, Add)
   DECL_SINGLE_REP_BINOP(Word32Add, WordBinop, Add, WordRepresentation::Word32())
@@ -165,6 +245,11 @@
   DECL_SINGLE_REP_BINOP(Float64Atan2, FloatBinop, Atan2,
                         FloatRepresentation::Float64())
 
+  OpIndex Shift(OpIndex left, OpIndex right, ShiftOp::Kind kind,
+                WordRepresentation rep) {
+    return stack().ReduceShift(left, right, kind, rep);
+  }
+
   DECL_MULTI_REP_BINOP(ShiftRightArithmeticShiftOutZeros, Shift,
                        WordRepresentation, ShiftRightArithmeticShiftOutZeros)
   DECL_SINGLE_REP_BINOP(Word32ShiftRightArithmeticShiftOutZeros, Shift,
@@ -222,6 +307,9 @@
                                 FloatRepresentation::Float32())
   DECL_SINGLE_REP_BINOP_NO_KIND(Float64Equal, Equal,
                                 FloatRepresentation::Float64())
+  OpIndex Equal(OpIndex left, OpIndex right, RegisterRepresentation rep) {
+    return stack().ReduceEqual(left, right, rep);
+  }
 
   DECL_MULTI_REP_BINOP(IntLessThan, Comparison, RegisterRepresentation,
                        SignedLessThan)
@@ -260,18 +348,24 @@
                         SignedLessThanOrEqual, FloatRepresentation::Float32())
   DECL_SINGLE_REP_BINOP(Float64LessThanOrEqual, Comparison,
                         SignedLessThanOrEqual, FloatRepresentation::Float64())
+  OpIndex Comparison(OpIndex left, OpIndex right, ComparisonOp::Kind kind,
+                     RegisterRepresentation rep) {
+    return stack().ReduceComparison(left, right, kind, rep);
+  }
 
 #undef DECL_SINGLE_REP_BINOP
 #undef DECL_MULTI_REP_BINOP
 #undef DECL_SINGLE_REP_BINOP_NO_KIND
 
-#define DECL_MULTI_REP_UNARY(name, operation, rep_type, kind)              \
-  OpIndex name(OpIndex input, rep_type rep) {                              \
-    return subclass().operation(input, operation##Op::Kind::k##kind, rep); \
-  }
-#define DECL_SINGLE_REP_UNARY(name, operation, kind, rep)                  \
-  OpIndex name(OpIndex input) {                                            \
-    return subclass().operation(input, operation##Op::Kind::k##kind, rep); \
+#define DECL_MULTI_REP_UNARY(name, operation, rep_type, kind)             \
+  OpIndex name(OpIndex input, rep_type rep) {                             \
+    return stack().Reduce##operation(input, operation##Op::Kind::k##kind, \
+                                     rep);                                \
+  }
+#define DECL_SINGLE_REP_UNARY(name, operation, kind, rep)                 \
+  OpIndex name(OpIndex input) {                                           \
+    return stack().Reduce##operation(input, operation##Op::Kind::k##kind, \
+                                     rep);                                \
   }
 
   DECL_MULTI_REP_UNARY(FloatAbs, FloatUnary, FloatRepresentation, Abs)
@@ -391,23 +485,32 @@
 #undef DECL_SINGLE_REP_UNARY
 #undef DECL_MULTI_REP_UNARY
 
-  OpIndex Word32Select(OpIndex condition, OpIndex left, OpIndex right) {
-    return subclass().Select(condition, left, right,
-                             WordRepresentation::Word32());
-  }
-  OpIndex Word64Select(OpIndex condition, OpIndex left, OpIndex right) {
-    return subclass().Select(condition, left, right,
-                             WordRepresentation::Word64());
+  OpIndex Float64InsertWord32(OpIndex float64, OpIndex word32,
+                              Float64InsertWord32Op::Kind kind) {
+    return stack().ReduceFloat64InsertWord32(float64, word32, kind);
+  }
+
+  OpIndex TaggedBitcast(OpIndex input, RegisterRepresentation from,
+                        RegisterRepresentation to) {
+    return stack().ReduceTaggedBitcast(input, from, to);
+  }
+  OpIndex BitcastTaggedToWord(OpIndex tagged) {
+    return TaggedBitcast(tagged, RegisterRepresentation::Tagged(),
+                         RegisterRepresentation::PointerSized());
+  }
+  OpIndex BitcastWordToTagged(OpIndex word) {
+    return TaggedBitcast(word, RegisterRepresentation::PointerSized(),
+                         RegisterRepresentation::Tagged());
   }
 
   OpIndex Word32Constant(uint32_t value) {
-    return subclass().Constant(ConstantOp::Kind::kWord32, uint64_t{value});
+    return stack().ReduceConstant(ConstantOp::Kind::kWord32, uint64_t{value});
   }
   OpIndex Word32Constant(int32_t value) {
     return Word32Constant(static_cast<uint32_t>(value));
   }
   OpIndex Word64Constant(uint64_t value) {
-    return subclass().Constant(ConstantOp::Kind::kWord64, value);
+    return stack().ReduceConstant(ConstantOp::Kind::kWord64, value);
   }
   OpIndex Word64Constant(int64_t value) {
     return Word64Constant(static_cast<uint64_t>(value));
@@ -421,10 +524,10 @@
     }
   }
   OpIndex Float32Constant(float value) {
-    return subclass().Constant(ConstantOp::Kind::kFloat32, value);
+    return stack().ReduceConstant(ConstantOp::Kind::kFloat32, value);
   }
   OpIndex Float64Constant(double value) {
-    return subclass().Constant(ConstantOp::Kind::kFloat64, value);
+    return stack().ReduceConstant(ConstantOp::Kind::kFloat64, value);
   }
   OpIndex FloatConstant(double value, FloatRepresentation rep) {
     switch (rep.value()) {
@@ -435,40 +538,41 @@
     }
   }
   OpIndex NumberConstant(double value) {
-    return subclass().Constant(ConstantOp::Kind::kNumber, value);
+    return stack().ReduceConstant(ConstantOp::Kind::kNumber, value);
   }
   OpIndex TaggedIndexConstant(int32_t value) {
-    return subclass().Constant(ConstantOp::Kind::kTaggedIndex,
-                               uint64_t{static_cast<uint32_t>(value)});
+    return stack().ReduceConstant(ConstantOp::Kind::kTaggedIndex,
+                                  uint64_t{static_cast<uint32_t>(value)});
   }
   OpIndex HeapConstant(Handle<HeapObject> value) {
-    return subclass().Constant(ConstantOp::Kind::kHeapObject, value);
+    return stack().ReduceConstant(ConstantOp::Kind::kHeapObject, value);
   }
   OpIndex CompressedHeapConstant(Handle<HeapObject> value) {
-    return subclass().Constant(ConstantOp::Kind::kHeapObject, value);
+    return stack().ReduceConstant(ConstantOp::Kind::kHeapObject, value);
   }
   OpIndex ExternalConstant(ExternalReference value) {
-    return subclass().Constant(ConstantOp::Kind::kExternal, value);
+    return stack().ReduceConstant(ConstantOp::Kind::kExternal, value);
   }
   OpIndex RelocatableConstant(int64_t value, RelocInfo::Mode mode) {
     DCHECK_EQ(mode, any_of(RelocInfo::WASM_CALL, RelocInfo::WASM_STUB_CALL));
-    return subclass().Constant(mode == RelocInfo::WASM_CALL
-                                   ? ConstantOp::Kind::kRelocatableWasmCall
-                                   : ConstantOp::Kind::kRelocatableWasmStubCall,
-                               static_cast<uint64_t>(value));
+    return stack().ReduceConstant(
+        mode == RelocInfo::WASM_CALL
+            ? ConstantOp::Kind::kRelocatableWasmCall
+            : ConstantOp::Kind::kRelocatableWasmStubCall,
+        static_cast<uint64_t>(value));
   }
 
 #define DECL_CHANGE(name, kind, assumption, from, to)                  \
   OpIndex name(OpIndex input) {                                        \
-    return subclass().Change(                                          \
+    return stack().ReduceChange(                                       \
         input, ChangeOp::Kind::kind, ChangeOp::Assumption::assumption, \
         RegisterRepresentation::from(), RegisterRepresentation::to()); \
   }
-#define DECL_TRY_CHANGE(name, kind, from, to)                   \
-  OpIndex name(OpIndex input) {                                 \
-    return subclass().TryChange(input, TryChangeOp::Kind::kind, \
-                                FloatRepresentation::from(),    \
-                                WordRepresentation::to());      \
+#define DECL_TRY_CHANGE(name, kind, from, to)                      \
+  OpIndex name(OpIndex input) {                                    \
+    return stack().ReduceTryChange(input, TryChangeOp::Kind::kind, \
+                                   FloatRepresentation::from(),    \
+                                   WordRepresentation::to());      \
   }
 
   DECL_CHANGE(BitcastWord32ToWord64, kBitcast, kNoAssumption, Word32, Word64)
@@ -554,120 +658,211 @@
 #undef DECL_CHANGE
 #undef DECL_TRY_CHANGE
 
-  using Base::Tuple;
-  OpIndex Tuple(OpIndex a, OpIndex b) {
-    return subclass().Tuple(base::VectorOf({a, b}));
+  OpIndex Load(OpIndex base, LoadOp::Kind kind, MemoryRepresentation loaded_rep,
+               int32_t offset = 0) {
+    return Load(base, OpIndex::Invalid(), kind, loaded_rep, offset);
+  }
+  OpIndex Load(OpIndex base, OpIndex index, LoadOp::Kind kind,
+               MemoryRepresentation loaded_rep, int32_t offset = 0,
+               uint8_t element_size_log2 = 0) {
+    return stack().ReduceLoad(base, index, kind, loaded_rep,
+                              loaded_rep.ToRegisterRepresentation(), offset,
+                              element_size_log2);
+  }
+  void Store(OpIndex base, OpIndex value, StoreOp::Kind kind,
+             MemoryRepresentation stored_rep, WriteBarrierKind write_barrier,
+             int32_t offset = 0) {
+    Store(base, OpIndex::Invalid(), value, kind, stored_rep, write_barrier,
+          offset);
+  }
+  void Store(OpIndex base, OpIndex index, OpIndex value, StoreOp::Kind kind,
+             MemoryRepresentation stored_rep, WriteBarrierKind write_barrier,
+             int32_t offset = 0, uint8_t element_size_log2 = 0) {
+    stack().ReduceStore(base, index, value, kind, stored_rep, write_barrier,
+                        offset, element_size_log2);
   }
 
- private:
-  Subclass& subclass() { return *static_cast<Subclass*>(this); }
-};
+  void Retain(OpIndex value) { stack().ReduceRetain(value); }
 
-// This empty base-class is used to provide default-implementations of plain
-// methods emitting operations.
-template <class Assembler>
-class AssemblerBase {
- public:
-#define EMIT_OP(Name)                                                       \
-  template <class... Args>                                                  \
-  OpIndex Name(Args... args) {                                              \
-    return static_cast<Assembler*>(this)->template Emit<Name##Op>(args...); \
+  OpIndex StackPointerGreaterThan(OpIndex limit, StackCheckKind kind) {
+    return stack().ReduceStackPointerGreaterThan(limit, kind);
   }
-  TURBOSHAFT_OPERATION_LIST(EMIT_OP)
-#undef EMIT_OP
-};
-
-class Assembler
-    : public AssemblerInterface<Assembler, AssemblerBase<Assembler>>,
-      public OperationMatching<Assembler> {
- public:
-  Block* NewBlock(Block::Kind kind) { return graph_.NewBlock(kind); }
 
-  void EnterBlock(const Block& block) { USE(block); }
-  void ExitBlock(const Block& block) { USE(block); }
+  OpIndex StackCheckOffset() {
+    return stack().ReduceFrameConstant(
+        FrameConstantOp::Kind::kStackCheckOffset);
+  }
+  OpIndex FramePointer() {
+    return stack().ReduceFrameConstant(FrameConstantOp::Kind::kFramePointer);
+  }
+  OpIndex ParentFramePointer() {
+    return stack().ReduceFrameConstant(
+        FrameConstantOp::Kind::kParentFramePointer);
+  }
 
-  V8_INLINE bool Bind(Block* block) {
-    if (!graph().Add(block)) return false;
-    DCHECK_NULL(current_block_);
-    current_block_ = block;
-    return true;
+  OpIndex StackSlot(int size, int alignment) {
+    return stack().ReduceStackSlot(size, alignment);
   }
 
-  void SetCurrentOrigin(OpIndex operation_origin) {
-    current_operation_origin_ = operation_origin;
+  void Goto(Block* destination) { stack().ReduceGoto(destination); }
+  void Branch(OpIndex condition, Block* if_true, Block* if_false) {
+    stack().ReduceBranch(condition, if_true, if_false);
   }
+  OpIndex Select(OpIndex cond, OpIndex vtrue, OpIndex vfalse,
+                 RegisterRepresentation rep, BranchHint hint,
+                 SelectOp::Implementation implem) {
+    return stack().ReduceSelect(cond, vtrue, vfalse, rep, hint, implem);
+  }
+  void Switch(OpIndex input, base::Vector<const SwitchOp::Case> cases,
+              Block* default_case) {
+    stack().ReduceSwitch(input, cases, default_case);
+  }
+  OpIndex CatchException(OpIndex call, Block* if_success, Block* if_exception) {
+    return stack().ReduceCatchException(call, if_success, if_exception);
+  }
+  void Unreachable() { stack().ReduceUnreachable(); }
 
-  OpIndex Phi(base::Vector<const OpIndex> inputs, RegisterRepresentation rep) {
-    DCHECK(current_block()->IsMerge() &&
-           inputs.size() == current_block()->Predecessors().size());
-    return Base::Phi(inputs, rep);
+  OpIndex Parameter(int index, const char* debug_name = nullptr) {
+    return stack().ReduceParameter(index, debug_name);
+  }
+  OpIndex OsrValue(int index) { return stack().ReduceOsrValue(index); }
+  void Return(OpIndex pop_count, base::Vector<OpIndex> return_values) {
+    stack().ReduceReturn(pop_count, return_values);
+  }
+  void Return(OpIndex result) {
+    Return(Word32Constant(0), base::VectorOf({result}));
   }
 
-  template <class... Args>
-  OpIndex PendingLoopPhi(Args... args) {
-    DCHECK(current_block()->IsLoop());
-    return Base::PendingLoopPhi(args...);
+  OpIndex Call(OpIndex callee, base::Vector<const OpIndex> arguments,
+               const CallDescriptor* descriptor) {
+    return stack().ReduceCall(callee, arguments, descriptor);
+  }
+  OpIndex CallMaybeDeopt(OpIndex callee, base::Vector<const OpIndex> arguments,
+                         const CallDescriptor* descriptor,
+                         OpIndex frame_state) {
+    OpIndex call = stack().ReduceCall(callee, arguments, descriptor);
+    stack().ReduceCheckLazyDeopt(call, frame_state);
+    return call;
+  }
+  void TailCall(OpIndex callee, base::Vector<const OpIndex> arguments,
+                const CallDescriptor* descriptor) {
+    stack().ReduceTailCall(callee, arguments, descriptor);
   }
 
-  OpIndex Goto(Block* destination) {
-    destination->AddPredecessor(current_block());
-    return Base::Goto(destination);
+  OpIndex FrameState(base::Vector<const OpIndex> inputs, bool inlined,
+                     const FrameStateData* data) {
+    return stack().ReduceFrameState(inputs, inlined, data);
+  }
+  void DeoptimizeIf(OpIndex condition, OpIndex frame_state,
+                    const DeoptimizeParameters* parameters) {
+    stack().ReduceDeoptimizeIf(condition, frame_state, false, parameters);
+  }
+  void DeoptimizeIfNot(OpIndex condition, OpIndex frame_state,
+                       const DeoptimizeParameters* parameters) {
+    stack().ReduceDeoptimizeIf(condition, frame_state, true, parameters);
+  }
+  void Deoptimize(OpIndex frame_state, const DeoptimizeParameters* parameters) {
+    stack().ReduceDeoptimize(frame_state, parameters);
   }
 
-  OpIndex Branch(OpIndex condition, Block* if_true, Block* if_false) {
-    if_true->AddPredecessor(current_block());
-    if_false->AddPredecessor(current_block());
-    return Base::Branch(condition, if_true, if_false);
+  void TrapIf(OpIndex condition, TrapId trap_id) {
+    stack().ReduceTrapIf(condition, false, trap_id);
+  }
+  void TrapIfNot(OpIndex condition, TrapId trap_id) {
+    stack().ReduceTrapIf(condition, true, trap_id);
   }
 
-  OpIndex CatchException(OpIndex call, Block* if_success, Block* if_exception) {
-    if_success->AddPredecessor(current_block());
-    if_exception->AddPredecessor(current_block());
-    return Base::CatchException(call, if_success, if_exception);
+  OpIndex Phi(base::Vector<const OpIndex> inputs, RegisterRepresentation rep) {
+    return stack().ReducePhi(inputs, rep);
+  }
+  OpIndex PendingLoopPhi(OpIndex first, RegisterRepresentation rep,
+                         OpIndex old_backedge_index) {
+    return stack().ReducePendingLoopPhi(first, rep, old_backedge_index);
+  }
+  OpIndex PendingLoopPhi(OpIndex first, RegisterRepresentation rep,
+                         Node* old_backedge_index) {
+    return stack().ReducePendingLoopPhi(first, rep, old_backedge_index);
   }
 
-  OpIndex Switch(OpIndex input, base::Vector<const SwitchOp::Case> cases,
-                 Block* default_case) {
-    for (SwitchOp::Case c : cases) {
-      c.destination->AddPredecessor(current_block());
-    }
-    default_case->AddPredecessor(current_block());
-    return Base::Switch(input, cases, default_case);
+  OpIndex Tuple(OpIndex a, OpIndex b) {
+    return stack().ReduceTuple(base::VectorOf({a, b}));
   }
+  OpIndex Projection(OpIndex tuple, uint16_t index) {
+    return stack().ReduceProjection(tuple, index);
+  }
+
+ private:
+  Assembler& stack() { return *static_cast<Assembler*>(this); }
+};
 
-  explicit Assembler(Graph* graph, Zone* phase_zone)
-      : graph_(*graph), phase_zone_(phase_zone) {
-    graph_.Reset();
+template <template <class> class... Reducers>
+class Assembler
+    : public GraphVisitor<Assembler<Reducers...>>,
+      public ReducerStack<Assembler<Reducers...>, Reducers..., ReducerBase>,
+      public OperationMatching<Assembler<Reducers...>>,
+      public AssemblerOpInterface<Assembler<Reducers...>> {
+  using Stack = ReducerStack<Assembler<Reducers...>, Reducers...,
+                             v8::internal::compiler::turboshaft::ReducerBase>;
+
+ public:
+  explicit Assembler(Graph& input_graph, Graph& output_graph, Zone* phase_zone,
+                     compiler::NodeOriginTable* origins = nullptr)
+      : GraphVisitor<Assembler>(input_graph, output_graph, phase_zone,
+                                origins) {
     SupportedOperations::Initialize();
   }
 
-  Block* current_block() { return current_block_; }
-  Zone* graph_zone() { return graph().graph_zone(); }
-  Graph& graph() { return graph_; }
-  Zone* phase_zone() { return phase_zone_; }
+  Block* NewBlock(Block::Kind kind) {
+    return this->output_graph().NewBlock(kind);
+  }
 
- private:
-  friend class AssemblerBase<Assembler>;
-  void FinalizeBlock() {
-    graph().Finalize(current_block_);
-    current_block_ = nullptr;
+  using OperationMatching<Assembler<Reducers...>>::Get;
+
+  V8_INLINE V8_WARN_UNUSED_RESULT bool Bind(Block* block,
+                                            const Block* origin = nullptr) {
+    if (!this->output_graph().Add(block)) return false;
+    DCHECK_NULL(current_block_);
+    current_block_ = block;
+    Stack::Bind(block, origin);
+    return true;
+  }
+
+  V8_INLINE void BindReachable(Block* block, const Block* origin = nullptr) {
+    bool bound = Bind(block, origin);
+    DCHECK(bound);
+    USE(bound);
+  }
+
+  void SetCurrentOrigin(OpIndex operation_origin) {
+    current_operation_origin_ = operation_origin;
   }
 
+  Block* current_block() const { return current_block_; }
+  OpIndex current_operation_origin() const { return current_operation_origin_; }
+
   template <class Op, class... Args>
   OpIndex Emit(Args... args) {
     static_assert((std::is_base_of<Operation, Op>::value));
     static_assert(!(std::is_same<Op, Operation>::value));
     DCHECK_NOT_NULL(current_block_);
-    OpIndex result = graph().Add<Op>(args...);
-    graph().operation_origins()[result] = current_operation_origin_;
-    if (Op::properties.is_block_terminator) FinalizeBlock();
+    OpIndex result = this->output_graph().next_operation_index();
+    Op& op = this->output_graph().template Add<Op>(args...);
+    this->output_graph().operation_origins()[result] =
+        current_operation_origin_;
+    if (op.Properties().is_block_terminator) FinalizeBlock();
     return result;
   }
 
+ private:
+  void FinalizeBlock() {
+    this->output_graph().Finalize(current_block_);
+    current_block_ = nullptr;
+  }
+
   Block* current_block_ = nullptr;
-  Graph& graph_;
+  // TODO(dmercadier,tebbi): remove {current_operation_origin_} and pass instead
+  // additional parameters to ReduceXXX methods.
   OpIndex current_operation_origin_ = OpIndex::Invalid();
-  Zone* const phase_zone_;
 };
 
 }  // namespace v8::internal::compiler::turboshaft
diff -r -u --color up/v8/src/compiler/turboshaft/decompression-optimization.cc nw/v8/src/compiler/turboshaft/decompression-optimization.cc
--- up/v8/src/compiler/turboshaft/decompression-optimization.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/turboshaft/decompression-optimization.cc	2023-01-19 16:46:36.135609592 -0500
@@ -77,15 +77,7 @@
     case Opcode::kStore: {
       auto& store = op.Cast<StoreOp>();
       MarkAsNeedsDecompression(store.base());
-      if (!store.stored_rep.IsTagged()) {
-        MarkAsNeedsDecompression(store.value());
-      }
-      break;
-    }
-    case Opcode::kIndexedStore: {
-      auto& store = op.Cast<IndexedStoreOp>();
-      MarkAsNeedsDecompression(store.base());
-      MarkAsNeedsDecompression(store.index());
+      if (store.index().valid()) MarkAsNeedsDecompression(store.index());
       if (!store.stored_rep.IsTagged()) {
         MarkAsNeedsDecompression(store.value());
       }
@@ -151,7 +143,6 @@
       }
       break;
     }
-    case Opcode::kIndexedLoad:
     case Opcode::kLoad:
     case Opcode::kConstant:
       if (!NeedsDecompression(op)) {
@@ -197,16 +188,6 @@
         if (load.loaded_rep.IsTagged()) {
           DCHECK_EQ(load.result_rep,
                     any_of(RegisterRepresentation::Tagged(),
-                           RegisterRepresentation::Compressed()));
-          load.result_rep = RegisterRepresentation::Compressed();
-        }
-        break;
-      }
-      case Opcode::kIndexedLoad: {
-        auto& load = op.Cast<IndexedLoadOp>();
-        if (load.loaded_rep.IsTagged()) {
-          DCHECK_EQ(load.result_rep,
-                    any_of(RegisterRepresentation::Tagged(),
                            RegisterRepresentation::Compressed()));
           load.result_rep = RegisterRepresentation::Compressed();
         }
diff -r -u --color up/v8/src/compiler/turboshaft/graph-builder.cc nw/v8/src/compiler/turboshaft/graph-builder.cc
--- up/v8/src/compiler/turboshaft/graph-builder.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/turboshaft/graph-builder.cc	2023-01-19 16:46:36.135609592 -0500
@@ -36,7 +36,7 @@
   Zone* graph_zone;
   Zone* phase_zone;
   Schedule& schedule;
-  Assembler assembler;
+  Assembler<> assembler;
   SourcePositionTable* source_positions;
   NodeOriginTable* origins;
 
@@ -48,7 +48,7 @@
  private:
   OpIndex Map(Node* old_node) {
     OpIndex result = op_mapping.Get(old_node);
-    DCHECK(assembler.graph().IsValid(result));
+    DCHECK(assembler.output_graph().IsValid(result));
     return result;
   }
   Block* Map(BasicBlock* block) {
@@ -59,11 +59,11 @@
 
   void FixLoopPhis(Block* loop, Block* backedge) {
     DCHECK(loop->IsLoop());
-    for (Operation& op : assembler.graph().operations(*loop)) {
+    for (Operation& op : assembler.output_graph().operations(*loop)) {
       if (!op.Is<PendingLoopPhiOp>()) continue;
       auto& pending_phi = op.Cast<PendingLoopPhiOp>();
-      assembler.graph().Replace<PhiOp>(
-          assembler.graph().Index(pending_phi),
+      assembler.output_graph().Replace<PhiOp>(
+          assembler.output_graph().Index(pending_phi),
           base::VectorOf(
               {pending_phi.first(), Map(pending_phi.old_backedge_node)}),
           pending_phi.rep);
@@ -223,17 +223,18 @@
   }
 
   if (source_positions->IsEnabled()) {
-    for (OpIndex index : assembler.graph().AllOperationIndices()) {
-      compiler::NodeId origin =
-          assembler.graph().operation_origins()[index].DecodeTurbofanNodeId();
-      assembler.graph().source_positions()[index] =
+    for (OpIndex index : assembler.output_graph().AllOperationIndices()) {
+      compiler::NodeId origin = assembler.output_graph()
+                                    .operation_origins()[index]
+                                    .DecodeTurbofanNodeId();
+      assembler.output_graph().source_positions()[index] =
           source_positions->GetSourcePosition(origin);
     }
   }
 
   if (origins) {
-    for (OpIndex index : assembler.graph().AllOperationIndices()) {
-      OpIndex origin = assembler.graph().operation_origins()[index];
+    for (OpIndex index : assembler.output_graph().AllOperationIndices()) {
+      OpIndex origin = assembler.output_graph().operation_origins()[index];
       origins->SetNodeOrigin(index.id(), origin.DecodeTurbofanNodeId());
     }
   }
@@ -270,7 +271,8 @@
       // Use the `CatchExceptionOp` that has already been produced when
       // processing the call.
       OpIndex catch_exception = Map(node);
-      DCHECK(assembler.graph().Get(catch_exception).Is<CatchExceptionOp>());
+      DCHECK(
+          assembler.output_graph().Get(catch_exception).Is<CatchExceptionOp>());
       return catch_exception;
     }
 
@@ -373,6 +375,7 @@
       BINOP_CASE(Int32AddWithOverflow, Int32AddCheckOverflow)
       BINOP_CASE(Int64AddWithOverflow, Int64AddCheckOverflow)
       BINOP_CASE(Int32MulWithOverflow, Int32MulCheckOverflow)
+      BINOP_CASE(Int64MulWithOverflow, Int64MulCheckOverflow)
       BINOP_CASE(Int32SubWithOverflow, Int32SubCheckOverflow)
       BINOP_CASE(Int64SubWithOverflow, Int64SubCheckOverflow)
 
@@ -567,47 +570,59 @@
                                      RegisterRepresentation::PointerSized(),
                                      RegisterRepresentation::Tagged());
 
+    case IrOpcode::kSelect: {
+      OpIndex cond = Map(node->InputAt(0));
+      OpIndex vtrue = Map(node->InputAt(1));
+      OpIndex vfalse = Map(node->InputAt(2));
+      const SelectParameters& params = SelectParametersOf(op);
+      return assembler.Select(cond, vtrue, vfalse,
+                              RegisterRepresentation::FromMachineRepresentation(
+                                  params.representation()),
+                              params.hint(), SelectOp::Implementation::kBranch);
+    }
     case IrOpcode::kWord32Select:
-      return assembler.Word32Select(
-          Map(node->InputAt(0)), Map(node->InputAt(1)), Map(node->InputAt(2)));
+      return assembler.Select(
+          Map(node->InputAt(0)), Map(node->InputAt(1)), Map(node->InputAt(2)),
+          RegisterRepresentation::Word32(), BranchHint::kNone,
+          SelectOp::Implementation::kCMove);
     case IrOpcode::kWord64Select:
-      return assembler.Word64Select(
-          Map(node->InputAt(0)), Map(node->InputAt(1)), Map(node->InputAt(2)));
+      return assembler.Select(
+          Map(node->InputAt(0)), Map(node->InputAt(1)), Map(node->InputAt(2)),
+          RegisterRepresentation::Word64(), BranchHint::kNone,
+          SelectOp::Implementation::kCMove);
 
     case IrOpcode::kLoad:
     case IrOpcode::kLoadImmutable:
     case IrOpcode::kUnalignedLoad: {
       MemoryRepresentation loaded_rep =
           MemoryRepresentation::FromMachineType(LoadRepresentationOf(op));
-      RegisterRepresentation result_rep = loaded_rep.ToRegisterRepresentation();
       Node* base = node->InputAt(0);
       Node* index = node->InputAt(1);
       // It's ok to merge LoadImmutable into Load after scheduling.
       LoadOp::Kind kind = opcode == IrOpcode::kUnalignedLoad
-                              ? LoadOp::Kind::kRawUnaligned
-                              : LoadOp::Kind::kRawAligned;
+                              ? LoadOp::Kind::RawUnaligned()
+                              : LoadOp::Kind::RawAligned();
       if (index->opcode() == IrOpcode::kInt32Constant) {
         int32_t offset = OpParameter<int32_t>(index->op());
-        return assembler.Load(Map(base), kind, loaded_rep, result_rep, offset);
+        return assembler.Load(Map(base), kind, loaded_rep, offset);
       }
       if (index->opcode() == IrOpcode::kInt64Constant) {
         int64_t offset = OpParameter<int64_t>(index->op());
         if (base::IsValueInRangeForNumericType<int32_t>(offset)) {
-          return assembler.Load(Map(base), kind, loaded_rep, result_rep,
+          return assembler.Load(Map(base), kind, loaded_rep,
                                 static_cast<int32_t>(offset));
         }
       }
       int32_t offset = 0;
       uint8_t element_size_log2 = 0;
-      return assembler.IndexedLoad(Map(base), Map(index), kind, loaded_rep,
-                                   result_rep, offset, element_size_log2);
+      return assembler.Load(Map(base), Map(index), kind, loaded_rep, offset,
+                            element_size_log2);
     }
     case IrOpcode::kProtectedLoad: {
       MemoryRepresentation loaded_rep =
           MemoryRepresentation::FromMachineType(LoadRepresentationOf(op));
-      RegisterRepresentation result_rep = loaded_rep.ToRegisterRepresentation();
-      return assembler.ProtectedLoad(
-          Map(node->InputAt(0)), Map(node->InputAt(1)), loaded_rep, result_rep);
+      return assembler.Load(Map(node->InputAt(0)), Map(node->InputAt(1)),
+                            LoadOp::Kind::Protected(), loaded_rep);
     }
 
     case IrOpcode::kStore:
@@ -618,57 +633,60 @@
                   : StoreRepresentation(UnalignedStoreRepresentationOf(op),
                                         WriteBarrierKind::kNoWriteBarrier);
       StoreOp::Kind kind = opcode == IrOpcode::kStore
-                               ? StoreOp::Kind::kRawAligned
-                               : StoreOp::Kind::kRawUnaligned;
+                               ? StoreOp::Kind::RawAligned()
+                               : StoreOp::Kind::RawUnaligned();
 
       Node* base = node->InputAt(0);
       Node* index = node->InputAt(1);
       Node* value = node->InputAt(2);
       if (index->opcode() == IrOpcode::kInt32Constant) {
         int32_t offset = OpParameter<int32_t>(index->op());
-        return assembler.Store(Map(base), Map(value), kind,
-                               MemoryRepresentation::FromMachineRepresentation(
-                                   store_rep.representation()),
-                               store_rep.write_barrier_kind(), offset);
+        assembler.Store(Map(base), Map(value), kind,
+                        MemoryRepresentation::FromMachineRepresentation(
+                            store_rep.representation()),
+                        store_rep.write_barrier_kind(), offset);
+        return OpIndex::Invalid();
       }
       if (index->opcode() == IrOpcode::kInt64Constant) {
         int64_t offset = OpParameter<int64_t>(index->op());
         if (base::IsValueInRangeForNumericType<int32_t>(offset)) {
-          return assembler.Store(
-              Map(base), Map(value), kind,
-              MemoryRepresentation::FromMachineRepresentation(
-                  store_rep.representation()),
-              store_rep.write_barrier_kind(), static_cast<int32_t>(offset));
+          assembler.Store(Map(base), Map(value), kind,
+                          MemoryRepresentation::FromMachineRepresentation(
+                              store_rep.representation()),
+                          store_rep.write_barrier_kind(),
+                          static_cast<int32_t>(offset));
+          return OpIndex::Invalid();
         }
       }
       int32_t offset = 0;
       uint8_t element_size_log2 = 0;
-      return assembler.IndexedStore(
-          Map(base), Map(index), Map(value), kind,
-          MemoryRepresentation::FromMachineRepresentation(
-              store_rep.representation()),
-          store_rep.write_barrier_kind(), offset, element_size_log2);
-    }
-    case IrOpcode::kProtectedStore: {
-      return assembler.ProtectedStore(
-          Map(node->InputAt(0)), Map(node->InputAt(1)), Map(node->InputAt(2)),
-          MemoryRepresentation::FromMachineRepresentation(
-              OpParameter<MachineRepresentation>(node->op())));
+      assembler.Store(Map(base), Map(index), Map(value), kind,
+                      MemoryRepresentation::FromMachineRepresentation(
+                          store_rep.representation()),
+                      store_rep.write_barrier_kind(), offset,
+                      element_size_log2);
+      return OpIndex::Invalid();
     }
+    case IrOpcode::kProtectedStore:
+      assembler.Store(Map(node->InputAt(0)), Map(node->InputAt(1)),
+                      Map(node->InputAt(2)), StoreOp::Kind::Protected(),
+                      MemoryRepresentation::FromMachineRepresentation(
+                          OpParameter<MachineRepresentation>(node->op())),
+                      WriteBarrierKind::kNoWriteBarrier);
+      return OpIndex::Invalid();
 
     case IrOpcode::kRetain:
-      return assembler.Retain(Map(node->InputAt(0)));
-
+      assembler.Retain(Map(node->InputAt(0)));
+      return OpIndex::Invalid();
     case IrOpcode::kStackPointerGreaterThan:
       return assembler.StackPointerGreaterThan(Map(node->InputAt(0)),
                                                StackCheckKindOf(op));
     case IrOpcode::kLoadStackCheckOffset:
-      return assembler.FrameConstant(FrameConstantOp::Kind::kStackCheckOffset);
+      return assembler.StackCheckOffset();
     case IrOpcode::kLoadFramePointer:
-      return assembler.FrameConstant(FrameConstantOp::Kind::kFramePointer);
+      return assembler.FramePointer();
     case IrOpcode::kLoadParentFramePointer:
-      return assembler.FrameConstant(
-          FrameConstantOp::Kind::kParentFramePointer);
+      return assembler.ParentFramePointer();
 
     case IrOpcode::kStackSlot:
       return assembler.StackSlot(StackSlotRepresentationOf(op).size(),
@@ -676,8 +694,9 @@
 
     case IrOpcode::kBranch:
       DCHECK_EQ(block->SuccessorCount(), 2);
-      return assembler.Branch(Map(node->InputAt(0)), Map(block->SuccessorAt(0)),
-                              Map(block->SuccessorAt(1)));
+      assembler.Branch(Map(node->InputAt(0)), Map(block->SuccessorAt(0)),
+                       Map(block->SuccessorAt(1)));
+      return OpIndex::Invalid();
 
     case IrOpcode::kSwitch: {
       BasicBlock* default_branch = block->successors().back();
@@ -689,9 +708,10 @@
         const IfValueParameters& p = IfValueParametersOf(branch->front()->op());
         cases.emplace_back(p.value(), Map(branch));
       }
-      return assembler.Switch(Map(node->InputAt(0)),
-                              graph_zone->CloneVector(base::VectorOf(cases)),
-                              Map(default_branch));
+      assembler.Switch(Map(node->InputAt(0)),
+                       graph_zone->CloneVector(base::VectorOf(cases)),
+                       Map(default_branch));
+      return OpIndex::Invalid();
     }
 
     case IrOpcode::kCall: {
@@ -705,13 +725,13 @@
            ++i) {
         arguments.emplace_back(Map(node->InputAt(i)));
       }
-      OpIndex call =
-          assembler.Call(callee, base::VectorOf(arguments), call_descriptor);
-      if (!call_descriptor->NeedsFrameState()) return call;
-      FrameState frame_state{
-          node->InputAt(static_cast<int>(call_descriptor->InputCount()))};
-      assembler.CheckLazyDeopt(call, Map(frame_state));
-      return call;
+      if (call_descriptor->NeedsFrameState()) {
+        FrameState frame_state{
+            node->InputAt(static_cast<int>(call_descriptor->InputCount()))};
+        return assembler.CallMaybeDeopt(callee, base::VectorOf(arguments),
+                                        call_descriptor, Map(frame_state));
+      }
+      return assembler.Call(callee, base::VectorOf(arguments), call_descriptor);
     }
 
     case IrOpcode::kTailCall: {
@@ -725,8 +745,8 @@
            ++i) {
         arguments.emplace_back(Map(node->InputAt(i)));
       }
-      return assembler.TailCall(callee, base::VectorOf(arguments),
-                                call_descriptor);
+      assembler.TailCall(callee, base::VectorOf(arguments), call_descriptor);
+      return OpIndex::Invalid();
     }
 
     case IrOpcode::kFrameState: {
@@ -740,24 +760,25 @@
     }
 
     case IrOpcode::kDeoptimizeIf:
-    case IrOpcode::kDeoptimizeUnless: {
-      OpIndex condition = Map(node->InputAt(0));
-      OpIndex frame_state = Map(node->InputAt(1));
-      bool negated = op->opcode() == IrOpcode::kDeoptimizeUnless;
-      return assembler.DeoptimizeIf(condition, frame_state, negated,
-                                    &DeoptimizeParametersOf(op));
-    }
+      assembler.DeoptimizeIf(Map(node->InputAt(0)), Map(node->InputAt(1)),
+                             &DeoptimizeParametersOf(op));
+      return OpIndex::Invalid();
+    case IrOpcode::kDeoptimizeUnless:
+      assembler.DeoptimizeIfNot(Map(node->InputAt(0)), Map(node->InputAt(1)),
+                                &DeoptimizeParametersOf(op));
+      return OpIndex::Invalid();
 
     case IrOpcode::kTrapIf:
-    case IrOpcode::kTrapUnless: {
-      OpIndex condition = Map(node->InputAt(0));
-      bool negated = op->opcode() == IrOpcode::kTrapUnless;
-      return assembler.TrapIf(condition, negated, TrapIdOf(op));
-    }
+      assembler.TrapIf(Map(node->InputAt(0)), TrapIdOf(op));
+      return OpIndex::Invalid();
+    case IrOpcode::kTrapUnless:
+      assembler.TrapIfNot(Map(node->InputAt(0)), TrapIdOf(op));
+      return OpIndex::Invalid();
 
     case IrOpcode::kDeoptimize: {
       OpIndex frame_state = Map(node->InputAt(0));
-      return assembler.Deoptimize(frame_state, &DeoptimizeParametersOf(op));
+      assembler.Deoptimize(frame_state, &DeoptimizeParametersOf(op));
+      return OpIndex::Invalid();
     }
 
     case IrOpcode::kReturn: {
@@ -766,7 +787,8 @@
       for (int i = 1; i < node->op()->ValueInputCount(); ++i) {
         return_values.push_back(Map(node->InputAt(i)));
       }
-      return assembler.Return(Map(pop_count), base::VectorOf(return_values));
+      assembler.Return(Map(pop_count), base::VectorOf(return_values));
+      return OpIndex::Invalid();
     }
 
     case IrOpcode::kUnreachable:
@@ -775,7 +797,8 @@
       }
       return OpIndex::Invalid();
     case IrOpcode::kThrow:
-      return assembler.Unreachable();
+      assembler.Unreachable();
+      return OpIndex::Invalid();
 
     case IrOpcode::kProjection: {
       Node* input = node->InputAt(0);
@@ -796,9 +819,10 @@
                                          Zone* phase_zone, Graph* graph,
                                          SourcePositionTable* source_positions,
                                          NodeOriginTable* origins) {
-  GraphBuilder builder{graph_zone,       phase_zone,
-                       *schedule,        Assembler(graph, phase_zone),
-                       source_positions, origins};
+  GraphBuilder builder{
+      graph_zone,       phase_zone,
+      *schedule,        Assembler<>(*graph, *graph, phase_zone),
+      source_positions, origins};
   return builder.Run();
 }
 
diff -r -u --color up/v8/src/compiler/turboshaft/graph-visualizer.cc nw/v8/src/compiler/turboshaft/graph-visualizer.cc
--- up/v8/src/compiler/turboshaft/graph-visualizer.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/turboshaft/graph-visualizer.cc	2023-01-19 16:46:36.135609592 -0500
@@ -4,6 +4,7 @@
 
 #include "src/compiler/graph-visualizer.h"
 
+#include "src/base/small-vector.h"
 #include "src/compiler/node-origin-table.h"
 #include "src/compiler/turboshaft/graph-visualizer.h"
 
@@ -37,7 +38,7 @@
       os_ << "{\"id\":" << index.id() << ",";
       os_ << "\"title\":\"" << OpcodeName(op.opcode) << "\",";
       os_ << "\"block_id\":" << block.index().id() << ",";
-      os_ << "\"op_properties_type\":\"" << op.properties() << "\"";
+      os_ << "\"op_properties_type\":\"" << op.Properties() << "\"";
       if (origins_) {
         NodeOrigin origin = origins_->GetNodeOrigin(index.id());
         if (origin.IsKnown()) {
@@ -58,7 +59,16 @@
   for (const Block& block : turboshaft_graph_.blocks()) {
     for (const Operation& op : turboshaft_graph_.operations(block)) {
       int target_id = turboshaft_graph_.Index(op).id();
-      for (OpIndex input : op.inputs()) {
+      base::SmallVector<OpIndex, 32> inputs{op.inputs()};
+      // Reorder the inputs to correspond to the order used in constructor and
+      // assembler functions.
+      if (auto* store = op.TryCast<StoreOp>()) {
+        if (store->index().valid()) {
+          DCHECK_EQ(store->input_count, 3);
+          inputs = {store->base(), store->index(), store->value()};
+        }
+      }
+      for (OpIndex input : inputs) {
         if (!first) os_ << ",\n";
         first = false;
         os_ << "{\"source\":" << input.id() << ",";
diff -r -u --color up/v8/src/compiler/turboshaft/graph.h nw/v8/src/compiler/turboshaft/graph.h
--- up/v8/src/compiler/turboshaft/graph.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/turboshaft/graph.h	2023-01-19 16:46:36.135609592 -0500
@@ -21,8 +21,8 @@
 
 namespace v8::internal::compiler::turboshaft {
 
+template <template <class> class... Reducers>
 class Assembler;
-class VarAssembler;
 
 // `OperationBuffer` is a growable, Zone-allocated buffer to store Turboshaft
 // operations. It is part of a `Graph`.
@@ -253,13 +253,14 @@
   friend class Block;
 #endif
 
-  int len_ = 0;
-  Derived* nxt_ = nullptr;
-  Derived* jmp_ = nullptr;
   // Myers' original datastructure requires to often check jmp_->len_, which is
   // not so great on modern computers (memory access, caches & co). To speed up
   // things a bit, we store here jmp_len_.
   int jmp_len_ = 0;
+
+  int len_ = 0;
+  Derived* nxt_ = nullptr;
+  Derived* jmp_ = nullptr;
 };
 
 // A basic block
@@ -383,7 +384,7 @@
     next_block_ = 0;
   }
 
-  const Operation& Get(OpIndex i) const {
+  V8_INLINE const Operation& Get(OpIndex i) const {
     // `Operation` contains const fields and can be overwritten with placement
     // new. Therefore, std::launder is necessary to avoid undefined behavior.
     const Operation* ptr =
@@ -392,7 +393,7 @@
     DCHECK_LT(OpcodeIndex(ptr->opcode), kNumberOfOpcodes);
     return *ptr;
   }
-  Operation& Get(OpIndex i) {
+  V8_INLINE Operation& Get(OpIndex i) {
     // `Operation` contains const fields and can be overwritten with placement
     // new. Therefore, std::launder is necessary to avoid undefined behavior.
     Operation* ptr =
@@ -412,10 +413,6 @@
     DCHECK_LT(i.id(), bound_blocks_.size());
     return *bound_blocks_[i.id()];
   }
-  Block* GetPtr(uint32_t index) {
-    DCHECK_LT(index, bound_blocks_.size());
-    return bound_blocks_[index];
-  }
 
   OpIndex Index(const Operation& op) const { return operations_.Index(op); }
 
@@ -429,8 +426,10 @@
   }
 
   template <class Op, class... Args>
-  V8_INLINE OpIndex Add(Args... args) {
+  V8_INLINE Op& Add(Args... args) {
+#ifdef DEBUG
     OpIndex result = next_operation_index();
+#endif  // DEBUG
     Op& op = Op::New(this, args...);
     IncrementInputUses(op);
     DCHECK_EQ(result, Index(op));
@@ -439,7 +438,7 @@
       DCHECK_LT(input, result);
     }
 #endif  // DEBUG
-    return result;
+    return op;
   }
 
   template <class Op, class... Args>
@@ -479,15 +478,17 @@
   V8_INLINE bool Add(Block* block) {
     DCHECK_EQ(block->graph_generation_, generation_);
     if (!bound_blocks_.empty() && !block->HasPredecessors()) return false;
-    bool deferred = true;
-    for (Block* pred = block->last_predecessor_; pred != nullptr;
-         pred = pred->neighboring_predecessor_) {
-      if (!pred->IsDeferred()) {
-        deferred = false;
-        break;
+    if (!block->IsDeferred()) {
+      bool deferred = true;
+      for (Block* pred = block->last_predecessor_; pred != nullptr;
+           pred = pred->neighboring_predecessor_) {
+        if (!pred->IsDeferred()) {
+          deferred = false;
+          break;
+        }
       }
+      block->SetDeferred(deferred);
     }
-    block->SetDeferred(deferred);
     DCHECK(!block->begin_.valid());
     block->begin_ = next_operation_index();
     DCHECK_EQ(block->index_, BlockIndex::Invalid());
Only in up/v8/src/compiler/turboshaft: machine-optimization-assembler.h
Only in nw/v8/src/compiler/turboshaft: machine-optimization-reducer.h
diff -r -u --color up/v8/src/compiler/turboshaft/operation-matching.h nw/v8/src/compiler/turboshaft/operation-matching.h
--- up/v8/src/compiler/turboshaft/operation-matching.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/turboshaft/operation-matching.h	2023-01-19 16:46:36.135609592 -0500
@@ -14,21 +14,21 @@
  public:
   template <class Op>
   bool Is(OpIndex op_idx) {
-    return assembler().graph().Get(op_idx).template Is<Op>();
+    return assembler().output_graph().Get(op_idx).template Is<Op>();
   }
 
   template <class Op>
   const Op* TryCast(OpIndex op_idx) {
-    return assembler().graph().Get(op_idx).template TryCast<Op>();
+    return assembler().output_graph().Get(op_idx).template TryCast<Op>();
   }
 
   template <class Op>
   const Op& Cast(OpIndex op_idx) {
-    return assembler().graph().Get(op_idx).template Cast<Op>();
+    return assembler().output_graph().Get(op_idx).template Cast<Op>();
   }
 
   const Operation& Get(OpIndex op_idx) {
-    return assembler().graph().Get(op_idx);
+    return assembler().output_graph().Get(op_idx);
   }
 
   bool MatchZero(OpIndex matched) {
diff -r -u --color up/v8/src/compiler/turboshaft/operations.cc nw/v8/src/compiler/turboshaft/operations.cc
--- up/v8/src/compiler/turboshaft/operations.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/turboshaft/operations.cc	2023-01-19 16:46:36.135609592 -0500
@@ -29,14 +29,8 @@
 
 std::ostream& operator<<(std::ostream& os, OperationPrintStyle styled_op) {
   const Operation& op = styled_op.op;
-  os << OpcodeName(op.opcode) << "(";
-  bool first = true;
-  for (OpIndex input : op.inputs()) {
-    if (!first) os << ", ";
-    first = false;
-    os << styled_op.op_index_prefix << input.id();
-  }
-  os << ")";
+  os << OpcodeName(op.opcode);
+  op.PrintInputs(os, styled_op.op_index_prefix);
   op.PrintOptions(os);
   return os;
 }
@@ -254,6 +248,15 @@
   }
 }
 
+std::ostream& operator<<(std::ostream& os, SelectOp::Implementation kind) {
+  switch (kind) {
+    case SelectOp::Implementation::kBranch:
+      return os << "Branch";
+    case SelectOp::Implementation::kCMove:
+      return os << "CMove";
+  }
+}
+
 std::ostream& operator<<(std::ostream& os, FrameConstantOp::Kind kind) {
   switch (kind) {
     case FrameConstantOp::Kind::kStackCheckOffset:
@@ -265,6 +268,18 @@
   }
 }
 
+void Operation::PrintInputs(std::ostream& os,
+                            const std::string& op_index_prefix) const {
+  switch (opcode) {
+#define SWITCH_CASE(Name)                              \
+  case Opcode::k##Name:                                \
+    Cast<Name##Op>().PrintInputs(os, op_index_prefix); \
+    break;
+    TURBOSHAFT_OPERATION_LIST(SWITCH_CASE)
+#undef SWITCH_CASE
+  }
+}
+
 void Operation::PrintOptions(std::ostream& os) const {
   switch (opcode) {
 #define SWITCH_CASE(Name)              \
@@ -322,25 +337,30 @@
   os << "]";
 }
 
-void LoadOp::PrintOptions(std::ostream& os) const {
-  os << "[";
-  os << (kind == Kind::kTaggedBase ? "tagged base" : "raw");
-  if (!IsAlignedAccess(kind)) os << ", unaligned";
-  os << ", " << loaded_rep;
-  if (offset != 0) os << ", offset: " << offset;
-  os << "]";
-}
-
 void ParameterOp::PrintOptions(std::ostream& os) const {
   os << "[" << parameter_index;
   if (debug_name) os << ", " << debug_name;
   os << "]";
 }
 
-void IndexedLoadOp::PrintOptions(std::ostream& os) const {
+void LoadOp::PrintInputs(std::ostream& os,
+                         const std::string& op_index_prefix) const {
+  os << " *(" << op_index_prefix << base().id();
+  if (offset < 0) {
+    os << " - " << -offset;
+  } else if (offset > 0) {
+    os << " + " << offset;
+  }
+  if (index().valid()) {
+    os << " + " << op_index_prefix << index().id();
+    if (element_size_log2 > 0) os << "*" << (1 << element_size_log2);
+  }
+  os << ") ";
+}
+void LoadOp::PrintOptions(std::ostream& os) const {
   os << "[";
-  os << (kind == Kind::kTaggedBase ? "tagged base" : "raw");
-  if (!IsAlignedAccess(kind)) os << ", unaligned";
+  os << (kind.tagged_base ? "tagged base" : "raw");
+  if (kind.maybe_unaligned) os << ", unaligned";
   os << ", " << loaded_rep;
   if (element_size_log2 != 0)
     os << ", element size: 2^" << int{element_size_log2};
@@ -348,20 +368,24 @@
   os << "]";
 }
 
-void StoreOp::PrintOptions(std::ostream& os) const {
-  os << "[";
-  os << (kind == Kind::kTaggedBase ? "tagged base" : "raw");
-  if (!IsAlignedAccess(kind)) os << ", unaligned";
-  os << ", " << stored_rep;
-  os << ", " << write_barrier;
-  if (offset != 0) os << ", offset: " << offset;
-  os << "]";
+void StoreOp::PrintInputs(std::ostream& os,
+                          const std::string& op_index_prefix) const {
+  os << " *(" << op_index_prefix << base().id();
+  if (offset < 0) {
+    os << " - " << -offset;
+  } else if (offset > 0) {
+    os << " + " << offset;
+  }
+  if (index().valid()) {
+    os << " + " << op_index_prefix << index().id();
+    if (element_size_log2 > 0) os << "*" << (1 << element_size_log2);
+  }
+  os << ") = " << op_index_prefix << value().id() << " ";
 }
-
-void IndexedStoreOp::PrintOptions(std::ostream& os) const {
+void StoreOp::PrintOptions(std::ostream& os) const {
   os << "[";
-  os << (kind == Kind::kTaggedBase ? "tagged base" : "raw");
-  if (!IsAlignedAccess(kind)) os << ", unaligned";
+  os << (kind.tagged_base ? "tagged base" : "raw");
+  if (kind.maybe_unaligned) os << ", unaligned";
   os << ", " << stored_rep;
   os << ", " << write_barrier;
   if (element_size_log2 != 0)
diff -r -u --color up/v8/src/compiler/turboshaft/operations.h nw/v8/src/compiler/turboshaft/operations.h
--- up/v8/src/compiler/turboshaft/operations.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/turboshaft/operations.h	2023-01-19 16:46:36.135609592 -0500
@@ -15,11 +15,13 @@
 
 #include "src/base/logging.h"
 #include "src/base/macros.h"
+#include "src/base/optional.h"
 #include "src/base/platform/mutex.h"
 #include "src/base/template-utils.h"
 #include "src/base/vector.h"
 #include "src/codegen/external-reference.h"
 #include "src/common/globals.h"
+#include "src/compiler/common-operator.h"
 #include "src/compiler/globals.h"
 #include "src/compiler/turboshaft/fast-hash.h"
 #include "src/compiler/turboshaft/representations.h"
@@ -59,6 +61,9 @@
 //   a variable arity operation where the constructor doesn't take the inputs as
 //   a single base::Vector<OpIndex> argument, it's also necessary to overwrite
 //   the static `New` function, see `CallOp` for an example.
+// - `OperatorProperties` as either a static constexpr member `properties` or a
+//   non-static method `Properties()` if the properties depend on the particular
+//   operation and not just the opcode.
 
 #define TURBOSHAFT_OPERATION_LIST(V) \
   V(WordBinop)                       \
@@ -77,11 +82,7 @@
   V(PendingLoopPhi)                  \
   V(Constant)                        \
   V(Load)                            \
-  V(IndexedLoad)                     \
-  V(ProtectedLoad)                   \
   V(Store)                           \
-  V(IndexedStore)                    \
-  V(ProtectedStore)                  \
   V(Retain)                          \
   V(Parameter)                       \
   V(OsrValue)                        \
@@ -333,9 +334,10 @@
     if (!Is<Op>()) return nullptr;
     return static_cast<Op*>(this);
   }
-  const OpProperties& properties() const;
+  OpProperties Properties() const;
 
   std::string ToString() const;
+  void PrintInputs(std::ostream& os, const std::string& op_index_prefix) const;
   void PrintOptions(std::ostream& os) const;
 
  protected:
@@ -364,6 +366,14 @@
 
 OperationStorageSlot* AllocateOpStorage(Graph* graph, size_t slot_count);
 
+// Determine if an operation declares `properties`, which means that its
+// properties are static and don't depend on inputs or options.
+template <class Op, class = void>
+struct HasStaticProperties : std::bool_constant<false> {};
+template <class Op>
+struct HasStaticProperties<Op, std::void_t<decltype(Op::properties)>>
+    : std::bool_constant<true> {};
+
 // This template knows the complete type of the operation and is plugged into
 // the inheritance hierarchy. It removes boilerplate from the concrete
 // `Operation` subclasses, defining everything that can be expressed
@@ -376,7 +386,14 @@
 
   static const Opcode opcode;
 
-  static constexpr OpProperties properties() { return Derived::properties; }
+  static constexpr OpProperties Properties() { return Derived::properties; }
+
+  static constexpr base::Optional<OpProperties> PropertiesIfStatic() {
+    if constexpr (HasStaticProperties<Derived>::value) {
+      return Derived::Properties();
+    }
+    return base::nullopt;
+  }
 
   Derived& derived_this() { return *static_cast<Derived*>(this); }
   const Derived& derived_this() const {
@@ -451,6 +468,17 @@
                              derived_this().options());
   }
 
+  void PrintInputs(std::ostream& os, const std::string& op_index_prefix) const {
+    os << "(";
+    bool first = true;
+    for (OpIndex input : inputs()) {
+      if (!first) os << ", ";
+      first = false;
+      os << op_index_prefix << input.id();
+    }
+    os << ")";
+  }
+
   void PrintOptions(std::ostream& os) const {
     const auto& options = derived_this().options();
     constexpr size_t options_count =
@@ -1080,23 +1108,33 @@
 };
 
 struct SelectOp : FixedArityOperationT<3, SelectOp> {
-  // TODO(12783): Support all register reps.
-  WordRepresentation rep;
+  enum class Implementation : uint8_t { kBranch, kCMove };
+
   static constexpr OpProperties properties = OpProperties::Pure();
+  RegisterRepresentation rep;
+  BranchHint hint;
+  Implementation implem;
 
-  OpIndex condition() const { return Base::input(0); }
-  OpIndex left() const { return Base::input(1); }
-  OpIndex right() const { return Base::input(2); }
-
-  SelectOp(OpIndex condition, OpIndex left, OpIndex right,
-           WordRepresentation rep)
-      : Base(condition, left, right), rep(rep) {
-    DCHECK(rep == WordRepresentation::Word32()
-               ? SupportedOperations::word32_select()
-               : SupportedOperations::word64_select());
+  SelectOp(OpIndex cond, OpIndex vtrue, OpIndex vfalse,
+           RegisterRepresentation rep, BranchHint hint, Implementation implem)
+      : Base(cond, vtrue, vfalse), rep(rep), hint(hint), implem(implem) {
+#ifdef DEBUG
+    if (implem == Implementation::kCMove) {
+      DCHECK((rep == RegisterRepresentation::Word32() &&
+              SupportedOperations::word32_select()) ||
+             (rep == RegisterRepresentation::Word64() &&
+              SupportedOperations::word64_select()));
+    }
+#endif
   }
-  auto options() const { return std::tuple{rep}; }
+
+  OpIndex cond() const { return input(0); }
+  OpIndex vtrue() const { return input(1); }
+  OpIndex vfalse() const { return input(2); }
+
+  auto options() const { return std::tuple{rep, hint, implem}; }
 };
+std::ostream& operator<<(std::ostream& os, SelectOp::Implementation kind);
 
 struct PhiOp : OperationT<PhiOp> {
   RegisterRepresentation rep;
@@ -1365,74 +1403,52 @@
   }
 };
 
-// Load loaded_rep from: base + offset.
-// For Kind::tagged_base: subtract kHeapObjectTag,
-//                        `base` has to be the object start.
-// For (u)int8/16, the value will be sign- or zero-extended to Word32.
-// When result_rep is RegisterRepresentation::Compressed(), then the load does
-// not decompress the value.
-struct LoadOp : FixedArityOperationT<1, LoadOp> {
-  enum class Kind : uint8_t { kTaggedBase, kRawAligned, kRawUnaligned };
-  Kind kind;
-  MemoryRepresentation loaded_rep;
-  RegisterRepresentation result_rep;
-  int32_t offset;
-
-  static constexpr OpProperties properties = OpProperties::Reading();
-
-  OpIndex base() const { return input(0); }
-
-  LoadOp(OpIndex base, Kind kind, MemoryRepresentation loaded_rep,
-         RegisterRepresentation result_rep, int32_t offset)
-      : Base(base),
-        kind(kind),
-        loaded_rep(loaded_rep),
-        result_rep(result_rep),
-        offset(offset) {
-    DCHECK(loaded_rep.ToRegisterRepresentation() == result_rep ||
-           (loaded_rep.IsTagged() &&
-            result_rep == RegisterRepresentation::Compressed()));
-  }
-  void PrintOptions(std::ostream& os) const;
-  auto options() const {
-    return std::tuple{kind, loaded_rep, result_rep, offset};
-  }
-};
-
-inline bool IsAlignedAccess(LoadOp::Kind kind) {
-  switch (kind) {
-    case LoadOp::Kind::kTaggedBase:
-    case LoadOp::Kind::kRawAligned:
-      return true;
-    case LoadOp::Kind::kRawUnaligned:
-      return false;
-  }
-}
-
 // Load `loaded_rep` from: base + offset + index * 2^element_size_log2.
 // For Kind::tagged_base: subtract kHeapObjectTag,
 //                        `base` has to be the object start.
 // For (u)int8/16, the value will be sign- or zero-extended to Word32.
 // When result_rep is RegisterRepresentation::Compressed(), then the load does
 // not decompress the value.
-struct IndexedLoadOp : FixedArityOperationT<2, IndexedLoadOp> {
-  using Kind = LoadOp::Kind;
+struct LoadOp : OperationT<LoadOp> {
+  struct Kind {
+    // The `base` input is a tagged pointer to a HeapObject.
+    bool tagged_base : 1;
+    // The effective address might be unaligned.
+    bool maybe_unaligned : 1;
+    // There is a Wasm trap handler for out-of-bounds accesses.
+    bool with_trap_handler : 1;
+
+    static constexpr Kind TaggedBase() { return Kind{true, false, false}; }
+    static constexpr Kind RawAligned() { return Kind{false, false, false}; }
+    static constexpr Kind RawUnaligned() { return Kind{false, true, false}; }
+    static constexpr Kind Protected() { return Kind{false, false, true}; }
+
+    bool operator==(const Kind& other) const {
+      return tagged_base == other.tagged_base &&
+             maybe_unaligned == other.maybe_unaligned &&
+             with_trap_handler == other.with_trap_handler;
+    }
+  };
   Kind kind;
   MemoryRepresentation loaded_rep;
   RegisterRepresentation result_rep;
   uint8_t element_size_log2;  // multiply index with 2^element_size_log2
   int32_t offset;             // add offset to scaled index
 
-  static constexpr OpProperties properties = OpProperties::Reading();
+  OpProperties Properties() const {
+    return kind.with_trap_handler ? OpProperties::ReadingAndCanAbort()
+                                  : OpProperties::Reading();
+  }
 
   OpIndex base() const { return input(0); }
-  OpIndex index() const { return input(1); }
+  OpIndex index() const {
+    return input_count == 2 ? input(1) : OpIndex::Invalid();
+  }
 
-  IndexedLoadOp(OpIndex base, OpIndex index, Kind kind,
-                MemoryRepresentation loaded_rep,
-                RegisterRepresentation result_rep, int32_t offset,
-                uint8_t element_size_log2)
-      : Base(base, index),
+  LoadOp(OpIndex base, OpIndex index, Kind kind,
+         MemoryRepresentation loaded_rep, RegisterRepresentation result_rep,
+         int32_t offset, uint8_t element_size_log2)
+      : Base(1 + index.valid()),
         kind(kind),
         loaded_rep(loaded_rep),
         result_rep(result_rep),
@@ -1441,91 +1457,75 @@
     DCHECK(loaded_rep.ToRegisterRepresentation() == result_rep ||
            (loaded_rep.IsTagged() &&
             result_rep == RegisterRepresentation::Compressed()));
+    DCHECK_IMPLIES(element_size_log2 > 0, index.valid());
+    input(0) = base;
+    if (index.valid()) input(1) = index;
+  }
+  static LoadOp& New(Graph* graph, OpIndex base, OpIndex index, Kind kind,
+                     MemoryRepresentation loaded_rep,
+                     RegisterRepresentation result_rep, int32_t offset,
+                     uint8_t element_size_log2) {
+    return Base::New(graph, 1 + index.valid(), base, index, kind, loaded_rep,
+                     result_rep, offset, element_size_log2);
   }
+  void PrintInputs(std::ostream& os, const std::string& op_index_prefix) const;
   void PrintOptions(std::ostream& os) const;
   auto options() const {
-    return std::tuple{kind, loaded_rep, offset, element_size_log2};
+    return std::tuple{kind, loaded_rep, result_rep, offset, element_size_log2};
   }
 };
 
-// A protected load registers a trap handler which handles out-of-bounds memory
-// accesses.
-struct ProtectedLoadOp : FixedArityOperationT<2, ProtectedLoadOp> {
-  MemoryRepresentation loaded_rep;
-  RegisterRepresentation result_rep;
-
-  static constexpr OpProperties properties = OpProperties::ReadingAndCanAbort();
-
-  OpIndex base() const { return input(0); }
-  OpIndex index() const { return input(1); }
-
-  ProtectedLoadOp(OpIndex base, OpIndex index, MemoryRepresentation loaded_rep,
-                  RegisterRepresentation result_rep)
-      : Base(base, index), loaded_rep(loaded_rep), result_rep(result_rep) {
-    DCHECK(loaded_rep.ToRegisterRepresentation() == result_rep ||
-           (loaded_rep.IsTagged() &&
-            result_rep == RegisterRepresentation::Compressed()));
-  }
-
-  auto options() const { return std::tuple{loaded_rep, result_rep}; }
-};
+V8_INLINE size_t hash_value(LoadOp::Kind kind) {
+  return base::hash_value(static_cast<int>(kind.tagged_base) |
+                          (kind.maybe_unaligned << 1) |
+                          (kind.with_trap_handler << 2));
+}
 
-// Store `value` to: base + offset.
+// Store `value` to: base + offset + index * 2^element_size_log2.
 // For Kind::tagged_base: subtract kHeapObjectTag,
 //                        `base` has to be the object start.
-struct StoreOp : FixedArityOperationT<2, StoreOp> {
+struct StoreOp : OperationT<StoreOp> {
   using Kind = LoadOp::Kind;
   Kind kind;
   MemoryRepresentation stored_rep;
   WriteBarrierKind write_barrier;
-  int32_t offset;
+  uint8_t element_size_log2;  // multiply index with 2^element_size_log2
+  int32_t offset;             // add offset to scaled index
 
-  static constexpr OpProperties properties = OpProperties::Writing();
+  OpProperties Properties() const {
+    return kind.with_trap_handler ? OpProperties::WritingAndCanAbort()
+                                  : OpProperties::Writing();
+  }
 
   OpIndex base() const { return input(0); }
   OpIndex value() const { return input(1); }
+  OpIndex index() const {
+    return input_count == 3 ? input(2) : OpIndex::Invalid();
+  }
 
-  StoreOp(OpIndex base, OpIndex value, Kind kind,
+  StoreOp(OpIndex base, OpIndex index, OpIndex value, Kind kind,
           MemoryRepresentation stored_rep, WriteBarrierKind write_barrier,
-          int32_t offset)
-      : Base(base, value),
+          int32_t offset, uint8_t element_size_log2)
+      : Base(2 + index.valid()),
         kind(kind),
         stored_rep(stored_rep),
         write_barrier(write_barrier),
-        offset(offset) {}
-  void PrintOptions(std::ostream& os) const;
-  auto options() const {
-    return std::tuple{kind, stored_rep, write_barrier, offset};
+        element_size_log2(element_size_log2),
+        offset(offset) {
+    DCHECK_IMPLIES(element_size_log2 > 0, index.valid());
+    input(0) = base;
+    input(1) = value;
+    if (index.valid()) input(2) = index;
+  }
+  static StoreOp& New(Graph* graph, OpIndex base, OpIndex index, OpIndex value,
+                      Kind kind, MemoryRepresentation stored_rep,
+                      WriteBarrierKind write_barrier, int32_t offset,
+                      uint8_t element_size_log2) {
+    return Base::New(graph, 2 + index.valid(), base, index, value, kind,
+                     stored_rep, write_barrier, offset, element_size_log2);
   }
-};
 
-// Store `value` to: base + offset + index * 2^element_size_log2.
-// For Kind::tagged_base: subtract kHeapObjectTag,
-//                        `base` has to be the object start.
-struct IndexedStoreOp : FixedArityOperationT<3, IndexedStoreOp> {
-  using Kind = StoreOp::Kind;
-  Kind kind;
-  MemoryRepresentation stored_rep;
-  WriteBarrierKind write_barrier;
-  uint8_t element_size_log2;  // multiply index with 2^element_size_log2
-  int32_t offset;             // add offset to scaled index
-
-  static constexpr OpProperties properties = OpProperties::Writing();
-
-  OpIndex base() const { return input(0); }
-  OpIndex index() const { return input(1); }
-  OpIndex value() const { return input(2); }
-
-  IndexedStoreOp(OpIndex base, OpIndex index, OpIndex value, Kind kind,
-                 MemoryRepresentation stored_rep,
-                 WriteBarrierKind write_barrier, int32_t offset,
-                 uint8_t element_size_log2)
-      : Base(base, index, value),
-        kind(kind),
-        stored_rep(stored_rep),
-        write_barrier(write_barrier),
-        element_size_log2(element_size_log2),
-        offset(offset) {}
+  void PrintInputs(std::ostream& os, const std::string& op_index_prefix) const;
   void PrintOptions(std::ostream& os) const;
   auto options() const {
     return std::tuple{kind, stored_rep, write_barrier, offset,
@@ -1533,23 +1533,6 @@
   }
 };
 
-// A protected store registers a trap handler which handles out-of-bounds memory
-// accesses.
-struct ProtectedStoreOp : FixedArityOperationT<3, ProtectedStoreOp> {
-  MemoryRepresentation stored_rep;
-
-  static constexpr OpProperties properties = OpProperties::WritingAndCanAbort();
-
-  OpIndex base() const { return input(0); }
-  OpIndex index() const { return input(1); }
-  OpIndex value() const { return input(2); }
-
-  ProtectedStoreOp(OpIndex base, OpIndex index, OpIndex value,
-                   MemoryRepresentation stored_rep)
-      : Base(base, index, value), stored_rep(stored_rep) {}
-  auto options() const { return std::tuple{stored_rep}; }
-};
-
 // Retain a HeapObject to prevent it from being garbage collected too early.
 struct RetainOp : FixedArityOperationT<1, RetainOp> {
   OpIndex retained() const { return input(0); }
@@ -1880,9 +1863,10 @@
   auto options() const { return std::tuple{index}; }
 };
 
-#define OPERATION_PROPERTIES_CASE(Name) Name##Op::properties,
-static constexpr OpProperties kOperationPropertiesTable[kNumberOfOpcodes] = {
-    TURBOSHAFT_OPERATION_LIST(OPERATION_PROPERTIES_CASE)};
+#define OPERATION_PROPERTIES_CASE(Name) Name##Op::PropertiesIfStatic(),
+static constexpr base::Optional<OpProperties>
+    kOperationPropertiesTable[kNumberOfOpcodes] = {
+        TURBOSHAFT_OPERATION_LIST(OPERATION_PROPERTIES_CASE)};
 #undef OPERATION_PROPERTIES_CASE
 
 template <class Op>
@@ -1923,8 +1907,18 @@
   return {ptr, input_count};
 }
 
-inline const OpProperties& Operation::properties() const {
-  return kOperationPropertiesTable[OpcodeIndex(opcode)];
+inline OpProperties Operation::Properties() const {
+  if (auto prop = kOperationPropertiesTable[OpcodeIndex(opcode)]) {
+    return *prop;
+  }
+  switch (opcode) {
+    case Opcode::kLoad:
+      return Cast<LoadOp>().Properties();
+    case Opcode::kStore:
+      return Cast<StoreOp>().Properties();
+    default:
+      UNREACHABLE();
+  }
 }
 
 // static
diff -r -u --color up/v8/src/compiler/turboshaft/optimization-phase.h nw/v8/src/compiler/turboshaft/optimization-phase.h
--- up/v8/src/compiler/turboshaft/optimization-phase.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/turboshaft/optimization-phase.h	2023-01-19 16:46:36.135609592 -0500
@@ -35,13 +35,15 @@
   bool OpIsUsed(OpIndex i) const {
     const Operation& op = graph.Get(i);
     return op.saturated_use_count > 0 ||
-           op.properties().is_required_when_unused;
+           op.Properties().is_required_when_unused;
   }
 
   explicit AnalyzerBase(const Graph& graph, Zone* phase_zone)
       : phase_zone(phase_zone), graph(graph) {}
 };
 
+// TODO(dmercadier, tebbi): transform this analyzer into a reducer, and plug in
+// into some reducer stacks.
 struct LivenessAnalyzer : AnalyzerBase {
   using Base = AnalyzerBase;
   // Using `uint8_t` instead of `bool` prevents `std::vector` from using a
@@ -74,7 +76,7 @@
       --it;
       OpIndex index = *it;
       const Operation& op = graph.Get(index);
-      if (op.properties().is_required_when_unused) {
+      if (op.Properties().is_required_when_unused) {
         op_used[index.id()] = true;
       } else if (!OpIsUsed(index)) {
         continue;
@@ -100,172 +102,168 @@
   }
 };
 
-enum class VisitOrder { kAsEmitted, kDominator };
-
-template <class Analyzer, class Assembler>
+template <template <class> class... Reducers>
 class OptimizationPhase {
- private:
-  struct Impl;
-
  public:
-  static void Run(Graph* input, Zone* phase_zone, NodeOriginTable* origins,
-                  VisitOrder visit_order = VisitOrder::kAsEmitted) {
-    Impl phase{*input, phase_zone, origins, visit_order};
+  static void Run(Graph* input, Zone* phase_zone, NodeOriginTable* origins) {
+    Assembler<Reducers...> phase(*input, input->GetOrCreateCompanion(),
+                                 phase_zone, origins);
     if (v8_flags.turboshaft_trace_reduction) {
-      phase.template Run<true>();
+      phase.template VisitGraph<true>();
     } else {
-      phase.template Run<false>();
+      phase.template VisitGraph<false>();
     }
   }
-  static void RunWithoutTracing(
-      Graph* input, Zone* phase_zone,
-      VisitOrder visit_order = VisitOrder::kAsEmitted) {
-    Impl phase{*input, phase_zone, visit_order};
-    phase.template Run<false>();
+  static void RunWithoutTracing(Graph* input, Zone* phase_zone) {
+    Assembler<Reducers...> phase(input, input->GetOrCreateCompanion(),
+                                 phase_zone);
+    phase->template VisitGraph<false>();
   }
 };
 
-template <class Analyzer, class Assembler>
-struct OptimizationPhase<Analyzer, Assembler>::Impl {
-  Graph& input_graph;
-  Zone* phase_zone;
-  compiler::NodeOriginTable* origins;
-  VisitOrder visit_order;
-
-  Analyzer analyzer{input_graph, phase_zone};
-  Assembler assembler{&input_graph.GetOrCreateCompanion(), phase_zone};
-  const Block* current_input_block = nullptr;
-  // Mappings from the old graph to the new graph.
-  std::vector<Block*> block_mapping{input_graph.block_count(), nullptr};
-  std::vector<OpIndex> op_mapping{input_graph.op_id_count(),
-                                  OpIndex::Invalid()};
+template <class Assembler>
+class GraphVisitor {
+ public:
+  GraphVisitor(Graph& input_graph, Graph& output_graph, Zone* phase_zone,
+               compiler::NodeOriginTable* origins = nullptr)
+      : input_graph_(input_graph),
+        output_graph_(output_graph),
+        phase_zone_(phase_zone),
+        origins_(origins),
+        current_input_block_(nullptr),
+        block_mapping_(input_graph.block_count(), nullptr, phase_zone),
+        op_mapping_(input_graph.op_id_count(), OpIndex::Invalid(), phase_zone) {
+    output_graph_.Reset();
+  }
 
   // `trace_reduction` is a template parameter to avoid paying for tracing at
   // runtime.
   template <bool trace_reduction>
-  void Run() {
-    analyzer.Run();
-
-    for (const Block& input_block : input_graph.blocks()) {
-      block_mapping[input_block.index().id()] =
-          assembler.NewBlock(input_block.kind());
+  void VisitGraph() {
+    // Creating initial old-to-new Block mapping.
+    for (const Block& input_block : input_graph().blocks()) {
+      block_mapping_[input_block.index().id()] =
+          assembler().NewBlock(input_block.kind());
     }
 
-    if (visit_order == VisitOrder::kDominator) {
-      RunDominatorOrder<trace_reduction>();
-    } else {
-      RunAsEmittedOrder<trace_reduction>();
-    }
+    // Visiting the graph.
+    VisitAllBlocks<trace_reduction>();
 
-    if (!input_graph.source_positions().empty()) {
-      for (OpIndex index : assembler.graph().AllOperationIndices()) {
-        OpIndex origin = assembler.graph().operation_origins()[index];
-        assembler.graph().source_positions()[index] =
-            input_graph.source_positions()[origin];
+    // Updating the source_positions.
+    if (!input_graph().source_positions().empty()) {
+      for (OpIndex index : output_graph_.AllOperationIndices()) {
+        OpIndex origin = output_graph_.operation_origins()[index];
+        output_graph_.source_positions()[index] =
+            input_graph().source_positions()[origin];
       }
     }
-    if (origins) {
-      for (OpIndex index : assembler.graph().AllOperationIndices()) {
-        OpIndex origin = assembler.graph().operation_origins()[index];
-        origins->SetNodeOrigin(index.id(), origin.id());
+    // Updating the operation origins.
+    if (origins_) {
+      for (OpIndex index : assembler().output_graph().AllOperationIndices()) {
+        OpIndex origin = assembler().output_graph().operation_origins()[index];
+        origins_->SetNodeOrigin(index.id(), origin.id());
       }
     }
 
-    input_graph.SwapWithCompanion();
+    input_graph_.SwapWithCompanion();
   }
 
-  template <bool trace_reduction>
-  void RunAsEmittedOrder() {
-    for (const Block& input_block : input_graph.blocks()) {
-      VisitBlock<trace_reduction>(input_block);
-    }
-  }
+  Zone* graph_zone() const { return input_graph().graph_zone(); }
+  const Graph& input_graph() const { return input_graph_; }
+  Graph& output_graph() const { return output_graph_; }
+  Zone* phase_zone() { return phase_zone_; }
 
+ private:
   template <bool trace_reduction>
-  void RunDominatorOrder() {
-    base::SmallVector<Block*, 128> dominator_visit_stack;
+  void VisitAllBlocks() {
+    base::SmallVector<const Block*, 128> visit_stack;
 
-    dominator_visit_stack.push_back(input_graph.GetPtr(0));
-    while (!dominator_visit_stack.empty()) {
-      Block* block = dominator_visit_stack.back();
-      dominator_visit_stack.pop_back();
-      VisitBlock<trace_reduction>(*block);
+    visit_stack.push_back(&input_graph().StartBlock());
+    while (!visit_stack.empty()) {
+      const Block* block = visit_stack.back();
+      visit_stack.pop_back();
+      VisitBlock<trace_reduction>(block);
 
       for (Block* child = block->LastChild(); child != nullptr;
            child = child->NeighboringChild()) {
-        dominator_visit_stack.push_back(child);
+        visit_stack.push_back(child);
       }
     }
   }
 
   template <bool trace_reduction>
-  void VisitBlock(const Block& input_block) {
-    assembler.EnterBlock(input_block);
-    current_input_block = &input_block;
+  void VisitBlock(const Block* input_block) {
+    current_input_block_ = input_block;
     if constexpr (trace_reduction) {
-      std::cout << PrintAsBlockHeader{input_block} << "\n";
+      std::cout << PrintAsBlockHeader{*input_block} << "\n";
     }
-    if (!assembler.Bind(MapToNewGraph(input_block.index()))) {
+    if (!assembler().Bind(MapToNewGraph(input_block->index()))) {
       if constexpr (trace_reduction) TraceBlockUnreachable();
       // If we eliminate a loop backedge, we need to turn the loop into a
       // single-predecessor merge block.
       const Operation& last_op =
-          *base::Reversed(input_graph.operations(input_block)).begin();
+          *base::Reversed(input_graph().operations(*input_block)).begin();
       if (auto* final_goto = last_op.TryCast<GotoOp>()) {
         if (final_goto->destination->IsLoop()) {
           Block* new_loop = MapToNewGraph(final_goto->destination->index());
           DCHECK(new_loop->IsLoop());
           if (new_loop->IsLoop() && new_loop->PredecessorCount() == 1) {
-            assembler.graph().TurnLoopIntoMerge(new_loop);
+            output_graph_.TurnLoopIntoMerge(new_loop);
           }
         }
       }
-      assembler.ExitBlock(input_block);
       return;
     }
-    assembler.current_block()->SetDeferred(input_block.IsDeferred());
-    for (OpIndex index : input_graph.OperationIndices(input_block)) {
-      if (!assembler.current_block()) break;
-      assembler.SetCurrentOrigin(index);
-      OpIndex first_output_index = assembler.graph().next_operation_index();
-      USE(first_output_index);
-      if constexpr (trace_reduction) TraceReductionStart(index);
-      if (!analyzer.OpIsUsed(index)) {
-        if constexpr (trace_reduction) TraceOperationUnused();
-        continue;
-      }
-      const Operation& op = input_graph.Get(index);
-      OpIndex new_index;
-      if (input_block.IsLoop() && op.Is<PhiOp>()) {
-        const PhiOp& phi = op.Cast<PhiOp>();
-        new_index = assembler.PendingLoopPhi(MapToNewGraph(phi.inputs()[0]),
+    assembler().current_block()->SetDeferred(input_block->IsDeferred());
+    for (OpIndex index : input_graph().OperationIndices(*input_block)) {
+      if (!VisitOp<trace_reduction>(index, input_block)) break;
+    }
+    if constexpr (trace_reduction) TraceBlockFinished();
+  }
+
+  template <bool trace_reduction>
+  bool VisitOp(OpIndex index, const Block* input_block) {
+    if (!assembler().current_block()) return false;
+    assembler().SetCurrentOrigin(index);
+    OpIndex first_output_index =
+        assembler().output_graph().next_operation_index();
+    USE(first_output_index);
+    const Operation& op = input_graph().Get(index);
+    if (op.saturated_use_count == 0 &&
+        !op.Properties().is_required_when_unused) {
+      if constexpr (trace_reduction) TraceOperationUnused();
+      return true;
+    }
+    if constexpr (trace_reduction) TraceReductionStart(index);
+    OpIndex new_index;
+    if (input_block->IsLoop() && op.Is<PhiOp>()) {
+      const PhiOp& phi = op.Cast<PhiOp>();
+      new_index = assembler().PendingLoopPhi(MapToNewGraph(phi.inputs()[0]),
                                              phi.rep, phi.inputs()[1]);
-        if constexpr (trace_reduction) {
-          TraceReductionResult(first_output_index, new_index);
-        }
-      } else {
-        switch (op.opcode) {
-#define EMIT_INSTR_CASE(Name)                            \
-  case Opcode::k##Name:                                  \
-    new_index = this->Reduce##Name(op.Cast<Name##Op>()); \
+      if constexpr (trace_reduction) {
+        TraceReductionResult(first_output_index, new_index);
+      }
+    } else {
+      switch (op.opcode) {
+#define EMIT_INSTR_CASE(Name)                           \
+  case Opcode::k##Name:                                 \
+    new_index = this->Visit##Name(op.Cast<Name##Op>()); \
     break;
-          TURBOSHAFT_OPERATION_LIST(EMIT_INSTR_CASE)
+        TURBOSHAFT_OPERATION_LIST(EMIT_INSTR_CASE)
 #undef EMIT_INSTR_CASE
-        }
-        if constexpr (trace_reduction) {
-          TraceReductionResult(first_output_index, new_index);
-        }
       }
-      op_mapping[index.id()] = new_index;
+      if constexpr (trace_reduction) {
+        TraceReductionResult(first_output_index, new_index);
+      }
     }
-    assembler.ExitBlock(input_block);
-    if constexpr (trace_reduction) TraceBlockFinished();
+    op_mapping_[index.id()] = new_index;
+    return true;
   }
 
   void TraceReductionStart(OpIndex index) {
     std::cout << "╭── o" << index.id() << ": "
               << PaddingSpace{5 - CountDecimalDigits(index.id())}
-              << OperationPrintStyle{input_graph.Get(index), "#o"} << "\n";
+              << OperationPrintStyle{input_graph().Get(index), "#o"} << "\n";
   }
   void TraceOperationUnused() { std::cout << "╰─> unused\n\n"; }
   void TraceBlockUnreachable() { std::cout << "╰─> unreachable\n\n"; }
@@ -275,9 +273,9 @@
       std::cout << "╰─> #n" << new_index.id() << "\n";
     }
     bool before_arrow = new_index >= first_output_index;
-    for (const Operation& op : assembler.graph().operations(
-             first_output_index, assembler.graph().next_operation_index())) {
-      OpIndex index = assembler.graph().Index(op);
+    for (const Operation& op : output_graph_.operations(
+             first_output_index, output_graph_.next_operation_index())) {
+      OpIndex index = output_graph_.Index(op);
       const char* prefix;
       if (index == new_index) {
         prefix = "╰─>";
@@ -289,8 +287,7 @@
       }
       std::cout << prefix << " n" << index.id() << ": "
                 << PaddingSpace{5 - CountDecimalDigits(index.id())}
-                << OperationPrintStyle{assembler.graph().Get(index), "#n"}
-                << "\n";
+                << OperationPrintStyle{output_graph_.Get(index), "#n"} << "\n";
     }
     std::cout << "\n";
   }
@@ -300,50 +297,51 @@
   // to emit a corresponding operation in the new graph, translating inputs and
   // blocks accordingly.
 
-  V8_INLINE OpIndex ReduceGoto(const GotoOp& op) {
+  V8_INLINE OpIndex VisitGoto(const GotoOp& op) {
     Block* destination = MapToNewGraph(op.destination->index());
-    assembler.current_block()->SetOrigin(current_input_block);
-    assembler.Goto(destination);
+    assembler().current_block()->SetOrigin(current_input_block_);
+    assembler().ReduceGoto(destination);
     if (destination->IsBound()) {
       DCHECK(destination->IsLoop());
       FixLoopPhis(destination);
     }
     return OpIndex::Invalid();
   }
-  V8_INLINE OpIndex ReduceBranch(const BranchOp& op) {
+  V8_INLINE OpIndex VisitBranch(const BranchOp& op) {
     Block* if_true = MapToNewGraph(op.if_true->index());
     Block* if_false = MapToNewGraph(op.if_false->index());
-    return assembler.Branch(MapToNewGraph(op.condition()), if_true, if_false);
+    return assembler().ReduceBranch(MapToNewGraph(op.condition()), if_true,
+                                    if_false);
   }
-  OpIndex ReduceCatchException(const CatchExceptionOp& op) {
+  OpIndex VisitCatchException(const CatchExceptionOp& op) {
     Block* if_success = MapToNewGraph(op.if_success->index());
     Block* if_exception = MapToNewGraph(op.if_exception->index());
-    return assembler.CatchException(MapToNewGraph(op.call()), if_success,
-                                    if_exception);
+    return assembler().ReduceCatchException(MapToNewGraph(op.call()),
+                                            if_success, if_exception);
   }
-  OpIndex ReduceSwitch(const SwitchOp& op) {
+  OpIndex VisitSwitch(const SwitchOp& op) {
     base::SmallVector<SwitchOp::Case, 16> cases;
     for (SwitchOp::Case c : op.cases) {
       cases.emplace_back(c.value, MapToNewGraph(c.destination->index()));
     }
-    return assembler.Switch(
+    return assembler().ReduceSwitch(
         MapToNewGraph(op.input()),
-        assembler.graph_zone()->CloneVector(base::VectorOf(cases)),
+        graph_zone()->CloneVector(base::VectorOf(cases)),
         MapToNewGraph(op.default_case->index()));
   }
-  OpIndex ReducePhi(const PhiOp& op) {
+  OpIndex VisitPhi(const PhiOp& op) {
     base::Vector<const OpIndex> old_inputs = op.inputs();
     base::SmallVector<OpIndex, 8> new_inputs;
-    Block* old_pred = current_input_block->LastPredecessor();
-    Block* new_pred = assembler.current_block()->LastPredecessor();
+    Block* old_pred = current_input_block_->LastPredecessor();
+    Block* new_pred = assembler().current_block()->LastPredecessor();
     // Control predecessors might be missing after the optimization phase. So we
     // need to skip phi inputs that belong to control predecessors that have no
     // equivalent in the new graph.
 
-    // When iterating the graph in kAsEmitted order (ie, going through all of
-    // the blocks in linear order), we assume that the order of control
-    // predecessors did not change. In kDominator order, the order of control
-    // predecessor might or might not change.
+    // We first assume that the order if the predecessors of the current block
+    // did not change. If it did, {new_pred} won't be nullptr at the end of this
+    // loop, and we'll instead fall back to the slower code below to compute the
+    // inputs of the Phi.
     for (OpIndex input : base::Reversed(old_inputs)) {
       if (new_pred && new_pred->Origin() == old_pred) {
         new_inputs.push_back(MapToNewGraph(input));
@@ -354,10 +352,9 @@
     DCHECK_IMPLIES(new_pred == nullptr, old_pred == nullptr);
 
     if (new_pred != nullptr) {
-      DCHECK_EQ(visit_order, VisitOrder::kDominator);
       // If {new_pred} is nullptr, then the order of the predecessors changed.
-      // This should only happen when {visit_order} is kDominator. For instance,
-      // consider this (partial) dominator tree:
+      // This should only happen with blocks that were introduced in the
+      // previous graph. For instance, consider this (partial) dominator tree:
       //
       //     ╠ 7
       //     ║ ╠ 8
@@ -366,7 +363,7 @@
       //     ╚ 11
       //
       // Where the predecessors of block 11 are blocks 9 and 10 (in that order).
-      // In kDominator visit order, block 10 will be visited before block 9.
+      // In dominator visit order, block 10 will be visited before block 9.
       // Since blocks are added to predecessors when the predecessors are
       // visited, it means that in the new graph, the predecessors of block 11
       // are [10, 9] rather than [9, 10].
@@ -374,7 +371,7 @@
       // inputs from blocks that vanished.
 
       base::SmallVector<uint32_t, 16> old_pred_vec;
-      for (old_pred = current_input_block->LastPredecessor();
+      for (old_pred = current_input_block_->LastPredecessor();
            old_pred != nullptr; old_pred = old_pred->NeighboringPredecessor()) {
         old_pred_vec.push_back(old_pred->index().id());
         // Checking that predecessors are indeed sorted.
@@ -388,9 +385,10 @@
       // predecessor, we check the index of the input corresponding to the old
       // predecessor, and we put it next in {new_inputs}.
       new_inputs.clear();
-      for (new_pred = assembler.current_block()->LastPredecessor();
+      for (new_pred = assembler().current_block()->LastPredecessor();
            new_pred != nullptr; new_pred = new_pred->NeighboringPredecessor()) {
         const Block* origin = new_pred->Origin();
+        DCHECK_NOT_NULL(origin);
         // {old_pred_vec} is sorted. We can thus use a binary search to find the
         // index of {origin} in {old_pred_vec}: the index is the index of the
         // old input corresponding to {new_pred}.
@@ -402,160 +400,168 @@
       }
     }
 
+    DCHECK_EQ(new_inputs.size(),
+              assembler().current_block()->PredecessorCount());
+
+    if (new_inputs.size() == 1) {
+      // This Operation used to be a Phi in a Merge, but since one (or more) of
+      // the inputs of the merge have been removed, there is no need for a Phi
+      // anymore.
+      return new_inputs[0];
+    }
+
     std::reverse(new_inputs.begin(), new_inputs.end());
-    return assembler.Phi(base::VectorOf(new_inputs), op.rep);
+    return assembler().ReducePhi(base::VectorOf(new_inputs), op.rep);
   }
-  OpIndex ReducePendingLoopPhi(const PendingLoopPhiOp& op) { UNREACHABLE(); }
-  V8_INLINE OpIndex ReduceFrameState(const FrameStateOp& op) {
+  OpIndex VisitPendingLoopPhi(const PendingLoopPhiOp& op) { UNREACHABLE(); }
+  V8_INLINE OpIndex VisitFrameState(const FrameStateOp& op) {
     auto inputs = MapToNewGraph<32>(op.inputs());
-    return assembler.FrameState(base::VectorOf(inputs), op.inlined, op.data);
+    return assembler().ReduceFrameState(base::VectorOf(inputs), op.inlined,
+                                        op.data);
   }
-  OpIndex ReduceCall(const CallOp& op) {
+  OpIndex VisitCall(const CallOp& op) {
     OpIndex callee = MapToNewGraph(op.callee());
     auto arguments = MapToNewGraph<16>(op.arguments());
-    return assembler.Call(callee, base::VectorOf(arguments), op.descriptor);
+    return assembler().ReduceCall(callee, base::VectorOf(arguments),
+                                  op.descriptor);
   }
-  OpIndex ReduceTailCall(const TailCallOp& op) {
+  OpIndex VisitTailCall(const TailCallOp& op) {
     OpIndex callee = MapToNewGraph(op.callee());
     auto arguments = MapToNewGraph<16>(op.arguments());
-    return assembler.TailCall(callee, base::VectorOf(arguments), op.descriptor);
+    return assembler().ReduceTailCall(callee, base::VectorOf(arguments),
+                                      op.descriptor);
   }
-  OpIndex ReduceReturn(const ReturnOp& op) {
+  OpIndex VisitReturn(const ReturnOp& op) {
     // We very rarely have tuples longer than 4.
     auto return_values = MapToNewGraph<4>(op.return_values());
-    return assembler.Return(MapToNewGraph(op.pop_count()),
-                            base::VectorOf(return_values));
+    return assembler().ReduceReturn(MapToNewGraph(op.pop_count()),
+                                    base::VectorOf(return_values));
   }
-  OpIndex ReduceOverflowCheckedBinop(const OverflowCheckedBinopOp& op) {
-    return assembler.OverflowCheckedBinop(
+  OpIndex VisitOverflowCheckedBinop(const OverflowCheckedBinopOp& op) {
+    return assembler().ReduceOverflowCheckedBinop(
         MapToNewGraph(op.left()), MapToNewGraph(op.right()), op.kind, op.rep);
   }
-  OpIndex ReduceWordUnary(const WordUnaryOp& op) {
-    return assembler.WordUnary(MapToNewGraph(op.input()), op.kind, op.rep);
-  }
-  OpIndex ReduceFloatUnary(const FloatUnaryOp& op) {
-    return assembler.FloatUnary(MapToNewGraph(op.input()), op.kind, op.rep);
-  }
-  OpIndex ReduceShift(const ShiftOp& op) {
-    return assembler.Shift(MapToNewGraph(op.left()), MapToNewGraph(op.right()),
-                           op.kind, op.rep);
-  }
-  OpIndex ReduceEqual(const EqualOp& op) {
-    return assembler.Equal(MapToNewGraph(op.left()), MapToNewGraph(op.right()),
-                           op.rep);
-  }
-  OpIndex ReduceComparison(const ComparisonOp& op) {
-    return assembler.Comparison(MapToNewGraph(op.left()),
-                                MapToNewGraph(op.right()), op.kind, op.rep);
-  }
-  OpIndex ReduceChange(const ChangeOp& op) {
-    return assembler.Change(MapToNewGraph(op.input()), op.kind, op.assumption,
-                            op.from, op.to);
-  }
-  OpIndex ReduceTryChange(const TryChangeOp& op) {
-    return assembler.TryChange(MapToNewGraph(op.input()), op.kind, op.from,
-                               op.to);
-  }
-
-  OpIndex ReduceFloat64InsertWord32(const Float64InsertWord32Op& op) {
-    return assembler.Float64InsertWord32(MapToNewGraph(op.float64()),
-                                         MapToNewGraph(op.word32()), op.kind);
-  }
-  OpIndex ReduceTaggedBitcast(const TaggedBitcastOp& op) {
-    return assembler.TaggedBitcast(MapToNewGraph(op.input()), op.from, op.to);
-  }
-  OpIndex ReduceSelect(const SelectOp& op) {
-    return assembler.Select(MapToNewGraph(op.condition()),
-                            MapToNewGraph(op.left()), MapToNewGraph(op.right()),
-                            op.rep);
-  }
-  OpIndex ReduceConstant(const ConstantOp& op) {
-    return assembler.Constant(op.kind, op.storage);
-  }
-  OpIndex ReduceLoad(const LoadOp& op) {
-    return assembler.Load(MapToNewGraph(op.base()), op.kind, op.loaded_rep,
-                          op.result_rep, op.offset);
-  }
-  OpIndex ReduceIndexedLoad(const IndexedLoadOp& op) {
-    return assembler.IndexedLoad(
-        MapToNewGraph(op.base()), MapToNewGraph(op.index()), op.kind,
-        op.loaded_rep, op.result_rep, op.offset, op.element_size_log2);
+  OpIndex VisitWordUnary(const WordUnaryOp& op) {
+    return assembler().ReduceWordUnary(MapToNewGraph(op.input()), op.kind,
+                                       op.rep);
+  }
+  OpIndex VisitFloatUnary(const FloatUnaryOp& op) {
+    return assembler().ReduceFloatUnary(MapToNewGraph(op.input()), op.kind,
+                                        op.rep);
+  }
+  OpIndex VisitShift(const ShiftOp& op) {
+    return assembler().ReduceShift(MapToNewGraph(op.left()),
+                                   MapToNewGraph(op.right()), op.kind, op.rep);
+  }
+  OpIndex VisitEqual(const EqualOp& op) {
+    return assembler().ReduceEqual(MapToNewGraph(op.left()),
+                                   MapToNewGraph(op.right()), op.rep);
   }
-  OpIndex ReduceProtectedLoad(const ProtectedLoadOp& op) {
-    return assembler.ProtectedLoad(MapToNewGraph(op.base()),
-                                   MapToNewGraph(op.index()), op.loaded_rep,
-                                   op.result_rep);
-  }
-  OpIndex ReduceStore(const StoreOp& op) {
-    return assembler.Store(MapToNewGraph(op.base()), MapToNewGraph(op.value()),
-                           op.kind, op.stored_rep, op.write_barrier, op.offset);
+  OpIndex VisitComparison(const ComparisonOp& op) {
+    return assembler().ReduceComparison(
+        MapToNewGraph(op.left()), MapToNewGraph(op.right()), op.kind, op.rep);
   }
-  OpIndex ReduceIndexedStore(const IndexedStoreOp& op) {
-    return assembler.IndexedStore(
-        MapToNewGraph(op.base()), MapToNewGraph(op.index()),
+  OpIndex VisitChange(const ChangeOp& op) {
+    return assembler().ReduceChange(MapToNewGraph(op.input()), op.kind,
+                                    op.assumption, op.from, op.to);
+  }
+  OpIndex VisitTryChange(const TryChangeOp& op) {
+    return assembler().ReduceTryChange(MapToNewGraph(op.input()), op.kind,
+                                       op.from, op.to);
+  }
+
+  OpIndex VisitFloat64InsertWord32(const Float64InsertWord32Op& op) {
+    return assembler().ReduceFloat64InsertWord32(
+        MapToNewGraph(op.float64()), MapToNewGraph(op.word32()), op.kind);
+  }
+  OpIndex VisitTaggedBitcast(const TaggedBitcastOp& op) {
+    return assembler().ReduceTaggedBitcast(MapToNewGraph(op.input()), op.from,
+                                           op.to);
+  }
+  OpIndex VisitSelect(const SelectOp& op) {
+    return assembler().ReduceSelect(
+        MapToNewGraph(op.cond()), MapToNewGraph(op.vtrue()),
+        MapToNewGraph(op.vfalse()), op.rep, op.hint, op.implem);
+  }
+  OpIndex VisitConstant(const ConstantOp& op) {
+    return assembler().ReduceConstant(op.kind, op.storage);
+  }
+  OpIndex VisitLoad(const LoadOp& op) {
+    return assembler().ReduceLoad(
+        MapToNewGraph(op.base()),
+        op.index().valid() ? MapToNewGraph(op.index()) : OpIndex::Invalid(),
+        op.kind, op.loaded_rep, op.result_rep, op.offset, op.element_size_log2);
+  }
+  OpIndex VisitStore(const StoreOp& op) {
+    return assembler().ReduceStore(
+        MapToNewGraph(op.base()),
+        op.index().valid() ? MapToNewGraph(op.index()) : OpIndex::Invalid(),
         MapToNewGraph(op.value()), op.kind, op.stored_rep, op.write_barrier,
         op.offset, op.element_size_log2);
   }
-  OpIndex ReduceProtectedStore(const ProtectedStoreOp& op) {
-    return assembler.ProtectedStore(MapToNewGraph(op.base()),
-                                    MapToNewGraph(op.index()),
-                                    MapToNewGraph(op.value()), op.stored_rep);
+  OpIndex VisitRetain(const RetainOp& op) {
+    return assembler().ReduceRetain(MapToNewGraph(op.retained()));
   }
-  OpIndex ReduceRetain(const RetainOp& op) {
-    return assembler.Retain(MapToNewGraph(op.retained()));
+  OpIndex VisitParameter(const ParameterOp& op) {
+    return assembler().ReduceParameter(op.parameter_index, op.debug_name);
   }
-  OpIndex ReduceParameter(const ParameterOp& op) {
-    return assembler.Parameter(op.parameter_index, op.debug_name);
+  OpIndex VisitOsrValue(const OsrValueOp& op) {
+    return assembler().ReduceOsrValue(op.index);
   }
-  OpIndex ReduceOsrValue(const OsrValueOp& op) {
-    return assembler.OsrValue(op.index);
+  OpIndex VisitStackPointerGreaterThan(const StackPointerGreaterThanOp& op) {
+    return assembler().ReduceStackPointerGreaterThan(
+        MapToNewGraph(op.stack_limit()), op.kind);
   }
-  OpIndex ReduceStackPointerGreaterThan(const StackPointerGreaterThanOp& op) {
-    return assembler.StackPointerGreaterThan(MapToNewGraph(op.stack_limit()),
-                                             op.kind);
+  OpIndex VisitStackSlot(const StackSlotOp& op) {
+    return assembler().ReduceStackSlot(op.size, op.alignment);
   }
-  OpIndex ReduceStackSlot(const StackSlotOp& op) {
-    return assembler.StackSlot(op.size, op.alignment);
+  OpIndex VisitFrameConstant(const FrameConstantOp& op) {
+    return assembler().ReduceFrameConstant(op.kind);
   }
-  OpIndex ReduceFrameConstant(const FrameConstantOp& op) {
-    return assembler.FrameConstant(op.kind);
+  OpIndex VisitCheckLazyDeopt(const CheckLazyDeoptOp& op) {
+    return assembler().ReduceCheckLazyDeopt(MapToNewGraph(op.call()),
+                                            MapToNewGraph(op.frame_state()));
   }
-  OpIndex ReduceCheckLazyDeopt(const CheckLazyDeoptOp& op) {
-    return assembler.CheckLazyDeopt(MapToNewGraph(op.call()),
-                                    MapToNewGraph(op.frame_state()));
+  OpIndex VisitDeoptimize(const DeoptimizeOp& op) {
+    return assembler().ReduceDeoptimize(MapToNewGraph(op.frame_state()),
+                                        op.parameters);
   }
-  OpIndex ReduceDeoptimize(const DeoptimizeOp& op) {
-    return assembler.Deoptimize(MapToNewGraph(op.frame_state()), op.parameters);
+  OpIndex VisitDeoptimizeIf(const DeoptimizeIfOp& op) {
+    return assembler().ReduceDeoptimizeIf(MapToNewGraph(op.condition()),
+                                          MapToNewGraph(op.frame_state()),
+                                          op.negated, op.parameters);
   }
-  OpIndex ReduceDeoptimizeIf(const DeoptimizeIfOp& op) {
-    return assembler.DeoptimizeIf(MapToNewGraph(op.condition()),
-                                  MapToNewGraph(op.frame_state()), op.negated,
-                                  op.parameters);
+  OpIndex VisitTrapIf(const TrapIfOp& op) {
+    return assembler().ReduceTrapIf(MapToNewGraph(op.condition()), op.negated,
+                                    op.trap_id);
   }
-  OpIndex ReduceTrapIf(const TrapIfOp& op) {
-    return assembler.TrapIf(MapToNewGraph(op.condition()), op.negated,
-                            op.trap_id);
+  OpIndex VisitTuple(const TupleOp& op) {
+    return assembler().ReduceTuple(
+        base::VectorOf(MapToNewGraph<4>(op.inputs())));
   }
-  OpIndex ReduceTuple(const TupleOp& op) {
-    return assembler.Tuple(base::VectorOf(MapToNewGraph<4>(op.inputs())));
+  OpIndex VisitProjection(const ProjectionOp& op) {
+    return assembler().ReduceProjection(MapToNewGraph(op.input()), op.index);
   }
-  OpIndex ReduceProjection(const ProjectionOp& op) {
-    return assembler.Projection(MapToNewGraph(op.input()), op.index);
+  OpIndex VisitWordBinop(const WordBinopOp& op) {
+    return assembler().ReduceWordBinop(
+        MapToNewGraph(op.left()), MapToNewGraph(op.right()), op.kind, op.rep);
   }
-  OpIndex ReduceWordBinop(const WordBinopOp& op) {
-    return assembler.WordBinop(MapToNewGraph(op.left()),
-                               MapToNewGraph(op.right()), op.kind, op.rep);
+  OpIndex VisitFloatBinop(const FloatBinopOp& op) {
+    return assembler().ReduceFloatBinop(
+        MapToNewGraph(op.left()), MapToNewGraph(op.right()), op.kind, op.rep);
   }
-  OpIndex ReduceFloatBinop(const FloatBinopOp& op) {
-    return assembler.FloatBinop(MapToNewGraph(op.left()),
-                                MapToNewGraph(op.right()), op.kind, op.rep);
+  OpIndex VisitUnreachable(const UnreachableOp& op) {
+    return assembler().ReduceUnreachable();
   }
-  OpIndex ReduceUnreachable(const UnreachableOp& op) {
-    return assembler.Unreachable();
+
+  Block* MapToNewGraph(BlockIndex old_index) const {
+    Block* result = block_mapping_[old_index.id()];
+    DCHECK_NOT_NULL(result);
+    return result;
   }
 
   OpIndex MapToNewGraph(OpIndex old_index) {
-    OpIndex result = op_mapping[old_index.id()];
+    OpIndex result = op_mapping_[old_index.id()];
     DCHECK(result.valid());
     return result;
   }
@@ -570,24 +576,33 @@
     return result;
   }
 
-  Block* MapToNewGraph(BlockIndex old_index) {
-    Block* result = block_mapping[old_index.id()];
-    DCHECK_NOT_NULL(result);
-    return result;
-  }
-
   void FixLoopPhis(Block* loop) {
     DCHECK(loop->IsLoop());
-    for (Operation& op : assembler.graph().operations(*loop)) {
+    for (Operation& op : assembler().output_graph().operations(*loop)) {
       if (auto* pending_phi = op.TryCast<PendingLoopPhiOp>()) {
-        assembler.graph().template Replace<PhiOp>(
-            assembler.graph().Index(*pending_phi),
+        assembler().output_graph().template Replace<PhiOp>(
+            assembler().output_graph().Index(*pending_phi),
             base::VectorOf({pending_phi->first(),
                             MapToNewGraph(pending_phi->old_backedge_index)}),
             pending_phi->rep);
       }
     }
   }
+
+  // TODO(dmercadier,tebbi): unify the ways we refer to the Assembler.
+  // Currently, we have Asm(), assembler(), and to a certain extent, stack().
+  Assembler& assembler() { return static_cast<Assembler&>(*this); }
+
+  Graph& input_graph_;
+  Graph& output_graph_;
+  Zone* phase_zone_;
+  compiler::NodeOriginTable* origins_;
+
+  const Block* current_input_block_;
+
+  // Mappings from the old graph to the new graph.
+  ZoneVector<Block*> block_mapping_;
+  ZoneVector<OpIndex> op_mapping_;
 };
 
 }  // namespace v8::internal::compiler::turboshaft
diff -r -u --color up/v8/src/compiler/turboshaft/recreate-schedule.cc nw/v8/src/compiler/turboshaft/recreate-schedule.cc
--- up/v8/src/compiler/turboshaft/recreate-schedule.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/turboshaft/recreate-schedule.cc	2023-01-19 16:46:36.135609592 -0500
@@ -353,7 +353,8 @@
           o = machine.Int64SubWithOverflow();
           break;
         case OverflowCheckedBinopOp::Kind::kSignedMul:
-          UNREACHABLE();
+          o = machine.Int64MulWithOverflow();
+          break;
       }
       break;
     default:
@@ -835,13 +836,21 @@
   return AddNode(o, {GetNode(op.input())});
 }
 Node* ScheduleBuilder::ProcessOperation(const SelectOp& op) {
-  const Operator* o = op.rep == WordRepresentation::Word32()
+  // If there is a Select, then it should only be one that is supported by the
+  // machine, and it should be meant to be implementation with cmove.
+  DCHECK_EQ(op.implem, SelectOp::Implementation::kCMove);
+  DCHECK((op.rep == RegisterRepresentation::Word32() &&
+          SupportedOperations::word32_select()) ||
+         (op.rep == RegisterRepresentation::Word64() &&
+          SupportedOperations::word64_select()));
+
+  const Operator* o = op.rep == RegisterRepresentation::Word32()
                           ? machine.Word32Select().op()
                           : machine.Word64Select().op();
+
   return AddNode(
-      o, {GetNode(op.condition()), GetNode(op.left()), GetNode(op.right())});
+      o, {GetNode(op.cond()), GetNode(op.vtrue()), GetNode(op.vfalse())});
 }
-
 Node* ScheduleBuilder::ProcessOperation(const PendingLoopPhiOp& op) {
   UNREACHABLE();
 }
@@ -879,34 +888,27 @@
                                        RelocInfo::WASM_STUB_CALL);
   }
 }
+
 Node* ScheduleBuilder::ProcessOperation(const LoadOp& op) {
   intptr_t offset = op.offset;
-  if (op.kind == LoadOp::Kind::kTaggedBase) {
-    CHECK_GE(offset, std::numeric_limits<int32_t>::min() + kHeapObjectTag);
-    offset -= kHeapObjectTag;
-  }
-  Node* base = GetNode(op.base());
-  return AddNode(op.kind == LoadOp::Kind::kRawAligned
-                     ? machine.Load(op.loaded_rep.ToMachineType())
-                 : op.kind == LoadOp::Kind::kRawUnaligned
-                     ? machine.UnalignedLoad(op.loaded_rep.ToMachineType())
-                     : machine.ProtectedLoad(op.loaded_rep.ToMachineType()),
-                 {base, IntPtrConstant(offset)});
-}
-Node* ScheduleBuilder::ProcessOperation(const IndexedLoadOp& op) {
-  intptr_t offset = op.offset;
-  if (op.kind == LoadOp::Kind::kTaggedBase) {
+  if (op.kind.tagged_base) {
     CHECK_GE(offset, std::numeric_limits<int32_t>::min() + kHeapObjectTag);
     offset -= kHeapObjectTag;
   }
   Node* base = GetNode(op.base());
-  Node* index = GetNode(op.index());
-  if (op.element_size_log2 != 0) {
-    index = IntPtrShl(index, IntPtrConstant(op.element_size_log2));
-  }
-  if (offset != 0) {
-    index = IntPtrAdd(index, IntPtrConstant(offset));
+  Node* index;
+  if (op.index().valid()) {
+    index = GetNode(op.index());
+    if (op.element_size_log2 != 0) {
+      index = IntPtrShl(index, IntPtrConstant(op.element_size_log2));
+    }
+    if (offset != 0) {
+      index = IntPtrAdd(index, IntPtrConstant(offset));
+    }
+  } else {
+    index = IntPtrConstant(offset);
   }
+
   MachineType loaded_rep = op.loaded_rep.ToMachineType();
   if (op.result_rep == RegisterRepresentation::Compressed()) {
     if (loaded_rep == MachineType::AnyTagged()) {
@@ -915,64 +917,56 @@
       loaded_rep = MachineType::CompressedPointer();
     }
   }
-  return AddNode(op.kind == LoadOp::Kind::kRawAligned ? machine.Load(loaded_rep)
-                 : op.kind == LoadOp::Kind::kRawUnaligned
-                     ? machine.UnalignedLoad(loaded_rep)
-                     : machine.ProtectedLoad(loaded_rep),
-                 {base, index});
-}
-Node* ScheduleBuilder::ProcessOperation(const ProtectedLoadOp& op) {
-  return AddNode(machine.ProtectedLoad(op.loaded_rep.ToMachineType()),
-                 {GetNode(op.base()), GetNode(op.index())});
-}
-Node* ScheduleBuilder::ProcessOperation(const StoreOp& op) {
-  intptr_t offset = op.offset;
-  if (op.kind == StoreOp::Kind::kTaggedBase) {
-    CHECK(offset >= std::numeric_limits<int32_t>::min() + kHeapObjectTag);
-    offset -= kHeapObjectTag;
-  }
-  Node* base = GetNode(op.base());
-  Node* value = GetNode(op.value());
   const Operator* o;
-  if (IsAlignedAccess(op.kind)) {
-    o = machine.Store(StoreRepresentation(
-        op.stored_rep.ToMachineType().representation(), op.write_barrier));
+  if (op.kind.maybe_unaligned) {
+    DCHECK(!op.kind.with_trap_handler);
+    o = machine.UnalignedLoad(loaded_rep);
+  } else if (op.kind.with_trap_handler) {
+    DCHECK(!op.kind.maybe_unaligned);
+    o = machine.ProtectedLoad(loaded_rep);
   } else {
-    DCHECK_EQ(op.write_barrier, WriteBarrierKind::kNoWriteBarrier);
-    o = machine.UnalignedStore(op.stored_rep.ToMachineType().representation());
+    o = machine.Load(loaded_rep);
   }
-  return AddNode(o, {base, IntPtrConstant(offset), value});
+  return AddNode(o, {base, index});
 }
-Node* ScheduleBuilder::ProcessOperation(const IndexedStoreOp& op) {
+
+Node* ScheduleBuilder::ProcessOperation(const StoreOp& op) {
   intptr_t offset = op.offset;
-  if (op.kind == IndexedStoreOp::Kind::kTaggedBase) {
+  if (op.kind.tagged_base) {
     CHECK(offset >= std::numeric_limits<int32_t>::min() + kHeapObjectTag);
     offset -= kHeapObjectTag;
   }
   Node* base = GetNode(op.base());
-  Node* index = GetNode(op.index());
-  Node* value = GetNode(op.value());
-  if (op.element_size_log2 != 0) {
-    index = IntPtrShl(index, IntPtrConstant(op.element_size_log2));
-  }
-  if (offset != 0) {
-    index = IntPtrAdd(index, IntPtrConstant(offset));
+  Node* index;
+  if (op.index().valid()) {
+    index = GetNode(op.index());
+    if (op.element_size_log2 != 0) {
+      index = IntPtrShl(index, IntPtrConstant(op.element_size_log2));
+    }
+    if (offset != 0) {
+      index = IntPtrAdd(index, IntPtrConstant(offset));
+    }
+  } else {
+    index = IntPtrConstant(offset);
   }
+  Node* value = GetNode(op.value());
+
   const Operator* o;
-  if (IsAlignedAccess(op.kind)) {
-    o = machine.Store(StoreRepresentation(
-        op.stored_rep.ToMachineType().representation(), op.write_barrier));
-  } else {
+  if (op.kind.maybe_unaligned) {
+    DCHECK(!op.kind.with_trap_handler);
     DCHECK_EQ(op.write_barrier, WriteBarrierKind::kNoWriteBarrier);
     o = machine.UnalignedStore(op.stored_rep.ToMachineType().representation());
+  } else if (op.kind.with_trap_handler) {
+    DCHECK(!op.kind.maybe_unaligned);
+    DCHECK_EQ(op.write_barrier, WriteBarrierKind::kNoWriteBarrier);
+    o = machine.ProtectedStore(op.stored_rep.ToMachineType().representation());
+  } else {
+    o = machine.Store(StoreRepresentation(
+        op.stored_rep.ToMachineType().representation(), op.write_barrier));
   }
   return AddNode(o, {base, index, value});
 }
-Node* ScheduleBuilder::ProcessOperation(const ProtectedStoreOp& op) {
-  return AddNode(
-      machine.ProtectedStore(op.stored_rep.ToMachineType().representation()),
-      {GetNode(op.base()), GetNode(op.index()), GetNode(op.value())});
-}
+
 Node* ScheduleBuilder::ProcessOperation(const RetainOp& op) {
   return AddNode(common.Retain(), {GetNode(op.retained())});
 }
Only in nw/v8/src/compiler/turboshaft: select-lowering-reducer.h
diff -r -u --color up/v8/src/compiler/turboshaft/sidetable.h nw/v8/src/compiler/turboshaft/sidetable.h
--- up/v8/src/compiler/turboshaft/sidetable.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/turboshaft/sidetable.h	2023-01-19 16:46:36.135609592 -0500
@@ -55,7 +55,7 @@
 
   // Returns `true` if the table never contained any values, even before
   // `Reset()`.
-  bool empty() { return table_.empty(); }
+  bool empty() const { return table_.empty(); }
 
  private:
   mutable ZoneVector<T> table_;
Only in nw/v8/src/compiler/turboshaft: snapshot-table.h
diff -r -u --color up/v8/src/compiler/turboshaft/utils.cc nw/v8/src/compiler/turboshaft/utils.cc
--- up/v8/src/compiler/turboshaft/utils.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/turboshaft/utils.cc	2023-01-19 16:46:36.135609592 -0500
@@ -13,10 +13,10 @@
 bool ShouldSkipOptimizationStep() {
   static std::atomic<uint64_t> counter{0};
   uint64_t current = counter++;
-  if (current == FLAG_turboshaft_opt_bisect_break) {
+  if (current == v8_flags.turboshaft_opt_bisect_break) {
     base::OS::DebugBreak();
   }
-  if (current >= FLAG_turboshaft_opt_bisect_limit) {
+  if (current >= v8_flags.turboshaft_opt_bisect_limit) {
     return true;
   }
   return false;
Only in up/v8/src/compiler/turboshaft: value-numbering-assembler.h
Only in nw/v8/src/compiler/turboshaft: value-numbering-reducer.h
diff -r -u --color up/v8/src/compiler/typer.cc nw/v8/src/compiler/typer.cc
--- up/v8/src/compiler/typer.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/typer.cc	2023-01-19 16:46:36.146442922 -0500
@@ -88,6 +88,7 @@
       SIMPLIFIED_BIGINT_BINOP_LIST(DECLARE_BINARY_CASE)
       SIMPLIFIED_SPECULATIVE_NUMBER_BINOP_LIST(DECLARE_BINARY_CASE)
       SIMPLIFIED_SPECULATIVE_BIGINT_BINOP_LIST(DECLARE_BINARY_CASE)
+      TYPER_SUPPORTED_MACHINE_BINOP_LIST(DECLARE_BINARY_CASE)
 #undef DECLARE_BINARY_CASE
 #define DECLARE_OTHER_CASE(x, ...) \
   case IrOpcode::k##x:             \
@@ -125,7 +126,139 @@
       SIMPLIFIED_CHECKED_OP_LIST(DECLARE_IMPOSSIBLE_CASE)
       IF_WASM(SIMPLIFIED_WASM_OP_LIST, DECLARE_IMPOSSIBLE_CASE)
       MACHINE_SIMD_OP_LIST(DECLARE_IMPOSSIBLE_CASE)
-      MACHINE_OP_LIST(DECLARE_IMPOSSIBLE_CASE)
+      MACHINE_UNOP_32_LIST(DECLARE_IMPOSSIBLE_CASE)
+      DECLARE_IMPOSSIBLE_CASE(Word32Xor)
+      DECLARE_IMPOSSIBLE_CASE(Word32Sar)
+      DECLARE_IMPOSSIBLE_CASE(Word32Rol)
+      DECLARE_IMPOSSIBLE_CASE(Word32Ror)
+      DECLARE_IMPOSSIBLE_CASE(Int32AddWithOverflow)
+      DECLARE_IMPOSSIBLE_CASE(Int32SubWithOverflow)
+      DECLARE_IMPOSSIBLE_CASE(Int32Mul)
+      DECLARE_IMPOSSIBLE_CASE(Int32MulWithOverflow)
+      DECLARE_IMPOSSIBLE_CASE(Int32MulHigh)
+      DECLARE_IMPOSSIBLE_CASE(Int32Div)
+      DECLARE_IMPOSSIBLE_CASE(Int32Mod)
+      DECLARE_IMPOSSIBLE_CASE(Uint32Mod)
+      DECLARE_IMPOSSIBLE_CASE(Uint32MulHigh)
+      DECLARE_IMPOSSIBLE_CASE(Word64Or)
+      DECLARE_IMPOSSIBLE_CASE(Word64Xor)
+      DECLARE_IMPOSSIBLE_CASE(Word64Sar)
+      DECLARE_IMPOSSIBLE_CASE(Word64Rol)
+      DECLARE_IMPOSSIBLE_CASE(Word64Ror)
+      DECLARE_IMPOSSIBLE_CASE(Word64RolLowerable)
+      DECLARE_IMPOSSIBLE_CASE(Word64RorLowerable)
+      DECLARE_IMPOSSIBLE_CASE(Int64AddWithOverflow)
+      DECLARE_IMPOSSIBLE_CASE(Int64SubWithOverflow)
+      DECLARE_IMPOSSIBLE_CASE(Int64Mul)
+      DECLARE_IMPOSSIBLE_CASE(Int64MulHigh)
+      DECLARE_IMPOSSIBLE_CASE(Int64MulWithOverflow)
+      DECLARE_IMPOSSIBLE_CASE(Int64Div)
+      DECLARE_IMPOSSIBLE_CASE(Int64Mod)
+      DECLARE_IMPOSSIBLE_CASE(Uint64Mod)
+      DECLARE_IMPOSSIBLE_CASE(Uint64MulHigh)
+      DECLARE_IMPOSSIBLE_CASE(Word64Equal)
+      DECLARE_IMPOSSIBLE_CASE(Int32LessThan)
+      DECLARE_IMPOSSIBLE_CASE(Int32LessThanOrEqual)
+      DECLARE_IMPOSSIBLE_CASE(Int64LessThan)
+      DECLARE_IMPOSSIBLE_CASE(Int64LessThanOrEqual)
+      DECLARE_IMPOSSIBLE_CASE(Uint64LessThan)
+      DECLARE_IMPOSSIBLE_CASE(Float32Equal)
+      DECLARE_IMPOSSIBLE_CASE(Float32LessThan)
+      DECLARE_IMPOSSIBLE_CASE(Float32LessThanOrEqual)
+      DECLARE_IMPOSSIBLE_CASE(Float64Equal)
+      DECLARE_IMPOSSIBLE_CASE(Float64LessThan)
+      DECLARE_IMPOSSIBLE_CASE(Float64LessThanOrEqual)
+      MACHINE_FLOAT32_BINOP_LIST(DECLARE_IMPOSSIBLE_CASE)
+      MACHINE_FLOAT32_UNOP_LIST(DECLARE_IMPOSSIBLE_CASE)
+      MACHINE_FLOAT64_BINOP_LIST(DECLARE_IMPOSSIBLE_CASE)
+      MACHINE_FLOAT64_UNOP_LIST(DECLARE_IMPOSSIBLE_CASE)
+      MACHINE_ATOMIC_OP_LIST(DECLARE_IMPOSSIBLE_CASE)
+      DECLARE_IMPOSSIBLE_CASE(AbortCSADcheck)
+      DECLARE_IMPOSSIBLE_CASE(DebugBreak)
+      DECLARE_IMPOSSIBLE_CASE(Comment)
+      DECLARE_IMPOSSIBLE_CASE(LoadImmutable)
+      DECLARE_IMPOSSIBLE_CASE(Store)
+      DECLARE_IMPOSSIBLE_CASE(StackSlot)
+      DECLARE_IMPOSSIBLE_CASE(Word32Popcnt)
+      DECLARE_IMPOSSIBLE_CASE(Word64Popcnt)
+      DECLARE_IMPOSSIBLE_CASE(Word64Clz)
+      DECLARE_IMPOSSIBLE_CASE(Word64Ctz)
+      DECLARE_IMPOSSIBLE_CASE(Word64ClzLowerable)
+      DECLARE_IMPOSSIBLE_CASE(Word64CtzLowerable)
+      DECLARE_IMPOSSIBLE_CASE(Word64ReverseBits)
+      DECLARE_IMPOSSIBLE_CASE(Word64ReverseBytes)
+      DECLARE_IMPOSSIBLE_CASE(Simd128ReverseBytes)
+      DECLARE_IMPOSSIBLE_CASE(Int64AbsWithOverflow)
+      DECLARE_IMPOSSIBLE_CASE(BitcastTaggedToWord)
+      DECLARE_IMPOSSIBLE_CASE(BitcastTaggedToWordForTagAndSmiBits)
+      DECLARE_IMPOSSIBLE_CASE(BitcastWordToTagged)
+      DECLARE_IMPOSSIBLE_CASE(BitcastWordToTaggedSigned)
+      DECLARE_IMPOSSIBLE_CASE(TruncateFloat64ToWord32)
+      DECLARE_IMPOSSIBLE_CASE(ChangeFloat32ToFloat64)
+      DECLARE_IMPOSSIBLE_CASE(ChangeFloat64ToInt32)
+      DECLARE_IMPOSSIBLE_CASE(ChangeFloat64ToInt64)
+      DECLARE_IMPOSSIBLE_CASE(ChangeFloat64ToUint32)
+      DECLARE_IMPOSSIBLE_CASE(ChangeFloat64ToUint64)
+      DECLARE_IMPOSSIBLE_CASE(Float64SilenceNaN)
+      DECLARE_IMPOSSIBLE_CASE(TruncateFloat64ToInt64)
+      DECLARE_IMPOSSIBLE_CASE(TruncateFloat64ToUint32)
+      DECLARE_IMPOSSIBLE_CASE(TruncateFloat32ToInt32)
+      DECLARE_IMPOSSIBLE_CASE(TruncateFloat32ToUint32)
+      DECLARE_IMPOSSIBLE_CASE(TryTruncateFloat32ToInt64)
+      DECLARE_IMPOSSIBLE_CASE(TryTruncateFloat64ToInt64)
+      DECLARE_IMPOSSIBLE_CASE(TryTruncateFloat32ToUint64)
+      DECLARE_IMPOSSIBLE_CASE(TryTruncateFloat64ToUint64)
+      DECLARE_IMPOSSIBLE_CASE(TryTruncateFloat64ToInt32)
+      DECLARE_IMPOSSIBLE_CASE(TryTruncateFloat64ToUint32)
+      DECLARE_IMPOSSIBLE_CASE(ChangeInt32ToFloat64)
+      DECLARE_IMPOSSIBLE_CASE(BitcastWord32ToWord64)
+      DECLARE_IMPOSSIBLE_CASE(ChangeInt32ToInt64)
+      DECLARE_IMPOSSIBLE_CASE(ChangeInt64ToFloat64)
+      DECLARE_IMPOSSIBLE_CASE(ChangeUint32ToFloat64)
+      DECLARE_IMPOSSIBLE_CASE(ChangeUint32ToUint64)
+      DECLARE_IMPOSSIBLE_CASE(TruncateFloat64ToFloat32)
+      DECLARE_IMPOSSIBLE_CASE(TruncateInt64ToInt32)
+      DECLARE_IMPOSSIBLE_CASE(RoundFloat64ToInt32)
+      DECLARE_IMPOSSIBLE_CASE(RoundInt32ToFloat32)
+      DECLARE_IMPOSSIBLE_CASE(RoundInt64ToFloat32)
+      DECLARE_IMPOSSIBLE_CASE(RoundInt64ToFloat64)
+      DECLARE_IMPOSSIBLE_CASE(RoundUint32ToFloat32)
+      DECLARE_IMPOSSIBLE_CASE(RoundUint64ToFloat32)
+      DECLARE_IMPOSSIBLE_CASE(RoundUint64ToFloat64)
+      DECLARE_IMPOSSIBLE_CASE(BitcastFloat32ToInt32)
+      DECLARE_IMPOSSIBLE_CASE(BitcastFloat64ToInt64)
+      DECLARE_IMPOSSIBLE_CASE(BitcastInt32ToFloat32)
+      DECLARE_IMPOSSIBLE_CASE(BitcastInt64ToFloat64)
+      DECLARE_IMPOSSIBLE_CASE(Float64ExtractLowWord32)
+      DECLARE_IMPOSSIBLE_CASE(Float64ExtractHighWord32)
+      DECLARE_IMPOSSIBLE_CASE(Float64InsertLowWord32)
+      DECLARE_IMPOSSIBLE_CASE(Float64InsertHighWord32)
+      DECLARE_IMPOSSIBLE_CASE(Word32Select)
+      DECLARE_IMPOSSIBLE_CASE(Word64Select)
+      DECLARE_IMPOSSIBLE_CASE(Float32Select)
+      DECLARE_IMPOSSIBLE_CASE(Float64Select)
+      DECLARE_IMPOSSIBLE_CASE(LoadStackCheckOffset)
+      DECLARE_IMPOSSIBLE_CASE(LoadFramePointer)
+      DECLARE_IMPOSSIBLE_CASE(LoadParentFramePointer)
+      DECLARE_IMPOSSIBLE_CASE(UnalignedLoad)
+      DECLARE_IMPOSSIBLE_CASE(UnalignedStore)
+      DECLARE_IMPOSSIBLE_CASE(Int32PairAdd)
+      DECLARE_IMPOSSIBLE_CASE(Int32PairSub)
+      DECLARE_IMPOSSIBLE_CASE(Int32PairMul)
+      DECLARE_IMPOSSIBLE_CASE(Word32PairShl)
+      DECLARE_IMPOSSIBLE_CASE(Word32PairShr)
+      DECLARE_IMPOSSIBLE_CASE(Word32PairSar)
+      DECLARE_IMPOSSIBLE_CASE(ProtectedLoad)
+      DECLARE_IMPOSSIBLE_CASE(ProtectedStore)
+      DECLARE_IMPOSSIBLE_CASE(MemoryBarrier)
+      DECLARE_IMPOSSIBLE_CASE(SignExtendWord8ToInt32)
+      DECLARE_IMPOSSIBLE_CASE(SignExtendWord16ToInt32)
+      DECLARE_IMPOSSIBLE_CASE(SignExtendWord8ToInt64)
+      DECLARE_IMPOSSIBLE_CASE(SignExtendWord16ToInt64)
+      DECLARE_IMPOSSIBLE_CASE(SignExtendWord32ToInt64)
+      DECLARE_IMPOSSIBLE_CASE(StackPointerGreaterThan)
+      DECLARE_IMPOSSIBLE_CASE(TraceInstruction)
+
 #undef DECLARE_IMPOSSIBLE_CASE
       UNREACHABLE();
     }
@@ -228,6 +361,7 @@
   SIMPLIFIED_BIGINT_BINOP_LIST(DECLARE_METHOD)
   SIMPLIFIED_SPECULATIVE_NUMBER_BINOP_LIST(DECLARE_METHOD)
   SIMPLIFIED_SPECULATIVE_BIGINT_BINOP_LIST(DECLARE_METHOD)
+  TYPER_SUPPORTED_MACHINE_BINOP_LIST(DECLARE_METHOD)
 #undef DECLARE_METHOD
 #define DECLARE_METHOD(Name, ...)                  \
   inline Type Type##Name(Type left, Type right) {  \
@@ -243,6 +377,7 @@
   SIMPLIFIED_BIGINT_BINOP_LIST(DECLARE_METHOD)
   SIMPLIFIED_SPECULATIVE_NUMBER_BINOP_LIST(DECLARE_METHOD)
   SIMPLIFIED_SPECULATIVE_BIGINT_BINOP_LIST(DECLARE_METHOD)
+  TYPER_SUPPORTED_MACHINE_BINOP_LIST(DECLARE_METHOD)
 #undef DECLARE_METHOD
 #define DECLARE_METHOD(Name, ...) \
   inline Type Type##Name(Type input) { return TypeUnaryOp(input, Name); }
@@ -728,9 +863,9 @@
 
 Type Typer::Visitor::TypeRetain(Node* node) { UNREACHABLE(); }
 
-Type Typer::Visitor::TypeInt32Constant(Node* node) { UNREACHABLE(); }
+Type Typer::Visitor::TypeInt32Constant(Node* node) { return Type::Machine(); }
 
-Type Typer::Visitor::TypeInt64Constant(Node* node) { UNREACHABLE(); }
+Type Typer::Visitor::TypeInt64Constant(Node* node) { return Type::Machine(); }
 
 Type Typer::Visitor::TypeTaggedIndexConstant(Node* node) { UNREACHABLE(); }
 
@@ -774,6 +909,14 @@
   return type;
 }
 
+Type Typer::Visitor::TypeEnterMachineGraph(Node* node) {
+  return Type::Machine();
+}
+
+Type Typer::Visitor::TypeExitMachineGraph(Node* node) {
+  return ExitMachineGraphParametersOf(node->op()).output_type();
+}
+
 Type Typer::Visitor::TypeInductionVariablePhi(Node* node) {
   int arity = NodeProperties::GetControlInput(node)->op()->ControlInputCount();
   DCHECK_EQ(IrOpcode::kLoop, NodeProperties::GetControlInput(node)->opcode());
@@ -1483,7 +1626,7 @@
 }
 
 Type Typer::Visitor::TypeJSFindNonDefaultConstructorOrConstruct(Node* node) {
-  return Type::Tuple(Type::Boolean(), Type::Object(), zone());
+  return Type::Tuple(Type::Boolean(), Type::ReceiverOrNull(), zone());
 }
 
 // JS context operators.
@@ -1540,6 +1683,10 @@
 
 Type Typer::Visitor::TypeDateNow(Node* node) { return Type::Number(); }
 
+Type Typer::Visitor::TypeDoubleArrayMin(Node* node) { return Type::Number(); }
+
+Type Typer::Visitor::TypeDoubleArrayMax(Node* node) { return Type::Number(); }
+
 Type Typer::Visitor::TypeUnsigned32Divide(Node* node) {
   Type lhs = Operand(node, 0);
   return Type::Range(0, lhs.Max(), zone());
diff -r -u --color up/v8/src/compiler/types.h nw/v8/src/compiler/types.h
--- up/v8/src/compiler/types.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/types.h	2023-01-19 16:46:36.146442922 -0500
@@ -49,6 +49,7 @@
 //
 //   Constant(x) < T  iff instance_type(map(x)) < T
 //
+//    None <= Machine <= Any
 //
 // RANGE TYPES
 //
@@ -140,7 +141,8 @@
 // We split the macro list into two parts because the Torque equivalent in
 // turbofan-types.tq uses two 32bit bitfield structs.
 #define PROPER_ATOMIC_BITSET_TYPE_HIGH_LIST(V) \
-  V(SandboxedPointer,         uint64_t{1} << 32)
+  V(SandboxedPointer,         uint64_t{1} << 32) \
+  V(Machine,                  uint64_t{1} << 33)
 
 #define PROPER_BITSET_TYPE_LIST(V) \
   V(None,                     uint64_t{0}) \
@@ -208,6 +210,7 @@
   V(Object,                       kDetectableObject | kOtherUndetectable) \
   V(Receiver,                     kObject | kProxy | kWasmObject) \
   V(ReceiverOrUndefined,          kReceiver | kUndefined) \
+  V(ReceiverOrNull,               kReceiver | kNull) \
   V(ReceiverOrNullOrUndefined,    kReceiver | kNull | kUndefined) \
   V(SymbolOrReceiver,             kSymbol | kReceiver) \
   V(StringOrReceiver,             kString | kReceiver) \
diff -r -u --color up/v8/src/compiler/verifier.cc nw/v8/src/compiler/verifier.cc
--- up/v8/src/compiler/verifier.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/verifier.cc	2023-01-19 16:46:36.146442922 -0500
@@ -312,8 +312,16 @@
       }
       CHECK_EQ(1, count_true);
       CHECK_EQ(1, count_false);
-      // The condition must be a Boolean.
-      CheckValueInputIs(node, 0, Type::Boolean());
+      switch (BranchParametersOf(node->op()).semantics()) {
+        case BranchSemantics::kJS:
+        case BranchSemantics::kUnspecified:
+          // The condition must be a Boolean.
+          CheckValueInputIs(node, 0, Type::Boolean());
+          break;
+        case BranchSemantics::kMachine:
+          CheckValueInputIs(node, 0, Type::Machine());
+          break;
+      }
       CheckNotTyped(node);
       break;
     }
@@ -415,8 +423,16 @@
       CheckTypeIs(node, Type::Any());
       break;
     }
-    case IrOpcode::kInt32Constant:  // TODO(turbofan): rename Word32Constant?
-    case IrOpcode::kInt64Constant:  // TODO(turbofan): rename Word64Constant?
+    case IrOpcode::kInt32Constant:    // TODO(turbofan): rename Word32Constant?
+    case IrOpcode::kInt64Constant: {  // TODO(turbofan): rename Word64Constant?
+      // Constants have no inputs.
+      CHECK_EQ(0, input_count);
+      // Wasm numeric constants have types. However, since wasm only gets
+      // verified in untyped mode, we do not need to check that the types match.
+      // TODO(manoskouk): Verify the type if wasm runs in typed mode.
+      if (code_type != kWasm) CheckTypeIs(node, Type::Machine());
+      break;
+    }
     case IrOpcode::kFloat32Constant:
     case IrOpcode::kFloat64Constant: {
       // Constants have no inputs.
@@ -606,6 +622,12 @@
     case IrOpcode::kTailCall:
       // TODO(bmeurer): what are the constraints on these?
       break;
+    case IrOpcode::kEnterMachineGraph:
+      CheckTypeIs(node, Type::Machine());
+      break;
+    case IrOpcode::kExitMachineGraph:
+      CheckValueInputIs(node, 0, Type::Machine());
+      break;
 
     // JavaScript operators
     // --------------------
@@ -972,6 +994,7 @@
     case IrOpcode::kSpeculativeBigIntSubtract:
     case IrOpcode::kSpeculativeBigIntMultiply:
     case IrOpcode::kSpeculativeBigIntDivide:
+    case IrOpcode::kSpeculativeBigIntModulus:
     case IrOpcode::kSpeculativeBigIntBitwiseAnd:
       CheckTypeIs(node, Type::BigInt());
       break;
@@ -987,6 +1010,7 @@
     case IrOpcode::kBigIntSubtract:
     case IrOpcode::kBigIntMultiply:
     case IrOpcode::kBigIntDivide:
+    case IrOpcode::kBigIntModulus:
     case IrOpcode::kBigIntBitwiseAnd:
       CheckValueInputIs(node, 0, Type::BigInt());
       CheckValueInputIs(node, 1, Type::BigInt());
@@ -1528,11 +1552,19 @@
     case IrOpcode::kCheckedTaggedToTaggedSigned:
     case IrOpcode::kCheckedTaggedToTaggedPointer:
     case IrOpcode::kCheckedTruncateTaggedToWord32:
-    case IrOpcode::kCheckedBigInt64Add:
+    case IrOpcode::kCheckedInt64Add:
+    case IrOpcode::kCheckedInt64Sub:
+    case IrOpcode::kCheckedInt64Mul:
+    case IrOpcode::kCheckedInt64Div:
+    case IrOpcode::kCheckedInt64Mod:
     case IrOpcode::kAssertType:
     case IrOpcode::kVerifyType:
       break;
-
+    case IrOpcode::kDoubleArrayMin:
+    case IrOpcode::kDoubleArrayMax:
+      CheckValueInputIs(node, 0, Type::Any());
+      CheckTypeIs(node, Type::Number());
+      break;
     case IrOpcode::kCheckFloat64Hole:
       CheckValueInputIs(node, 0, Type::NumberOrHole());
       CheckTypeIs(node, Type::NumberOrUndefined());
@@ -1647,8 +1679,8 @@
       CheckValueInputIs(node, 0, Type::Any());
       CheckTypeIs(node, Type::BigInt());
       break;
-    case IrOpcode::kCheckBigInt64:
-      CheckValueInputIs(node, 0, Type::Any());
+    case IrOpcode::kCheckedBigIntToBigInt64:
+      CheckValueInputIs(node, 0, Type::BigInt());
       CheckTypeIs(node, Type::SignedBigInt64());
       break;
     case IrOpcode::kFastApiCall:
diff -r -u --color up/v8/src/compiler/wasm-compiler-definitions.h nw/v8/src/compiler/wasm-compiler-definitions.h
--- up/v8/src/compiler/wasm-compiler-definitions.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/wasm-compiler-definitions.h	2023-01-19 16:46:36.146442922 -0500
@@ -13,31 +13,33 @@
 #include <ostream>
 
 #include "src/base/functional.h"
+#include "src/wasm/value-type.h"
 
 namespace v8 {
 namespace internal {
 namespace compiler {
 
+// If {to} is nullable, it means that null passes the check.
+// {from} may change in compiler optimization passes as the object's type gets
+// narrowed.
+// TODO(12166): Add modules if we have cross-module inlining.
 struct WasmTypeCheckConfig {
-  bool object_can_be_null;
-  bool null_succeeds;
-  uint8_t rtt_depth;
+  wasm::ValueType from;
+  const wasm::ValueType to;
 };
 
 V8_INLINE std::ostream& operator<<(std::ostream& os,
                                    WasmTypeCheckConfig const& p) {
-  return os << (p.object_can_be_null ? "nullable" : "non-nullable")
-            << ", depth=" << static_cast<int>(p.rtt_depth);
+  return os << p.from.name() << " -> " << p.to.name();
 }
 
 V8_INLINE size_t hash_value(WasmTypeCheckConfig const& p) {
-  return base::hash_combine(p.object_can_be_null, p.rtt_depth);
+  return base::hash_combine(p.from.raw_bit_field(), p.to.raw_bit_field());
 }
 
 V8_INLINE bool operator==(const WasmTypeCheckConfig& p1,
                           const WasmTypeCheckConfig& p2) {
-  return p1.object_can_be_null == p2.object_can_be_null &&
-         p1.rtt_depth == p2.rtt_depth;
+  return p1.from == p2.from && p1.to == p2.to;
 }
 
 }  // namespace compiler
diff -r -u --color up/v8/src/compiler/wasm-compiler.cc nw/v8/src/compiler/wasm-compiler.cc
--- up/v8/src/compiler/wasm-compiler.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/wasm-compiler.cc	2023-01-19 16:46:36.146442922 -0500
@@ -282,14 +282,19 @@
 }
 
 Node* WasmGraphBuilder::RefFunc(uint32_t function_index) {
-  return gasm_->CallRuntimeStub(wasm::WasmCode::kWasmRefFunc,
-                                Operator::kNoThrow,
-                                gasm_->Uint32Constant(function_index));
-}
-
-Node* WasmGraphBuilder::RefAsNonNull(Node* arg,
-                                     wasm::WasmCodePosition position) {
-  return AssertNotNull(arg, position);
+  Node* functions =
+      LOAD_INSTANCE_FIELD(WasmInternalFunctions, MachineType::TaggedPointer());
+  Node* maybe_function =
+      gasm_->LoadFixedArrayElementPtr(functions, function_index);
+  auto done = gasm_->MakeLabel(MachineRepresentation::kTaggedPointer);
+  gasm_->GotoIfNot(gasm_->TaggedEqual(maybe_function, UndefinedValue()), &done,
+                   maybe_function);
+  Node* function_from_builtin =
+      gasm_->CallRuntimeStub(wasm::WasmCode::kWasmRefFunc, Operator::kNoThrow,
+                             gasm_->Uint32Constant(function_index));
+  gasm_->Goto(&done, function_from_builtin);
+  gasm_->Bind(&done);
+  return done.PhiAt(0);
 }
 
 Node* WasmGraphBuilder::NoContextConstant() {
@@ -1124,7 +1129,6 @@
 
 Node* WasmGraphBuilder::AssertNotNull(Node* object,
                                       wasm::WasmCodePosition position) {
-  if (v8_flags.experimental_wasm_skip_null_checks) return object;
   Node* result = gasm_->AssertNotNull(object);
   SetSourcePosition(result, position);
   return result;
@@ -3964,34 +3968,6 @@
   lowering_special_case_->replacements.insert({original, replacement});
 }
 
-CallDescriptor* WasmGraphBuilder::GetI32AtomicWaitCallDescriptor() {
-  if (i32_atomic_wait_descriptor_) return i32_atomic_wait_descriptor_;
-
-  i32_atomic_wait_descriptor_ = GetBuiltinCallDescriptor(
-      Builtin::kWasmI32AtomicWait64, zone_, StubCallMode::kCallWasmRuntimeStub);
-
-  AddInt64LoweringReplacement(
-      i32_atomic_wait_descriptor_,
-      GetBuiltinCallDescriptor(Builtin::kWasmI32AtomicWait32, zone_,
-                               StubCallMode::kCallWasmRuntimeStub));
-
-  return i32_atomic_wait_descriptor_;
-}
-
-CallDescriptor* WasmGraphBuilder::GetI64AtomicWaitCallDescriptor() {
-  if (i64_atomic_wait_descriptor_) return i64_atomic_wait_descriptor_;
-
-  i64_atomic_wait_descriptor_ = GetBuiltinCallDescriptor(
-      Builtin::kWasmI64AtomicWait64, zone_, StubCallMode::kCallWasmRuntimeStub);
-
-  AddInt64LoweringReplacement(
-      i64_atomic_wait_descriptor_,
-      GetBuiltinCallDescriptor(Builtin::kWasmI64AtomicWait32, zone_,
-                               StubCallMode::kCallWasmRuntimeStub));
-
-  return i64_atomic_wait_descriptor_;
-}
-
 void WasmGraphBuilder::LowerInt64(Signature<MachineRepresentation>* sig) {
   if (mcgraph()->machine()->Is64()) return;
   Int64Lowering r(mcgraph()->graph(), mcgraph()->machine(), mcgraph()->common(),
@@ -4005,6 +3981,45 @@
   LowerInt64(CreateMachineSignature(mcgraph()->zone(), sig_, origin));
 }
 
+CallDescriptor* WasmGraphBuilder::GetI64ToBigIntCallDescriptor(
+    StubCallMode stub_mode) {
+  CallDescriptor** i64_to_bigint_descriptor =
+      stub_mode == StubCallMode::kCallCodeObject
+          ? &i64_to_bigint_stub_descriptor_
+          : &i64_to_bigint_builtin_descriptor_;
+  if (*i64_to_bigint_descriptor) return *i64_to_bigint_descriptor;
+
+  *i64_to_bigint_descriptor =
+      GetBuiltinCallDescriptor(Builtin::kI64ToBigInt, zone_, stub_mode);
+
+  AddInt64LoweringReplacement(
+      *i64_to_bigint_descriptor,
+      GetBuiltinCallDescriptor(Builtin::kI32PairToBigInt, zone_, stub_mode));
+  return *i64_to_bigint_descriptor;
+}
+
+Node* WasmGraphBuilder::BuildChangeInt64ToBigInt(Node* input,
+                                                 StubCallMode stub_mode) {
+  Node* target;
+  if (mcgraph()->machine()->Is64()) {
+    target = (stub_mode == StubCallMode::kCallWasmRuntimeStub)
+                 ? mcgraph()->RelocatableIntPtrConstant(
+                       wasm::WasmCode::kI64ToBigInt, RelocInfo::WASM_STUB_CALL)
+                 : gasm_->GetBuiltinPointerTarget(Builtin::kI64ToBigInt);
+  } else {
+    DCHECK(mcgraph()->machine()->Is32());
+    // On 32-bit platforms we already set the target to the
+    // I32PairToBigInt builtin here, so that we don't have to replace the
+    // target in the int64-lowering.
+    target =
+        (stub_mode == StubCallMode::kCallWasmRuntimeStub)
+            ? mcgraph()->RelocatableIntPtrConstant(
+                  wasm::WasmCode::kI32PairToBigInt, RelocInfo::WASM_STUB_CALL)
+            : gasm_->GetBuiltinPointerTarget(Builtin::kI32PairToBigInt);
+  }
+  return gasm_->Call(GetI64ToBigIntCallDescriptor(stub_mode), target, input);
+}
+
 void WasmGraphBuilder::SetSourcePosition(Node* node,
                                          wasm::WasmCodePosition position) {
   DCHECK_NE(position, wasm::kNoCodePosition);
@@ -4971,29 +4986,31 @@
                                     inputs[1]);
 
     case wasm::kExprI32AtomicWait: {
-      auto* call_descriptor = GetI32AtomicWaitCallDescriptor();
+      constexpr StubCallMode kStubMode = StubCallMode::kCallWasmRuntimeStub;
+      auto* call_descriptor = GetBuiltinCallDescriptor(
+          Builtin::kWasmI32AtomicWait, zone_, kStubMode);
 
-      intptr_t target = mcgraph()->machine()->Is64()
-                            ? wasm::WasmCode::kWasmI32AtomicWait64
-                            : wasm::WasmCode::kWasmI32AtomicWait32;
+      intptr_t target = wasm::WasmCode::kWasmI32AtomicWait;
       Node* call_target = mcgraph()->RelocatableIntPtrConstant(
           target, RelocInfo::WASM_STUB_CALL);
 
       return gasm_->Call(call_descriptor, call_target, effective_offset,
-                         inputs[1], inputs[2]);
+                         inputs[1],
+                         BuildChangeInt64ToBigInt(inputs[2], kStubMode));
     }
 
     case wasm::kExprI64AtomicWait: {
-      auto* call_descriptor = GetI64AtomicWaitCallDescriptor();
+      constexpr StubCallMode kStubMode = StubCallMode::kCallWasmRuntimeStub;
+      auto* call_descriptor = GetBuiltinCallDescriptor(
+          Builtin::kWasmI64AtomicWait, zone_, kStubMode);
 
-      intptr_t target = mcgraph()->machine()->Is64()
-                            ? wasm::WasmCode::kWasmI64AtomicWait64
-                            : wasm::WasmCode::kWasmI64AtomicWait32;
+      intptr_t target = wasm::WasmCode::kWasmI64AtomicWait;
       Node* call_target = mcgraph()->RelocatableIntPtrConstant(
           target, RelocInfo::WASM_STUB_CALL);
 
       return gasm_->Call(call_descriptor, call_target, effective_offset,
-                         inputs[1], inputs[2]);
+                         BuildChangeInt64ToBigInt(inputs[1], kStubMode),
+                         BuildChangeInt64ToBigInt(inputs[2], kStubMode));
     }
 
     default:
@@ -5268,9 +5285,9 @@
       ObjectAccess(MachineType::Uint32(), kNoWriteBarrier), a,
       wasm::ObjectAccess::ToTagged(WasmArray::kLengthOffset), length);
 
-  // Initialize the array elements. Use memset for large arrays initialized with
-  // zeroes (through an external function), and a loop for all other ones. The
-  // size limit was determined by running array-copy-benchmark.js.
+  // Initialize the array. Use an external function for large arrays with
+  // null/number initializer. Use a loop for small arrays and reference arrays
+  // with a non-null initial value.
   auto done = gasm_->MakeLabel();
   // TODO(manoskouk): If the loop is ever removed here, we have to update
   // ArrayNew() in graph-builder-interface.cc to not mark the current
@@ -5278,22 +5295,66 @@
   auto loop = gasm_->MakeLoopLabel(MachineRepresentation::kWord32);
   Node* start_offset = gasm_->IntPtrConstant(
       wasm::ObjectAccess::ToTagged(WasmArray::kHeaderSize));
-  Node* element_size = gasm_->IntPtrConstant(element_type.value_kind_size());
-  Node* end_offset =
-      gasm_->IntAdd(start_offset, gasm_->IntMul(element_size, length));
 
-  if (initial_value == nullptr && element_type.is_numeric()) {
-    constexpr uint32_t kArrayNewMinimumSizeForMemSet = 10;
+  if ((initial_value == nullptr && (element_type.kind() == wasm::kRefNull ||
+                                    element_type.kind() == wasm::kS128)) ||
+      (element_type.is_numeric() && element_type != wasm::kWasmS128)) {
+    constexpr uint32_t kArrayNewMinimumSizeForMemSet = 16;
     gasm_->GotoIf(gasm_->Uint32LessThan(
                       length, Int32Constant(kArrayNewMinimumSizeForMemSet)),
                   &loop, BranchHint::kNone, start_offset);
     Node* function = gasm_->ExternalConstant(
-        ExternalReference::wasm_array_fill_with_zeroes());
+        ExternalReference::wasm_array_fill_with_number_or_null());
+
+    Node* initial_value_i64 = nullptr;
+    if (initial_value == nullptr && element_type.is_numeric()) {
+      initial_value_i64 = Int64Constant(0);
+    } else {
+      switch (element_type.kind()) {
+        case wasm::kI32:
+        case wasm::kI8:
+        case wasm::kI16:
+          initial_value_i64 = graph()->NewNode(
+              mcgraph()->machine()->ChangeInt32ToInt64(), initial_value);
+          break;
+        case wasm::kI64:
+          initial_value_i64 = initial_value;
+          break;
+        case wasm::kF32:
+          initial_value_i64 = graph()->NewNode(
+              mcgraph()->machine()->ChangeInt32ToInt64(),
+              graph()->NewNode(mcgraph()->machine()->BitcastFloat32ToInt32(),
+                               initial_value));
+          break;
+        case wasm::kF64:
+          initial_value_i64 = graph()->NewNode(
+              mcgraph()->machine()->BitcastFloat64ToInt64(), initial_value);
+          break;
+        case wasm::kRefNull:
+          initial_value_i64 =
+              initial_value == nullptr ? gasm_->Null() : initial_value;
+          if (kSystemPointerSize == 4) {
+            initial_value_i64 = graph()->NewNode(
+                mcgraph()->machine()->ChangeInt32ToInt64(), initial_value_i64);
+          }
+          break;
+        case wasm::kS128:
+        case wasm::kRtt:
+        case wasm::kRef:
+        case wasm::kVoid:
+        case wasm::kBottom:
+          UNREACHABLE();
+      }
+    }
+
+    Node* stack_slot = StoreArgsInStackSlot(
+        {{MachineRepresentation::kWord64, initial_value_i64}});
+
     MachineType arg_types[]{MachineType::TaggedPointer(), MachineType::Uint32(),
-                            MachineType::Uint32()};
-    MachineSignature sig(0, 3, arg_types);
+                            MachineType::Uint32(), MachineType::Pointer()};
+    MachineSignature sig(0, 4, arg_types);
     BuildCCall(&sig, function, a, length,
-               Int32Constant(element_type.value_kind_size()));
+               Int32Constant(element_type.raw_bit_field()), stack_slot);
     gasm_->Goto(&done);
   } else {
     gasm_->Goto(&loop, start_offset);
@@ -5304,6 +5365,9 @@
     initial_value = DefaultValue(element_type);
     object_access.write_barrier_kind = kNoWriteBarrier;
   }
+  Node* element_size = gasm_->IntPtrConstant(element_type.value_kind_size());
+  Node* end_offset =
+      gasm_->IntAdd(start_offset, gasm_->IntMul(element_size, length));
   {
     Node* offset = loop.PhiAt(0);
     Node* check = gasm_->UintLessThan(offset, end_offset);
@@ -5431,12 +5495,12 @@
 
 void WasmGraphBuilder::DataCheck(Node* object, bool object_can_be_null,
                                  Callbacks callbacks, bool null_succeeds) {
+  // TODO(7748): Only used for backwards compatibility in combination with
+  // v8_flags.wasm_gc_structref_as_dataref. Remove.
   if (object_can_be_null) {
     if (null_succeeds) {
       callbacks.succeed_if(IsNull(object), BranchHint::kFalse);
     } else {
-      // TODO(7748): Is the extra null check actually beneficial for
-      // performance?
       callbacks.fail_if(IsNull(object), BranchHint::kFalse);
     }
   }
@@ -5530,8 +5594,8 @@
       return RefIsEq(object, is_nullable, null_succeeds);
     case wasm::HeapType::kI31:
       return RefIsI31(object, null_succeeds);
-    case wasm::HeapType::kData:
-      return RefIsData(object, is_nullable, null_succeeds);
+    case wasm::HeapType::kStruct:
+      return RefIsStruct(object, is_nullable, null_succeeds);
     case wasm::HeapType::kArray:
       return RefIsArray(object, is_nullable, null_succeeds);
     case wasm::HeapType::kAny:
@@ -5548,6 +5612,28 @@
   return gasm_->WasmTypeCast(object, rtt, config);
 }
 
+Node* WasmGraphBuilder::RefCastAbstract(Node* object, wasm::HeapType type,
+                                        wasm::WasmCodePosition position,
+                                        bool null_succeeds) {
+  bool is_nullable =
+      compiler::NodeProperties::GetType(object).AsWasm().type.is_nullable();
+  switch (type.representation()) {
+    case wasm::HeapType::kEq:
+      return RefAsEq(object, is_nullable, position, null_succeeds);
+    case wasm::HeapType::kI31:
+      return RefAsI31(object, position, null_succeeds);
+    case wasm::HeapType::kStruct:
+      return RefAsStruct(object, is_nullable, position, null_succeeds);
+    case wasm::HeapType::kArray:
+      return RefAsArray(object, is_nullable, position, null_succeeds);
+    case wasm::HeapType::kAny:
+      // Any may never need a cast as it is either implicitly convertible or
+      // never convertible for any given type.
+    default:
+      UNREACHABLE();
+  }
+}
+
 void WasmGraphBuilder::BrOnCast(Node* object, Node* rtt,
                                 WasmTypeCheckConfig config,
                                 Node** match_control, Node** match_effect,
@@ -5572,36 +5658,63 @@
   return done.PhiAt(0);
 }
 
-Node* WasmGraphBuilder::RefIsData(Node* object, bool object_can_be_null,
-                                  bool null_succeeds) {
+Node* WasmGraphBuilder::RefAsEq(Node* object, bool object_can_be_null,
+                                wasm::WasmCodePosition position,
+                                bool null_succeeds) {
+  auto done = gasm_->MakeLabel();
+  EqCheck(object, object_can_be_null, CastCallbacks(&done, position),
+          null_succeeds);
+  gasm_->Goto(&done);
+  gasm_->Bind(&done);
+  return object;
+}
+
+Node* WasmGraphBuilder::RefIsStruct(Node* object, bool object_can_be_null,
+                                    bool null_succeeds) {
   auto done = gasm_->MakeLabel(MachineRepresentation::kWord32);
-  DataCheck(object, object_can_be_null, TestCallbacks(&done), null_succeeds);
+  if (!v8_flags.wasm_gc_structref_as_dataref) {
+    ManagedObjectInstanceCheck(object, object_can_be_null, WASM_STRUCT_TYPE,
+                               TestCallbacks(&done), null_succeeds);
+  } else {
+    DataCheck(object, object_can_be_null, TestCallbacks(&done), null_succeeds);
+  }
   gasm_->Goto(&done, Int32Constant(1));
   gasm_->Bind(&done);
   return done.PhiAt(0);
 }
 
-Node* WasmGraphBuilder::RefAsData(Node* object, bool object_can_be_null,
-                                  wasm::WasmCodePosition position) {
-  bool null_succeeds = false;
+Node* WasmGraphBuilder::RefAsStruct(Node* object, bool object_can_be_null,
+                                    wasm::WasmCodePosition position,
+                                    bool null_succeeds) {
   auto done = gasm_->MakeLabel();
-  DataCheck(object, object_can_be_null, CastCallbacks(&done, position),
-            null_succeeds);
+  if (!v8_flags.wasm_gc_structref_as_dataref) {
+    ManagedObjectInstanceCheck(object, object_can_be_null, WASM_STRUCT_TYPE,
+                               CastCallbacks(&done, position), null_succeeds);
+  } else {
+    DataCheck(object, object_can_be_null, CastCallbacks(&done, position),
+              null_succeeds);
+  }
   gasm_->Goto(&done);
   gasm_->Bind(&done);
   return object;
 }
 
-void WasmGraphBuilder::BrOnData(Node* object, Node* /*rtt*/,
-                                WasmTypeCheckConfig config,
-                                Node** match_control, Node** match_effect,
-                                Node** no_match_control,
-                                Node** no_match_effect) {
+void WasmGraphBuilder::BrOnStruct(Node* object, Node* /*rtt*/,
+                                  WasmTypeCheckConfig config,
+                                  Node** match_control, Node** match_effect,
+                                  Node** no_match_control,
+                                  Node** no_match_effect) {
   bool null_succeeds = false;
   BrOnCastAbs(match_control, match_effect, no_match_control, no_match_effect,
               [=](Callbacks callbacks) -> void {
-                return DataCheck(object, config.object_can_be_null, callbacks,
-                                 null_succeeds);
+                if (!v8_flags.wasm_gc_structref_as_dataref) {
+                  return ManagedObjectInstanceCheck(
+                      object, config.from.is_nullable(), WASM_STRUCT_TYPE,
+                      callbacks, null_succeeds);
+                } else {
+                  return DataCheck(object, config.from.is_nullable(), callbacks,
+                                   null_succeeds);
+                }
               });
 }
 
@@ -5616,8 +5729,8 @@
 }
 
 Node* WasmGraphBuilder::RefAsArray(Node* object, bool object_can_be_null,
-                                   wasm::WasmCodePosition position) {
-  bool null_succeeds = false;
+                                   wasm::WasmCodePosition position,
+                                   bool null_succeeds) {
   auto done = gasm_->MakeLabel();
   ManagedObjectInstanceCheck(object, object_can_be_null, WASM_ARRAY_TYPE,
                              CastCallbacks(&done, position), null_succeeds);
@@ -5635,7 +5748,7 @@
   BrOnCastAbs(match_control, match_effect, no_match_control, no_match_effect,
               [=](Callbacks callbacks) -> void {
                 return ManagedObjectInstanceCheck(
-                    object, config.object_can_be_null, WASM_ARRAY_TYPE,
+                    object, config.from.is_nullable(), WASM_ARRAY_TYPE,
                     callbacks, null_succeeds);
               });
 }
@@ -5652,8 +5765,16 @@
   return gasm_->IsI31(object);
 }
 
-Node* WasmGraphBuilder::RefAsI31(Node* object,
-                                 wasm::WasmCodePosition position) {
+Node* WasmGraphBuilder::RefAsI31(Node* object, wasm::WasmCodePosition position,
+                                 bool null_succeeds) {
+  if (null_succeeds) {
+    auto done = gasm_->MakeLabel();
+    gasm_->GotoIf(gasm_->IsNull(object), &done);
+    TrapIfFalse(wasm::kTrapIllegalCast, gasm_->IsI31(object), position);
+    gasm_->Goto(&done);
+    gasm_->Bind(&done);
+    return object;
+  }
   TrapIfFalse(wasm::kTrapIllegalCast, gasm_->IsI31(object), position);
   return object;
 }
@@ -6189,18 +6310,6 @@
         stub_mode_(stub_mode),
         enabled_features_(features) {}
 
-  CallDescriptor* GetI64ToBigIntCallDescriptor() {
-    if (i64_to_bigint_descriptor_) return i64_to_bigint_descriptor_;
-
-    i64_to_bigint_descriptor_ =
-        GetBuiltinCallDescriptor(Builtin::kI64ToBigInt, zone_, stub_mode_);
-
-    AddInt64LoweringReplacement(
-        i64_to_bigint_descriptor_,
-        GetBuiltinCallDescriptor(Builtin::kI32PairToBigInt, zone_, stub_mode_));
-    return i64_to_bigint_descriptor_;
-  }
-
   CallDescriptor* GetBigIntToI64CallDescriptor(bool needs_frame_state) {
     if (bigint_to_i64_descriptor_) return bigint_to_i64_descriptor_;
 
@@ -6366,7 +6475,7 @@
       case wasm::kI32:
         return BuildChangeInt32ToNumber(node);
       case wasm::kI64:
-        return BuildChangeInt64ToBigInt(node);
+        return BuildChangeInt64ToBigInt(node, stub_mode_);
       case wasm::kF32:
         return BuildChangeFloat32ToNumber(node);
       case wasm::kF64:
@@ -6394,45 +6503,17 @@
                       WasmInternalFunction::kExternalOffset));
             }
           }
-          case wasm::HeapType::kEq: {
-            // TODO(7748): Update this when JS interop is settled.
-            auto done = gasm_->MakeLabel(MachineRepresentation::kTaggedPointer);
-            // Do not wrap i31s.
-            gasm_->GotoIf(IsSmi(node), &done, node);
-            if (type.kind() == wasm::kRefNull) {
-              // Do not wrap {null}.
-              gasm_->GotoIf(IsNull(node), &done, node);
-            }
-            gasm_->Goto(&done, BuildAllocateObjectWrapper(node, context));
-            gasm_->Bind(&done);
-            return done.PhiAt(0);
-          }
-          case wasm::HeapType::kData:
+          case wasm::HeapType::kEq:
+          case wasm::HeapType::kStruct:
           case wasm::HeapType::kArray:
-            // TODO(7748): Update this when JS interop is settled.
-            if (type.kind() == wasm::kRefNull) {
-              auto done =
-                  gasm_->MakeLabel(MachineRepresentation::kTaggedPointer);
-              // Do not wrap {null}.
-              gasm_->GotoIf(IsNull(node), &done, node);
-              gasm_->Goto(&done, BuildAllocateObjectWrapper(node, context));
-              gasm_->Bind(&done);
-              return done.PhiAt(0);
-            } else {
-              return BuildAllocateObjectWrapper(node, context);
-            }
           case wasm::HeapType::kString:
-            // Either {node} is already a tagged JS string, or if type.kind() is
-            // wasm::kRefNull, it's the null object.  Either way it's good to go
-            // already to JS.
-            return node;
           case wasm::HeapType::kExtern:
+          case wasm::HeapType::kAny:
             return node;
           case wasm::HeapType::kNone:
           case wasm::HeapType::kNoFunc:
           case wasm::HeapType::kNoExtern:
           case wasm::HeapType::kI31:
-          case wasm::HeapType::kAny:
             UNREACHABLE();
           default:
             DCHECK(type.has_index());
@@ -6459,36 +6540,11 @@
     }
   }
 
-  // TODO(7748): Temporary solution to allow round-tripping of Wasm objects
-  // through JavaScript, where they show up as opaque boxes. This will disappear
-  // once we have a proper WasmGC <-> JS interaction story.
-  Node* BuildAllocateObjectWrapper(Node* input, Node* context) {
-    if (v8_flags.wasm_gc_js_interop) return input;
-    return gasm_->CallBuiltin(Builtin::kWasmAllocateObjectWrapper,
-                              Operator::kEliminatable, input, context);
-  }
-
   enum UnwrapExternalFunctions : bool {
     kUnwrapWasmExternalFunctions = true,
     kLeaveFunctionsAlone = false
   };
 
-  Node* BuildChangeInt64ToBigInt(Node* input) {
-    Node* target;
-    if (mcgraph()->machine()->Is64()) {
-      target = GetTargetForBuiltinCall(wasm::WasmCode::kI64ToBigInt,
-                                       Builtin::kI64ToBigInt);
-    } else {
-      DCHECK(mcgraph()->machine()->Is32());
-      // On 32-bit platforms we already set the target to the
-      // I32PairToBigInt builtin here, so that we don't have to replace the
-      // target in the int64-lowering.
-      target = GetTargetForBuiltinCall(wasm::WasmCode::kI32PairToBigInt,
-                                       Builtin::kI32PairToBigInt);
-    }
-    return gasm_->Call(GetI64ToBigIntCallDescriptor(), target, input);
-  }
-
   Node* BuildChangeBigIntToInt64(Node* input, Node* context,
                                  Node* frame_state) {
     Node* target;
@@ -6548,7 +6604,7 @@
           case wasm::HeapType::kI31:
             UNREACHABLE();
           case wasm::HeapType::kFunc:
-          case wasm::HeapType::kData:
+          case wasm::HeapType::kStruct:
           case wasm::HeapType::kArray:
           case wasm::HeapType::kEq:
           default: {
@@ -7642,7 +7698,6 @@
   SetOncePointer<const Operator> tagged_to_float64_operator_;
   wasm::WasmFeatures enabled_features_;
   CallDescriptor* bigint_to_i64_descriptor_ = nullptr;
-  CallDescriptor* i64_to_bigint_descriptor_ = nullptr;
 };
 
 }  // namespace
diff -r -u --color up/v8/src/compiler/wasm-compiler.h nw/v8/src/compiler/wasm-compiler.h
--- up/v8/src/compiler/wasm-compiler.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/wasm-compiler.h	2023-01-19 16:46:36.146442922 -0500
@@ -255,7 +255,7 @@
   Node* EffectPhi(unsigned count, Node** effects_and_control);
   Node* RefNull();
   Node* RefFunc(uint32_t function_index);
-  Node* RefAsNonNull(Node* arg, wasm::WasmCodePosition position);
+  Node* AssertNotNull(Node* object, wasm::WasmCodePosition position);
   Node* TraceInstruction(uint32_t mark_id);
   Node* Int32Constant(int32_t value);
   Node* Int64Constant(int64_t value);
@@ -497,24 +497,29 @@
   Node* RefTestAbstract(Node* object, wasm::HeapType type, bool null_succeeds);
   Node* RefCast(Node* object, Node* rtt, WasmTypeCheckConfig config,
                 wasm::WasmCodePosition position);
+  Node* RefCastAbstract(Node* object, wasm::HeapType type,
+                        wasm::WasmCodePosition position, bool null_succeeds);
   void BrOnCast(Node* object, Node* rtt, WasmTypeCheckConfig config,
                 Node** match_control, Node** match_effect,
                 Node** no_match_control, Node** no_match_effect);
   Node* RefIsEq(Node* object, bool object_can_be_null, bool null_succeeds);
-  Node* RefIsData(Node* object, bool object_can_be_null, bool null_succeeds);
-  Node* RefAsData(Node* object, bool object_can_be_null,
-                  wasm::WasmCodePosition position);
-  void BrOnData(Node* object, Node* rtt, WasmTypeCheckConfig config,
-                Node** match_control, Node** match_effect,
-                Node** no_match_control, Node** no_match_effect);
+  Node* RefAsEq(Node* object, bool object_can_be_null,
+                wasm::WasmCodePosition position, bool null_succeeds);
+  Node* RefIsStruct(Node* object, bool object_can_be_null, bool null_succeeds);
+  Node* RefAsStruct(Node* object, bool object_can_be_null,
+                    wasm::WasmCodePosition position, bool null_succeeds);
+  void BrOnStruct(Node* object, Node* rtt, WasmTypeCheckConfig config,
+                  Node** match_control, Node** match_effect,
+                  Node** no_match_control, Node** no_match_effect);
   Node* RefIsArray(Node* object, bool object_can_be_null, bool null_succeeds);
   Node* RefAsArray(Node* object, bool object_can_be_null,
-                   wasm::WasmCodePosition position);
+                   wasm::WasmCodePosition position, bool null_succeeds);
   void BrOnArray(Node* object, Node* rtt, WasmTypeCheckConfig config,
                  Node** match_control, Node** match_effect,
                  Node** no_match_control, Node** no_match_effect);
   Node* RefIsI31(Node* object, bool null_succeeds);
-  Node* RefAsI31(Node* object, wasm::WasmCodePosition position);
+  Node* RefAsI31(Node* object, wasm::WasmCodePosition position,
+                 bool null_succeeds);
   void BrOnI31(Node* object, Node* rtt, WasmTypeCheckConfig config,
                Node** match_control, Node** match_effect,
                Node** no_match_control, Node** no_match_effect);
@@ -739,8 +744,6 @@
   void MemTypeToUintPtrOrOOBTrap(std::initializer_list<Node**> nodes,
                                  wasm::WasmCodePosition position);
 
-  Node* AssertNotNull(Node* object, wasm::WasmCodePosition position);
-
   void GetGlobalBaseAndOffset(const wasm::WasmGlobal&, Node** base_node,
                               Node** offset_node);
 
@@ -831,9 +834,9 @@
   void AddInt64LoweringReplacement(CallDescriptor* original,
                                    CallDescriptor* replacement);
 
-  CallDescriptor* GetI32AtomicWaitCallDescriptor();
+  Node* BuildChangeInt64ToBigInt(Node* input, StubCallMode stub_mode);
 
-  CallDescriptor* GetI64AtomicWaitCallDescriptor();
+  CallDescriptor* GetI64ToBigIntCallDescriptor(StubCallMode stub_mode);
 
   Node* StoreArgsInStackSlot(
       std::initializer_list<std::pair<MachineRepresentation, Node*>> args);
@@ -863,8 +866,8 @@
   SetOncePointer<Node> instance_node_;
 
   std::unique_ptr<Int64LoweringSpecialCase> lowering_special_case_;
-  CallDescriptor* i32_atomic_wait_descriptor_ = nullptr;
-  CallDescriptor* i64_atomic_wait_descriptor_ = nullptr;
+  CallDescriptor* i64_to_bigint_builtin_descriptor_ = nullptr;
+  CallDescriptor* i64_to_bigint_stub_descriptor_ = nullptr;
 };
 
 enum WasmCallKind { kWasmFunction, kWasmImportWrapper, kWasmCapiFunction };
diff -r -u --color up/v8/src/compiler/wasm-gc-lowering.cc nw/v8/src/compiler/wasm-gc-lowering.cc
--- up/v8/src/compiler/wasm-gc-lowering.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/wasm-gc-lowering.cc	2023-01-19 16:46:36.146442922 -0500
@@ -13,16 +13,20 @@
 #include "src/compiler/wasm-compiler-definitions.h"
 #include "src/compiler/wasm-graph-assembler.h"
 #include "src/wasm/object-access.h"
+#include "src/wasm/wasm-engine.h"
 #include "src/wasm/wasm-linkage.h"
 #include "src/wasm/wasm-objects.h"
+#include "src/wasm/wasm-subtyping.h"
 
 namespace v8 {
 namespace internal {
 namespace compiler {
 
-WasmGCLowering::WasmGCLowering(Editor* editor, MachineGraph* mcgraph)
+WasmGCLowering::WasmGCLowering(Editor* editor, MachineGraph* mcgraph,
+                               const wasm::WasmModule* module)
     : AdvancedReducer(editor),
       gasm_(mcgraph, mcgraph->zone()),
+      module_(module),
       dead_(mcgraph->Dead()),
       instance_node_(nullptr) {
   // Find and store the instance node.
@@ -64,6 +68,7 @@
 }
 
 Node* WasmGCLowering::RootNode(RootIndex index) {
+  // TODO(13449): Use root register instead of isolate.
   Node* isolate_root = gasm_.LoadImmutable(
       MachineType::Pointer(), instance_node_,
       WasmInstanceObject::kIsolateRootOffset - kHeapObjectTag);
@@ -73,6 +78,13 @@
 
 Node* WasmGCLowering::Null() { return RootNode(RootIndex::kNullValue); }
 
+Node* WasmGCLowering::IsNull(Node* object) {
+  Tagged_t static_null = wasm::GetWasmEngine()->compressed_null_value_or_zero();
+  Node* null_value =
+      static_null != 0 ? gasm_.UintPtrConstant(static_null) : Null();
+  return gasm_.TaggedEqual(object, null_value);
+}
+
 // TODO(manoskouk): Use the Callbacks infrastructure from wasm-compiler.h to
 // unify all check/cast implementations.
 // TODO(manoskouk): Find a way to optimize branches on typechecks.
@@ -84,22 +96,28 @@
   Node* effect_input = NodeProperties::GetEffectInput(node);
   Node* control_input = NodeProperties::GetControlInput(node);
   auto config = OpParameter<WasmTypeCheckConfig>(node->op());
-  int rtt_depth = config.rtt_depth;
-  bool object_can_be_null = config.object_can_be_null;
+  int rtt_depth = wasm::GetSubtypingDepth(module_, config.to.ref_index());
+  bool object_can_be_null = config.from.is_nullable();
+  bool object_can_be_i31 =
+      wasm::IsSubtypeOf(wasm::kWasmI31Ref.AsNonNull(), config.from, module_);
 
   gasm_.InitializeEffectControl(effect_input, control_input);
 
   auto end_label = gasm_.MakeLabel(MachineRepresentation::kWord32);
+  bool is_cast_from_any = config.from.is_reference_to(wasm::HeapType::kAny);
 
-  if (object_can_be_null) {
-    const int kResult = config.null_succeeds ? 1 : 0;
-    gasm_.GotoIf(gasm_.TaggedEqual(object, Null()), &end_label,
-                 BranchHint::kFalse, gasm_.Int32Constant(kResult));
+  // Skip the null check if casting from any and if null results in check
+  // failure. In that case the instance type check will identify null as not
+  // being a wasm object and return 0 (failure).
+  if (object_can_be_null && (!is_cast_from_any || config.to.is_nullable())) {
+    const int kResult = config.to.is_nullable() ? 1 : 0;
+    gasm_.GotoIf(IsNull(object), &end_label, BranchHint::kFalse,
+                 gasm_.Int32Constant(kResult));
   }
 
-  // TODO(7748): In some cases the Smi check is redundant. If we had information
-  // about the source type, we could skip it in those cases.
-  gasm_.GotoIf(gasm_.IsI31(object), &end_label, gasm_.Int32Constant(0));
+  if (object_can_be_i31) {
+    gasm_.GotoIf(gasm_.IsI31(object), &end_label, gasm_.Int32Constant(0));
+  }
 
   Node* map = gasm_.LoadMap(object);
 
@@ -108,6 +126,13 @@
   gasm_.GotoIf(gasm_.TaggedEqual(map, rtt), &end_label, BranchHint::kTrue,
                gasm_.Int32Constant(1));
 
+  // Check if map instance type identifies a wasm object.
+  if (is_cast_from_any) {
+    Node* is_wasm_obj = gasm_.IsDataRefMap(map);
+    gasm_.GotoIfNot(is_wasm_obj, &end_label, BranchHint::kTrue,
+                    gasm_.Int32Constant(0));
+  }
+
   Node* type_info = gasm_.LoadWasmTypeInfo(map);
   DCHECK_GE(rtt_depth, 0);
   // If the depth of the rtt is known to be less that the minimum supertype
@@ -146,16 +171,30 @@
   Node* effect_input = NodeProperties::GetEffectInput(node);
   Node* control_input = NodeProperties::GetControlInput(node);
   auto config = OpParameter<WasmTypeCheckConfig>(node->op());
-  int rtt_depth = config.rtt_depth;
-  bool object_can_be_null = config.object_can_be_null;
+  int rtt_depth = wasm::GetSubtypingDepth(module_, config.to.ref_index());
+  bool object_can_be_null = config.from.is_nullable();
+  bool object_can_be_i31 =
+      wasm::IsSubtypeOf(wasm::kWasmI31Ref.AsNonNull(), config.from, module_);
 
   gasm_.InitializeEffectControl(effect_input, control_input);
 
   auto end_label = gasm_.MakeLabel();
+  bool is_cast_from_any = config.from.is_reference_to(wasm::HeapType::kAny);
 
-  if (object_can_be_null) {
-    gasm_.GotoIf(gasm_.TaggedEqual(object, Null()), &end_label,
-                 BranchHint::kFalse);
+  // Skip the null check if casting from any and if null results in check
+  // failure. In that case the instance type check will identify null as not
+  // being a wasm object and trap.
+  if (object_can_be_null && (!is_cast_from_any || config.to.is_nullable())) {
+    Node* is_null = IsNull(object);
+    if (config.to.is_nullable()) {
+      gasm_.GotoIf(is_null, &end_label, BranchHint::kFalse);
+    } else if (!v8_flags.experimental_wasm_skip_null_checks) {
+      gasm_.TrapIf(is_null, TrapId::kTrapIllegalCast);
+    }
+  }
+
+  if (object_can_be_i31) {
+    gasm_.TrapIf(gasm_.IsI31(object), TrapId::kTrapIllegalCast);
   }
 
   Node* map = gasm_.LoadMap(object);
@@ -164,6 +203,12 @@
   // speedups.
   gasm_.GotoIf(gasm_.TaggedEqual(map, rtt), &end_label, BranchHint::kTrue);
 
+  // Check if map instance type identifies a wasm object.
+  if (is_cast_from_any) {
+    Node* is_wasm_obj = gasm_.IsDataRefMap(map);
+    gasm_.TrapUnless(is_wasm_obj, TrapId::kTrapIllegalCast);
+  }
+
   Node* type_info = gasm_.LoadWasmTypeInfo(map);
   DCHECK_GE(rtt_depth, 0);
   // If the depth of the rtt is known to be less that the minimum supertype
@@ -202,7 +247,9 @@
   Node* control = NodeProperties::GetControlInput(node);
   Node* object = NodeProperties::GetValueInput(node, 0);
   gasm_.InitializeEffectControl(effect, control);
-  gasm_.TrapIf(gasm_.TaggedEqual(object, Null()), TrapId::kTrapNullDereference);
+  if (!v8_flags.experimental_wasm_skip_null_checks) {
+    gasm_.TrapIf(IsNull(object), TrapId::kTrapNullDereference);
+  }
 
   ReplaceWithValue(node, object, gasm_.effect(), gasm_.control());
   node->Kill();
@@ -217,14 +264,13 @@
 Reduction WasmGCLowering::ReduceIsNull(Node* node) {
   DCHECK_EQ(node->opcode(), IrOpcode::kIsNull);
   Node* object = NodeProperties::GetValueInput(node, 0);
-  return Replace(gasm_.TaggedEqual(object, Null()));
+  return Replace(IsNull(object));
 }
 
 Reduction WasmGCLowering::ReduceIsNotNull(Node* node) {
   DCHECK_EQ(node->opcode(), IrOpcode::kIsNotNull);
   Node* object = NodeProperties::GetValueInput(node, 0);
-  return Replace(gasm_.Word32Equal(gasm_.TaggedEqual(object, Null()),
-                                   gasm_.Int32Constant(0)));
+  return Replace(gasm_.Word32Equal(IsNull(object), gasm_.Int32Constant(0)));
 }
 
 Reduction WasmGCLowering::ReduceRttCanon(Node* node) {
@@ -248,66 +294,20 @@
 
 Reduction WasmGCLowering::ReduceWasmExternInternalize(Node* node) {
   DCHECK_EQ(node->opcode(), IrOpcode::kWasmExternInternalize);
-  Node* effect = NodeProperties::GetEffectInput(node);
-  Node* control = NodeProperties::GetControlInput(node);
   Node* object = NodeProperties::GetValueInput(node, 0);
-  gasm_.InitializeEffectControl(effect, control);
-  auto end = gasm_.MakeLabel(MachineRepresentation::kTaggedPointer);
-
-  if (!v8_flags.wasm_gc_js_interop) {
-    Node* context = gasm_.LoadImmutable(
-        MachineType::TaggedPointer(), instance_node_,
-        WasmInstanceObject::kNativeContextOffset - kHeapObjectTag);
-    Node* obj = gasm_.CallBuiltin(
-        Builtin::kWasmGetOwnProperty, Operator::kEliminatable, object,
-        RootNode(RootIndex::kwasm_wrapped_object_symbol), context);
-    // Invalid object wrappers (i.e. any other JS object that doesn't have the
-    // magic hidden property) will return {undefined}. Map that to {object}.
-    Node* is_undefined =
-        gasm_.TaggedEqual(obj, RootNode(RootIndex::kUndefinedValue));
-    gasm_.GotoIf(is_undefined, &end, object);
-    gasm_.Goto(&end, obj);
-  } else {
-    gasm_.Goto(&end, object);
-  }
-  gasm_.Bind(&end);
-  Node* replacement = end.PhiAt(0);
-  ReplaceWithValue(node, replacement, gasm_.effect(), gasm_.control());
+  // TODO(7748): Canonicalize HeapNumbers.
+  ReplaceWithValue(node, object);
   node->Kill();
-  return Replace(replacement);
+  return Replace(object);
 }
 
+// TODO(7748): WasmExternExternalize is a no-op. Consider removing it.
 Reduction WasmGCLowering::ReduceWasmExternExternalize(Node* node) {
   DCHECK_EQ(node->opcode(), IrOpcode::kWasmExternExternalize);
-  Node* effect = NodeProperties::GetEffectInput(node);
-  Node* control = NodeProperties::GetControlInput(node);
   Node* object = NodeProperties::GetValueInput(node, 0);
-  gasm_.InitializeEffectControl(effect, control);
-
-  auto end = gasm_.MakeLabel(MachineRepresentation::kTaggedPointer);
-  if (!v8_flags.wasm_gc_js_interop) {
-    auto wrap = gasm_.MakeLabel();
-    gasm_.GotoIf(gasm_.IsI31(object), &end, object);
-    gasm_.GotoIf(gasm_.IsDataRefMap(gasm_.LoadMap(object)), &wrap);
-    // This includes the case where {node == null}.
-    gasm_.Goto(&end, object);
-
-    gasm_.Bind(&wrap);
-    Node* context = gasm_.LoadImmutable(
-        MachineType::TaggedPointer(), instance_node_,
-        WasmInstanceObject::kNativeContextOffset - kHeapObjectTag);
-    Node* wrapped = gasm_.CallBuiltin(Builtin::kWasmAllocateObjectWrapper,
-                                      Operator::kEliminatable, object, context);
-    gasm_.Goto(&end, wrapped);
-  } else {
-    gasm_.Goto(&end, object);
-  }
-
-  gasm_.Bind(&end);
-  Node* replacement = end.PhiAt(0);
-  ReplaceWithValue(node, replacement, gasm_.effect(), gasm_.control());
+  ReplaceWithValue(node, object);
   node->Kill();
-  return Replace(replacement);
+  return Replace(object);
 }
 
 }  // namespace compiler
diff -r -u --color up/v8/src/compiler/wasm-gc-lowering.h nw/v8/src/compiler/wasm-gc-lowering.h
--- up/v8/src/compiler/wasm-gc-lowering.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/wasm-gc-lowering.h	2023-01-19 16:46:36.146442922 -0500
@@ -21,7 +21,8 @@
 
 class WasmGCLowering final : public AdvancedReducer {
  public:
-  WasmGCLowering(Editor* editor, MachineGraph* mcgraph);
+  WasmGCLowering(Editor* editor, MachineGraph* mcgraph,
+                 const wasm::WasmModule* module);
 
   const char* reducer_name() const override { return "WasmGCLowering"; }
 
@@ -40,7 +41,9 @@
   Reduction ReduceWasmExternExternalize(Node* node);
   Node* RootNode(RootIndex index);
   Node* Null();
+  Node* IsNull(Node* object);
   WasmGraphAssembler gasm_;
+  const wasm::WasmModule* module_;
   Node* dead_;
   Node* instance_node_;
 };
diff -r -u --color up/v8/src/compiler/wasm-gc-operator-reducer.cc nw/v8/src/compiler/wasm-gc-operator-reducer.cc
--- up/v8/src/compiler/wasm-gc-operator-reducer.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/wasm-gc-operator-reducer.cc	2023-01-19 16:46:36.146442922 -0500
@@ -135,14 +135,13 @@
       wasm::TypeInModule object_type = ObjectTypeFromContext(object, branch);
       if (object_type.type.is_bottom()) return NoChange();
 
-      Node* rtt = NodeProperties::GetValueInput(condition_node, 1);
-      wasm::ValueType rtt_type = wasm::ValueType::RefNull(
-          NodeProperties::GetType(rtt).AsWasm().type.ref_index());
+      wasm::ValueType to_type =
+          OpParameter<WasmTypeCheckConfig>(condition_node->op()).to;
 
-      // TODO(manoskouk): Think about {module_} below if we have cross-module
+      // TODO(12166): Think about {module_} below if we have cross-module
       // inlining.
       wasm::TypeInModule new_type =
-          wasm::Intersection(object_type, {rtt_type, module_});
+          wasm::Intersection(object_type, {to_type, module_});
       return UpdateNodeAndAliasesTypes(node, parent_state, object, new_type,
                                        true);
     }
@@ -253,14 +252,21 @@
   if (object_type.type.is_bottom()) return NoChange();
   if (InDeadBranch(rtt)) return NoChange();
   wasm::TypeInModule rtt_type = NodeProperties::GetType(rtt).AsWasm();
+  bool to_nullable =
+      OpParameter<WasmTypeCheckConfig>(node->op()).to.is_nullable();
 
   if (wasm::IsHeapSubtypeOf(object_type.type.heap_type(),
                             wasm::HeapType(rtt_type.type.ref_index()),
                             object_type.module, rtt_type.module)) {
-    // Type cast will always succeed. Remove it.
-    ReplaceWithValue(node, object);
-    node->Kill();
-    return Replace(object);
+    if (to_nullable) {
+      // Type cast will always succeed. Remove it.
+      ReplaceWithValue(node, object);
+      node->Kill();
+      return Replace(object);
+    } else {
+      gasm_.InitializeEffectControl(effect, control);
+      return Replace(gasm_.AssertNotNull(object));
+    }
   }
 
   if (wasm::HeapTypesUnrelated(object_type.type.heap_type(),
@@ -269,7 +275,7 @@
     gasm_.InitializeEffectControl(effect, control);
     // A cast between unrelated types can only succeed if the argument is null.
     // Otherwise, it always fails.
-    Node* non_trapping_condition = object_type.type.is_nullable()
+    Node* non_trapping_condition = object_type.type.is_nullable() && to_nullable
                                        ? gasm_.IsNull(object)
                                        : gasm_.Int32Constant(0);
     gasm_.TrapUnless(SetType(non_trapping_condition, wasm::kWasmI32),
@@ -280,17 +286,14 @@
     return Replace(null_node);
   }
 
-  // Remove the null check from the cast if able.
-  if (!object_type.type.is_nullable()) {
-    uint8_t rtt_depth = OpParameter<WasmTypeCheckConfig>(node->op()).rtt_depth;
-    NodeProperties::ChangeOp(
-        node, gasm_.simplified()->WasmTypeCast({false,  // object_can_be_null
-                                                false,  // null_succeeds
-                                                rtt_depth}));
-  }
+  // TODO(12166): Think about modules below if we have cross-module inlining.
+
+  // Update the from-type in the type cast.
+  WasmTypeCheckConfig current_config =
+      OpParameter<WasmTypeCheckConfig>(node->op());
+  NodeProperties::ChangeOp(node, gasm_.simplified()->WasmTypeCast(
+                                     {object_type.type, current_config.to}));
 
-  // TODO(manoskouk): Think about {module_} below if we have cross-module
-  // inlining.
   wasm::TypeInModule new_type = wasm::Intersection(
       object_type,
       {wasm::ValueType::RefNull(rtt_type.type.ref_index()), module_});
@@ -315,7 +318,7 @@
                             wasm::HeapType(rtt_type.type.ref_index()),
                             object_type.module, rtt_type.module)) {
     bool null_succeeds =
-        OpParameter<WasmTypeCheckConfig>(node->op()).null_succeeds;
+        OpParameter<WasmTypeCheckConfig>(node->op()).to.is_nullable();
     // Type cast will fail only on null.
     gasm_.InitializeEffectControl(effect, control);
     Node* condition = SetType(object_type.type.is_nullable() && !null_succeeds
@@ -331,7 +334,7 @@
                                wasm::HeapType(rtt_type.type.ref_index()),
                                object_type.module, rtt_type.module)) {
     bool null_succeeds =
-        OpParameter<WasmTypeCheckConfig>(node->op()).null_succeeds;
+        OpParameter<WasmTypeCheckConfig>(node->op()).to.is_nullable();
     Node* condition = nullptr;
     if (null_succeeds && object_type.type.is_nullable()) {
       // The cast only succeeds in case of null.
@@ -346,14 +349,13 @@
     return Replace(condition);
   }
 
-  // Remove the null check from the typecheck if able.
-  if (!object_type.type.is_nullable()) {
-    uint8_t rtt_depth = OpParameter<WasmTypeCheckConfig>(node->op()).rtt_depth;
-    NodeProperties::ChangeOp(
-        node, gasm_.simplified()->WasmTypeCheck({false,  // object_can_be_null
-                                                 false,  // null_succeeds
-                                                 rtt_depth}));
-  }
+  // TODO(12166): Think about modules below if we have cross-module inlining.
+
+  // Update the from-type in the type cast.
+  WasmTypeCheckConfig current_config =
+      OpParameter<WasmTypeCheckConfig>(node->op());
+  NodeProperties::ChangeOp(node, gasm_.simplified()->WasmTypeCheck(
+                                     {object_type.type, current_config.to}));
 
   return TakeStatesFromFirstControl(node);
 }
diff -r -u --color up/v8/src/compiler/wasm-graph-assembler.h nw/v8/src/compiler/wasm-graph-assembler.h
--- up/v8/src/compiler/wasm-graph-assembler.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/wasm-graph-assembler.h	2023-01-19 16:46:36.146442922 -0500
@@ -41,11 +41,12 @@
 class WasmGraphAssembler : public GraphAssembler {
  public:
   WasmGraphAssembler(MachineGraph* mcgraph, Zone* zone)
-      : GraphAssembler(mcgraph, zone), simplified_(zone) {}
+      : GraphAssembler(mcgraph, zone, BranchSemantics::kMachine),
+        simplified_(zone) {}
 
   template <typename... Args>
   Node* CallRuntimeStub(wasm::WasmCode::RuntimeStubId stub_id,
-                        Operator::Properties properties, Args*... args) {
+                        Operator::Properties properties, Args... args) {
     auto* call_descriptor = GetBuiltinCallDescriptor(
         WasmRuntimeStubIdToBuiltinName(stub_id), temp_zone(),
         StubCallMode::kCallWasmRuntimeStub, false, properties);
@@ -63,7 +64,7 @@
 
   template <typename... Args>
   Node* CallBuiltin(Builtin name, Operator::Properties properties,
-                    Args*... args) {
+                    Args... args) {
     auto* call_descriptor = GetBuiltinCallDescriptor(
         name, temp_zone(), StubCallMode::kCallBuiltinPointer, false,
         properties);
@@ -268,7 +269,7 @@
                              effect(), control()));
   }
 
-  SimplifiedOperatorBuilder* simplified() { return &simplified_; }
+  SimplifiedOperatorBuilder* simplified() override { return &simplified_; }
 
  private:
   SimplifiedOperatorBuilder simplified_;
diff -r -u --color up/v8/src/compiler/wasm-inlining.cc nw/v8/src/compiler/wasm-inlining.cc
--- up/v8/src/compiler/wasm-inlining.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/wasm-inlining.cc	2023-01-19 16:46:36.146442922 -0500
@@ -179,9 +179,27 @@
     size_t subgraph_min_node_id = graph()->NodeCount();
     Node* inlinee_start;
     Node* inlinee_end;
-    const wasm::FunctionBody inlinee_body(inlinee->sig, inlinee->code.offset(),
+    const wasm::FunctionBody inlinee_body{inlinee->sig, inlinee->code.offset(),
                                           function_bytes.begin(),
-                                          function_bytes.end());
+                                          function_bytes.end()};
+
+    // If the inlinee was not validated before, do that now.
+    if (!module()->function_was_validated(candidate.inlinee_index)) {
+      wasm::WasmFeatures unused_detected_features;
+      if (ValidateFunctionBody(zone()->allocator(), env_->enabled_features,
+                               module(), &unused_detected_features,
+                               inlinee_body)
+              .failed()) {
+        Trace(candidate, "function is invalid");
+        // At this point we cannot easily raise a compilation error any more.
+        // Since this situation is highly unlikely though, we just ignore this
+        // inlinee and move on. The same validation error will be triggered
+        // again when actually compiling the invalid function.
+        continue;
+      }
+      module()->set_function_validated(candidate.inlinee_index);
+    }
+
     WasmGraphBuilder builder(env_, zone(), mcgraph_, inlinee_body.sig,
                              source_positions_);
     {
@@ -193,15 +211,10 @@
           NodeProperties::IsExceptionalCall(call)
               ? wasm::kInlinedHandledCall
               : wasm::kInlinedNonHandledCall);
-      if (result.ok()) {
-        builder.LowerInt64(WasmGraphBuilder::kCalledFromWasm);
-        inlinee_start = graph()->start();
-        inlinee_end = graph()->end();
-      } else {
-        // Otherwise report failure.
-        Trace(candidate, "failed to compile");
-        return;
-      }
+      CHECK(result.ok());
+      builder.LowerInt64(WasmGraphBuilder::kCalledFromWasm);
+      inlinee_start = graph()->start();
+      inlinee_end = graph()->end();
     }
 
     size_t additional_nodes = graph()->NodeCount() - subgraph_min_node_id;
@@ -364,11 +377,21 @@
         // The first input of a return node is always the 0 constant.
         return_inputs.push_back(graph()->NewNode(common()->Int32Constant(0)));
         if (return_arity == 1) {
+          // Tail calls are untyped; we have to type the node here.
+          NodeProperties::SetType(
+              input, Type::Wasm({inlinee_sig->GetReturn(0), module()},
+                                graph()->zone()));
           return_inputs.push_back(input);
         } else if (return_arity > 1) {
           for (int i = 0; i < return_arity; i++) {
-            return_inputs.push_back(
-                graph()->NewNode(common()->Projection(i), input, input));
+            Node* ith_projection =
+                graph()->NewNode(common()->Projection(i), input, input);
+            // Similarly here we have to type the call's projections.
+            NodeProperties::SetType(
+                ith_projection,
+                Type::Wasm({inlinee_sig->GetReturn(i), module()},
+                           graph()->zone()));
+            return_inputs.push_back(ith_projection);
           }
         }
 
diff -r -u --color up/v8/src/compiler/wasm-typer.cc nw/v8/src/compiler/wasm-typer.cc
--- up/v8/src/compiler/wasm-typer.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/compiler/wasm-typer.cc	2023-01-19 16:46:36.146442922 -0500
@@ -9,6 +9,7 @@
 #include "src/compiler/node-matchers.h"
 #include "src/compiler/node-properties.h"
 #include "src/compiler/opcodes.h"
+#include "src/compiler/wasm-compiler-definitions.h"
 #include "src/utils/utils.h"
 #include "src/wasm/object-access.h"
 #include "src/wasm/wasm-objects.h"
@@ -86,13 +87,10 @@
       TypeInModule object_type =
           NodeProperties::GetType(NodeProperties::GetValueInput(node, 0))
               .AsWasm();
-      TypeInModule rtt_type =
-          NodeProperties::GetType(NodeProperties::GetValueInput(node, 1))
-              .AsWasm();
-      wasm::ValueType to_type =
-          wasm::ValueType::RefNull(rtt_type.type.ref_index());
-      computed_type = wasm::Intersection(object_type.type, to_type,
-                                         object_type.module, rtt_type.module);
+      wasm::ValueType to_type = OpParameter<WasmTypeCheckConfig>(node->op()).to;
+      // TODO(12166): Change module parameters if we have cross-module inlining.
+      computed_type = wasm::Intersection(
+          object_type.type, to_type, object_type.module, object_type.module);
       break;
     }
     case IrOpcode::kAssertNotNull: {
@@ -126,6 +124,9 @@
           object = initial_object;
           control = previous_control;
           effect = previous_effect;
+          // We untype the node, because its new input might have a type not
+          // compatible with its current type.
+          NodeProperties::RemoveType(node);
         }
       }
 
@@ -190,7 +191,7 @@
       if (!NodeProperties::IsTyped(object)) return NoChange();
       TypeInModule object_type = NodeProperties::GetType(object).AsWasm();
       // This can happen in unreachable branches.
-      if (object_type.type.is_bottom()) {
+      if (object_type.type.is_bottom() || object_type.type.is_uninhabited()) {
         computed_type = {wasm::kWasmBottom, object_type.module};
         break;
       }
diff -r -u --color up/v8/src/d8/d8-platforms.h nw/v8/src/d8/d8-platforms.h
--- up/v8/src/d8/d8-platforms.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/d8/d8-platforms.h	2023-01-19 16:46:36.146442922 -0500
@@ -14,7 +14,7 @@
 class Platform;
 
 // Returns a predictable v8::Platform implementation.
-// orker threads are disabled, idle tasks are disallowed, and the time reported
+// Worker threads are disabled, idle tasks are disallowed, and the time reported
 // by {MonotonicallyIncreasingTime} is deterministic.
 std::unique_ptr<Platform> MakePredictablePlatform(
     std::unique_ptr<Platform> platform);
diff -r -u --color up/v8/src/d8/d8-test.cc nw/v8/src/d8/d8-test.cc
--- up/v8/src/d8/d8-test.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/d8/d8-test.cc	2023-01-19 16:46:36.146442922 -0500
@@ -471,6 +471,19 @@
   }
 
 #ifdef V8_USE_SIMULATOR_WITH_GENERIC_C_CALLS
+  static AnyCType AddAll32BitIntFastCallback_8ArgsPatch(
+      AnyCType receiver, AnyCType should_fallback, AnyCType arg1_i32,
+      AnyCType arg2_i32, AnyCType arg3_i32, AnyCType arg4_u32,
+      AnyCType arg5_u32, AnyCType arg6_u32, AnyCType arg7_u32,
+      AnyCType arg8_u32, AnyCType options) {
+    AnyCType ret;
+    ret.int32_value = AddAll32BitIntFastCallback_8Args(
+        receiver.object_value, should_fallback.bool_value, arg1_i32.int32_value,
+        arg2_i32.int32_value, arg3_i32.int32_value, arg4_u32.uint32_value,
+        arg5_u32.uint32_value, arg6_u32.uint32_value, arg7_u32.uint32_value,
+        arg8_u32.uint32_value, *options.options_value);
+    return ret;
+  }
   static AnyCType AddAll32BitIntFastCallback_6ArgsPatch(
       AnyCType receiver, AnyCType should_fallback, AnyCType arg1_i32,
       AnyCType arg2_i32, AnyCType arg3_i32, AnyCType arg4_u32,
@@ -494,6 +507,26 @@
   }
 #endif  //  V8_USE_SIMULATOR_WITH_GENERIC_C_CALLS
 
+  static int AddAll32BitIntFastCallback_8Args(
+      Local<Object> receiver, bool should_fallback, int32_t arg1_i32,
+      int32_t arg2_i32, int32_t arg3_i32, uint32_t arg4_u32, uint32_t arg5_u32,
+      uint32_t arg6_u32, uint32_t arg7_u32, uint32_t arg8_u32,
+      FastApiCallbackOptions& options) {
+    FastCApiObject* self = UnwrapObject(receiver);
+    CHECK_SELF_OR_FALLBACK(0);
+    self->fast_call_count_++;
+
+    if (should_fallback) {
+      options.fallback = true;
+      return 0;
+    }
+
+    int64_t result = static_cast<int64_t>(arg1_i32) + arg2_i32 + arg3_i32 +
+                     arg4_u32 + arg5_u32 + arg6_u32 + arg7_u32 + arg8_u32;
+    if (result > INT_MAX) return INT_MAX;
+    if (result < INT_MIN) return INT_MIN;
+    return static_cast<int>(result);
+  }
   static int AddAll32BitIntFastCallback_6Args(
       Local<Object> receiver, bool should_fallback, int32_t arg1_i32,
       int32_t arg2_i32, int32_t arg3_i32, uint32_t arg4_u32, uint32_t arg5_u32,
@@ -531,24 +564,29 @@
 
     HandleScope handle_scope(isolate);
 
+    Local<Context> context = isolate->GetCurrentContext();
     double sum = 0;
     if (args.Length() > 1 && args[1]->IsNumber()) {
-      sum += args[1]->Int32Value(isolate->GetCurrentContext()).FromJust();
+      sum += args[1]->Int32Value(context).FromJust();
     }
     if (args.Length() > 2 && args[2]->IsNumber()) {
-      sum += args[2]->Int32Value(isolate->GetCurrentContext()).FromJust();
+      sum += args[2]->Int32Value(context).FromJust();
     }
     if (args.Length() > 3 && args[3]->IsNumber()) {
-      sum += args[3]->Int32Value(isolate->GetCurrentContext()).FromJust();
+      sum += args[3]->Int32Value(context).FromJust();
     }
     if (args.Length() > 4 && args[4]->IsNumber()) {
-      sum += args[4]->Uint32Value(isolate->GetCurrentContext()).FromJust();
+      sum += args[4]->Uint32Value(context).FromJust();
     }
     if (args.Length() > 5 && args[5]->IsNumber()) {
-      sum += args[5]->Uint32Value(isolate->GetCurrentContext()).FromJust();
+      sum += args[5]->Uint32Value(context).FromJust();
     }
     if (args.Length() > 6 && args[6]->IsNumber()) {
-      sum += args[6]->Uint32Value(isolate->GetCurrentContext()).FromJust();
+      sum += args[6]->Uint32Value(context).FromJust();
+    }
+    if (args.Length() > 7 && args[7]->IsNumber() && args[8]->IsNumber()) {
+      sum += args[7]->Uint32Value(context).FromJust();
+      sum += args[8]->Uint32Value(context).FromJust();
     }
 
     args.GetReturnValue().Set(Number::New(isolate, sum));
@@ -758,6 +796,9 @@
   template <typename IntegerT>
   static double ClampCompareCompute(bool in_range, double real_arg,
                                     IntegerT checked_arg) {
+    if (i::v8_flags.fuzzing) {
+      return static_cast<double>(checked_arg);
+    }
     if (!in_range) {
       IntegerT lower_bound = std::numeric_limits<IntegerT>::min();
       IntegerT upper_bound = std::numeric_limits<IntegerT>::max();
@@ -847,8 +888,17 @@
       if (std::isnan(checked_arg_dbl)) {
         clamped = 0;
       } else {
-        clamped = std::clamp(checked_arg, std::numeric_limits<IntegerT>::min(),
-                             std::numeric_limits<IntegerT>::max());
+        IntegerT lower_bound = std::numeric_limits<IntegerT>::min();
+        IntegerT upper_bound = std::numeric_limits<IntegerT>::max();
+        if (lower_bound < internal::kMinSafeInteger) {
+          lower_bound = static_cast<IntegerT>(internal::kMinSafeInteger);
+        }
+        if (upper_bound > internal::kMaxSafeInteger) {
+          upper_bound = static_cast<IntegerT>(internal::kMaxSafeInteger);
+        }
+
+        clamped = std::clamp(real_arg, static_cast<double>(lower_bound),
+                             static_cast<double>(upper_bound));
       }
       args.GetReturnValue().Set(Number::New(isolate, clamped));
     }
@@ -1160,6 +1210,9 @@
             signature, 1, ConstructorBehavior::kThrow,
             SideEffectType::kHasSideEffect, {add_all_invalid_overloads, 2}));
 
+    CFunction add_all_32bit_int_8args_c_func = CFunction::Make(
+        FastCApiObject::AddAll32BitIntFastCallback_8Args V8_IF_USE_SIMULATOR(
+            FastCApiObject::AddAll32BitIntFastCallback_8ArgsPatch));
     CFunction add_all_32bit_int_6args_c_func = CFunction::Make(
         FastCApiObject::AddAll32BitIntFastCallback_6Args V8_IF_USE_SIMULATOR(
             FastCApiObject::AddAll32BitIntFastCallback_6ArgsPatch));
@@ -1177,6 +1230,13 @@
             SideEffectType::kHasSideEffect, {c_function_overloads, 2}));
 
     api_obj_ctor->PrototypeTemplate()->Set(
+        isolate, "overloaded_add_all_8args",
+        FunctionTemplate::New(
+            isolate, FastCApiObject::AddAll32BitIntSlowCallback, Local<Value>(),
+            signature, 1, ConstructorBehavior::kThrow,
+            SideEffectType::kHasSideEffect, &add_all_32bit_int_8args_c_func));
+
+    api_obj_ctor->PrototypeTemplate()->Set(
         isolate, "overloaded_add_all_32bit_int_no_sig",
         FunctionTemplate::NewWithCFunctionOverloads(
             isolate, FastCApiObject::AddAll32BitIntSlowCallback, Local<Value>(),
diff -r -u --color up/v8/src/d8/d8.cc nw/v8/src/d8/d8.cc
--- up/v8/src/d8/d8.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/d8/d8.cc	2023-01-19 16:46:36.157276252 -0500
@@ -113,8 +113,6 @@
 
 namespace {
 
-const int kMB = 1024 * 1024;
-
 #ifdef V8_FUZZILLI
 // REPRL = read-eval-print-reset-loop
 // These file descriptors are being opened when Fuzzilli uses fork & execve to
@@ -128,9 +126,6 @@
 bool fuzzilli_reprl = false;
 #endif  // V8_FUZZILLI
 
-const int kMaxSerializerMemoryUsage =
-    1 * kMB;  // Arbitrary maximum for testing.
-
 // Base class for shell ArrayBuffer allocators. It forwards all opertions to
 // the default v8 allocator.
 class ArrayBufferAllocatorBase : public v8::ArrayBuffer::Allocator {
@@ -210,7 +205,7 @@
 
  private:
   size_t Adjust(size_t length) {
-    const size_t kAllocationLimit = 10 * kMB;
+    const size_t kAllocationLimit = 10 * i::MB;
     return length > kAllocationLimit ? i::AllocatePageSize() : length;
   }
 };
@@ -381,7 +376,7 @@
   // which is not enough to parse the big literal expressions used in tests.
   // The stack size should be at least StackGuard::kLimitSize + some
   // OS-specific padding for thread startup code.  2Mbytes seems to be enough.
-  return base::Thread::Options(name, 2 * kMB);
+  return base::Thread::Options(name, 2 * i::MB);
 }
 
 }  // namespace
@@ -1759,7 +1754,7 @@
     return delta.InMillisecondsF();
   }
 }
-int64_t Shell::GetTracingTimestampFromPerformanceTimestamp(
+uint64_t Shell::GetTracingTimestampFromPerformanceTimestamp(
     double performance_timestamp) {
   // Don't use this in --verify-predictable mode, predictable timestamps don't
   // work well with tracing.
@@ -1767,7 +1762,9 @@
   base::TimeDelta delta =
       base::TimeDelta::FromMillisecondsD(performance_timestamp);
   // See TracingController::CurrentTimestampMicroseconds().
-  return (delta + kInitialTicks).ToInternalValue();
+  int64_t internal_value = (delta + kInitialTicks).ToInternalValue();
+  DCHECK(internal_value >= 0);
+  return internal_value;
 }
 
 // performance.now() returns GetTimestamp().
@@ -3954,7 +3951,7 @@
 V8_NOINLINE void FuzzerMonitor::UndefinedBehavior() {
   // Caught by UBSAN.
   int32_t val = -1;
-  USE(val << 8);
+  USE(val << val);
 }
 
 V8_NOINLINE void FuzzerMonitor::UseAfterFree() {
@@ -4958,6 +4955,10 @@
     } else if (strncmp(argv[i], "--repeat-compile=", 17) == 0) {
       options.repeat_compile = atoi(argv[i] + 17);
       argv[i] = nullptr;
+    } else if (strncmp(argv[i], "--max-serializer-memory=", 24) == 0) {
+      // Value is expressed in MB.
+      options.max_serializer_memory = atoi(argv[i] + 24) * i::MB;
+      argv[i] = nullptr;
 #ifdef V8_FUZZILLI
     } else if (strcmp(argv[i], "--no-fuzzilli-enable-builtins-coverage") == 0) {
       options.fuzzilli_enable_builtins_coverage = false;
@@ -5370,7 +5371,9 @@
     // Not accurate, because we don't take into account reallocated buffers,
     // but this is fine for testing.
     current_memory_usage_ += size;
-    if (current_memory_usage_ > kMaxSerializerMemoryUsage) return nullptr;
+    if (current_memory_usage_ > Shell::options.max_serializer_memory) {
+      return nullptr;
+    }
 
     void* result = base::Realloc(old_buffer, size);
     *actual_size = result ? size : 0;
@@ -5436,7 +5439,9 @@
 
       auto backing_store = array_buffer->GetBackingStore();
       data_->backing_stores_.push_back(std::move(backing_store));
-      array_buffer->Detach();
+      if (array_buffer->Detach(v8::Local<v8::Value>()).IsNothing()) {
+        return Nothing<bool>();
+      }
     }
 
     return Just(true);
diff -r -u --color up/v8/src/d8/d8.h nw/v8/src/d8/d8.h
--- up/v8/src/d8/d8.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/d8/d8.h	2023-01-19 16:46:36.157276252 -0500
@@ -486,6 +486,8 @@
       "throw-on-failed-access-check", false};
   DisallowReassignment<bool> noop_on_failed_access_check = {
       "noop-on-failed-access-check", false};
+  DisallowReassignment<size_t> max_serializer_memory = {"max-serializer-memory",
+                                                        1 * i::MB};
 };
 
 class Shell : public i::AllStatic {
@@ -541,7 +543,7 @@
   static void MapCounters(v8::Isolate* isolate, const char* name);
 
   static double GetTimestamp();
-  static int64_t GetTracingTimestampFromPerformanceTimestamp(
+  static uint64_t GetTracingTimestampFromPerformanceTimestamp(
       double performance_timestamp);
 
   static void PerformanceNow(const v8::FunctionCallbackInfo<v8::Value>& args);
diff -r -u --color up/v8/src/debug/debug-evaluate.cc nw/v8/src/debug/debug-evaluate.cc
--- up/v8/src/debug/debug-evaluate.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/debug/debug-evaluate.cc	2023-01-19 16:46:36.157276252 -0500
@@ -274,7 +274,11 @@
     scope_info->SetIsDebugEvaluateScope();
 
     if (v8_flags.experimental_reuse_locals_blocklists) {
-      if (rit == context_chain_.rbegin()) {
+      // In the case where the "paused function scope" is the script scope
+      // itself, we don't need (and don't have) a blocklist.
+      const bool paused_scope_is_script_scope =
+          scope_iterator_.Done() || scope_iterator_.InInnerScope();
+      if (rit == context_chain_.rbegin() && !paused_scope_is_script_scope) {
         // The DebugEvaluateContext we create for the closure scope is the only
         // DebugEvaluateContext with a block list. This means we'll retrieve
         // the existing block list from the paused function scope
diff -r -u --color up/v8/src/debug/debug-frames.h nw/v8/src/debug/debug-frames.h
--- up/v8/src/debug/debug-frames.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/debug/debug-frames.h	2023-01-19 16:46:36.157276252 -0500
@@ -19,7 +19,7 @@
 class CommonFrame;
 class WasmFrame;
 
-class FrameInspector {
+class V8_EXPORT_PRIVATE FrameInspector {
  public:
   FrameInspector(CommonFrame* frame, int inlined_frame_index, Isolate* isolate);
   FrameInspector(const FrameInspector&) = delete;
diff -r -u --color up/v8/src/debug/debug-interface.cc nw/v8/src/debug/debug-interface.cc
--- up/v8/src/debug/debug-interface.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/debug/debug-interface.cc	2023-01-19 16:46:36.157276252 -0500
@@ -340,10 +340,12 @@
 void ChangeBreakOnException(Isolate* isolate, ExceptionBreakState type) {
   i::Isolate* i_isolate = reinterpret_cast<i::Isolate*>(isolate);
   DCHECK_NO_SCRIPT_NO_EXCEPTION(i_isolate);
-  i_isolate->debug()->ChangeBreakOnException(i::BreakException,
-                                             type == BreakOnAnyException);
-  i_isolate->debug()->ChangeBreakOnException(i::BreakUncaughtException,
-                                             type != NoBreakOnException);
+  i_isolate->debug()->ChangeBreakOnException(
+      i::BreakCaughtException,
+      type == BreakOnCaughtException || type == BreakOnAnyException);
+  i_isolate->debug()->ChangeBreakOnException(
+      i::BreakUncaughtException,
+      type == BreakOnUncaughtException || type == BreakOnAnyException);
 }
 
 void SetBreakPointsActive(Isolate* v8_isolate, bool is_active) {
@@ -1387,10 +1389,6 @@
       i::Handle<i::JSMessageObject>::cast(maybeMessage));
 }
 
-bool isExperimentalAsyncStackTaggingApiEnabled() {
-  return i::v8_flags.experimental_async_stack_tagging_api;
-}
-
 bool isExperimentalRemoveInternalScopesPropertyEnabled() {
   return i::v8_flags.experimental_remove_internal_scopes_property;
 }
@@ -1400,10 +1398,15 @@
   isolate->CountUsage(v8::Isolate::kAsyncStackTaggingCreateTaskCall);
 }
 
+void NotifyDebuggerPausedEventSent(v8::Isolate* v8_isolate) {
+  i::Isolate* isolate = reinterpret_cast<i::Isolate*>(v8_isolate);
+  isolate->debug()->NotifyDebuggerPausedEventSent();
+}
+
 std::unique_ptr<PropertyIterator> PropertyIterator::Create(
     Local<Context> context, Local<Object> object, bool skip_indices) {
   internal::Isolate* isolate =
-      reinterpret_cast<i::Isolate*>(object->GetIsolate());
+      reinterpret_cast<i::Isolate*>(context->GetIsolate());
   if (isolate->is_execution_terminating()) {
     return nullptr;
   }
diff -r -u --color up/v8/src/debug/debug-interface.h nw/v8/src/debug/debug-interface.h
--- up/v8/src/debug/debug-interface.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/debug/debug-interface.h	2023-01-19 16:46:36.157276252 -0500
@@ -99,8 +99,9 @@
 
 enum ExceptionBreakState {
   NoBreakOnException = 0,
-  BreakOnUncaughtException = 1,
-  BreakOnAnyException = 2
+  BreakOnCaughtException = 1,
+  BreakOnUncaughtException = 2,
+  BreakOnAnyException = 3,
 };
 
 /**
@@ -286,9 +287,15 @@
       v8::Local<v8::Context> paused_context,
       const std::vector<debug::BreakpointId>& inspector_break_points_hit,
       base::EnumSet<BreakReason> break_reasons = {}) {}
-  virtual void BreakOnInstrumentation(
+  enum PauseAfterInstrumentation {
+    kPauseAfterInstrumentationRequested,
+    kNoPauseAfterInstrumentationRequested
+  };
+  virtual PauseAfterInstrumentation BreakOnInstrumentation(
       v8::Local<v8::Context> paused_context,
-      const debug::BreakpointId instrumentationId) {}
+      const debug::BreakpointId instrumentationId) {
+    return kNoPauseAfterInstrumentationRequested;
+  }
   virtual void ExceptionThrown(v8::Local<v8::Context> paused_context,
                                v8::Local<v8::Value> exception,
                                v8::Local<v8::Value> promise, bool is_uncaught,
@@ -545,8 +552,9 @@
 
 int GetDebuggingId(v8::Local<v8::Function> function);
 
-bool SetFunctionBreakpoint(v8::Local<v8::Function> function,
-                           v8::Local<v8::String> condition, BreakpointId* id);
+V8_EXPORT_PRIVATE bool SetFunctionBreakpoint(v8::Local<v8::Function> function,
+                                             v8::Local<v8::String> condition,
+                                             BreakpointId* id);
 
 v8::Platform* GetCurrentPlatform();
 
@@ -670,11 +678,12 @@
 
 MaybeLocal<Message> GetMessageFromPromise(Local<Promise> promise);
 
-bool isExperimentalAsyncStackTaggingApiEnabled();
 bool isExperimentalRemoveInternalScopesPropertyEnabled();
 
 void RecordAsyncStackTaggingCreateTaskCall(v8::Isolate* isolate);
 
+void NotifyDebuggerPausedEventSent(v8::Isolate* isolate);
+
 }  // namespace debug
 }  // namespace v8
 
diff -r -u --color up/v8/src/debug/debug-scopes.cc nw/v8/src/debug/debug-scopes.cc
--- up/v8/src/debug/debug-scopes.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/debug/debug-scopes.cc	2023-01-19 16:46:36.157276252 -0500
@@ -87,6 +87,8 @@
   current_scope_ = start_scope_;
   DCHECK_NOT_NULL(current_scope_);
   UnwrapEvaluationContext();
+  seen_script_scope_ = false;
+  calculate_blocklists_ = false;
 }
 
 namespace {
@@ -105,6 +107,7 @@
       : scope_(scope),
         break_scope_start_(function->shared().StartPosition()),
         break_scope_end_(function->shared().EndPosition()),
+        break_scope_type_(function->shared().scope_info().scope_type()),
         position_(position) {
     DCHECK_NOT_NULL(scope);
     RetrieveScopes();
@@ -117,6 +120,7 @@
   DeclarationScope* scope_;
   const int break_scope_start_;
   const int break_scope_end_;
+  const ScopeType break_scope_type_;
   const int position_;
 
   DeclarationScope* closure_scope_ = nullptr;
@@ -137,11 +141,11 @@
 
   bool RetrieveClosureScope(Scope* scope) {
     // The closure scope is the scope that matches exactly the function we
-    // paused in. There is one quirk though, member initializder functions have
-    // the same source position as their class scope, so when looking for the
-    // declaration scope of the member initializer, we need to skip the
-    // corresponding class scope and keep looking.
-    if (!scope->is_class_scope() &&
+    // paused in.
+    // Note that comparing the position alone is not enough and we also need to
+    // match the scope type. E.g. class member initializer have the exact same
+    // scope positions as their class scope.
+    if (break_scope_type_ == scope->scope_type() &&
         break_scope_start_ == scope->start_position() &&
         break_scope_end_ == scope->end_position()) {
       closure_scope_ = scope->AsDeclarationScope();
@@ -267,7 +271,10 @@
   info_ = std::make_unique<ParseInfo>(isolate_, flags, &compile_state,
                                       reusable_compile_state_.get());
 
-  const bool parse_result =
+  bool has_source_code = shared_info->HasSourceCode();
+  bool parse_result = false;
+  if (has_source_code)
+    parse_result =
       flags.is_toplevel()
           ? parsing::ParseProgram(info_.get(), script, maybe_outer_scope,
                                   isolate_, parsing::ReportStatisticsMode::kNo)
@@ -847,7 +854,13 @@
   }
 
   for (Variable* var : *current_scope_->locals()) {
-    if (ScopeInfo::VariableIsSynthetic(*var->name())) continue;
+    if (ScopeInfo::VariableIsSynthetic(*var->name())) {
+      // We want to materialize "new.target" for debug-evaluate.
+      if (mode != Mode::STACK ||
+          !var->name()->Equals(*isolate_->factory()->dot_new_target_string())) {
+        continue;
+      }
+    }
 
     int index = var->index();
     Handle<Object> value;
@@ -868,6 +881,8 @@
               generator_->parameters_and_registers();
           DCHECK_LT(index, parameters_and_registers.length());
           value = handle(parameters_and_registers.get(index), isolate_);
+        } else if (var->IsReceiver()) {
+          value = frame_inspector_->GetReceiver();
         } else {
           value = frame_inspector_->GetParameter(index);
         }
@@ -1221,12 +1236,12 @@
   SharedFunctionInfo::ScriptIterator iterator(isolate_, *script_);
   for (SharedFunctionInfo info = iterator.Next(); !info.is_null();
        info = iterator.Next()) {
-    if (scope->start_position() == info.StartPosition() &&
-        scope->end_position() == info.EndPosition()) {
-      if (info.is_compiled() && !info.scope_info().is_null()) {
-        return handle(info.scope_info(), isolate_);
-      }
-      return Handle<ScopeInfo>();
+    ScopeInfo scope_info = info.scope_info();
+    if (info.is_compiled() && !scope_info.is_null() &&
+        scope->start_position() == info.StartPosition() &&
+        scope->end_position() == info.EndPosition() &&
+        scope->scope_type() == scope_info.scope_type()) {
+      return handle(scope_info, isolate_);
     }
   }
   return Handle<ScopeInfo>();
@@ -1291,7 +1306,10 @@
 }  // namespace
 
 void ScopeIterator::MaybeCollectAndStoreLocalBlocklists() const {
-  if (!calculate_blocklists_ || current_scope_ != closure_scope_) return;
+  if (!calculate_blocklists_ || current_scope_ != closure_scope_ ||
+      Type() == ScopeTypeScript) {
+    return;
+  }
 
   CHECK(v8_flags.experimental_reuse_locals_blocklists);
   DCHECK(isolate_
diff -r -u --color up/v8/src/debug/debug-scopes.h nw/v8/src/debug/debug-scopes.h
--- up/v8/src/debug/debug-scopes.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/debug/debug-scopes.h	2023-01-19 16:46:36.157276252 -0500
@@ -18,7 +18,7 @@
 // The iteration proceeds from the innermost visible nested scope outwards.
 // All scopes are backed by an actual context except the local scope,
 // which is inserted "artificially" in the context chain.
-class ScopeIterator {
+class V8_EXPORT_PRIVATE ScopeIterator {
  public:
   enum ScopeType {
     ScopeTypeGlobal = 0,
diff -r -u --color up/v8/src/debug/debug.cc nw/v8/src/debug/debug.cc
--- up/v8/src/debug/debug.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/debug/debug.cc	2023-01-19 16:46:36.157276252 -0500
@@ -95,7 +95,7 @@
       is_suppressed_(false),
       break_disabled_(false),
       break_points_active_(true),
-      break_on_exception_(false),
+      break_on_caught_exception_(false),
       break_on_uncaught_exception_(false),
       side_effect_check_failed_(false),
       debug_info_list_(nullptr),
@@ -480,16 +480,19 @@
   debug_delegate_ = nullptr;
 }
 
-void Debug::OnInstrumentationBreak() {
+debug::DebugDelegate::PauseAfterInstrumentation
+Debug::OnInstrumentationBreak() {
   RCS_SCOPE(isolate_, RuntimeCallCounterId::kDebugger);
-  if (!debug_delegate_) return;
+  if (!debug_delegate_) {
+    return debug::DebugDelegate::kNoPauseAfterInstrumentationRequested;
+  }
   DCHECK(in_debug_scope());
   HandleScope scope(isolate_);
   DisableBreak no_recursive_break(this);
 
   Handle<Context> native_context(isolate_->native_context());
-  debug_delegate_->BreakOnInstrumentation(v8::Utils::ToLocal(native_context),
-                                          kInstrumentationId);
+  return debug_delegate_->BreakOnInstrumentation(
+      v8::Utils::ToLocal(native_context), kInstrumentationId);
 }
 
 void Debug::Break(JavaScriptFrame* frame, Handle<JSFunction> break_target) {
@@ -512,21 +515,28 @@
   BreakLocation location = BreakLocation::FromFrame(debug_info, frame);
   const bool hitInstrumentationBreak =
       IsBreakOnInstrumentation(debug_info, location);
+  bool shouldPauseAfterInstrumentation = false;
   if (hitInstrumentationBreak) {
-    OnInstrumentationBreak();
+    debug::DebugDelegate::PauseAfterInstrumentation pauseDuringInstrumentation =
+        OnInstrumentationBreak();
+    shouldPauseAfterInstrumentation =
+        pauseDuringInstrumentation ==
+        debug::DebugDelegate::kPauseAfterInstrumentationRequested;
   }
 
   // Find actual break points, if any, and trigger debug break event.
   bool has_break_points;
+  bool scheduled_break =
+      scheduled_break_on_function_call() || shouldPauseAfterInstrumentation;
   MaybeHandle<FixedArray> break_points_hit =
       CheckBreakPoints(debug_info, &location, &has_break_points);
   if (!break_points_hit.is_null() || break_on_next_function_call() ||
-      scheduled_break_on_function_call()) {
+      scheduled_break) {
     StepAction lastStepAction = last_step_action();
     DCHECK_IMPLIES(scheduled_break_on_function_call(),
                    lastStepAction == StepNone);
     debug::BreakReasons break_reasons;
-    if (scheduled_break_on_function_call()) {
+    if (scheduled_break) {
       break_reasons.Add(debug::BreakReason::kScheduled);
     }
     // Clear all current stepping setup.
@@ -1015,7 +1025,7 @@
   if (type == BreakUncaughtException) {
     break_on_uncaught_exception_ = enable;
   } else {
-    break_on_exception_ = enable;
+    break_on_caught_exception_ = enable;
   }
 }
 
@@ -1023,7 +1033,7 @@
   if (type == BreakUncaughtException) {
     return break_on_uncaught_exception_;
   } else {
-    return break_on_exception_;
+    return break_on_caught_exception_;
   }
 }
 
@@ -1584,6 +1594,17 @@
       kRelaxedStore);
 }
 
+namespace {
+
+bool IsJSFunctionAndNeedsTrampoline(Object maybe_function) {
+  if (!maybe_function.IsJSFunction()) return false;
+
+  SharedFunctionInfo shared = JSFunction::cast(maybe_function).shared();
+  return shared.HasDebugInfo() && shared.GetDebugInfo().CanBreakAtEntry();
+}
+
+}  // namespace
+
 void Debug::InstallDebugBreakTrampoline() {
   RCS_SCOPE(isolate_, RuntimeCallCounterId::kDebugger);
   // Check the list of debug infos whether the debug break trampoline needs to
@@ -1609,27 +1630,71 @@
 
   Handle<CodeT> trampoline = BUILTIN_CODE(isolate_, DebugBreakTrampoline);
   std::vector<Handle<JSFunction>> needs_compile;
+  using AccessorPairWithContext =
+      std::pair<Handle<AccessorPair>, Handle<NativeContext>>;
+  std::vector<AccessorPairWithContext> needs_instantiate;
   {
+    // Deduplicate {needs_instantiate} by recording all collected AccessorPairs.
+    std::set<AccessorPair> recorded;
     HeapObjectIterator iterator(isolate_->heap());
+    DisallowGarbageCollection no_gc;
     for (HeapObject obj = iterator.Next(); !obj.is_null();
          obj = iterator.Next()) {
       if (needs_to_clear_ic && obj.IsFeedbackVector()) {
         FeedbackVector::cast(obj).ClearSlots(isolate_);
         continue;
-      } else if (obj.IsJSFunction()) {
+      } else if (IsJSFunctionAndNeedsTrampoline(obj)) {
         JSFunction fun = JSFunction::cast(obj);
-        SharedFunctionInfo shared = fun.shared();
-        if (!shared.HasDebugInfo()) continue;
-        if (!shared.GetDebugInfo().CanBreakAtEntry()) continue;
         if (!fun.is_compiled()) {
           needs_compile.push_back(handle(fun, isolate_));
         } else {
           fun.set_code(*trampoline);
         }
+      } else if (obj.IsJSObject()) {
+        JSObject object = JSObject::cast(obj);
+        DescriptorArray descriptors =
+            object.map().instance_descriptors(kRelaxedLoad);
+
+        for (InternalIndex i : object.map().IterateOwnDescriptors()) {
+          if (descriptors.GetDetails(i).kind() == PropertyKind::kAccessor) {
+            Object value = descriptors.GetStrongValue(i);
+            if (!value.IsAccessorPair()) continue;
+
+            AccessorPair accessor_pair = AccessorPair::cast(value);
+            if (!accessor_pair.getter().IsFunctionTemplateInfo() &&
+                !accessor_pair.setter().IsFunctionTemplateInfo()) {
+              continue;
+            }
+            if (recorded.find(accessor_pair) != recorded.end()) continue;
+
+            needs_instantiate.emplace_back(
+                handle(accessor_pair, isolate_),
+                object.GetCreationContext().ToHandleChecked());
+            recorded.insert(accessor_pair);
+          }
+        }
       }
     }
   }
 
+  // Forcibly instantiate all lazy accessor pairs to make sure that they
+  // properly hit the debug break trampoline.
+  for (AccessorPairWithContext tuple : needs_instantiate) {
+    Handle<AccessorPair> accessor_pair = tuple.first;
+    Handle<NativeContext> native_context = tuple.second;
+    Handle<Object> getter = AccessorPair::GetComponent(
+        isolate_, native_context, accessor_pair, ACCESSOR_GETTER);
+    if (IsJSFunctionAndNeedsTrampoline(*getter)) {
+      Handle<JSFunction>::cast(getter)->set_code(*trampoline);
+    }
+
+    Handle<Object> setter = AccessorPair::GetComponent(
+        isolate_, native_context, accessor_pair, ACCESSOR_SETTER);
+    if (IsJSFunctionAndNeedsTrampoline(*setter)) {
+      Handle<JSFunction>::cast(setter)->set_code(*trampoline);
+    }
+  }
+
   // By overwriting the function code with DebugBreakTrampoline, which tailcalls
   // to shared code, we bypass CompileLazy. Perform CompileLazy here instead.
   for (Handle<JSFunction> fun : needs_compile) {
@@ -2204,7 +2269,7 @@
   if (!debug_delegate_) return;
 
   // Return if we are not interested in exception events.
-  if (!break_on_exception_ && !break_on_uncaught_exception_) return;
+  if (!break_on_caught_exception_ && !break_on_uncaught_exception_) return;
 
   Isolate::CatchType catch_type = isolate_->PredictExceptionCatcher();
 
@@ -2229,11 +2294,14 @@
     }
   }
 
-  // Return if the exception is caught and we only care about uncaught
-  // exceptions.
-  if (!uncaught && !break_on_exception_) {
-    DCHECK(break_on_uncaught_exception_);
-    return;
+  if (!uncaught) {
+    if (!break_on_caught_exception_) {
+      return;
+    }
+  } else {
+    if (!break_on_uncaught_exception_) {
+      return;
+    }
   }
 
   {
@@ -2620,6 +2688,7 @@
       prev_(reinterpret_cast<DebugScope*>(
           base::Relaxed_Load(&debug->thread_local_.current_debug_scope_))),
       no_interrupts_(debug_->isolate_) {
+  timer_.Start();
   // Link recursive debugger entry.
   base::Relaxed_Store(&debug_->thread_local_.current_debug_scope_,
                       reinterpret_cast<base::AtomicWord>(this));
@@ -2638,6 +2707,10 @@
 
 void DebugScope::set_terminate_on_resume() { terminate_on_resume_ = true; }
 
+base::TimeDelta DebugScope::ElapsedTimeSinceCreation() {
+  return timer_.Elapsed();
+}
+
 DebugScope::~DebugScope() {
   // Terminate on resume must have been handled by retrieving it, if this is
   // the outer scope.
@@ -2957,5 +3030,13 @@
   PrepareStep(StepInto);
 }
 
+void Debug::NotifyDebuggerPausedEventSent() {
+  DebugScope* scope = reinterpret_cast<DebugScope*>(
+      base::Relaxed_Load(&thread_local_.current_debug_scope_));
+  CHECK(scope);
+  isolate_->counters()->debug_pause_to_paused_event()->AddTimedSample(
+      scope->ElapsedTimeSinceCreation());
+}
+
 }  // namespace internal
 }  // namespace v8
diff -r -u --color up/v8/src/debug/debug.h nw/v8/src/debug/debug.h
--- up/v8/src/debug/debug.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/debug/debug.h	2023-01-19 16:46:36.157276252 -0500
@@ -9,6 +9,7 @@
 #include <vector>
 
 #include "src/base/enum-set.h"
+#include "src/base/platform/elapsed-timer.h"
 #include "src/codegen/source-position-table.h"
 #include "src/common/globals.h"
 #include "src/debug/debug-interface.h"
@@ -41,7 +42,10 @@
 };
 
 // Type of exception break. NOTE: These values are in macros.py as well.
-enum ExceptionBreakType { BreakException = 0, BreakUncaughtException = 1 };
+enum ExceptionBreakType {
+  BreakCaughtException = 0,
+  BreakUncaughtException = 1,
+};
 
 // Type of debug break. NOTE: The order matters for the predicates
 // below inside BreakLocation, so be careful when adding / removing.
@@ -218,7 +222,7 @@
   // Debug event triggers.
   void OnDebugBreak(Handle<FixedArray> break_points_hit, StepAction stepAction,
                     debug::BreakReasons break_reasons = {});
-  void OnInstrumentationBreak();
+  debug::DebugDelegate::PauseAfterInstrumentation OnInstrumentationBreak();
 
   base::Optional<Object> OnThrow(Handle<Object> exception)
       V8_WARN_UNUSED_RESULT;
@@ -426,6 +430,9 @@
 
   void RemoveBreakInfoAndMaybeFree(Handle<DebugInfo> debug_info);
 
+  // Stops the timer for the top-most `DebugScope` and records a UMA event.
+  void NotifyDebuggerPausedEventSent();
+
   static char* Iterate(RootVisitor* v, char* thread_storage);
 
  private:
@@ -528,8 +535,8 @@
   bool break_disabled_;
   // Do not break on break points.
   bool break_points_active_;
-  // Trigger debug break events for all exceptions.
-  bool break_on_exception_;
+  // Trigger debug break events for caught exceptions.
+  bool break_on_caught_exception_;
   // Trigger debug break events for uncaught exceptions.
   bool break_on_uncaught_exception_;
   // Termination exception because side effect check has failed.
@@ -641,6 +648,8 @@
 
   void set_terminate_on_resume();
 
+  base::TimeDelta ElapsedTimeSinceCreation();
+
  private:
   Isolate* isolate() { return debug_->isolate_; }
 
@@ -650,6 +659,10 @@
   PostponeInterruptsScope no_interrupts_;
   // This is used as a boolean.
   bool terminate_on_resume_ = false;
+
+  // Measures (for UMA) the duration beginning when we enter this `DebugScope`
+  // until we potentially send a "Debugger.paused" response in the inspector.
+  base::ElapsedTimer timer_;
 };
 
 // This scope is used to handle return values in nested debug break points.
diff -r -u --color up/v8/src/debug/liveedit.cc nw/v8/src/debug/liveedit.cc
--- up/v8/src/debug/liveedit.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/debug/liveedit.cc	2023-01-19 16:46:36.157276252 -0500
@@ -821,7 +821,7 @@
 
 void LiveEdit::PatchScript(Isolate* isolate, Handle<Script> script,
                            Handle<String> new_source, bool preview,
-                           bool allow_top_frame_live_editing_param,
+                           bool allow_top_frame_live_editing,
                            debug::LiveEditResult* result) {
   std::vector<SourceChangeRange> diffs;
   LiveEdit::CompareStrings(isolate,
@@ -878,8 +878,6 @@
   }
   function_data_map.Fill(isolate);
 
-  const bool allow_top_frame_live_editing =
-      allow_top_frame_live_editing_param && v8_flags.live_edit_top_frame;
   if (!CanPatchScript(changed, script, new_script, function_data_map,
                       allow_top_frame_live_editing, result)) {
     return;
diff -r -u --color up/v8/src/deoptimizer/OWNERS nw/v8/src/deoptimizer/OWNERS
--- up/v8/src/deoptimizer/OWNERS	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/deoptimizer/OWNERS	2023-01-19 16:46:36.157276252 -0500
@@ -2,3 +2,4 @@
 leszeks@chromium.org
 nicohartmann@chromium.org
 victorgomes@chromium.org
+tebbi@chromium.org
diff -r -u --color up/v8/src/deoptimizer/deoptimize-reason.h nw/v8/src/deoptimizer/deoptimize-reason.h
--- up/v8/src/deoptimizer/deoptimize-reason.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/deoptimizer/deoptimize-reason.h	2023-01-19 16:46:36.168109582 -0500
@@ -40,6 +40,7 @@
   V(NaN, "NaN")                                                                \
   V(NoCache, "no cache")                                                       \
   V(NotABigInt, "not a BigInt")                                                \
+  V(NotABigInt64, "not a BigInt64")                                            \
   V(NotAHeapNumber, "not a heap number")                                       \
   V(NotAJavaScriptObject, "not a JavaScript object")                           \
   V(NotAJavaScriptObjectOrNullOrUndefined,                                     \
diff -r -u --color up/v8/src/deoptimizer/deoptimizer.cc nw/v8/src/deoptimizer/deoptimizer.cc
--- up/v8/src/deoptimizer/deoptimizer.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/deoptimizer/deoptimizer.cc	2023-01-19 16:46:36.168109582 -0500
@@ -432,7 +432,7 @@
   Object element = native_context.OptimizedCodeListHead();
   Isolate* isolate = native_context.GetIsolate();
   while (!element.IsUndefined(isolate)) {
-    Code code = FromCodeT(CodeT::cast(element));
+    CodeT code = CodeT::cast(element);
     CHECK(CodeKindCanDeoptimize(code.kind()));
     code.set_marked_for_deoptimization(true);
     element = code.next_code_link();
@@ -635,7 +635,7 @@
     NativeContext native_context = NativeContext::cast(context);
     Object element = native_context.DeoptimizedCodeListHead();
     while (!element.IsUndefined(isolate)) {
-      Code code = FromCodeT(CodeT::cast(element));
+      CodeT code = CodeT::cast(element);
       DCHECK(CodeKindCanDeoptimize(code.kind()));
       if (!code.marked_for_deoptimization()) {
         length++;
diff -r -u --color up/v8/src/diagnostics/ia32/disasm-ia32.cc nw/v8/src/diagnostics/ia32/disasm-ia32.cc
--- up/v8/src/diagnostics/ia32/disasm-ia32.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/diagnostics/ia32/disasm-ia32.cc	2023-01-19 16:46:36.168109582 -0500
@@ -752,16 +752,16 @@
         // have the same opcodes but differ by rex_w.
         if (vex_w()) {
           switch (opcode) {
-            FMA_SS_INSTRUCTION_LIST(DECLARE_FMA_DISASM)
-            FMA_PS_INSTRUCTION_LIST(DECLARE_FMA_DISASM)
+            FMA_SD_INSTRUCTION_LIST(DECLARE_FMA_DISASM)
+            FMA_PD_INSTRUCTION_LIST(DECLARE_FMA_DISASM)
             default: {
               UnimplementedInstruction();
             }
           }
         } else {
           switch (opcode) {
-            FMA_SD_INSTRUCTION_LIST(DECLARE_FMA_DISASM)
-            FMA_PD_INSTRUCTION_LIST(DECLARE_FMA_DISASM)
+            FMA_SS_INSTRUCTION_LIST(DECLARE_FMA_DISASM)
+            FMA_PS_INSTRUCTION_LIST(DECLARE_FMA_DISASM)
             default: {
               UnimplementedInstruction();
             }
diff -r -u --color up/v8/src/diagnostics/objects-debug.cc nw/v8/src/diagnostics/objects-debug.cc
--- up/v8/src/diagnostics/objects-debug.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/diagnostics/objects-debug.cc	2023-01-19 16:46:36.178942912 -0500
@@ -561,6 +561,7 @@
       if (maybe_cell.IsCell()) CHECK(maybe_cell.InSharedHeap());
       CHECK(!is_extensible());
       CHECK(!is_prototype_map());
+      CHECK(OnlyHasSimpleProperties());
       CHECK(instance_descriptors(isolate).InSharedHeap());
       if (IsJSSharedArrayMap()) {
         CHECK(has_shared_array_elements());
@@ -1099,7 +1100,7 @@
       // when external code space is not enabled.
       CHECK_EQ(code.kind(), kind());
       CHECK_EQ(code.builtin_id(), builtin_id());
-      if (V8_REMOVE_BUILTINS_CODE_OBJECTS) {
+      if (V8_EXTERNAL_CODE_SPACE_BOOL) {
         // When v8_flags.interpreted_frames_native_stack is enabled each
         // interpreted function gets its own copy of the
         // InterpreterEntryTrampoline. Thus, there could be Code'ful builtins.
@@ -1268,7 +1269,6 @@
     CHECK(details.representation().IsTagged());
     FieldIndex field_index = FieldIndex::ForDescriptor(struct_map, i);
     CHECK(RawFastPropertyAt(field_index).IsShared());
-    CHECK(field_index.is_inobject());
   }
 }
 
diff -r -u --color up/v8/src/diagnostics/objects-printer.cc nw/v8/src/diagnostics/objects-printer.cc
--- up/v8/src/diagnostics/objects-printer.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/diagnostics/objects-printer.cc	2023-01-19 16:46:36.178942912 -0500
@@ -1553,6 +1553,7 @@
   os << "\n - backing_store: " << backing_store();
   os << "\n - byte_length: " << byte_length();
   os << "\n - max_byte_length: " << max_byte_length();
+  os << "\n - detach key: " << detach_key();
   if (is_external()) os << "\n - external";
   if (is_detachable()) os << "\n - detachable";
   if (was_detached()) os << "\n - detached";
@@ -2065,7 +2066,7 @@
   PRINT_WASM_INSTANCE_FIELD(imported_function_refs, Brief);
   PRINT_OPTIONAL_WASM_INSTANCE_FIELD(indirect_function_table_refs, Brief);
   PRINT_OPTIONAL_WASM_INSTANCE_FIELD(tags_table, Brief);
-  PRINT_OPTIONAL_WASM_INSTANCE_FIELD(wasm_internal_functions, Brief);
+  PRINT_WASM_INSTANCE_FIELD(wasm_internal_functions, Brief);
   PRINT_WASM_INSTANCE_FIELD(managed_object_maps, Brief);
   PRINT_WASM_INSTANCE_FIELD(feedback_vectors, Brief);
   PRINT_WASM_INSTANCE_FIELD(memory_start, to_void_ptr);
diff -r -u --color up/v8/src/diagnostics/x64/disasm-x64.cc nw/v8/src/diagnostics/x64/disasm-x64.cc
--- up/v8/src/diagnostics/x64/disasm-x64.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/diagnostics/x64/disasm-x64.cc	2023-01-19 16:46:36.178942912 -0500
@@ -950,16 +950,16 @@
         // have the same opcodes but differ by rex_w.
         if (rex_w()) {
           switch (opcode) {
-            FMA_SS_INSTRUCTION_LIST(DECLARE_FMA_DISASM)
-            FMA_PS_INSTRUCTION_LIST(DECLARE_FMA_DISASM)
+            FMA_SD_INSTRUCTION_LIST(DECLARE_FMA_DISASM)
+            FMA_PD_INSTRUCTION_LIST(DECLARE_FMA_DISASM)
             default: {
               UnimplementedInstruction();
             }
           }
         } else {
           switch (opcode) {
-            FMA_SD_INSTRUCTION_LIST(DECLARE_FMA_DISASM)
-            FMA_PD_INSTRUCTION_LIST(DECLARE_FMA_DISASM)
+            FMA_SS_INSTRUCTION_LIST(DECLARE_FMA_DISASM)
+            FMA_PS_INSTRUCTION_LIST(DECLARE_FMA_DISASM)
             default: {
               UnimplementedInstruction();
             }
diff -r -u --color up/v8/src/execution/arguments-inl.h nw/v8/src/execution/arguments-inl.h
--- up/v8/src/execution/arguments-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/execution/arguments-inl.h	2023-01-19 16:46:36.178942912 -0500
@@ -48,6 +48,14 @@
   return (*this)[index].Number();
 }
 
+template <ArgumentsType T>
+Handle<Object> Arguments<T>::atOrUndefined(Isolate* isolate, int index) const {
+  if (index >= length_) {
+    return Handle<Object>::cast(isolate->factory()->undefined_value());
+  }
+  return at<Object>(index);
+}
+
 }  // namespace internal
 }  // namespace v8
 
diff -r -u --color up/v8/src/execution/arguments.h nw/v8/src/execution/arguments.h
--- up/v8/src/execution/arguments.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/execution/arguments.h	2023-01-19 16:46:36.178942912 -0500
@@ -65,6 +65,8 @@
 
   V8_INLINE double number_value_at(int index) const;
 
+  V8_INLINE Handle<Object> atOrUndefined(Isolate* isolate, int index) const;
+
   V8_INLINE Address* address_of_arg_at(int index) const {
     DCHECK_LE(static_cast<uint32_t>(index), static_cast<uint32_t>(length_));
     uintptr_t offset = index * kSystemPointerSize;
diff -r -u --color up/v8/src/execution/arm/frame-constants-arm.h nw/v8/src/execution/arm/frame-constants-arm.h
--- up/v8/src/execution/arm/frame-constants-arm.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/execution/arm/frame-constants-arm.h	2023-01-19 16:46:36.178942912 -0500
@@ -55,7 +55,7 @@
       (kNumCalleeSaved - 1) * kSystemPointerSize;
 };
 
-class WasmCompileLazyFrameConstants : public TypedFrameConstants {
+class WasmLiftoffSetupFrameConstants : public TypedFrameConstants {
  public:
   // Number of gp parameters, without the instance.
   static constexpr int kNumberOfSavedGpParamRegs = 3;
@@ -65,17 +65,18 @@
   // We spill:
   //   r3: param0 = instance
   //   r0, r2, r6: param1, param2, param3
-  // in the following FP-relative order: [r6, r3, r2, r0].
+  //   lr (== r14): internal usage of the caller
+  // in the following FP-relative order: [lr, r6, r3, r2, r0].
   static constexpr int kInstanceSpillOffset =
-      TYPED_FRAME_PUSHED_VALUE_OFFSET(1);
+      TYPED_FRAME_PUSHED_VALUE_OFFSET(2);
 
   static constexpr int kParameterSpillsOffset[] = {
-      TYPED_FRAME_PUSHED_VALUE_OFFSET(3), TYPED_FRAME_PUSHED_VALUE_OFFSET(2),
-      TYPED_FRAME_PUSHED_VALUE_OFFSET(0)};
+      TYPED_FRAME_PUSHED_VALUE_OFFSET(4), TYPED_FRAME_PUSHED_VALUE_OFFSET(3),
+      TYPED_FRAME_PUSHED_VALUE_OFFSET(1)};
 
   // SP-relative.
   static constexpr int kWasmInstanceOffset = 2 * kSystemPointerSize;
-  static constexpr int kFunctionIndexOffset = 1 * kSystemPointerSize;
+  static constexpr int kDeclaredFunctionIndexOffset = 1 * kSystemPointerSize;
   static constexpr int kNativeModuleOffset = 0;
 };
 
diff -r -u --color up/v8/src/execution/arm64/frame-constants-arm64.h nw/v8/src/execution/arm64/frame-constants-arm64.h
--- up/v8/src/execution/arm64/frame-constants-arm64.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/execution/arm64/frame-constants-arm64.h	2023-01-19 16:46:36.189776244 -0500
@@ -72,7 +72,7 @@
       kCalleeSavedRegisterBytesPushedBeforeFpLrPair;
 };
 
-class WasmCompileLazyFrameConstants : public TypedFrameConstants {
+class WasmLiftoffSetupFrameConstants : public TypedFrameConstants {
  public:
   // Number of gp parameters, without the instance.
   static constexpr int kNumberOfSavedGpParamRegs = 6;
@@ -80,22 +80,20 @@
 
   // On arm, spilled registers are implicitly sorted backwards by number.
   // We spill:
-  //   x7: param0 = instance
   //   x0, x2, x3, x4, x5, x6: param1, param2, ..., param6
-  //   x1: for alignment
-  // in the following FP-relative order: [x7, x6, x5, x4, x3, x2, x1, x0].
-  // For frame alignment, the first spill slot is at position '1', not at '0'.
+  // in the following FP-relative order: [x6, x5, x4, x3, x2, x0].
+  // The instance slot is in position '0', the first spill slot is at '1'.
   static constexpr int kInstanceSpillOffset =
-      TYPED_FRAME_PUSHED_VALUE_OFFSET(1);
+      TYPED_FRAME_PUSHED_VALUE_OFFSET(0);
 
   static constexpr int kParameterSpillsOffset[] = {
-      TYPED_FRAME_PUSHED_VALUE_OFFSET(8), TYPED_FRAME_PUSHED_VALUE_OFFSET(6),
-      TYPED_FRAME_PUSHED_VALUE_OFFSET(5), TYPED_FRAME_PUSHED_VALUE_OFFSET(4),
-      TYPED_FRAME_PUSHED_VALUE_OFFSET(3), TYPED_FRAME_PUSHED_VALUE_OFFSET(2)};
+      TYPED_FRAME_PUSHED_VALUE_OFFSET(6), TYPED_FRAME_PUSHED_VALUE_OFFSET(5),
+      TYPED_FRAME_PUSHED_VALUE_OFFSET(4), TYPED_FRAME_PUSHED_VALUE_OFFSET(3),
+      TYPED_FRAME_PUSHED_VALUE_OFFSET(2), TYPED_FRAME_PUSHED_VALUE_OFFSET(1)};
 
   // SP-relative.
   static constexpr int kWasmInstanceOffset = 2 * kSystemPointerSize;
-  static constexpr int kFunctionIndexOffset = 1 * kSystemPointerSize;
+  static constexpr int kDeclaredFunctionIndexOffset = 1 * kSystemPointerSize;
   static constexpr int kNativeModuleOffset = 0;
 };
 
diff -r -u --color up/v8/src/execution/frames-inl.h nw/v8/src/execution/frames-inl.h
--- up/v8/src/execution/frames-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/execution/frames-inl.h	2023-01-19 16:46:36.189776244 -0500
@@ -275,7 +275,7 @@
 inline CWasmEntryFrame::CWasmEntryFrame(StackFrameIteratorBase* iterator)
     : StubFrame(iterator) {}
 
-inline WasmCompileLazyFrame::WasmCompileLazyFrame(
+inline WasmLiftoffSetupFrame::WasmLiftoffSetupFrame(
     StackFrameIteratorBase* iterator)
     : TypedFrame(iterator) {}
 #endif  // V8_ENABLE_WEBASSEMBLY
diff -r -u --color up/v8/src/execution/frames.cc nw/v8/src/execution/frames.cc
--- up/v8/src/execution/frames.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/execution/frames.cc	2023-01-19 16:46:36.189776244 -0500
@@ -768,7 +768,7 @@
 #if V8_ENABLE_WEBASSEMBLY
     case WASM_TO_JS:
     case WASM:
-    case WASM_COMPILE_LAZY:
+    case WASM_LIFTOFF_SETUP:
     case WASM_EXIT:
     case WASM_DEBUG_BREAK:
     case JS_TO_WASM:
@@ -2693,32 +2693,33 @@
   DCHECK_NE(*state->pc_address, kNullAddress);
 }
 
-int WasmCompileLazyFrame::GetFunctionIndex() const {
+int WasmLiftoffSetupFrame::GetDeclaredFunctionIndex() const {
   Object func_index(Memory<Address>(
-      sp() + WasmCompileLazyFrameConstants::kFunctionIndexOffset));
+      sp() + WasmLiftoffSetupFrameConstants::kDeclaredFunctionIndexOffset));
   return Smi::ToInt(func_index);
 }
 
-wasm::NativeModule* WasmCompileLazyFrame::GetNativeModule() const {
+wasm::NativeModule* WasmLiftoffSetupFrame::GetNativeModule() const {
   return *reinterpret_cast<wasm::NativeModule**>(
-      sp() + WasmCompileLazyFrameConstants::kNativeModuleOffset);
+      sp() + WasmLiftoffSetupFrameConstants::kNativeModuleOffset);
 }
 
-FullObjectSlot WasmCompileLazyFrame::wasm_instance_slot() const {
+FullObjectSlot WasmLiftoffSetupFrame::wasm_instance_slot() const {
   return FullObjectSlot(&Memory<Address>(
-      sp() + WasmCompileLazyFrameConstants::kWasmInstanceOffset));
+      sp() + WasmLiftoffSetupFrameConstants::kWasmInstanceOffset));
 }
 
-void WasmCompileLazyFrame::Iterate(RootVisitor* v) const {
+void WasmLiftoffSetupFrame::Iterate(RootVisitor* v) const {
   FullObjectSlot spilled_instance_slot(&Memory<Address>(
-      fp() + WasmCompileLazyFrameConstants::kInstanceSpillOffset));
+      fp() + WasmLiftoffSetupFrameConstants::kInstanceSpillOffset));
   v->VisitRootPointer(Root::kStackRoots, "spilled wasm instance",
                       spilled_instance_slot);
   v->VisitRootPointer(Root::kStackRoots, "wasm instance parameter",
                       wasm_instance_slot());
 
-  int func_index = GetFunctionIndex();
   wasm::NativeModule* native_module = GetNativeModule();
+  int func_index = GetDeclaredFunctionIndex() +
+                   native_module->module()->num_imported_functions;
 
   // Scan the spill slots of the parameter registers. Parameters in WebAssembly
   // get reordered such that first all value parameters get put into registers.
@@ -2743,42 +2744,39 @@
   // There are no reference parameters, there is nothing to scan.
   if (num_ref_params == 0) return;
 
-  int num_int_params_in_registers = std::min(
-      num_int_params, WasmCompileLazyFrameConstants::kNumberOfSavedGpParamRegs);
-  int num_ref_params_in_registers = std::min(
-      num_ref_params, WasmCompileLazyFrameConstants::kNumberOfSavedGpParamRegs -
-                          num_int_params_in_registers);
+  int num_int_params_in_registers =
+      std::min(num_int_params,
+               WasmLiftoffSetupFrameConstants::kNumberOfSavedGpParamRegs);
+  int num_ref_params_in_registers =
+      std::min(num_ref_params,
+               WasmLiftoffSetupFrameConstants::kNumberOfSavedGpParamRegs -
+                   num_int_params_in_registers);
 
   for (int i = 0; i < num_ref_params_in_registers; ++i) {
     FullObjectSlot spill_slot(
-        fp() + WasmCompileLazyFrameConstants::kParameterSpillsOffset
+        fp() + WasmLiftoffSetupFrameConstants::kParameterSpillsOffset
                    [num_int_params_in_registers + i]);
 
     v->VisitRootPointer(Root::kStackRoots, "register parameter", spill_slot);
   }
 
   // Next we scan the slots of stack parameters.
-  // If there is no code, then lazy compilation failed (which can happen with
-  // lazy validation). In that case, just do not scan parameters, which will
-  // never be used anyway because the stack will get unwound when returning to
-  // the CEntry stub.
-  if (wasm::WasmCode* wasm_code = native_module->GetCode(func_index)) {
-    uint32_t first_tagged_stack_slot = wasm_code->first_tagged_parameter_slot();
-    uint32_t num_tagged_stack_slots = wasm_code->num_tagged_parameter_slots();
-
-    // Visit tagged parameters that have been passed to the function of this
-    // frame. Conceptionally these parameters belong to the parent frame.
-    // However, the exact count is only known by this frame (in the presence of
-    // tail calls, this information cannot be derived from the call site).
-    if (num_tagged_stack_slots > 0) {
-      FullObjectSlot tagged_parameter_base(&Memory<Address>(caller_sp()));
-      tagged_parameter_base += first_tagged_stack_slot;
-      FullObjectSlot tagged_parameter_limit =
-          tagged_parameter_base + num_tagged_stack_slots;
+  wasm::WasmCode* wasm_code = native_module->GetCode(func_index);
+  uint32_t first_tagged_stack_slot = wasm_code->first_tagged_parameter_slot();
+  uint32_t num_tagged_stack_slots = wasm_code->num_tagged_parameter_slots();
 
-      v->VisitRootPointers(Root::kStackRoots, "stack parameter",
-                           tagged_parameter_base, tagged_parameter_limit);
-    }
+  // Visit tagged parameters that have been passed to the function of this
+  // frame. Conceptionally these parameters belong to the parent frame.
+  // However, the exact count is only known by this frame (in the presence of
+  // tail calls, this information cannot be derived from the call site).
+  if (num_tagged_stack_slots > 0) {
+    FullObjectSlot tagged_parameter_base(&Memory<Address>(caller_sp()));
+    tagged_parameter_base += first_tagged_stack_slot;
+    FullObjectSlot tagged_parameter_limit =
+        tagged_parameter_base + num_tagged_stack_slots;
+
+    v->VisitRootPointers(Root::kStackRoots, "stack parameter",
+                         tagged_parameter_base, tagged_parameter_limit);
   }
 }
 #endif  // V8_ENABLE_WEBASSEMBLY
diff -r -u --color up/v8/src/execution/frames.h nw/v8/src/execution/frames.h
--- up/v8/src/execution/frames.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/execution/frames.h	2023-01-19 16:46:36.189776244 -0500
@@ -45,7 +45,7 @@
 //       - WasmExitFrame
 //       - WasmToJsFrame
 //     - WasmDebugBreakFrame
-//     - WasmCompileLazyFrame
+//     - WasmLiftoffSetupFrame
 //
 
 namespace v8 {
@@ -111,7 +111,7 @@
   IF_WASM(V, WASM_DEBUG_BREAK, WasmDebugBreakFrame)                       \
   IF_WASM(V, C_WASM_ENTRY, CWasmEntryFrame)                               \
   IF_WASM(V, WASM_EXIT, WasmExitFrame)                                    \
-  IF_WASM(V, WASM_COMPILE_LAZY, WasmCompileLazyFrame)                     \
+  IF_WASM(V, WASM_LIFTOFF_SETUP, WasmLiftoffSetupFrame)                   \
   V(INTERPRETED, InterpretedFrame)                                        \
   V(BASELINE, BaselineFrame)                                              \
   V(MAGLEV, MaglevFrame)                                                  \
@@ -235,7 +235,7 @@
 #if V8_ENABLE_WEBASSEMBLY
   bool is_wasm() const { return this->type() == WASM; }
   bool is_c_wasm_entry() const { return type() == C_WASM_ENTRY; }
-  bool is_wasm_compile_lazy() const { return type() == WASM_COMPILE_LAZY; }
+  bool is_wasm_liftoff_setup() const { return type() == WASM_LIFTOFF_SETUP; }
   bool is_wasm_debug_break() const { return type() == WASM_DEBUG_BREAK; }
   bool is_wasm_to_js() const {
     return type() == WASM_TO_JS || type() == WASM_TO_JS_FUNCTION;
@@ -1157,26 +1157,26 @@
   Type GetCallerState(State* state) const override;
 };
 
-class WasmCompileLazyFrame : public TypedFrame {
+class WasmLiftoffSetupFrame : public TypedFrame {
  public:
-  Type type() const override { return WASM_COMPILE_LAZY; }
+  Type type() const override { return WASM_LIFTOFF_SETUP; }
 
   FullObjectSlot wasm_instance_slot() const;
 
-  int GetFunctionIndex() const;
+  int GetDeclaredFunctionIndex() const;
 
   wasm::NativeModule* GetNativeModule() const;
 
   // Garbage collection support.
   void Iterate(RootVisitor* v) const override;
 
-  static WasmCompileLazyFrame* cast(StackFrame* frame) {
-    DCHECK(frame->is_wasm_compile_lazy());
-    return static_cast<WasmCompileLazyFrame*>(frame);
+  static WasmLiftoffSetupFrame* cast(StackFrame* frame) {
+    DCHECK(frame->is_wasm_liftoff_setup());
+    return static_cast<WasmLiftoffSetupFrame*>(frame);
   }
 
  protected:
-  inline explicit WasmCompileLazyFrame(StackFrameIteratorBase* iterator);
+  inline explicit WasmLiftoffSetupFrame(StackFrameIteratorBase* iterator);
 
  private:
   friend class StackFrameIteratorBase;
diff -r -u --color up/v8/src/execution/ia32/frame-constants-ia32.h nw/v8/src/execution/ia32/frame-constants-ia32.h
--- up/v8/src/execution/ia32/frame-constants-ia32.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/execution/ia32/frame-constants-ia32.h	2023-01-19 16:46:36.189776244 -0500
@@ -34,22 +34,23 @@
   static constexpr int kMicrotaskQueueArgOffset = +3 * kSystemPointerSize;
 };
 
-class WasmCompileLazyFrameConstants : public TypedFrameConstants {
+class WasmLiftoffSetupFrameConstants : public TypedFrameConstants {
  public:
   // Number of gp parameters, without the instance.
   static constexpr int kNumberOfSavedGpParamRegs = 3;
   static constexpr int kNumberOfSavedFpParamRegs = 6;
 
+  // There's one spilled value (which doesn't need visiting) below the instance.
   static constexpr int kInstanceSpillOffset =
-      TYPED_FRAME_PUSHED_VALUE_OFFSET(0);
+      TYPED_FRAME_PUSHED_VALUE_OFFSET(1);
 
   static constexpr int kParameterSpillsOffset[] = {
-      TYPED_FRAME_PUSHED_VALUE_OFFSET(1), TYPED_FRAME_PUSHED_VALUE_OFFSET(2),
-      TYPED_FRAME_PUSHED_VALUE_OFFSET(3)};
+      TYPED_FRAME_PUSHED_VALUE_OFFSET(2), TYPED_FRAME_PUSHED_VALUE_OFFSET(3),
+      TYPED_FRAME_PUSHED_VALUE_OFFSET(4)};
 
   // SP-relative.
   static constexpr int kWasmInstanceOffset = 2 * kSystemPointerSize;
-  static constexpr int kFunctionIndexOffset = 1 * kSystemPointerSize;
+  static constexpr int kDeclaredFunctionIndexOffset = 1 * kSystemPointerSize;
   static constexpr int kNativeModuleOffset = 0;
 };
 
diff -r -u --color up/v8/src/execution/isolate-inl.h nw/v8/src/execution/isolate-inl.h
--- up/v8/src/execution/isolate-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/execution/isolate-inl.h	2023-01-19 16:46:36.189776244 -0500
@@ -23,6 +23,24 @@
 namespace v8 {
 namespace internal {
 
+// static
+V8_INLINE Isolate::PerIsolateThreadData*
+Isolate::CurrentPerIsolateThreadData() {
+  return g_current_per_isolate_thread_data_;
+}
+
+// static
+V8_INLINE Isolate* Isolate::TryGetCurrent() { return g_current_isolate_; }
+
+// static
+V8_INLINE Isolate* Isolate::Current() {
+  Isolate* isolate = TryGetCurrent();
+  DCHECK_NOT_NULL(isolate);
+  return isolate;
+}
+
+bool Isolate::IsCurrent() const { return this == TryGetCurrent(); }
+
 void Isolate::set_context(Context context) {
   DCHECK(context.is_null() || context.IsContext());
   thread_local_top()->context_ = context;
diff -r -u --color up/v8/src/execution/isolate.cc nw/v8/src/execution/isolate.cc
--- up/v8/src/execution/isolate.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/execution/isolate.cc	2023-01-19 16:46:36.189776244 -0500
@@ -428,7 +428,7 @@
        ++builtin) {
     CodeT codet = builtins()->code(builtin);
 
-    if (V8_REMOVE_BUILTINS_CODE_OBJECTS) {
+    if (V8_EXTERNAL_CODE_SPACE_BOOL) {
 #ifdef V8_EXTERNAL_CODE_SPACE
       DCHECK(Internals::HasHeapObjectTag(codet.ptr()));
       uint8_t* const code_ptr = reinterpret_cast<uint8_t*>(codet.address());
@@ -437,8 +437,6 @@
       // hash code cage base and code entry point. Other data fields must
       // remain the same.
       static_assert(CodeDataContainer::kCodePointerFieldsStrongEndOffset ==
-                    CodeDataContainer::kCodeCageBaseUpper32BitsOffset);
-      static_assert(CodeDataContainer::kCodeCageBaseUpper32BitsOffsetEnd + 1 ==
                     CodeDataContainer::kCodeEntryPointOffset);
 
       static_assert(CodeDataContainer::kCodeEntryPointOffsetEnd + 1 ==
@@ -506,9 +504,9 @@
 
 Isolate* Isolate::process_wide_shared_space_isolate_{nullptr};
 
-base::Thread::LocalStorageKey Isolate::isolate_key_;
-base::Thread::LocalStorageKey Isolate::per_isolate_thread_data_key_;
-std::atomic<bool> Isolate::isolate_key_created_{false};
+thread_local Isolate::PerIsolateThreadData* g_current_per_isolate_thread_data_
+    V8_CONSTINIT = nullptr;
+thread_local Isolate* g_current_isolate_ V8_CONSTINIT = nullptr;
 
 namespace {
 // A global counter for all generated Isolates, might overflow.
@@ -563,23 +561,7 @@
   return per_thread;
 }
 
-void Isolate::InitializeOncePerProcess() {
-  isolate_key_ = base::Thread::CreateThreadLocalKey();
-  bool expected = false;
-  CHECK(isolate_key_created_.compare_exchange_strong(
-      expected, true, std::memory_order_relaxed));
-  per_isolate_thread_data_key_ = base::Thread::CreateThreadLocalKey();
-
-  Heap::InitializeOncePerProcess();
-}
-
-void Isolate::DisposeOncePerProcess() {
-  base::Thread::DeleteThreadLocalKey(isolate_key_);
-  bool expected = true;
-  CHECK(isolate_key_created_.compare_exchange_strong(
-      expected, false, std::memory_order_relaxed));
-  base::Thread::DeleteThreadLocalKey(per_isolate_thread_data_key_);
-}
+void Isolate::InitializeOncePerProcess() { Heap::InitializeOncePerProcess(); }
 
 Address Isolate::get_address_from_id(IsolateAddressId id) {
   return isolate_addresses_[id];
@@ -2127,11 +2109,10 @@
                             visited_frames);
       }
 
-      case StackFrame::WASM_COMPILE_LAZY: {
-        // Can only fail directly on invocation. This happens if an invalid
-        // function was validated lazily.
-        DCHECK(v8_flags.wasm_lazy_validation);
-        break;
+      case StackFrame::WASM_LIFTOFF_SETUP: {
+        // The WasmLiftoffFrameSetup builtin doesn't throw, and doesn't call
+        // out to user code that could throw.
+        UNREACHABLE();
       }
 #endif  // V8_ENABLE_WEBASSEMBLY
 
@@ -2972,30 +2953,6 @@
   return false;
 }
 
-bool Isolate::IsWasmSimdEnabled(Handle<Context> context) {
-#if V8_ENABLE_WEBASSEMBLY
-  if (wasm_simd_enabled_callback()) {
-    v8::Local<v8::Context> api_context = v8::Utils::ToLocal(context);
-    return wasm_simd_enabled_callback()(api_context);
-  }
-  return v8_flags.experimental_wasm_simd;
-#else
-  return false;
-#endif  // V8_ENABLE_WEBASSEMBLY
-}
-
-bool Isolate::AreWasmExceptionsEnabled(Handle<Context> context) {
-#if V8_ENABLE_WEBASSEMBLY
-  if (wasm_exceptions_enabled_callback()) {
-    v8::Local<v8::Context> api_context = v8::Utils::ToLocal(context);
-    return wasm_exceptions_enabled_callback()(api_context);
-  }
-  return v8_flags.experimental_wasm_eh;
-#else
-  return false;
-#endif  // V8_ENABLE_WEBASSEMBLY
-}
-
 Handle<Context> Isolate::GetIncumbentContext() {
   JavaScriptFrameIterator it(this);
 
@@ -3377,11 +3334,12 @@
   // direct pointer. We don't use Enter/Exit here to avoid
   // initializing the thread data.
   PerIsolateThreadData* saved_data = isolate->CurrentPerIsolateThreadData();
-  DCHECK_EQ(true, isolate_key_created_.load(std::memory_order_relaxed));
-  Isolate* saved_isolate = reinterpret_cast<Isolate*>(
-      base::Thread::GetThreadLocal(isolate->isolate_key_));
+  Isolate* saved_isolate = isolate->TryGetCurrent();
   SetIsolateThreadLocals(isolate, nullptr);
   isolate->set_thread_id(ThreadId::Current());
+  isolate->thread_local_top()->stack_ =
+      saved_isolate ? saved_isolate->thread_local_top()->stack_
+                    : ::heap::base::Stack(base::Stack::GetStackStart());
 
   bool owns_shared_isolate = isolate->owns_shared_isolate_;
   Isolate* maybe_shared_isolate = isolate->shared_isolate_;
@@ -3437,6 +3395,7 @@
       isolate_allocator_(std::move(isolate_allocator)),
       id_(isolate_counter.fetch_add(1, std::memory_order_relaxed)),
       allocator_(new TracingAccountingAllocator(this)),
+      traced_handles_(this),
       builtins_(this),
 #if defined(DEBUG) || defined(VERIFY_HEAP)
       num_active_deserializers_(0),
@@ -3738,8 +3697,8 @@
 
 void Isolate::SetIsolateThreadLocals(Isolate* isolate,
                                      PerIsolateThreadData* data) {
-  base::Thread::SetThreadLocal(isolate_key_, isolate);
-  base::Thread::SetThreadLocal(per_isolate_thread_data_key_, data);
+  g_current_isolate_ = isolate;
+  g_current_per_isolate_thread_data_ = data;
 }
 
 Isolate::~Isolate() {
@@ -4062,12 +4021,10 @@
   add_crash_key_callback_(v8::CrashKeyId::kReadonlySpaceFirstPageAddress,
                           ToHexString(ro_space_firstpage_address));
 
-  if (heap()->map_space()) {
-    const uintptr_t map_space_firstpage_address =
-        heap()->map_space()->FirstPageAddress();
-    add_crash_key_callback_(v8::CrashKeyId::kMapSpaceFirstPageAddress,
-                            ToHexString(map_space_firstpage_address));
-  }
+  const uintptr_t old_space_firstpage_address =
+      heap()->old_space()->FirstPageAddress();
+  add_crash_key_callback_(v8::CrashKeyId::kOldSpaceFirstPageAddress,
+                          ToHexString(old_space_firstpage_address));
 
   if (heap()->code_range_base()) {
     const uintptr_t code_range_base_address = heap()->code_range_base();
@@ -4075,7 +4032,7 @@
                             ToHexString(code_range_base_address));
   }
 
-  if (!V8_REMOVE_BUILTINS_CODE_OBJECTS || heap()->code_space()->first_page()) {
+  if (!V8_EXTERNAL_CODE_SPACE_BOOL || heap()->code_space()->first_page()) {
     const uintptr_t code_space_firstpage_address =
         heap()->code_space()->FirstPageAddress();
     add_crash_key_callback_(v8::CrashKeyId::kCodeSpaceFirstPageAddress,
@@ -4711,7 +4668,8 @@
       // Static conditions.
       v8_flags.trace_deopt || v8_flags.trace_turbo ||
       v8_flags.trace_turbo_graph || v8_flags.turbo_profiling ||
-      v8_flags.perf_prof || v8_flags.log_maps || v8_flags.log_ic ||
+      v8_flags.print_maglev_code || v8_flags.perf_prof || v8_flags.log_maps ||
+      v8_flags.log_ic ||
       // Dynamic conditions; changing any of these conditions triggers source
       // position collection for the entire heap
       // (CollectSourcePositionsForAllBytecodeArrays).
@@ -6092,9 +6050,9 @@
 
   void Run() override {
     v8::HandleScope scope(isolate_);
-    MicrotasksScope microtasks_scope(isolate_,
-                                     MicrotasksScope::kDoNotRunMicrotasks);
     v8::Local<v8::Context> context = context_.Get(isolate_);
+    MicrotasksScope microtasks_scope(context,
+                                     MicrotasksScope::kDoNotRunMicrotasks);
     v8::Local<v8::Promise::Resolver> resolver = resolver_.Get(isolate_);
     v8::Local<v8::Value> result = result_.Get(isolate_);
 
diff -r -u --color up/v8/src/execution/isolate.h nw/v8/src/execution/isolate.h
--- up/v8/src/execution/isolate.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/execution/isolate.h	2023-01-19 16:46:36.189776244 -0500
@@ -31,6 +31,7 @@
 #include "src/execution/shared-mutex-guard-if-off-thread.h"
 #include "src/execution/stack-guard.h"
 #include "src/handles/handles.h"
+#include "src/handles/traced-handles.h"
 #include "src/heap/factory.h"
 #include "src/heap/heap.h"
 #include "src/heap/read-only-heap.h"
@@ -621,7 +622,6 @@
   };
 
   static void InitializeOncePerProcess();
-  static void DisposeOncePerProcess();
 
   // Creates Isolate object. Must be used instead of constructing Isolate with
   // new operator.
@@ -642,26 +642,15 @@
 
   // Returns the PerIsolateThreadData for the current thread (or nullptr if one
   // is not currently set).
-  static PerIsolateThreadData* CurrentPerIsolateThreadData() {
-    return reinterpret_cast<PerIsolateThreadData*>(
-        base::Thread::GetThreadLocal(per_isolate_thread_data_key_));
-  }
+  V8_INLINE static PerIsolateThreadData* CurrentPerIsolateThreadData();
 
   // Returns the isolate inside which the current thread is running or nullptr.
-  V8_INLINE static Isolate* TryGetCurrent() {
-    DCHECK_EQ(true, isolate_key_created_.load(std::memory_order_relaxed));
-    return reinterpret_cast<Isolate*>(
-        base::Thread::GetExistingThreadLocal(isolate_key_));
-  }
+  V8_INLINE static Isolate* TryGetCurrent();
 
   // Returns the isolate inside which the current thread is running.
-  V8_INLINE static Isolate* Current() {
-    Isolate* isolate = TryGetCurrent();
-    DCHECK_NOT_NULL(isolate);
-    return isolate;
-  }
+  V8_INLINE static Isolate* Current();
 
-  bool IsCurrent() const { return this == TryGetCurrent(); }
+  inline bool IsCurrent() const;
 
   // Usually called by Init(), but can be called early e.g. to allow
   // testing components that require logging but not the whole
@@ -779,9 +768,6 @@
 
   bool IsSharedArrayBufferConstructorEnabled(Handle<Context> context);
 
-  bool IsWasmSimdEnabled(Handle<Context> context);
-  bool AreWasmExceptionsEnabled(Handle<Context> context);
-
   THREAD_LOCAL_TOP_ADDRESS(Context, pending_handler_context)
   THREAD_LOCAL_TOP_ADDRESS(Address, pending_handler_entrypoint)
   THREAD_LOCAL_TOP_ADDRESS(Address, pending_handler_constant_pool)
@@ -1302,6 +1288,8 @@
 
   GlobalHandles* global_handles() const { return global_handles_; }
 
+  TracedHandles* traced_handles() { return &traced_handles_; }
+
   EternalHandles* eternal_handles() const { return eternal_handles_; }
 
   ThreadManager* thread_manager() const { return thread_manager_; }
@@ -2123,10 +2111,6 @@
 
   static Isolate* process_wide_shared_space_isolate_;
 
-  static base::Thread::LocalStorageKey per_isolate_thread_data_key_;
-  static base::Thread::LocalStorageKey isolate_key_;
-  static std::atomic<bool> isolate_key_created_;
-
   void Deinit();
 
   static void SetIsolateThreadLocals(Isolate* isolate,
@@ -2231,6 +2215,7 @@
   AccountingAllocator* allocator_ = nullptr;
   InnerPointerToCodeCache* inner_pointer_to_code_cache_ = nullptr;
   GlobalHandles* global_handles_ = nullptr;
+  TracedHandles traced_handles_;
   EternalHandles* eternal_handles_ = nullptr;
   ThreadManager* thread_manager_ = nullptr;
   bigint::Processor* bigint_processor_ = nullptr;
@@ -2552,6 +2537,16 @@
   friend class SharedHeapNoClientsTest;
 };
 
+// The current entered Isolate and its thread data. Do not access these
+// directly! Use Isolate::Current and Isolate::CurrentPerIsolateThreadData.
+//
+// These are outside the Isolate class with extern storage because in clang-cl,
+// thread_local is incompatible with dllexport linkage caused by
+// V8_EXPORT_PRIVATE being applied to Isolate.
+extern thread_local Isolate::PerIsolateThreadData*
+    g_current_per_isolate_thread_data_ V8_CONSTINIT;
+extern thread_local Isolate* g_current_isolate_ V8_CONSTINIT;
+
 #undef FIELD_ACCESSOR
 #undef THREAD_LOCAL_TOP_ACCESSOR
 
diff -r -u --color up/v8/src/execution/loong64/frame-constants-loong64.h nw/v8/src/execution/loong64/frame-constants-loong64.h
--- up/v8/src/execution/loong64/frame-constants-loong64.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/execution/loong64/frame-constants-loong64.h	2023-01-19 16:46:36.189776244 -0500
@@ -20,7 +20,7 @@
   static constexpr int kCallerFPOffset = -3 * kSystemPointerSize;
 };
 
-class WasmCompileLazyFrameConstants : public TypedFrameConstants {
+class WasmLiftoffSetupFrameConstants : public TypedFrameConstants {
  public:
   // Number of gp parameters, without the instance.
   static constexpr int kNumberOfSavedGpParamRegs = 6;
@@ -29,9 +29,9 @@
 
   // On loong64, spilled registers are implicitly sorted backwards by number.
   // We spill:
-  //   a0: param0 = instance
   //   a2, a3, a4, a5, a6, a7: param1, param2, ..., param6
-  // in the following FP-relative order: [a7, a6, a5, a4, a3, a2, a0].
+  // in the following FP-relative order: [a7, a6, a5, a4, a3, a2].
+  // The instance slot is in position '6', the first spill slot is at '0'.
   static constexpr int kInstanceSpillOffset =
       TYPED_FRAME_PUSHED_VALUE_OFFSET(6);
 
@@ -42,7 +42,7 @@
 
   // SP-relative.
   static constexpr int kWasmInstanceOffset = 2 * kSystemPointerSize;
-  static constexpr int kFunctionIndexOffset = 1 * kSystemPointerSize;
+  static constexpr int kDeclaredFunctionIndexOffset = 1 * kSystemPointerSize;
   static constexpr int kNativeModuleOffset = 0;
 };
 
diff -r -u --color up/v8/src/execution/microtask-queue.cc nw/v8/src/execution/microtask-queue.cc
--- up/v8/src/execution/microtask-queue.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/execution/microtask-queue.cc	2023-01-19 16:46:36.189776244 -0500
@@ -163,7 +163,7 @@
   {
     SetIsRunningMicrotasks scope(&is_running_microtasks_);
     v8::Isolate::SuppressMicrotaskExecutionScope suppress(
-        reinterpret_cast<v8::Isolate*>(isolate));
+        reinterpret_cast<v8::Isolate*>(isolate), this);
     HandleScopeImplementer::EnteredContextRewindScope rewind_scope(
         isolate->handle_scope_implementer());
     TRACE_EVENT_BEGIN0("v8.execute", "RunMicrotasks");
diff -r -u --color up/v8/src/execution/mips64/frame-constants-mips64.h nw/v8/src/execution/mips64/frame-constants-mips64.h
--- up/v8/src/execution/mips64/frame-constants-mips64.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/execution/mips64/frame-constants-mips64.h	2023-01-19 16:46:36.189776244 -0500
@@ -20,7 +20,7 @@
   static constexpr int kCallerFPOffset = -3 * kSystemPointerSize;
 };
 
-class WasmCompileLazyFrameConstants : public TypedFrameConstants {
+class WasmLiftoffSetupFrameConstants : public TypedFrameConstants {
  public:
   // Number of gp parameters, without the instance.
   static constexpr int kNumberOfSavedGpParamRegs = 6;
@@ -29,9 +29,9 @@
 
   // On mips64, spilled registers are implicitly sorted backwards by number.
   // We spill:
-  //   a0: param0 = instance
   //   a2, a3, a4, a5, a6, a7: param1, param2, ..., param6
-  // in the following FP-relative order: [a7, a6, a5, a4, a3, a2, a0].
+  // in the following FP-relative order: [a7, a6, a5, a4, a3, a2].
+  // The instance slot is in position '6', the first spill slot is at '0'.
   static constexpr int kInstanceSpillOffset =
       TYPED_FRAME_PUSHED_VALUE_OFFSET(6);
 
@@ -42,7 +42,7 @@
 
   // SP-relative.
   static constexpr int kWasmInstanceOffset = 2 * kSystemPointerSize;
-  static constexpr int kFunctionIndexOffset = 1 * kSystemPointerSize;
+  static constexpr int kDeclaredFunctionIndexOffset = 1 * kSystemPointerSize;
   static constexpr int kNativeModuleOffset = 0;
 };
 
diff -r -u --color up/v8/src/execution/ppc/frame-constants-ppc.h nw/v8/src/execution/ppc/frame-constants-ppc.h
--- up/v8/src/execution/ppc/frame-constants-ppc.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/execution/ppc/frame-constants-ppc.h	2023-01-19 16:46:36.189776244 -0500
@@ -21,23 +21,24 @@
                                              : -3 * kSystemPointerSize;
 };
 
-class WasmCompileLazyFrameConstants : public TypedFrameConstants {
+class WasmLiftoffSetupFrameConstants : public TypedFrameConstants {
  public:
   // Number of gp parameters, without the instance.
   static constexpr int kNumberOfSavedGpParamRegs = 6;
   static constexpr int kNumberOfSavedFpParamRegs = 8;
 
+  // There's one spilled value (which doesn't need visiting) below the instance.
   static constexpr int kInstanceSpillOffset =
-      TYPED_FRAME_PUSHED_VALUE_OFFSET(0);
+      TYPED_FRAME_PUSHED_VALUE_OFFSET(1);
 
   static constexpr int kParameterSpillsOffset[] = {
-      TYPED_FRAME_PUSHED_VALUE_OFFSET(1), TYPED_FRAME_PUSHED_VALUE_OFFSET(2),
-      TYPED_FRAME_PUSHED_VALUE_OFFSET(3), TYPED_FRAME_PUSHED_VALUE_OFFSET(4),
-      TYPED_FRAME_PUSHED_VALUE_OFFSET(5), TYPED_FRAME_PUSHED_VALUE_OFFSET(6)};
+      TYPED_FRAME_PUSHED_VALUE_OFFSET(2), TYPED_FRAME_PUSHED_VALUE_OFFSET(3),
+      TYPED_FRAME_PUSHED_VALUE_OFFSET(4), TYPED_FRAME_PUSHED_VALUE_OFFSET(5),
+      TYPED_FRAME_PUSHED_VALUE_OFFSET(6), TYPED_FRAME_PUSHED_VALUE_OFFSET(7)};
 
   // SP-relative.
   static constexpr int kWasmInstanceOffset = 2 * kSystemPointerSize;
-  static constexpr int kFunctionIndexOffset = 1 * kSystemPointerSize;
+  static constexpr int kDeclaredFunctionIndexOffset = 1 * kSystemPointerSize;
   static constexpr int kNativeModuleOffset = 0;
 };
 
@@ -46,8 +47,8 @@
 // registers (see liftoff-assembler-defs.h).
 class WasmDebugBreakFrameConstants : public TypedFrameConstants {
  public:
-  static constexpr RegList kPushedGpRegs = {r3, r4, r5,  r6,  r7,
-                                            r8, r9, r10, r11, cp};
+  static constexpr RegList kPushedGpRegs = {r3, r4,  r5,  r6,  r7, r8,
+                                            r9, r10, r11, r15, cp};
 
   static constexpr DoubleRegList kPushedFpRegs = {d0, d1, d2, d3,  d4,  d5, d6,
                                                   d7, d8, d9, d10, d11, d12};
diff -r -u --color up/v8/src/execution/riscv/frame-constants-riscv.h nw/v8/src/execution/riscv/frame-constants-riscv.h
--- up/v8/src/execution/riscv/frame-constants-riscv.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/execution/riscv/frame-constants-riscv.h	2023-01-19 16:46:36.189776244 -0500
@@ -21,7 +21,7 @@
   static constexpr int kCallerFPOffset = -3 * kSystemPointerSize;
 };
 
-class WasmCompileLazyFrameConstants : public TypedFrameConstants {
+class WasmLiftoffSetupFrameConstants : public TypedFrameConstants {
  public:
   // Number of gp parameters, without the instance.
   static constexpr int kNumberOfSavedGpParamRegs =
@@ -39,7 +39,7 @@
 
   // SP-relative.
   static constexpr int kWasmInstanceOffset = 2 * kSystemPointerSize;
-  static constexpr int kFunctionIndexOffset = 1 * kSystemPointerSize;
+  static constexpr int kDeclaredFunctionIndexOffset = 1 * kSystemPointerSize;
   static constexpr int kNativeModuleOffset = 0;
 };
 
diff -r -u --color up/v8/src/execution/s390/frame-constants-s390.h nw/v8/src/execution/s390/frame-constants-s390.h
--- up/v8/src/execution/s390/frame-constants-s390.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/execution/s390/frame-constants-s390.h	2023-01-19 16:46:36.189776244 -0500
@@ -21,7 +21,7 @@
   static constexpr int kArgvOffset = 20 * kSystemPointerSize;
 };
 
-class WasmCompileLazyFrameConstants : public TypedFrameConstants {
+class WasmLiftoffSetupFrameConstants : public TypedFrameConstants {
  public:
   static constexpr int kNumberOfSavedGpParamRegs = 3;
 #ifdef V8_TARGET_ARCH_S390X
@@ -30,16 +30,17 @@
   static constexpr int kNumberOfSavedFpParamRegs = 2;
 #endif
 
+  // There's one spilled value (which doesn't need visiting) below the instance.
   static constexpr int kInstanceSpillOffset =
-      TYPED_FRAME_PUSHED_VALUE_OFFSET(0);
+      TYPED_FRAME_PUSHED_VALUE_OFFSET(1);
 
   static constexpr int kParameterSpillsOffset[] = {
-      TYPED_FRAME_PUSHED_VALUE_OFFSET(1), TYPED_FRAME_PUSHED_VALUE_OFFSET(2),
-      TYPED_FRAME_PUSHED_VALUE_OFFSET(3)};
+      TYPED_FRAME_PUSHED_VALUE_OFFSET(2), TYPED_FRAME_PUSHED_VALUE_OFFSET(3),
+      TYPED_FRAME_PUSHED_VALUE_OFFSET(4)};
 
   // SP-relative.
   static constexpr int kWasmInstanceOffset = 2 * kSystemPointerSize;
-  static constexpr int kFunctionIndexOffset = 1 * kSystemPointerSize;
+  static constexpr int kDeclaredFunctionIndexOffset = 1 * kSystemPointerSize;
   static constexpr int kNativeModuleOffset = 0;
 };
 
diff -r -u --color up/v8/src/execution/thread-local-top.cc nw/v8/src/execution/thread-local-top.cc
--- up/v8/src/execution/thread-local-top.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/execution/thread-local-top.cc	2023-01-19 16:46:36.200609574 -0500
@@ -37,12 +37,14 @@
   current_embedder_state_ = nullptr;
   failed_access_check_callback_ = nullptr;
   thread_in_wasm_flag_address_ = kNullAddress;
+  stack_ = ::heap::base::Stack();
 }
 
 void ThreadLocalTop::Initialize(Isolate* isolate) {
   Clear();
   isolate_ = isolate;
   thread_id_ = ThreadId::Current();
+  stack_.SetStackStart(base::Stack::GetStackStart());
 #if V8_ENABLE_WEBASSEMBLY
   thread_in_wasm_flag_address_ = reinterpret_cast<Address>(
       trap_handler::GetThreadInWasmThreadLocalAddress());
diff -r -u --color up/v8/src/execution/thread-local-top.h nw/v8/src/execution/thread-local-top.h
--- up/v8/src/execution/thread-local-top.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/execution/thread-local-top.h	2023-01-19 16:46:36.200609574 -0500
@@ -10,6 +10,7 @@
 #include "include/v8-unwinder.h"
 #include "src/common/globals.h"
 #include "src/execution/thread-id.h"
+#include "src/heap/base/stack.h"
 #include "src/objects/contexts.h"
 #include "src/utils/utils.h"
 
@@ -29,7 +30,7 @@
   // TODO(all): This is not particularly beautiful. We should probably
   // refactor this to really consist of just Addresses and 32-bit
   // integer fields.
-  static constexpr uint32_t kSizeInBytes = 25 * kSystemPointerSize;
+  static constexpr uint32_t kSizeInBytes = 27 * kSystemPointerSize;
 
   // Does early low-level initialization that does not depend on the
   // isolate being present.
@@ -146,6 +147,9 @@
 
   // Address of the thread-local "thread in wasm" flag.
   Address thread_in_wasm_flag_address_;
+
+  // Stack information.
+  ::heap::base::Stack stack_;
 };
 
 }  // namespace internal
diff -r -u --color up/v8/src/execution/tiering-manager.cc nw/v8/src/execution/tiering-manager.cc
--- up/v8/src/execution/tiering-manager.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/execution/tiering-manager.cc	2023-01-19 16:46:36.200609574 -0500
@@ -155,9 +155,11 @@
   return code_kind.has_value() && TiersUpToMaglev(code_kind.value());
 }
 
-int InterruptBudgetFor(base::Optional<CodeKind> code_kind) {
-  return TiersUpToMaglev(code_kind) ? v8_flags.interrupt_budget_for_maglev
-                                    : v8_flags.interrupt_budget;
+int InterruptBudgetFor(base::Optional<CodeKind> code_kind,
+                       TieringState tiering_state) {
+  return TiersUpToMaglev(code_kind) && tiering_state == TieringState::kNone
+             ? v8_flags.interrupt_budget_for_maglev
+             : v8_flags.interrupt_budget;
 }
 
 }  // namespace
@@ -165,7 +167,15 @@
 // static
 int TieringManager::InterruptBudgetFor(Isolate* isolate, JSFunction function) {
   if (function.has_feedback_vector()) {
-    return ::i::InterruptBudgetFor(function.GetActiveTier());
+    if (function.shared().GetBytecodeArray(isolate).length() >
+        v8_flags.max_optimized_bytecode_size) {
+      // Decrease times of interrupt budget underflow, the reason of not setting
+      // to INT_MAX is the interrupt budget may overflow when doing add
+      // operation for forward jump.
+      return INT_MAX / 2;
+    }
+    return ::i::InterruptBudgetFor(function.GetActiveTier(),
+                                   function.tiering_state());
   }
 
   DCHECK(!function.has_feedback_vector());
@@ -203,7 +213,7 @@
   static const int kOSRBytecodeSizeAllowanceBase = 119;
   static const int kOSRBytecodeSizeAllowancePerTick = 44;
   const double scale_factor_for_active_tier =
-      InterruptBudgetFor(code_kind) /
+      InterruptBudgetFor(code_kind, TieringState::kNone) /
       static_cast<double>(v8_flags.interrupt_budget);
 
   const double raw_limit = kOSRBytecodeSizeAllowanceBase +
@@ -276,8 +286,8 @@
   }
 
   if (V8_UNLIKELY(v8_flags.testing_d8_test_runner) &&
-      !PendingOptimizationTable::IsHeuristicOptimizationAllowed(isolate_,
-                                                                function)) {
+      ManualOptimizationTable::IsMarkedForManualOptimization(isolate_,
+                                                             function)) {
     TraceHeuristicOptimizationDisallowed(function);
     return;
   }
@@ -344,6 +354,9 @@
   }
 
   BytecodeArray bytecode = function.shared().GetBytecodeArray(isolate_);
+  if (bytecode.length() > v8_flags.max_optimized_bytecode_size) {
+    return OptimizationDecision::DoNotOptimize();
+  }
   const int ticks = function.feedback_vector().profiler_ticks();
   const int ticks_for_optimization =
       v8_flags.ticks_before_optimization +
@@ -392,11 +405,8 @@
   // Sparkplug only when reaching this point *with* a feedback vector.
   const bool had_feedback_vector = function->has_feedback_vector();
 
-  // Ensure that the feedback vector has been allocated, and reset the
-  // interrupt budget in preparation for the next tick.
-  if (had_feedback_vector) {
-    function->SetInterruptBudget(isolate_);
-  } else {
+  // Ensure that the feedback vector has been allocated.
+  if (!had_feedback_vector) {
     JSFunction::CreateAndAttachFeedbackVector(isolate_, function,
                                               &is_compiled_scope);
     DCHECK(is_compiled_scope.is_compiled());
@@ -432,11 +442,18 @@
   }
 
   // We only tier up beyond sparkplug if we already had a feedback vector.
-  if (!had_feedback_vector) return;
+  if (!had_feedback_vector) {
+    // The interrupt budget has already been set by
+    // JSFunction::CreateAndAttachFeedbackVector.
+    return;
+  }
 
   // Don't tier up if Turbofan is disabled.
   // TODO(jgruber): Update this for a multi-tier world.
-  if (V8_UNLIKELY(!isolate_->use_optimizer())) return;
+  if (V8_UNLIKELY(!isolate_->use_optimizer())) {
+    function->SetInterruptBudget(isolate_);
+    return;
+  }
 
   // --- We've decided to proceed for now. ---
 
@@ -447,6 +464,11 @@
   function_obj.feedback_vector().SaturatingIncrementProfilerTicks();
 
   MaybeOptimizeFrame(function_obj, code_kind);
+
+  // Make sure to set the interrupt budget after maybe starting an optimization,
+  // so that the interrupt budget size takes into account tiering state.
+  DCHECK(had_feedback_vector);
+  function->SetInterruptBudget(isolate_);
 }
 
 }  // namespace internal
diff -r -u --color up/v8/src/execution/x64/frame-constants-x64.h nw/v8/src/execution/x64/frame-constants-x64.h
--- up/v8/src/execution/x64/frame-constants-x64.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/execution/x64/frame-constants-x64.h	2023-01-19 16:46:36.200609574 -0500
@@ -43,23 +43,24 @@
 #endif
 };
 
-class WasmCompileLazyFrameConstants : public TypedFrameConstants {
+class WasmLiftoffSetupFrameConstants : public TypedFrameConstants {
  public:
   // Number of gp parameters, without the instance.
   static constexpr int kNumberOfSavedGpParamRegs = 5;
   static constexpr int kNumberOfSavedFpParamRegs = 6;
 
+  // There's one spilled value (which doesn't need visiting) below the instance.
   static constexpr int kInstanceSpillOffset =
-      TYPED_FRAME_PUSHED_VALUE_OFFSET(0);
+      TYPED_FRAME_PUSHED_VALUE_OFFSET(1);
 
   static constexpr int kParameterSpillsOffset[] = {
-      TYPED_FRAME_PUSHED_VALUE_OFFSET(1), TYPED_FRAME_PUSHED_VALUE_OFFSET(2),
-      TYPED_FRAME_PUSHED_VALUE_OFFSET(3), TYPED_FRAME_PUSHED_VALUE_OFFSET(4),
-      TYPED_FRAME_PUSHED_VALUE_OFFSET(5)};
+      TYPED_FRAME_PUSHED_VALUE_OFFSET(2), TYPED_FRAME_PUSHED_VALUE_OFFSET(3),
+      TYPED_FRAME_PUSHED_VALUE_OFFSET(4), TYPED_FRAME_PUSHED_VALUE_OFFSET(5),
+      TYPED_FRAME_PUSHED_VALUE_OFFSET(6)};
 
   // SP-relative.
   static constexpr int kWasmInstanceOffset = 2 * kSystemPointerSize;
-  static constexpr int kFunctionIndexOffset = 1 * kSystemPointerSize;
+  static constexpr int kDeclaredFunctionIndexOffset = 1 * kSystemPointerSize;
   static constexpr int kNativeModuleOffset = 0;
 };
 
diff -r -u --color up/v8/src/flags/flag-definitions.h nw/v8/src/flags/flag-definitions.h
--- up/v8/src/flags/flag-definitions.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/flags/flag-definitions.h	2023-01-19 16:46:36.200609574 -0500
@@ -36,16 +36,6 @@
 #define FLAG_READONLY(ftype, ctype, nam, def, cmt) \
   static constexpr FlagValue<ctype> nam{def};
 
-// Define a {FLAG_foo} alias per flag, pointing to {v8_flags.foo}.
-// This allows to still use the old and deprecated syntax for accessing flag
-// values. This will be removed after v10.7.
-// TODO(clemensb): Remove this after v10.7.
-#elif defined(FLAG_MODE_DEFINE_GLOBAL_ALIASES)
-#define FLAG_FULL(ftype, ctype, nam, def, cmt) \
-  inline auto& FLAG_##nam = v8_flags.nam;
-#define FLAG_READONLY(ftype, ctype, nam, def, cmt) \
-  inline auto constexpr& FLAG_##nam = v8_flags.nam;
-
 // We need to define all of our default values so that the Flag structure can
 // access them by pointer.  These are just used internally inside of one .cc,
 // for MODE_META, so there is no impact on the flags interface.
@@ -241,12 +231,11 @@
 #endif
 
 // Features that are complete (but still behind the --harmony flag).
-#define HARMONY_STAGED_BASE(V)                                    \
-  V(harmony_rab_gsab,                                             \
-    "harmony ResizableArrayBuffer / GrowableSharedArrayBuffer")   \
-  V(harmony_array_grouping, "harmony array grouping")             \
-  V(harmony_change_array_by_copy, "harmony change-Array-by-copy") \
-  V(harmony_symbol_as_weakmap_key, "harmony symbols as weakmap keys")
+#define HARMONY_STAGED_BASE(V)                                  \
+  V(harmony_rab_gsab,                                           \
+    "harmony ResizableArrayBuffer / GrowableSharedArrayBuffer") \
+  V(harmony_array_grouping, "harmony array grouping")           \
+  V(harmony_change_array_by_copy, "harmony change-Array-by-copy")
 
 #ifdef V8_INTL_SUPPORT
 #define HARMONY_STAGED(V) HARMONY_STAGED_BASE(V)
@@ -260,7 +249,8 @@
   V(harmony_atomics, "harmony atomics")                               \
   V(harmony_class_static_blocks, "harmony static initializer blocks") \
   V(harmony_array_find_last, "harmony array find last helpers")       \
-  V(harmony_import_assertions, "harmony import assertions")
+  V(harmony_import_assertions, "harmony import assertions")           \
+  V(harmony_symbol_as_weakmap_key, "harmony symbols as weakmap keys")
 
 #ifdef V8_INTL_SUPPORT
 #define HARMONY_SHIPPING(V) \
@@ -424,6 +414,11 @@
                      V8_ENABLE_CONSERVATIVE_STACK_SCANNING_BOOL,
                      "use conservative stack scanning")
 
+#if V8_ENABLE_WEBASSEMBLY
+DEFINE_NEG_IMPLICATION(conservative_stack_scanning,
+                       experimental_wasm_stack_switching)
+#endif  // V8_ENABLE_WEBASSEMBLY
+
 #ifdef V8_ENABLE_INNER_POINTER_RESOLUTION_OSB
 #define V8_ENABLE_INNER_POINTER_RESOLUTION_OSB_BOOL true
 #else
@@ -451,6 +446,9 @@
             "Implies all staged features that we want to ship in the "
             "not-too-far future")
 
+DEFINE_BOOL(lower_tier_as_toptier, false,
+            "remove tier-up logic from the top tier")
+
 #ifdef V8_ENABLE_MAGLEV
 #define V8_ENABLE_MAGLEV_BOOL true
 DEFINE_BOOL(maglev, false, "enable the maglev optimizing compiler")
@@ -491,9 +489,6 @@
 DEFINE_WEAK_IMPLICATION(future, short_builtin_calls)
 #endif
 DEFINE_WEAK_NEG_IMPLICATION(future, write_protect_code_memory)
-DEFINE_WEAK_NEG_IMPLICATION(future, use_map_space)
-DEFINE_WEAK_IMPLICATION(
-    future, merge_background_deserialized_script_with_compilation_cache)
 
 DEFINE_BOOL_READONLY(dict_property_const_tracking,
                      V8_DICT_PROPERTY_CONST_TRACKING_BOOL,
@@ -523,6 +518,8 @@
 DEFINE_BOOL(jitless, V8_LITE_BOOL,
             "Disable runtime allocation of executable memory.")
 
+DEFINE_WEAK_IMPLICATION(jitless, lower_tier_as_toptier)
+
 // Jitless V8 has a few implications:
 DEFINE_NEG_IMPLICATION(jitless, turbofan)
 // Field type tracking is only used by TurboFan.
@@ -607,7 +604,7 @@
 // Tiering: Maglev.
 // The Maglev interrupt budget is chosen to be roughly 1/10th of Turbofan's
 // overall budget (including the multiple required ticks).
-DEFINE_INT(interrupt_budget_for_maglev, 40 * KB,
+DEFINE_INT(interrupt_budget_for_maglev, 30 * KB,
            "interrupt budget which should be used for the profiler counter")
 
 // Tiering: Turbofan.
@@ -645,7 +642,7 @@
             "collect lazy source positions immediately after lazy compile")
 DEFINE_STRING(print_bytecode_filter, "*",
               "filter for selecting which functions to print bytecode")
-DEFINE_BOOL(omit_default_ctors, false, "omit calling default ctors in bytecode")
+DEFINE_BOOL(omit_default_ctors, true, "omit calling default ctors in bytecode")
 #ifdef V8_TRACE_UNOPTIMIZED
 DEFINE_BOOL(trace_unoptimized, false,
             "trace the bytecodes executed by all unoptimized execution")
@@ -764,6 +761,12 @@
 DEFINE_NEG_IMPLICATION(stress_concurrent_inlining, lazy_feedback_allocation)
 DEFINE_WEAK_VALUE_IMPLICATION(stress_concurrent_inlining, interrupt_budget,
                               15 * KB)
+DEFINE_BOOL(maglev_overwrite_budget, false,
+            "whether maglev resets the interrupt budget")
+DEFINE_WEAK_IMPLICATION(maglev, maglev_overwrite_budget)
+DEFINE_NEG_IMPLICATION(stress_concurrent_inlining, maglev_overwrite_budget)
+DEFINE_WEAK_VALUE_IMPLICATION(maglev_overwrite_budget, interrupt_budget,
+                              80 * KB)
 DEFINE_BOOL(stress_concurrent_inlining_attach_code, false,
             "create additional concurrent optimization jobs")
 DEFINE_IMPLICATION(stress_concurrent_inlining_attach_code,
@@ -835,7 +838,7 @@
 // non-ENABLE_VERIFY_CSA configuration.
 DEFINE_BOOL_READONLY(verify_csa, false,
                      "verify TurboFan machine graph of code stubs")
-#endif
+#endif  // ENABLE_VERIFY_CSA
 DEFINE_BOOL(trace_verify_csa, false, "trace code stubs verification")
 DEFINE_STRING(csa_trap_on_node, nullptr,
               "trigger break point when a node with given id is created in "
@@ -937,6 +940,9 @@
 #endif
 
 DEFINE_BOOL(
+    turbo_rab_gsab, true,
+    "optimize ResizableArrayBuffer / GrowableSharedArrayBuffer in TurboFan")
+DEFINE_BOOL(
     stress_gc_during_compilation, false,
     "simulate GC/compiler thread race related to https://crbug.com/v8/8520")
 DEFINE_BOOL(turbo_fast_api_calls, true, "enable fast API calls from TurboFan")
@@ -954,6 +960,8 @@
             "always use the mid-tier register allocator (for testing)")
 
 DEFINE_BOOL(turbo_optimize_apply, true, "optimize Function.prototype.apply")
+DEFINE_BOOL(turbo_optimize_math_minmax, true,
+            "optimize call math.min/max with double array")
 
 DEFINE_BOOL(turbo_collect_feedback_in_generic_lowering, true,
             "enable experimental feedback collection in generic lowering.")
@@ -993,10 +1001,9 @@
 DEFINE_BOOL(wasm_generic_wrapper, true,
             "allow use of the generic js-to-wasm wrapper instead of "
             "per-signature wrappers")
-DEFINE_BOOL(enable_wasm_arm64_generic_wrapper, false,
+DEFINE_BOOL(enable_wasm_arm64_generic_wrapper, true,
             "allow use of the generic js-to-wasm wrapper instead of "
             "per-signature wrappers on arm64")
-DEFINE_WEAK_IMPLICATION(future, enable_wasm_arm64_generic_wrapper)
 DEFINE_BOOL(expose_wasm, true, "expose wasm interface to JavaScript")
 DEFINE_INT(wasm_num_compilation_tasks, 128,
            "maximum number of parallel compilation tasks for wasm")
@@ -1115,7 +1122,8 @@
 DEFINE_IMPLICATION(experimental_wasm_stack_switching,
                    experimental_wasm_type_reflection)
 
-DEFINE_BOOL(wasm_gc_js_interop, true, "experimental WasmGC-JS interop")
+DEFINE_BOOL(wasm_gc_structref_as_dataref, true,
+            "compatibility mode: Treat structref as dataref")
 
 DEFINE_BOOL(wasm_staging, false, "enable staged wasm features")
 
@@ -1188,6 +1196,8 @@
 
 DEFINE_SIZE_T(wasm_max_module_size, wasm::kV8MaxWasmModuleSize,
               "maximum allowed size of wasm modules")
+DEFINE_SIZE_T(wasm_disassembly_max_mb, 1000,
+              "maximum size of produced disassembly (in MB, approximate)")
 
 DEFINE_BOOL(trace_wasm, false, "trace wasm function calls")
 
@@ -1242,8 +1252,6 @@
 DEFINE_BOOL(gc_global, false, "always perform global GCs")
 DEFINE_BOOL(shared_space, false,
             "Implement shared heap as shared space on a main isolate.")
-// Don't use a map space with --shared-space in order to avoid shared map space.
-DEFINE_NEG_IMPLICATION(shared_space, use_map_space)
 
 // TODO(12950): The next two flags only have an effect if
 // V8_ENABLE_ALLOCATION_TIMEOUT is set, so we should only define them in that
@@ -1419,11 +1427,6 @@
             "Perform compaction on full GCs based on V8's default heuristics")
 DEFINE_BOOL(compact_code_space, true,
             "Perform code space compaction on full collections.")
-DEFINE_BOOL(compact_maps, true,
-            "Perform compaction on maps on full collections.")
-DEFINE_BOOL(use_map_space, false, "Use separate space for maps.")
-// Without a map space we have to compact maps.
-DEFINE_NEG_VALUE_IMPLICATION(use_map_space, compact_maps, true)
 DEFINE_BOOL(compact_on_every_full_gc, false,
             "Perform compaction on every full GC")
 DEFINE_BOOL(compact_with_stack, true,
@@ -1581,7 +1584,7 @@
 DEFINE_BOOL(concurrent_cache_deserialization, true,
             "enable deserializing code caches on background")
 DEFINE_BOOL(
-    merge_background_deserialized_script_with_compilation_cache, false,
+    merge_background_deserialized_script_with_compilation_cache, true,
     "After deserializing code cache data on a background thread, merge it into "
     "an existing Script if one is found in the Isolate compilation cache")
 DEFINE_BOOL(disable_old_api_accessors, false,
@@ -1707,16 +1710,11 @@
 DEFINE_BOOL(hard_abort, true, "abort by crashing")
 DEFINE_NEG_IMPLICATION(fuzzing, hard_abort)
 
-DEFINE_BOOL(experimental_async_stack_tagging_api, true,
-            "enable experimental async stacks tagging API")
 DEFINE_BOOL(experimental_value_unavailable, false,
             "enable experimental <value unavailable> in scopes")
-DEFINE_BOOL(experimental_reuse_locals_blocklists, false,
+DEFINE_BOOL(experimental_reuse_locals_blocklists, true,
             "enable reuse of local blocklists across multiple debug-evaluates")
 
-DEFINE_BOOL(
-    live_edit_top_frame, true,
-    "enable support for live-editing the top-most function on the stack")
 DEFINE_BOOL(experimental_remove_internal_scopes_property, true,
             "don't report the artificial [[Scopes]] property for functions")
 
@@ -1794,7 +1792,7 @@
 DEFINE_IMPLICATION(log_maps, log_code)
 
 // parser.cc
-DEFINE_BOOL(allow_natives_syntax, false, "allow natives syntax")
+DEFINE_BOOL(allow_natives_syntax, true, "allow natives syntax")
 DEFINE_BOOL(allow_natives_for_differential_fuzzing, false,
             "allow only natives explicitly allowlisted for differential "
             "fuzzers")
@@ -1962,8 +1960,15 @@
     "Fuzzers use this flag to signal that they are ... fuzzing. This causes "
     "intrinsics to fail silently (e.g. return undefined) on invalid usage.")
 
+#if defined(V8_OS_AIX) && defined(COMPONENT_BUILD)
+// FreezeFlags relies on mprotect() method, which does not work by default on
+// shared mem: https://www.ibm.com/docs/en/aix/7.2?topic=m-mprotect-subroutine
+DEFINE_BOOL(freeze_flags_after_init, false,
+            "Disallow changes to flag values after initializing V8")
+#else
 DEFINE_BOOL(freeze_flags_after_init, true,
             "Disallow changes to flag values after initializing V8")
+#endif  // defined(V8_OS_AIX) && defined(COMPONENT_BUILD)
 
 // mksnapshot.cc
 DEFINE_STRING(embedded_src, nullptr,
@@ -2003,10 +2008,6 @@
             "perform young generation marking concurrently")
 DEFINE_NEG_NEG_IMPLICATION(concurrent_marking, concurrent_minor_mc_marking)
 
-DEFINE_BOOL(concurrent_minor_mc_sweeping, false,
-            "perform young generation sweeping concurrently")
-DEFINE_NEG_NEG_IMPLICATION(concurrent_sweeping, concurrent_minor_mc_sweeping)
-
 //
 // Dev shell flags
 //
@@ -2015,6 +2016,7 @@
 DEFINE_BOOL(print_flag_values, false, "Print all flag values of V8")
 
 // Slow histograms are also enabled via --dump-counters in d8.
+DEFINE_BOOL(nw_module, false, "Whether the input file is a module")
 DEFINE_BOOL(slow_histograms, false,
             "Enable slow histograms with more overhead.")
 
@@ -2116,9 +2118,9 @@
 //
 // Logging and profiling flags
 //
-// Logging flag dependencies are are also set separately in
+// Logging flag dependencies are also set separately in
 // V8::InitializeOncePerProcessImpl. Please add your flag to the log_all_flags
-// list in v8.cc to properly set FLAG_log and automatically enable it with
+// list in v8.cc to properly set v8_flags.log and automatically enable it with
 // --log-all.
 #undef FLAG
 #define FLAG FLAG_FULL
@@ -2392,7 +2394,6 @@
 #undef DEFINE_ALIAS_FLOAT
 
 #undef FLAG_MODE_DECLARE
-#undef FLAG_MODE_DEFINE_GLOBAL_ALIASES
 #undef FLAG_MODE_DEFINE_DEFAULTS
 #undef FLAG_MODE_META
 #undef FLAG_MODE_DEFINE_IMPLICATIONS
diff -r -u --color up/v8/src/flags/flags.h nw/v8/src/flags/flags.h
--- up/v8/src/flags/flags.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/flags/flags.h	2023-01-19 16:46:36.200609574 -0500
@@ -70,10 +70,6 @@
 
 V8_EXPORT_PRIVATE extern FlagValues v8_flags;
 
-// TODO(clemensb): Remove this after v10.7.
-#define FLAG_MODE_DEFINE_GLOBAL_ALIASES
-#include "src/flags/flag-definitions.h"  // NOLINT(build/include)
-
 // The global list of all flags.
 class V8_EXPORT_PRIVATE FlagList {
  public:
diff -r -u --color up/v8/src/handles/global-handles-inl.h nw/v8/src/handles/global-handles-inl.h
--- up/v8/src/handles/global-handles-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/handles/global-handles-inl.h	2023-01-19 16:46:36.200609574 -0500
@@ -27,12 +27,6 @@
   return obj;
 }
 
-// static
-Object GlobalHandles::Acquire(Address* location) {
-  return Object(reinterpret_cast<std::atomic<Address>*>(location)->load(
-      std::memory_order_acquire));
-}
-
 }  // namespace internal
 }  // namespace v8
 
diff -r -u --color up/v8/src/handles/global-handles.cc nw/v8/src/handles/global-handles.cc
--- up/v8/src/handles/global-handles.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/handles/global-handles.cc	2023-01-19 16:46:36.200609574 -0500
@@ -10,7 +10,6 @@
 #include <cstdint>
 #include <map>
 
-#include "include/v8-traced-handle.h"
 #include "src/api/api-inl.h"
 #include "src/base/compiler-specific.h"
 #include "src/base/logging.h"
@@ -196,7 +195,6 @@
  public:
   using BlockType = NodeBlock<NodeType>;
   using iterator = NodeIterator<BlockType>;
-  using NodeBounds = GlobalHandles::NodeBounds;
 
   static NodeSpace* From(NodeType* node);
   static void Release(NodeType* node);
@@ -213,8 +211,6 @@
   size_t TotalSize() const { return blocks_ * sizeof(NodeType) * kBlockSize; }
   size_t handles_count() const { return handles_count_; }
 
-  NodeBounds GetNodeBlockBounds() const;
-
  private:
   void PutNodesOnFreeList(BlockType* block);
   V8_INLINE void Free(NodeType* node);
@@ -258,21 +254,6 @@
 }
 
 template <class NodeType>
-typename GlobalHandles::NodeSpace<NodeType>::NodeBounds
-GlobalHandles::NodeSpace<NodeType>::GetNodeBlockBounds() const {
-  NodeBounds block_bounds;
-  for (BlockType* current = first_used_block_; current;
-       current = current->next_used()) {
-    block_bounds.push_back({current->begin_address(), current->end_address()});
-  }
-  std::sort(block_bounds.begin(), block_bounds.end(),
-            [](const auto& pair1, const auto& pair2) {
-              return pair1.first < pair2.first;
-            });
-  return block_bounds;
-}
-
-template <class NodeType>
 void GlobalHandles::NodeSpace<NodeType>::PutNodesOnFreeList(BlockType* block) {
   for (int32_t i = kBlockSize - 1; i >= 0; --i) {
     NodeType* node = block->at(i);
@@ -611,163 +592,19 @@
   friend class NodeBase<Node>;
 };
 
-class GlobalHandles::TracedNode final
-    : public NodeBase<GlobalHandles::TracedNode> {
- public:
-  TracedNode() {
-    DCHECK(!is_in_young_list());
-    DCHECK(!markbit());
-  }
-
-  // Copy and move ctors are used when constructing a TracedNode when recording
-  // a node for on-stack data structures. (Older compilers may refer to copy
-  // instead of move ctor.)
-  TracedNode(TracedNode&& other) V8_NOEXCEPT = default;
-  TracedNode(const TracedNode& other) V8_NOEXCEPT = default;
-
-  enum State { FREE = 0, NORMAL, NEAR_DEATH };
-
-  State state() const { return NodeState::decode(flags_); }
-  void set_state(State state) { flags_ = NodeState::update(flags_, state); }
-
-  void MarkAsFree() { set_state(FREE); }
-  void MarkAsUsed() { set_state(NORMAL); }
-  bool IsInUse() const { return state() != FREE; }
-  bool IsRetainer() const { return state() == NORMAL; }
-
-  bool is_in_young_list() const { return IsInYoungList::decode(flags_); }
-  void set_in_young_list(bool v) { flags_ = IsInYoungList::update(flags_, v); }
-
-  bool is_root() const { return IsRoot::decode(flags_); }
-  void set_root(bool v) { flags_ = IsRoot::update(flags_, v); }
-
-  template <AccessMode access_mode = AccessMode::NON_ATOMIC>
-  void set_markbit() {
-    if constexpr (access_mode == AccessMode::NON_ATOMIC) {
-      flags_ = Markbit::update(flags_, true);
-      return;
-    }
-    std::atomic<uint8_t>& atomic_flags =
-        reinterpret_cast<std::atomic<uint8_t>&>(flags_);
-    const uint8_t new_value =
-        Markbit::update(atomic_flags.load(std::memory_order_relaxed), true);
-    atomic_flags.fetch_or(new_value, std::memory_order_relaxed);
-  }
-
-  template <AccessMode access_mode = AccessMode::NON_ATOMIC>
-  bool markbit() const {
-    if constexpr (access_mode == AccessMode::NON_ATOMIC) {
-      return Markbit::decode(flags_);
-    }
-    const auto flags =
-        reinterpret_cast<const std::atomic<uint8_t>&>(flags_).load(
-            std::memory_order_relaxed);
-    return Markbit::decode(flags);
-  }
-
-  void clear_markbit() { flags_ = Markbit::update(flags_, false); }
-
-  void clear_object() {
-    reinterpret_cast<std::atomic<Address>*>(&object_)->store(
-        kNullAddress, std::memory_order_relaxed);
-  }
-
-  void CopyObjectReference(const TracedNode& other) {
-    reinterpret_cast<std::atomic<Address>*>(&object_)->store(
-        other.object_, std::memory_order_relaxed);
-  }
-
-  void ResetPhantomHandle() {
-    DCHECK(IsInUse());
-    NodeSpace<TracedNode>::Release(this);
-    DCHECK(!IsInUse());
-  }
-
-  static void Verify(const Address* const* slot);
-
- protected:
-  // Various state is managed in a bit field where some of the state is managed
-  // concurrently, whereas other state is managed only on the main thread when
-  // no concurrent thread has access to flags, e.g., in the atomic pause of the
-  // garbage collector. All state is made available to other threads using
-  // `Publish()`.
-  //
-  // The following state is modified only on the main thread.
-  using NodeState = base::BitField8<State, 0, 2>;
-  using IsInYoungList = NodeState::Next<bool, 1>;
-  using IsRoot = IsInYoungList::Next<bool, 1>;
-  // The markbit is the exception as it can be set from the main and marker
-  // threads at the same time.
-  using Markbit = IsRoot::Next<bool, 1>;
-
-  void ClearImplFields() { set_root(true); }
-
-  void CheckNodeIsFreeNodeImpl() const {
-    DCHECK(is_root());
-    DCHECK(!markbit());
-    DCHECK(!IsInUse());
-  }
-
-  friend class NodeBase<GlobalHandles::TracedNode>;
-};
-
-// static
-void GlobalHandles::EnableMarkingBarrier(Isolate* isolate) {
-  auto* global_handles = isolate->global_handles();
-  DCHECK(!global_handles->is_marking_);
-  global_handles->is_marking_ = true;
-}
-
-// static
-void GlobalHandles::DisableMarkingBarrier(Isolate* isolate) {
-  auto* global_handles = isolate->global_handles();
-  DCHECK(global_handles->is_marking_);
-  global_handles->is_marking_ = false;
-}
-
-// static
-void GlobalHandles::TracedNode::Verify(const Address* const* slot) {
-#ifdef DEBUG
-  const TracedNode* node = FromLocation(*slot);
-  auto* global_handles = GlobalHandles::From(node);
-  DCHECK(node->IsInUse());
-  Heap* heap = global_handles->isolate()->heap();
-  auto* incremental_marking = heap->incremental_marking();
-  if (incremental_marking && incremental_marking->IsMarking()) {
-    Object object = node->object();
-    if (object.IsHeapObject()) {
-      DCHECK_IMPLIES(node->markbit<AccessMode::ATOMIC>(),
-                     !heap->marking_state()->IsWhite(HeapObject::cast(object)));
-    }
-  }
-  DCHECK_IMPLIES(ObjectInYoungGeneration(node->object()),
-                 node->is_in_young_list());
-  const bool in_young_list =
-      std::find(global_handles->traced_young_nodes_.begin(),
-                global_handles->traced_young_nodes_.end(),
-                node) != global_handles->traced_young_nodes_.end();
-  DCHECK_EQ(in_young_list, node->is_in_young_list());
-#endif  // DEBUG
-}
-
-size_t GlobalHandles::TotalSize() const {
-  return regular_nodes_->TotalSize() + traced_nodes_->TotalSize();
-}
+size_t GlobalHandles::TotalSize() const { return regular_nodes_->TotalSize(); }
 
 size_t GlobalHandles::UsedSize() const {
-  return regular_nodes_->handles_count() * sizeof(Node) +
-         traced_nodes_->handles_count() * sizeof(TracedNode);
+  return regular_nodes_->handles_count() * sizeof(Node);
 }
 
 size_t GlobalHandles::handles_count() const {
-  return regular_nodes_->handles_count() + traced_nodes_->handles_count();
+  return regular_nodes_->handles_count();
 }
 
 GlobalHandles::GlobalHandles(Isolate* isolate)
     : isolate_(isolate),
-      regular_nodes_(std::make_unique<NodeSpace<GlobalHandles::Node>>(this)),
-      traced_nodes_(
-          std::make_unique<NodeSpace<GlobalHandles::TracedNode>>(this)) {}
+      regular_nodes_(std::make_unique<NodeSpace<GlobalHandles::Node>>(this)) {}
 
 GlobalHandles::~GlobalHandles() = default;
 
@@ -793,25 +630,6 @@
   return Create(Object(value));
 }
 
-Handle<Object> GlobalHandles::CreateTraced(Object value, Address* slot,
-                                           GlobalHandleStoreMode store_mode) {
-  GlobalHandles::TracedNode* node = traced_nodes_->Allocate();
-  if (NeedsTrackingInYoungNodes(value, node)) {
-    traced_young_nodes_.push_back(node);
-    node->set_in_young_list(true);
-  }
-  if (is_marking_ && store_mode != GlobalHandleStoreMode::kInitializingStore) {
-    node->set_markbit();
-    WriteBarrier::MarkingFromGlobalHandle(value);
-  }
-  return node->Publish(value);
-}
-
-Handle<Object> GlobalHandles::CreateTraced(Address value, Address* slot,
-                                           GlobalHandleStoreMode store_mode) {
-  return CreateTraced(Object(value), slot, store_mode);
-}
-
 Handle<Object> GlobalHandles::CopyGlobal(Address* location) {
   DCHECK_NOT_NULL(location);
   GlobalHandles* global_handles =
@@ -824,35 +642,6 @@
   return global_handles->Create(*location);
 }
 
-namespace {
-void SetSlotThreadSafe(Address** slot, Address* val) {
-  reinterpret_cast<std::atomic<Address*>*>(slot)->store(
-      val, std::memory_order_relaxed);
-}
-}  // namespace
-
-// static
-void GlobalHandles::CopyTracedReference(const Address* const* from,
-                                        Address** to) {
-  DCHECK_NOT_NULL(*from);
-  DCHECK_NULL(*to);
-  const TracedNode* from_node = TracedNode::FromLocation(*from);
-  DCHECK_NE(kGlobalHandleZapValue, from_node->raw_object());
-  GlobalHandles* global_handles =
-      GlobalHandles::From(const_cast<TracedNode*>(from_node));
-  Handle<Object> o = global_handles->CreateTraced(
-      from_node->object(), reinterpret_cast<Address*>(to),
-      GlobalHandleStoreMode::kAssigningStore);
-  SetSlotThreadSafe(to, o.location());
-  TracedNode::Verify(from);
-  TracedNode::Verify(to);
-#ifdef VERIFY_HEAP
-  if (v8_flags.verify_heap) {
-    Object(**to).ObjectVerify(global_handles->isolate());
-  }
-#endif  // VERIFY_HEAP
-}
-
 // static
 void GlobalHandles::MoveGlobal(Address** from, Address** to) {
   DCHECK_NOT_NULL(*from);
@@ -865,109 +654,12 @@
   // Strong handles do not require fixups.
 }
 
-// static
-void GlobalHandles::MoveTracedReference(Address** from, Address** to) {
-  // Fast path for moving from an empty reference.
-  if (!*from) {
-    DestroyTracedReference(*to);
-    SetSlotThreadSafe(to, nullptr);
-    return;
-  }
-
-  // Determining whether from or to are on stack.
-  TracedNode* from_node = TracedNode::FromLocation(*from);
-  DCHECK(from_node->IsInUse());
-  TracedNode* to_node = TracedNode::FromLocation(*to);
-  // Pure heap move.
-  DCHECK_IMPLIES(*to, to_node->IsInUse());
-  DCHECK_IMPLIES(*to, kGlobalHandleZapValue != to_node->raw_object());
-  DCHECK_NE(kGlobalHandleZapValue, from_node->raw_object());
-  DestroyTracedReference(*to);
-  SetSlotThreadSafe(to, *from);
-  to_node = from_node;
-  DCHECK_NOT_NULL(*from);
-  DCHECK_NOT_NULL(*to);
-  DCHECK_EQ(*from, *to);
-  if (GlobalHandles::From(to_node)->is_marking_) {
-    // Write barrier needs to cover node as well as object.
-    to_node->set_markbit<AccessMode::ATOMIC>();
-    WriteBarrier::MarkingFromGlobalHandle(to_node->object());
-  }
-  SetSlotThreadSafe(from, nullptr);
-  TracedNode::Verify(to);
-}
-
-// static
-GlobalHandles* GlobalHandles::From(const TracedNode* node) {
-  return NodeBlock<TracedNode>::From(node)->global_handles();
-}
-
-// static
-void GlobalHandles::MarkTraced(Address* location) {
-  TracedNode* node = TracedNode::FromLocation(location);
-  DCHECK(node->IsInUse());
-  node->set_markbit<AccessMode::ATOMIC>();
-}
-
-// static
-Object GlobalHandles::MarkTracedConservatively(
-    Address* inner_location, Address* traced_node_block_base) {
-  // Compute the `TracedNode` address based on its inner pointer.
-  const ptrdiff_t delta = reinterpret_cast<uintptr_t>(inner_location) -
-                          reinterpret_cast<uintptr_t>(traced_node_block_base);
-  const auto index = delta / sizeof(TracedNode);
-  TracedNode& node =
-      reinterpret_cast<TracedNode*>(traced_node_block_base)[index];
-  if (!node.IsInUse()) return Smi::zero();
-  node.set_markbit<AccessMode::ATOMIC>();
-  return node.object();
-}
-
 void GlobalHandles::Destroy(Address* location) {
   if (location != nullptr) {
     NodeSpace<Node>::Release(Node::FromLocation(location));
   }
 }
 
-// static
-void GlobalHandles::DestroyTracedReference(Address* location) {
-  if (!location) return;
-
-  TracedNode* node = TracedNode::FromLocation(location);
-  auto* global_handles = GlobalHandles::From(node);
-  DCHECK_IMPLIES(global_handles->is_marking_,
-                 !global_handles->is_sweeping_on_mutator_thread_);
-  DCHECK_IMPLIES(global_handles->is_sweeping_on_mutator_thread_,
-                 !global_handles->is_marking_);
-
-  // If sweeping on the mutator thread is running then the handle destruction
-  // may be a result of a Reset() call from a destructor. The node will be
-  // reclaimed on the next cycle.
-  //
-  // This allows v8::TracedReference::Reset() calls from destructors on
-  // objects that may be used from stack and heap.
-  if (global_handles->is_sweeping_on_mutator_thread_) {
-    return;
-  }
-
-  if (global_handles->is_marking_) {
-    // Incremental marking is on. This also covers the scavenge case which
-    // prohibits eagerly reclaiming nodes when marking is on during a scavenge.
-    //
-    // On-heap traced nodes are released in the atomic pause in
-    // `IterateWeakRootsForPhantomHandles()` when they are discovered as not
-    // marked. Eagerly clear out the object here to avoid needlessly marking it
-    // from this point on. The node will be reclaimed on the next cycle.
-    node->clear_object();
-    return;
-  }
-
-  // In case marking and sweeping are off, the handle may be freed immediately.
-  // Note that this includes also the case when invoking the first pass
-  // callbacks during the atomic pause which requires releasing a node fully.
-  NodeSpace<TracedNode>::Release(node);
-}
-
 using GenericCallback = v8::WeakCallbackInfo<void>::Callback;
 
 void GlobalHandles::MakeWeak(Address* location, void* parameter,
@@ -1018,42 +710,6 @@
   for (Node* node : *regular_nodes_) {
     if (node->IsWeakRetainer()) ResetWeakNodeIfDead(node, should_reset_handle);
   }
-  for (TracedNode* node : *traced_nodes_) {
-    if (!node->IsInUse()) continue;
-    // Detect unreachable nodes first.
-    if (!node->markbit()) {
-      // The handle itself is unreachable. We can clear it even if the target V8
-      // object is alive.
-      node->ResetPhantomHandle();
-      continue;
-    }
-    // Clear the markbit for the next GC.
-    node->clear_markbit();
-    DCHECK(node->IsInUse());
-    // TODO(v8:13141): Turn into a DCHECK after some time.
-    CHECK(!should_reset_handle(isolate()->heap(), node->location()));
-  }
-}
-
-void GlobalHandles::ComputeWeaknessForYoungObjects(
-    WeakSlotCallback is_unmodified) {
-  if (!v8_flags.reclaim_unmodified_wrappers) return;
-
-  // Treat all objects as roots during incremental marking to avoid corrupting
-  // marking worklists.
-  if (isolate()->heap()->incremental_marking()->IsMarking()) return;
-
-  auto* const handler = isolate()->heap()->GetEmbedderRootsHandler();
-  for (TracedNode* node : traced_young_nodes_) {
-    if (node->IsInUse()) {
-      DCHECK(node->is_root());
-      if (is_unmodified(node->location())) {
-        v8::Value* value = ToApi<v8::Value>(node->handle());
-        node->set_root(handler->IsRoot(
-            *reinterpret_cast<v8::TracedReference<v8::Value>*>(&value)));
-      }
-    }
-  }
 }
 
 void GlobalHandles::IterateYoungStrongAndDependentRoots(RootVisitor* v) {
@@ -1063,11 +719,6 @@
                           node->location());
     }
   }
-  for (TracedNode* node : traced_young_nodes_) {
-    if (node->IsInUse() && node->is_root()) {
-      v->VisitRootPointer(Root::kGlobalHandles, nullptr, node->location());
-    }
-  }
 }
 
 void GlobalHandles::ProcessWeakYoungObjects(
@@ -1082,29 +733,6 @@
                           node->location());
     }
   }
-
-  if (!v8_flags.reclaim_unmodified_wrappers) return;
-
-  auto* const handler = isolate()->heap()->GetEmbedderRootsHandler();
-  for (TracedNode* node : traced_young_nodes_) {
-    if (!node->IsInUse()) continue;
-
-    DCHECK_IMPLIES(node->is_root(),
-                   !should_reset_handle(isolate_->heap(), node->location()));
-    if (should_reset_handle(isolate_->heap(), node->location())) {
-      v8::Value* value = ToApi<v8::Value>(node->handle());
-      handler->ResetRoot(
-          *reinterpret_cast<v8::TracedReference<v8::Value>*>(&value));
-      // We cannot check whether a node is in use here as the reset behavior
-      // depends on whether incremental marking is running when reclaiming
-      // young objects.
-    } else {
-      if (!node->is_root()) {
-        node->set_root(true);
-        v->VisitRootPointer(Root::kGlobalHandles, nullptr, node->location());
-      }
-    }
-  }
 }
 
 void GlobalHandles::InvokeSecondPassPhantomCallbacks() {
@@ -1175,12 +803,10 @@
 
 void GlobalHandles::UpdateListOfYoungNodes() {
   UpdateListOfYoungNodesImpl(isolate_, &young_nodes_);
-  UpdateListOfYoungNodesImpl(isolate_, &traced_young_nodes_);
 }
 
 void GlobalHandles::ClearListOfYoungNodes() {
   ClearListOfYoungNodesImpl(isolate_, &young_nodes_);
-  ClearListOfYoungNodesImpl(isolate_, &traced_young_nodes_);
 }
 
 template <typename T>
@@ -1275,11 +901,6 @@
                           node->location());
     }
   }
-  for (TracedNode* node : *traced_nodes_) {
-    if (node->IsInUse()) {
-      v->VisitRootPointer(Root::kGlobalHandles, nullptr, node->location());
-    }
-  }
 }
 
 DISABLE_CFI_PERF
@@ -1290,11 +911,6 @@
                           node->location());
     }
   }
-  for (TracedNode* node : *traced_nodes_) {
-    if (node->IsRetainer()) {
-      v->VisitRootPointer(Root::kGlobalHandles, nullptr, node->location());
-    }
-  }
 }
 
 DISABLE_CFI_PERF
@@ -1305,11 +921,6 @@
                           node->location());
     }
   }
-  for (TracedNode* node : traced_young_nodes_) {
-    if (node->IsRetainer()) {
-      v->VisitRootPointer(Root::kGlobalHandles, nullptr, node->location());
-    }
-  }
 }
 
 DISABLE_CFI_PERF
@@ -1330,25 +941,6 @@
   }
 }
 
-GlobalHandles::NodeBounds GlobalHandles::GetTracedNodeBounds() const {
-  return traced_nodes_->GetNodeBlockBounds();
-}
-
-START_ALLOW_USE_DEPRECATED()
-
-DISABLE_CFI_PERF void GlobalHandles::IterateTracedNodes(
-    v8::EmbedderHeapTracer::TracedGlobalHandleVisitor* visitor) {
-  for (TracedNode* node : *traced_nodes_) {
-    if (node->IsInUse()) {
-      v8::Value* value = ToApi<v8::Value>(node->handle());
-      visitor->VisitTracedReference(
-          *reinterpret_cast<v8::TracedReference<v8::Value>*>(&value));
-    }
-  }
-}
-
-END_ALLOW_USE_DEPRECATED()
-
 void GlobalHandles::RecordStats(HeapStats* stats) {
   *stats->global_handle_count = 0;
   *stats->weak_global_handle_count = 0;
diff -r -u --color up/v8/src/handles/global-handles.h nw/v8/src/handles/global-handles.h
--- up/v8/src/handles/global-handles.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/handles/global-handles.h	2023-01-19 16:46:36.200609574 -0500
@@ -13,7 +13,6 @@
 #include "include/v8-callbacks.h"
 #include "include/v8-persistent-handle.h"
 #include "include/v8-profiler.h"
-#include "include/v8-traced-handle.h"
 #include "src/handles/handles.h"
 #include "src/heap/heap.h"
 #include "src/objects/heap-object.h"
@@ -30,9 +29,6 @@
 // callbacks and finalizers attached to them.
 class V8_EXPORT_PRIVATE GlobalHandles final {
  public:
-  static void EnableMarkingBarrier(Isolate*);
-  static void DisableMarkingBarrier(Isolate*);
-
   GlobalHandles(const GlobalHandles&) = delete;
   GlobalHandles& operator=(const GlobalHandles&) = delete;
 
@@ -71,19 +67,6 @@
   // Tells whether global handle is weak.
   static bool IsWeak(Address* location);
 
-  //
-  // API for traced handles.
-  //
-
-  static void MoveTracedReference(Address** from, Address** to);
-  static void CopyTracedReference(const Address* const* from, Address** to);
-  static void DestroyTracedReference(Address* location);
-  static void MarkTraced(Address* location);
-  static Object MarkTracedConservatively(Address* inner_location,
-                                         Address* traced_node_block_base);
-
-  V8_INLINE static Object Acquire(Address* location);
-
   explicit GlobalHandles(Isolate* isolate);
   ~GlobalHandles();
 
@@ -94,11 +77,6 @@
   template <typename T>
   inline Handle<T> Create(T value);
 
-  Handle<Object> CreateTraced(Object value, Address* slot,
-                              GlobalHandleStoreMode store_mode);
-  Handle<Object> CreateTraced(Address value, Address* slot,
-                              GlobalHandleStoreMode store_mode);
-
   void RecordStats(HeapStats* stats);
 
   size_t InvokeFirstPassWeakCallbacks();
@@ -113,14 +91,6 @@
   void IterateAllRoots(RootVisitor* v);
   void IterateAllYoungRoots(RootVisitor* v);
 
-  START_ALLOW_USE_DEPRECATED()
-
-  // Iterates over all traces handles represented by `v8::TracedReferenceBase`.
-  void IterateTracedNodes(
-      v8::EmbedderHeapTracer::TracedGlobalHandleVisitor* visitor);
-
-  END_ALLOW_USE_DEPRECATED()
-
   // Marks handles that are phantom or have callbacks based on the predicate
   // |should_reset_handle| as pending.
   void IterateWeakRootsForPhantomHandles(
@@ -146,13 +116,6 @@
   // empty.
   void ClearListOfYoungNodes();
 
-  // Computes whether young weak objects should be considered roots for young
-  // generation garbage collections  or just be treated weakly. Per default
-  // objects are considered as roots. Objects are treated not as root when both
-  // - `is_unmodified()` returns true;
-  // - the `EmbedderRootsHandler` also does not consider them as roots;
-  void ComputeWeaknessForYoungObjects(WeakSlotCallback is_unmodified);
-
   Isolate* isolate() const { return isolate_; }
 
   size_t TotalSize() const;
@@ -160,18 +123,8 @@
   // Number of global handles.
   size_t handles_count() const;
 
-  using NodeBounds = std::vector<std::pair<const void*, const void*>>;
-  NodeBounds GetTracedNodeBounds() const;
-
   void IterateAllRootsForTesting(v8::PersistentHandleVisitor* v);
 
-  void NotifyStartSweepingOnMutatorThread() {
-    is_sweeping_on_mutator_thread_ = true;
-  }
-  void NotifyEndSweepingOnMutatorThread() {
-    is_sweeping_on_mutator_thread_ = false;
-  }
-
 #ifdef DEBUG
   void PrintStats();
   void Print();
@@ -185,9 +138,6 @@
   template <class NodeType>
   class NodeSpace;
   class PendingPhantomCallback;
-  class TracedNode;
-
-  static GlobalHandles* From(const TracedNode*);
 
   template <typename T>
   size_t InvokeFirstPassWeakCallbacks(
@@ -204,17 +154,11 @@
                            WeakSlotCallbackWithHeap should_reset_node);
 
   Isolate* const isolate_;
-  bool is_marking_ = false;
-  bool is_sweeping_on_mutator_thread_ = false;
 
   std::unique_ptr<NodeSpace<Node>> regular_nodes_;
   // Contains all nodes holding young objects. Note: when the list
   // is accessed, some of the objects may have been promoted already.
   std::vector<Node*> young_nodes_;
-
-  std::unique_ptr<NodeSpace<TracedNode>> traced_nodes_;
-  std::vector<TracedNode*> traced_young_nodes_;
-
   std::vector<std::pair<Node*, PendingPhantomCallback>>
       regular_pending_phantom_callbacks_;
   std::vector<PendingPhantomCallback> second_pass_callbacks_;
Only in nw/v8/src/handles: traced-handles.cc
Only in nw/v8/src/handles: traced-handles.h
diff -r -u --color up/v8/src/heap/OWNERS nw/v8/src/heap/OWNERS
--- up/v8/src/heap/OWNERS	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/OWNERS	2023-01-19 16:46:36.211442904 -0500
@@ -2,6 +2,7 @@
 dinfuehr@chromium.org
 hpayer@chromium.org
 mlippautz@chromium.org
+nikolaos@chromium.org
 omerkatz@chromium.org
 
 per-file *factory*=file:../objects/OWNERS
Only in up/v8/src/heap/base/asm/arm: push_registers_asm.cc
Only in nw/v8/src/heap/base/asm/arm: save_registers_asm.cc
Only in up/v8/src/heap/base/asm/arm64: push_registers_asm.cc
Only in up/v8/src/heap/base/asm/arm64: push_registers_masm.S
Only in nw/v8/src/heap/base/asm/arm64: save_registers_asm.cc
Only in nw/v8/src/heap/base/asm/arm64: save_registers_masm.S
Only in up/v8/src/heap/base/asm/ia32: push_registers_asm.cc
Only in up/v8/src/heap/base/asm/ia32: push_registers_masm.asm
Only in nw/v8/src/heap/base/asm/ia32: save_registers_asm.cc
Only in nw/v8/src/heap/base/asm/ia32: save_registers_masm.asm
Only in up/v8/src/heap/base/asm/loong64: push_registers_asm.cc
Only in nw/v8/src/heap/base/asm/loong64: save_registers_asm.cc
Only in up/v8/src/heap/base/asm/mips64: push_registers_asm.cc
Only in nw/v8/src/heap/base/asm/mips64: save_registers_asm.cc
Only in up/v8/src/heap/base/asm/ppc: push_registers_asm.cc
Only in nw/v8/src/heap/base/asm/ppc: save_registers_asm.cc
Only in up/v8/src/heap/base/asm/riscv: push_registers_asm.cc
Only in nw/v8/src/heap/base/asm/riscv: save_registers_asm.cc
Only in up/v8/src/heap/base/asm/s390: push_registers_asm.cc
Only in nw/v8/src/heap/base/asm/s390: save_registers_asm.cc
Only in up/v8/src/heap/base/asm/x64: push_registers_asm.cc
Only in up/v8/src/heap/base/asm/x64: push_registers_masm.asm
Only in nw/v8/src/heap/base/asm/x64: save_registers_asm.cc
Only in nw/v8/src/heap/base/asm/x64: save_registers_masm.asm
diff -r -u --color up/v8/src/heap/base/stack.cc nw/v8/src/heap/base/stack.cc
--- up/v8/src/heap/base/stack.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/base/stack.cc	2023-01-19 16:46:36.211442904 -0500
@@ -6,17 +6,11 @@
 
 #include <limits>
 
-#include "src/base/platform/platform.h"
 #include "src/base/sanitizer/asan.h"
 #include "src/base/sanitizer/msan.h"
 #include "src/base/sanitizer/tsan.h"
 
-namespace heap {
-namespace base {
-
-using IterateStackCallback = void (*)(const Stack*, StackVisitor*, intptr_t*);
-extern "C" void PushAllRegistersAndIterateStack(const Stack*, StackVisitor*,
-                                                IterateStackCallback);
+namespace heap::base {
 
 Stack::Stack(const void* stack_start) : stack_start_(stack_start) {}
 
@@ -24,13 +18,12 @@
   stack_start_ = stack_start;
 }
 
-bool Stack::IsOnStack(void* slot) const {
+bool Stack::IsOnStack(const void* slot) const {
   DCHECK_NOT_NULL(stack_start_);
 #ifdef V8_USE_ADDRESS_SANITIZER
   // If the slot is part of a fake frame, then it is definitely on the stack.
   if (__asan_addr_is_in_fake_stack(__asan_get_current_fake_stack(),
-                                   reinterpret_cast<void*>(slot), nullptr,
-                                   nullptr)) {
+                                   const_cast<void*>(slot), nullptr, nullptr)) {
     return true;
   }
   // Fall through as there is still a regular stack present even when running
@@ -60,9 +53,10 @@
 // other thread may use a lock to synchronize the access.
 DISABLE_TSAN
 void IterateAsanFakeFrameIfNecessary(StackVisitor* visitor,
-                                     void* asan_fake_stack,
+                                     const void* asan_fake_stack,
                                      const void* stack_start,
-                                     const void* stack_end, void* address) {
+                                     const void* stack_end,
+                                     const void* address) {
   // When using ASAN fake stack a pointer to the fake frame is kept on the
   // native frame. In case |addr| points to a fake frame of the current stack
   // iterate the fake frame. Frame layout see
@@ -71,17 +65,19 @@
     void* fake_frame_begin;
     void* fake_frame_end;
     void* real_stack_frame = __asan_addr_is_in_fake_stack(
-        asan_fake_stack, address, &fake_frame_begin, &fake_frame_end);
+        const_cast<void*>(asan_fake_stack), const_cast<void*>(address),
+        &fake_frame_begin, &fake_frame_end);
     if (real_stack_frame) {
       // |address| points to a fake frame. Check that the fake frame is part
       // of this stack.
       if (stack_start >= real_stack_frame && real_stack_frame >= stack_end) {
         // Iterate the fake frame.
-        for (void** current = reinterpret_cast<void**>(fake_frame_begin);
+        for (const void* const* current =
+                 reinterpret_cast<const void* const*>(fake_frame_begin);
              current < fake_frame_end; ++current) {
-          void* addr = *current;
-          if (addr == nullptr) continue;
-          visitor->VisitPointer(addr);
+          const void* address = *current;
+          if (address == nullptr) continue;
+          visitor->VisitPointer(address);
         }
       }
     }
@@ -96,16 +92,17 @@
   // Source:
   // https://github.com/llvm/llvm-project/blob/main/compiler-rt/lib/safestack/safestack.cpp
   constexpr size_t kSafeStackAlignmentBytes = 16;
-  void* stack_end = __builtin___get_unsafe_stack_ptr();
-  void* stack_start = __builtin___get_unsafe_stack_top();
+  const void* stack_end = __builtin___get_unsafe_stack_ptr();
+  const void* stack_start = __builtin___get_unsafe_stack_top();
   CHECK_GT(stack_start, stack_end);
   CHECK_EQ(0u, reinterpret_cast<uintptr_t>(stack_end) &
                    (kSafeStackAlignmentBytes - 1));
   CHECK_EQ(0u, reinterpret_cast<uintptr_t>(stack_start) &
                    (kSafeStackAlignmentBytes - 1));
-  void** current = reinterpret_cast<void**>(stack_end);
-  for (; current < stack_start; ++current) {
-    void* address = *current;
+  for (const void* const* current =
+           reinterpret_cast<const void* const*>(stack_end);
+       current < stack_start; ++current) {
+    const void* address = *current;
     if (address == nullptr) continue;
     visitor->VisitPointer(address);
   }
@@ -123,26 +120,45 @@
 // thread, e.g., for interrupt handling. Atomic reads are not enough as the
 // other thread may use a lock to synchronize the access.
 DISABLE_TSAN
-void IteratePointersImpl(const Stack* stack, StackVisitor* visitor,
-                         intptr_t* stack_end) {
+void IteratePointersImpl(StackVisitor* visitor, const void* stack_start,
+                         const void* stack_end,
+                         const Stack::CalleeSavedRegisters* registers) {
+#ifdef V8_USE_ADDRESS_SANITIZER
+  const void* asan_fake_stack = __asan_get_current_fake_stack();
+#endif  // V8_USE_ADDRESS_SANITIZER
+
+  // Iterate through the registers.
+  if (registers != nullptr) {
+    for (intptr_t value : registers->buffer) {
+      const void* address = reinterpret_cast<const void*>(value);
+      MSAN_MEMORY_IS_INITIALIZED(&address, sizeof(address));
+      if (address == nullptr) continue;
+      visitor->VisitPointer(address);
 #ifdef V8_USE_ADDRESS_SANITIZER
-  void* asan_fake_stack = __asan_get_current_fake_stack();
+      IterateAsanFakeFrameIfNecessary(visitor, asan_fake_stack, stack_start,
+                                      stack_end, address);
 #endif  // V8_USE_ADDRESS_SANITIZER
+    }
+  }
+
+  // Iterate through the stack.
   // All supported platforms should have their stack aligned to at least
   // sizeof(void*).
   constexpr size_t kMinStackAlignment = sizeof(void*);
-  void** current = reinterpret_cast<void**>(stack_end);
-  CHECK_EQ(0u, reinterpret_cast<uintptr_t>(current) & (kMinStackAlignment - 1));
-  for (; current < stack->stack_start(); ++current) {
+  CHECK_EQ(0u,
+           reinterpret_cast<uintptr_t>(stack_end) & (kMinStackAlignment - 1));
+  for (const void* const* current =
+           reinterpret_cast<const void* const*>(stack_end);
+       current < stack_start; ++current) {
     // MSAN: Instead of unpoisoning the whole stack, the slot's value is copied
     // into a local which is unpoisoned.
-    void* address = *current;
+    const void* address = *current;
     MSAN_MEMORY_IS_INITIALIZED(&address, sizeof(address));
     if (address == nullptr) continue;
     visitor->VisitPointer(address);
 #ifdef V8_USE_ADDRESS_SANITIZER
-    IterateAsanFakeFrameIfNecessary(visitor, asan_fake_stack,
-                                    stack->stack_start(), stack_end, address);
+    IterateAsanFakeFrameIfNecessary(visitor, asan_fake_stack, stack_start,
+                                    stack_end, address);
 #endif  // V8_USE_ADDRESS_SANITIZER
   }
 }
@@ -151,7 +167,8 @@
 
 void Stack::IteratePointers(StackVisitor* visitor) const {
   DCHECK_NOT_NULL(stack_start_);
-  PushAllRegistersAndIterateStack(this, visitor, &IteratePointersImpl);
+  PushAllRegistersAndInvokeCallback(visitor, stack_start_,
+                                    &IteratePointersImpl);
   // No need to deal with callee-saved registers as they will be kept alive by
   // the regular conservative stack iteration.
   // TODO(chromium:1056170): Add support for SIMD and/or filtering.
@@ -159,9 +176,63 @@
 }
 
 void Stack::IteratePointersUnsafe(StackVisitor* visitor,
-                                  uintptr_t stack_end) const {
-  IteratePointersImpl(this, visitor, reinterpret_cast<intptr_t*>(stack_end));
+                                  const void* stack_end) const {
+  IteratePointersImpl(visitor, stack_start_, stack_end, nullptr);
+}
+
+namespace {
+// Function with architecture-specific implementation:
+// Saves all callee-saved registers in the specified buffer.
+extern "C" void SaveCalleeSavedRegisters(intptr_t* buffer);
+}  // namespace
+
+V8_NOINLINE void Stack::PushAllRegistersAndInvokeCallback(
+    StackVisitor* visitor, const void* stack_start, Callback callback) {
+  Stack::CalleeSavedRegisters registers;
+  SaveCalleeSavedRegisters(registers.buffer.data());
+  callback(visitor, stack_start, v8::base::Stack::GetCurrentStackPosition(),
+           &registers);
+}
+
+namespace {
+
+#ifdef DEBUG
+
+bool IsOnCurrentStack(const void* ptr) {
+  DCHECK_NOT_NULL(ptr);
+  const void* current_stack_start = v8::base::Stack::GetStackStart();
+  const void* current_stack_top = v8::base::Stack::GetCurrentStackPosition();
+  return ptr <= current_stack_start && ptr >= current_stack_top;
+}
+
+bool IsValidMarker(const void* stack_start, const void* stack_marker) {
+  const void* current_stack_top = v8::base::Stack::GetCurrentStackPosition();
+  return stack_marker <= stack_start && stack_marker >= current_stack_top;
+}
+
+#endif  // DEBUG
+
+}  // namespace
+
+// In the following three methods, the stored stack start needs not coincide
+// with the current (actual) stack start (e.g., in case it was explicitly set to
+// a lower address, in tests) but has to be inside the current stack.
+
+void Stack::set_marker(const void* stack_marker) {
+  DCHECK(IsOnCurrentStack(stack_start_));
+  DCHECK_NOT_NULL(stack_marker);
+  DCHECK(IsValidMarker(stack_start_, stack_marker));
+  stack_marker_ = stack_marker;
+}
+
+void Stack::clear_marker() {
+  DCHECK(IsOnCurrentStack(stack_start_));
+  stack_marker_ = nullptr;
+}
+
+const void* Stack::get_marker() const {
+  DCHECK_NOT_NULL(stack_marker_);
+  return stack_marker_;
 }
 
-}  // namespace base
-}  // namespace heap
+}  // namespace heap::base
diff -r -u --color up/v8/src/heap/base/stack.h nw/v8/src/heap/base/stack.h
--- up/v8/src/heap/base/stack.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/base/stack.h	2023-01-19 16:46:36.211442904 -0500
@@ -6,6 +6,7 @@
 #define V8_HEAP_BASE_STACK_H_
 
 #include "src/base/macros.h"
+#include "src/base/platform/platform.h"
 
 namespace heap::base {
 
@@ -21,13 +22,59 @@
 // - SafeStack: https://releases.llvm.org/10.0.0/tools/clang/docs/SafeStack.html
 class V8_EXPORT_PRIVATE Stack final {
  public:
+  // The following constant is architecture-specific. The size of the buffer
+  // for storing the callee-saved registers is going to be equal to
+  // NumberOfCalleeSavedRegisters * sizeof(intptr_t).
+
+#if V8_HOST_ARCH_IA32
+  // Must be consistent with heap/base/asm/ia32/.
+  static constexpr int NumberOfCalleeSavedRegisters = 3;
+#elif V8_HOST_ARCH_X64
+#ifdef _WIN64
+  // Must be consistent with heap/base/asm/x64/.
+  static constexpr int NumberOfCalleeSavedRegisters = 28;
+#else   // !_WIN64
+  // Must be consistent with heap/base/asm/x64/.
+  static constexpr int NumberOfCalleeSavedRegisters = 5;
+#endif  // !_WIN64
+#elif V8_HOST_ARCH_ARM64
+  // Must be consistent with heap/base/asm/arm64/.
+  static constexpr int NumberOfCalleeSavedRegisters = 11;
+#elif V8_HOST_ARCH_ARM
+  // Must be consistent with heap/base/asm/arm/.
+  static constexpr int NumberOfCalleeSavedRegisters = 8;
+#elif V8_HOST_ARCH_PPC64
+  // Must be consistent with heap/base/asm/ppc/.
+  static constexpr int NumberOfCalleeSavedRegisters = 20;
+#elif V8_HOST_ARCH_PPC
+  // Must be consistent with heap/base/asm/ppc/.
+  static constexpr int NumberOfCalleeSavedRegisters = 20;
+#elif V8_HOST_ARCH_MIPS64
+  // Must be consistent with heap/base/asm/mips64el/.
+  static constexpr int NumberOfCalleeSavedRegisters = 9;
+#elif V8_HOST_ARCH_LOONG64
+  // Must be consistent with heap/base/asm/loong64/.
+  static constexpr int NumberOfCalleeSavedRegisters = 11;
+#elif V8_HOST_ARCH_S390
+  // Must be consistent with heap/base/asm/s390/.
+  static constexpr int NumberOfCalleeSavedRegisters = 10;
+#elif V8_HOST_ARCH_RISCV32
+  // Must be consistent with heap/base/asm/riscv/.
+  static constexpr int NumberOfCalleeSavedRegisters = 12;
+#elif V8_HOST_ARCH_RISCV64
+  // Must be consistent with heap/base/asm/riscv/.
+  static constexpr int NumberOfCalleeSavedRegisters = 12;
+#else
+#error Unknown architecture.
+#endif
+
   explicit Stack(const void* stack_start = nullptr);
 
   // Sets the start of the stack.
   void SetStackStart(const void* stack_start);
 
   // Returns true if |slot| is part of the stack and false otherwise.
-  bool IsOnStack(void* slot) const;
+  bool IsOnStack(const void* slot) const;
 
   // Word-aligned iteration of the stack. Callee-saved registers are pushed to
   // the stack before iterating pointers. Slot values are passed on to
@@ -41,13 +88,36 @@
   // **Ignores:**
   // - Callee-saved registers.
   // - SafeStack.
-  void IteratePointersUnsafe(StackVisitor* visitor, uintptr_t stack_end) const;
+  void IteratePointersUnsafe(StackVisitor* visitor,
+                             const void* stack_end) const;
 
   // Returns the start of the stack.
   const void* stack_start() const { return stack_start_; }
 
+  // Sets, clears and gets the stack marker.
+  void set_marker(const void* stack_marker);
+  void clear_marker();
+  const void* get_marker() const;
+
+  // Mechanism for saving the callee-saved registers, required for conservative
+  // stack scanning.
+
+  struct CalleeSavedRegisters {
+    // We always double-align this buffer, to support for longer registers,
+    // e.g., 128-bit registers in WIN64.
+    alignas(2 * sizeof(intptr_t))
+        std::array<intptr_t, NumberOfCalleeSavedRegisters> buffer;
+  };
+
+  using Callback = void (*)(StackVisitor*, const void*, const void*,
+                            const CalleeSavedRegisters* registers);
+
+  static V8_NOINLINE void PushAllRegistersAndInvokeCallback(
+      StackVisitor* visitor, const void* stack_start, Callback callback);
+
  private:
   const void* stack_start_;
+  const void* stack_marker_ = nullptr;
 };
 
 }  // namespace heap::base
diff -r -u --color up/v8/src/heap/base-space.cc nw/v8/src/heap/base-space.cc
--- up/v8/src/heap/base-space.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/base-space.cc	2023-01-19 16:46:36.211442904 -0500
@@ -13,8 +13,6 @@
       return "new_space";
     case OLD_SPACE:
       return "old_space";
-    case MAP_SPACE:
-      return "map_space";
     case CODE_SPACE:
       return "code_space";
     case SHARED_SPACE:
diff -r -u --color up/v8/src/heap/combined-heap.h nw/v8/src/heap/combined-heap.h
--- up/v8/src/heap/combined-heap.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/combined-heap.h	2023-01-19 16:46:36.211442904 -0500
@@ -26,7 +26,7 @@
   HeapObject Next();
 
  private:
-  SafepointScope safepoint_scope_;
+  IsolateSafepointScope safepoint_scope_;
   HeapObjectIterator heap_iterator_;
   ReadOnlyHeapObjectIterator ro_heap_iterator_;
 };
diff -r -u --color up/v8/src/heap/concurrent-allocator.cc nw/v8/src/heap/concurrent-allocator.cc
--- up/v8/src/heap/concurrent-allocator.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/concurrent-allocator.cc	2023-01-19 16:46:36.211442904 -0500
@@ -178,7 +178,7 @@
                                                AllocationOrigin origin) {
   DCHECK(!space_->is_compaction_space());
   DCHECK(space_->identity() == OLD_SPACE || space_->identity() == CODE_SPACE ||
-         space_->identity() == MAP_SPACE || space_->identity() == SHARED_SPACE);
+         space_->identity() == SHARED_SPACE);
   DCHECK(origin == AllocationOrigin::kRuntime ||
          origin == AllocationOrigin::kGC);
   DCHECK_IMPLIES(!local_heap_, origin == AllocationOrigin::kGC);
diff -r -u --color up/v8/src/heap/cppgc/heap-base.cc nw/v8/src/heap/cppgc/heap-base.cc
--- up/v8/src/heap/cppgc/heap-base.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/cppgc/heap-base.cc	2023-01-19 16:46:36.211442904 -0500
@@ -4,7 +4,11 @@
 
 #include "src/heap/cppgc/heap-base.h"
 
+#include <memory>
+
 #include "include/cppgc/heap-consistency.h"
+#include "include/cppgc/platform.h"
+#include "src/base/logging.h"
 #include "src/base/platform/platform.h"
 #include "src/base/sanitizer/lsan-page-allocator.h"
 #include "src/heap/base/stack.h"
@@ -90,6 +94,41 @@
 };
 #endif  // defined(CPPGC_YOUNG_GENERATION)
 
+class PlatformWithPageAllocator final : public cppgc::Platform {
+ public:
+  explicit PlatformWithPageAllocator(std::shared_ptr<cppgc::Platform> delegate)
+      : delegate_(std::move(delegate)),
+        page_allocator_(GetGlobalPageAllocator()) {
+    // This platform wrapper should only be used if the platform doesn't provide
+    // a `PageAllocator`.
+    CHECK_NULL(delegate->GetPageAllocator());
+  }
+  ~PlatformWithPageAllocator() override = default;
+
+  PageAllocator* GetPageAllocator() final { return &page_allocator_; }
+
+  double MonotonicallyIncreasingTime() final {
+    return delegate_->MonotonicallyIncreasingTime();
+  }
+
+  std::shared_ptr<TaskRunner> GetForegroundTaskRunner() final {
+    return delegate_->GetForegroundTaskRunner();
+  }
+
+  std::unique_ptr<JobHandle> PostJob(TaskPriority priority,
+                                     std::unique_ptr<JobTask> job_task) final {
+    return delegate_->PostJob(std::move(priority), std::move(job_task));
+  }
+
+  TracingController* GetTracingController() final {
+    return delegate_->GetTracingController();
+  }
+
+ private:
+  std::shared_ptr<cppgc::Platform> delegate_;
+  cppgc::PageAllocator& page_allocator_;
+};
+
 }  // namespace
 
 HeapBase::HeapBase(
@@ -98,7 +137,11 @@
     StackSupport stack_support, MarkingType marking_support,
     SweepingType sweeping_support, GarbageCollector& garbage_collector)
     : raw_heap_(this, custom_spaces),
-      platform_(std::move(platform)),
+      platform_(platform->GetPageAllocator()
+                    ? std::move(platform)
+                    : std::static_pointer_cast<cppgc::Platform>(
+                          std::make_shared<PlatformWithPageAllocator>(
+                              std::move(platform)))),
       oom_handler_(std::make_unique<FatalOutOfMemoryHandler>(this)),
 #if defined(LEAK_SANITIZER)
       lsan_page_allocator_(std::make_unique<v8::base::LsanPageAllocator>(
diff -r -u --color up/v8/src/heap/cppgc/heap-base.h nw/v8/src/heap/cppgc/heap-base.h
--- up/v8/src/heap/cppgc/heap-base.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/cppgc/heap-base.h	2023-01-19 16:46:36.211442904 -0500
@@ -108,8 +108,6 @@
     return stats_collector_.get();
   }
 
-  heap::base::Stack* stack() { return stack_.get(); }
-
   PreFinalizerHandler* prefinalizer_handler() {
     return prefinalizer_handler_.get();
   }
@@ -161,6 +159,8 @@
 
   size_t ObjectPayloadSize() const;
 
+  virtual heap::base::Stack* stack() { return stack_.get(); }
+
   StackSupport stack_support() const { return stack_support_; }
   const EmbedderStackState* override_stack_state() const {
     return override_stack_state_.get();
diff -r -u --color up/v8/src/heap/cppgc/heap.cc nw/v8/src/heap/cppgc/heap.cc
--- up/v8/src/heap/cppgc/heap.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/cppgc/heap.cc	2023-01-19 16:46:36.211442904 -0500
@@ -63,9 +63,6 @@
 
 void CheckConfig(GCConfig config, HeapBase::MarkingType marking_support,
                  HeapBase::SweepingType sweeping_support) {
-  CHECK_WITH_MSG((config.collection_type != CollectionType::kMinor) ||
-                     (config.stack_state == StackState::kNoHeapPointers),
-                 "Minor GCs with stack is currently not supported");
   CHECK_LE(static_cast<int>(config.marking_type),
            static_cast<int>(marking_support));
   CHECK_LE(static_cast<int>(config.sweeping_type),
diff -r -u --color up/v8/src/heap/cppgc/marker.cc nw/v8/src/heap/cppgc/marker.cc
--- up/v8/src/heap/cppgc/marker.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/cppgc/marker.cc	2023-01-19 16:46:36.222276234 -0500
@@ -443,11 +443,13 @@
         heap().stats_collector(), StatsCollector::kMarkVisitStack);
     heap().stack()->IteratePointers(&stack_visitor());
   }
+
 #if defined(CPPGC_YOUNG_GENERATION)
   if (config_.collection_type == CollectionType::kMinor) {
     StatsCollector::EnabledScope stats_scope(
         heap().stats_collector(), StatsCollector::kMarkVisitRememberedSets);
-    heap().remembered_set().Visit(visitor(), mutator_marking_state_);
+    heap().remembered_set().Visit(visitor(), conservative_visitor(),
+                                  mutator_marking_state_);
   }
 #endif  // defined(CPPGC_YOUNG_GENERATION)
 }
diff -r -u --color up/v8/src/heap/cppgc/marking-verifier.cc nw/v8/src/heap/cppgc/marking-verifier.cc
--- up/v8/src/heap/cppgc/marking-verifier.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/cppgc/marking-verifier.cc	2023-01-19 16:46:36.222276234 -0500
@@ -63,7 +63,8 @@
 #if !defined(THREAD_SANITIZER) && !defined(CPPGC_POINTER_COMPRESSION)
   if (stack_state == StackState::kMayContainHeapPointers) {
     in_construction_objects_ = &in_construction_objects_stack_;
-    heap_.stack()->IteratePointersUnsafe(this, stack_end);
+    heap_.stack()->IteratePointersUnsafe(
+        this, reinterpret_cast<const void*>(stack_end));
     // The objects found through the unsafe iteration are only a subset of the
     // regular iteration as they miss objects held alive only from callee-saved
     // registers that are never pushed on the stack and SafeStack.
diff -r -u --color up/v8/src/heap/cppgc/marking-visitor.cc nw/v8/src/heap/cppgc/marking-visitor.cc
--- up/v8/src/heap/cppgc/marking-visitor.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/cppgc/marking-visitor.cc	2023-01-19 16:46:36.222276234 -0500
@@ -70,6 +70,14 @@
   // hold a reference to themselves.
   if (!marking_state_.MarkNoPush(header)) return;
   marking_state_.AccountMarkedBytes(header);
+#if defined(CPPGC_YOUNG_GENERATION)
+  // An in-construction object can add a reference to a young object that may
+  // miss the write-barrier on an initializing store. Remember object in the
+  // root-set to be retraced on the next GC.
+  if (heap_.generational_gc_supported()) {
+    heap_.remembered_set().AddInConstructionObjectToBeRetraced(header);
+  }
+#endif  // defined(CPPGC_YOUNG_GENERATION)
   callback(this, header);
 }
 
diff -r -u --color up/v8/src/heap/cppgc/platform.cc nw/v8/src/heap/cppgc/platform.cc
--- up/v8/src/heap/cppgc/platform.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/cppgc/platform.cc	2023-01-19 16:46:36.222276234 -0500
@@ -22,6 +22,28 @@
 namespace cppgc {
 namespace internal {
 
+namespace {
+
+PageAllocator* g_page_allocator = nullptr;
+
+PageAllocator& CreateAllocatorIfNeeded(PageAllocator* page_allocator) {
+  if (!page_allocator) {
+    static v8::base::LeakyObject<v8::base::PageAllocator>
+        default_page_allocator;
+    page_allocator = default_page_allocator.get();
+  }
+#if defined(LEAK_SANITIZER)
+  // If lsan is enabled, override the given allocator with the custom lsan
+  // allocator.
+  static v8::base::LeakyObject<v8::base::LsanPageAllocator> lsan_page_allocator(
+      page_allocator);
+  page_allocator = lsan_page_allocator.get();
+#endif  // LEAK_SANITIZER
+  return *page_allocator;
+}
+
+}  // namespace
+
 void Fatal(const std::string& reason, const SourceLocation& loc) {
 #ifdef DEBUG
   V8_Fatal(loc.FileName(), static_cast<int>(loc.Line()), "%s", reason.c_str());
@@ -53,28 +75,12 @@
   return oom_handler;
 }
 
-}  // namespace internal
-
-namespace {
-PageAllocator* g_page_allocator = nullptr;
-
-PageAllocator& GetAllocator(PageAllocator* page_allocator) {
-  if (!page_allocator) {
-    static v8::base::LeakyObject<v8::base::PageAllocator>
-        default_page_allocator;
-    page_allocator = default_page_allocator.get();
-  }
-#if defined(LEAK_SANITIZER)
-  // If lsan is enabled, override the given allocator with the custom lsan
-  // allocator.
-  static v8::base::LeakyObject<v8::base::LsanPageAllocator> lsan_page_allocator(
-      page_allocator);
-  page_allocator = lsan_page_allocator.get();
-#endif  // LEAK_SANITIZER
-  return *page_allocator;
+PageAllocator& GetGlobalPageAllocator() {
+  CHECK_NOT_NULL(g_page_allocator);
+  return *g_page_allocator;
 }
 
-}  // namespace
+}  // namespace internal
 
 TracingController* Platform::GetTracingController() {
   static v8::base::LeakyObject<TracingController> tracing_controller;
@@ -93,16 +99,16 @@
   CHECK_EQ(0u, internal::kAllocationGranularity % poisoning_granularity);
 #endif
 
-  auto& allocator = GetAllocator(page_allocator);
+  auto& allocator = internal::CreateAllocatorIfNeeded(page_allocator);
 
-  CHECK(!g_page_allocator);
+  CHECK(!internal::g_page_allocator);
   internal::GlobalGCInfoTable::Initialize(allocator);
 #if defined(CPPGC_CAGED_HEAP)
   internal::CagedHeap::InitializeIfNeeded(allocator);
 #endif  // defined(CPPGC_CAGED_HEAP)
-  g_page_allocator = &allocator;
+  internal::g_page_allocator = &allocator;
 }
 
-void ShutdownProcess() { g_page_allocator = nullptr; }
+void ShutdownProcess() { internal::g_page_allocator = nullptr; }
 
 }  // namespace cppgc
diff -r -u --color up/v8/src/heap/cppgc/platform.h nw/v8/src/heap/cppgc/platform.h
--- up/v8/src/heap/cppgc/platform.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/cppgc/platform.h	2023-01-19 16:46:36.222276234 -0500
@@ -7,11 +7,11 @@
 
 #include <string>
 
+#include "include/cppgc/platform.h"
 #include "include/cppgc/source-location.h"
 #include "src/base/macros.h"
 
-namespace cppgc {
-namespace internal {
+namespace cppgc::internal {
 
 class HeapBase;
 
@@ -40,7 +40,9 @@
 // Gets the global OOM handler that is not bound to any specific Heap instance.
 FatalOutOfMemoryHandler& GetGlobalOOMHandler();
 
-}  // namespace internal
-}  // namespace cppgc
+// Gets the gobal PageAllocator that is not bound to any specific Heap instance.
+PageAllocator& GetGlobalPageAllocator();
+
+}  // namespace cppgc::internal
 
 #endif  // V8_HEAP_CPPGC_PLATFORM_H_
diff -r -u --color up/v8/src/heap/cppgc/remembered-set.cc nw/v8/src/heap/cppgc/remembered-set.cc
--- up/v8/src/heap/cppgc/remembered-set.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/cppgc/remembered-set.cc	2023-01-19 16:46:36.222276234 -0500
@@ -93,11 +93,6 @@
   // may have mixed young/old objects. Check here precisely if the object is
   // old.
   if (slot_header.IsYoung()) return;
-  // The design of young generation requires collections to be executed at the
-  // top level (with the guarantee that no objects are currently being in
-  // construction). This can be ensured by running young GCs from safe points
-  // or by reintroducing nested allocation scopes that avoid finalization.
-  DCHECK(!slot_header.template IsInConstruction<AccessMode::kNonAtomic>());
 
 #if defined(CPPGC_POINTER_COMPRESSION)
   void* value = nullptr;
@@ -241,11 +236,6 @@
     // may have mixed young/old objects. Check here precisely if the object is
     // old.
     if (source_hoh->IsYoung()) continue;
-    // The design of young generation requires collections to be executed at the
-    // top level (with the guarantee that no objects are currently being in
-    // construction). This can be ensured by running young GCs from safe points
-    // or by reintroducing nested allocation scopes that avoid finalization.
-    DCHECK(!source_hoh->template IsInConstruction<AccessMode::kNonAtomic>());
 
     const TraceCallback trace_callback =
         GlobalGCInfoTable::GCInfoFromIndex(source_hoh->GetGCInfoIndex()).trace;
@@ -255,6 +245,28 @@
   }
 }
 
+// Revisit in-construction objects from previous GCs. We must do it to make
+// sure that we don't miss any initializing pointer writes if a previous GC
+// happened while an object was in-construction.
+void RevisitInConstructionObjects(
+    std::set<HeapObjectHeader*>& remembered_in_construction_objects,
+    Visitor& visitor, ConservativeTracingVisitor& conservative_visitor) {
+  for (HeapObjectHeader* hoh : remembered_in_construction_objects) {
+    DCHECK(hoh);
+    // The object must be marked on previous GC.
+    DCHECK(hoh->IsMarked());
+
+    if (hoh->template IsInConstruction<AccessMode::kNonAtomic>()) {
+      conservative_visitor.TraceConservatively(*hoh);
+    } else {
+      // If the object is fully constructed, trace precisely.
+      const TraceCallback trace_callback =
+          GlobalGCInfoTable::GCInfoFromIndex(hoh->GetGCInfoIndex()).trace;
+      trace_callback(&visitor, hoh->ObjectStart());
+    }
+  }
+}
+
 }  // namespace
 
 void OldToNewRememberedSet::AddSlot(void* slot) {
@@ -297,6 +309,12 @@
   remembered_weak_callbacks_.insert(item);
 }
 
+void OldToNewRememberedSet::AddInConstructionObjectToBeRetraced(
+    HeapObjectHeader& hoh) {
+  DCHECK(heap_.generational_gc_supported());
+  remembered_in_construction_objects_.current.insert(&hoh);
+}
+
 void OldToNewRememberedSet::InvalidateRememberedSlotsInRange(void* begin,
                                                              void* end) {
   DCHECK(heap_.generational_gc_supported());
@@ -313,12 +331,15 @@
   remembered_source_objects_.erase(&header);
 }
 
-void OldToNewRememberedSet::Visit(Visitor& visitor,
-                                  MutatorMarkingState& marking_state) {
+void OldToNewRememberedSet::Visit(
+    Visitor& visitor, ConservativeTracingVisitor& conservative_visitor,
+    MutatorMarkingState& marking_state) {
   DCHECK(heap_.generational_gc_supported());
   VisitRememberedSlots(heap_, marking_state, remembered_uncompressed_slots_,
                        remembered_slots_for_verification_);
   VisitRememberedSourceObjects(remembered_source_objects_, visitor);
+  RevisitInConstructionObjects(remembered_in_construction_objects_.previous,
+                               visitor, conservative_visitor);
 }
 
 void OldToNewRememberedSet::ExecuteCustomCallbacks(LivenessBroker broker) {
@@ -342,6 +363,8 @@
 #if DEBUG
   remembered_slots_for_verification_.clear();
 #endif  // DEBUG
+  remembered_in_construction_objects_.Reset();
+  // Custom weak callbacks is alive across GCs.
 }
 
 bool OldToNewRememberedSet::IsEmpty() const {
@@ -351,6 +374,18 @@
          remembered_weak_callbacks_.empty();
 }
 
+void OldToNewRememberedSet::RememberedInConstructionObjects::Reset() {
+  // Make sure to keep the still-in-construction objects in the remembered set,
+  // as otherwise, being marked, the marker won't be able to observe them.
+  std::copy_if(previous.begin(), previous.end(),
+               std::inserter(current, current.begin()),
+               [](const HeapObjectHeader* h) {
+                 return h->template IsInConstruction<AccessMode::kNonAtomic>();
+               });
+  previous = std::move(current);
+  current.clear();
+}
+
 }  // namespace internal
 }  // namespace cppgc
 
diff -r -u --color up/v8/src/heap/cppgc/remembered-set.h nw/v8/src/heap/cppgc/remembered-set.h
--- up/v8/src/heap/cppgc/remembered-set.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/cppgc/remembered-set.h	2023-01-19 16:46:36.222276234 -0500
@@ -42,10 +42,13 @@
   void AddSourceObject(HeapObjectHeader& source_hoh);
   void AddWeakCallback(WeakCallbackItem);
 
+  // Remembers an in-construction object to be retraced on the next minor GC.
+  void AddInConstructionObjectToBeRetraced(HeapObjectHeader&);
+
   void InvalidateRememberedSlotsInRange(void* begin, void* end);
   void InvalidateRememberedSourceObject(HeapObjectHeader& source_hoh);
 
-  void Visit(Visitor&, MutatorMarkingState&);
+  void Visit(Visitor&, ConservativeTracingVisitor&, MutatorMarkingState&);
 
   void ExecuteCustomCallbacks(LivenessBroker);
   void ReleaseCustomCallbacks();
@@ -57,6 +60,14 @@
  private:
   friend class MinorGCTest;
 
+  // The class keeps track of inconstruction objects that should be revisited.
+  struct RememberedInConstructionObjects final {
+    void Reset();
+
+    std::set<HeapObjectHeader*> previous;
+    std::set<HeapObjectHeader*> current;
+  };
+
   static constexpr struct {
     bool operator()(const WeakCallbackItem& lhs,
                     const WeakCallbackItem& rhs) const {
@@ -72,6 +83,7 @@
   // whereas uncompressed are stored in std::set.
   std::set<void*> remembered_uncompressed_slots_;
   std::set<void*> remembered_slots_for_verification_;
+  RememberedInConstructionObjects remembered_in_construction_objects_;
 };
 
 }  // namespace internal
diff -r -u --color up/v8/src/heap/cppgc/visitor.cc nw/v8/src/heap/cppgc/visitor.cc
--- up/v8/src/heap/cppgc/visitor.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/cppgc/visitor.cc	2023-01-19 16:46:36.222276234 -0500
@@ -31,10 +31,8 @@
     HeapBase& heap, PageBackend& page_backend, cppgc::Visitor& visitor)
     : heap_(heap), page_backend_(page_backend), visitor_(visitor) {}
 
-namespace {
-
-void TraceConservatively(ConservativeTracingVisitor* conservative_visitor,
-                         const HeapObjectHeader& header) {
+void ConservativeTracingVisitor::TraceConservatively(
+    const HeapObjectHeader& header) {
   const auto object_view = ObjectView<>(header);
   uintptr_t* word = reinterpret_cast<uintptr_t*>(object_view.Start());
   for (size_t i = 0; i < (object_view.Size() / sizeof(uintptr_t)); ++i) {
@@ -47,7 +45,7 @@
 #endif
     // First, check the full pointer.
     if (maybe_full_ptr > SentinelPointer::kSentinelValue)
-      conservative_visitor->TraceConservativelyIfNeeded(
+      this->TraceConservativelyIfNeeded(
           reinterpret_cast<Address>(maybe_full_ptr));
 #if defined(CPPGC_POINTER_COMPRESSION)
     // Then, check for compressed pointers.
@@ -55,19 +53,17 @@
         CompressedPointer::Decompress(static_cast<uint32_t>(maybe_full_ptr)));
     if (decompressed_low >
         reinterpret_cast<void*>(SentinelPointer::kSentinelValue))
-      conservative_visitor->TraceConservativelyIfNeeded(decompressed_low);
+      this->TraceConservativelyIfNeeded(decompressed_low);
     auto decompressed_high = reinterpret_cast<Address>(
         CompressedPointer::Decompress(static_cast<uint32_t>(
             maybe_full_ptr >> (sizeof(uint32_t) * CHAR_BIT))));
     if (decompressed_high >
         reinterpret_cast<void*>(SentinelPointer::kSentinelValue))
-      conservative_visitor->TraceConservativelyIfNeeded(decompressed_high);
+      this->TraceConservativelyIfNeeded(decompressed_high);
 #endif  // !defined(CPPGC_POINTER_COMPRESSION)
   }
 }
 
-}  // namespace
-
 void ConservativeTracingVisitor::TryTracePointerConservatively(
     Address pointer) {
 #if defined(CPPGC_CAGED_HEAP)
@@ -130,7 +126,11 @@
   if (!header.IsInConstruction<AccessMode::kNonAtomic>()) {
     VisitFullyConstructedConservatively(header);
   } else {
-    VisitInConstructionConservatively(header, TraceConservatively);
+    VisitInConstructionConservatively(
+        header,
+        [](ConservativeTracingVisitor* v, const HeapObjectHeader& header) {
+          v->TraceConservatively(header);
+        });
   }
 }
 
diff -r -u --color up/v8/src/heap/cppgc/visitor.h nw/v8/src/heap/cppgc/visitor.h
--- up/v8/src/heap/cppgc/visitor.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/cppgc/visitor.h	2023-01-19 16:46:36.222276234 -0500
@@ -57,6 +57,7 @@
 
   virtual void TraceConservativelyIfNeeded(const void*);
   void TraceConservativelyIfNeeded(HeapObjectHeader&);
+  void TraceConservatively(const HeapObjectHeader&);
 
  protected:
   using TraceConservativelyCallback = void(ConservativeTracingVisitor*,
diff -r -u --color up/v8/src/heap/cppgc-js/cpp-heap.cc nw/v8/src/heap/cppgc-js/cpp-heap.cc
--- up/v8/src/heap/cppgc-js/cpp-heap.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/cppgc-js/cpp-heap.cc	2023-01-19 16:46:36.211442904 -0500
@@ -15,12 +15,13 @@
 #include "include/v8-platform.h"
 #include "src/base/logging.h"
 #include "src/base/macros.h"
+#include "src/base/optional.h"
 #include "src/base/platform/platform.h"
 #include "src/base/platform/time.h"
 #include "src/execution/isolate-inl.h"
 #include "src/flags/flags.h"
-#include "src/handles/global-handles.h"
 #include "src/handles/handles.h"
+#include "src/handles/traced-handles.h"
 #include "src/heap/base/stack.h"
 #include "src/heap/cppgc-js/cpp-marking-state.h"
 #include "src/heap/cppgc-js/cpp-snapshot.h"
@@ -151,7 +152,7 @@
   DCHECK(isolate);
   V8ToCppGCReferencesVisitor forwarding_visitor(marking_state, isolate,
                                                 wrapper_descriptor);
-  isolate->global_handles()->IterateTracedNodes(&forwarding_visitor);
+  isolate->traced_handles()->Iterate(&forwarding_visitor);
 }
 
 }  // namespace
@@ -557,23 +558,35 @@
 
 namespace {
 
+class SweepingOnMutatorThreadForGlobalHandlesScope final {
+ public:
+  explicit SweepingOnMutatorThreadForGlobalHandlesScope(
+      TracedHandles& traced_handles)
+      : traced_handles_(traced_handles) {
+    traced_handles_.SetIsSweepingOnMutatorThread(true);
+  }
+  ~SweepingOnMutatorThreadForGlobalHandlesScope() {
+    traced_handles_.SetIsSweepingOnMutatorThread(false);
+  }
+
+  TracedHandles& traced_handles_;
+};
+
 class SweepingOnMutatorThreadForGlobalHandlesObserver final
     : public cppgc::internal::Sweeper::SweepingOnMutatorThreadObserver {
  public:
   SweepingOnMutatorThreadForGlobalHandlesObserver(CppHeap& cpp_heap,
-                                                  GlobalHandles& global_handles)
+                                                  TracedHandles& traced_handles)
       : cppgc::internal::Sweeper::SweepingOnMutatorThreadObserver(
             cpp_heap.sweeper()),
-        global_handles_(global_handles) {}
+        traced_handles_(traced_handles) {}
 
-  void Start() override {
-    global_handles_.NotifyStartSweepingOnMutatorThread();
-  }
+  void Start() override { traced_handles_.SetIsSweepingOnMutatorThread(true); }
 
-  void End() override { global_handles_.NotifyEndSweepingOnMutatorThread(); }
+  void End() override { traced_handles_.SetIsSweepingOnMutatorThread(false); }
 
  private:
-  GlobalHandles& global_handles_;
+  TracedHandles& traced_handles_;
 };
 
 }  // namespace
@@ -593,7 +606,7 @@
   ReduceGCCapabilititesFromFlags();
   sweeping_on_mutator_thread_observer_ =
       std::make_unique<SweepingOnMutatorThreadForGlobalHandlesObserver>(
-          *this, *isolate_->global_handles());
+          *this, *isolate_->traced_handles());
   no_gc_scope_--;
 }
 
@@ -625,6 +638,10 @@
   no_gc_scope_++;
 }
 
+::heap::base::Stack* CppHeap::stack() {
+  return isolate_ ? &isolate_->heap()->stack() : HeapBase::stack();
+}
+
 namespace {
 
 bool IsMemoryReducingGC(CppHeap::GarbageCollectionFlags flags) {
@@ -768,7 +785,8 @@
   in_atomic_pause_ = true;
   auto& marker = marker_.get()->To<UnifiedHeapMarker>();
   // Scan global handles conservatively in case we are attached to an Isolate.
-  if (isolate_) {
+  // TODO(1029379): Support global handle marking visitors with minor GC.
+  if (isolate_ && !generational_gc_supported()) {
     auto& heap = *isolate()->heap();
     marker.conservative_visitor().SetGlobalHandlesMarkingVisitor(
         std::make_unique<GlobalHandleMarkingVisitor>(
@@ -830,7 +848,15 @@
   {
     cppgc::subtle::NoGarbageCollectionScope no_gc(*this);
     cppgc::internal::SweepingConfig::CompactableSpaceHandling
-        compactable_space_handling = compactor_.CompactSpacesIfEnabled();
+        compactable_space_handling;
+    {
+      base::Optional<SweepingOnMutatorThreadForGlobalHandlesScope>
+          global_handles_scope;
+      if (isolate_) {
+        global_handles_scope.emplace(*isolate_->traced_handles());
+      }
+      compactable_space_handling = compactor_.CompactSpacesIfEnabled();
+    }
     const cppgc::internal::SweepingConfig sweeping_config{
         SelectSweepingType(), compactable_space_handling,
         ShouldReduceMemory(current_gc_flags_)
@@ -848,13 +874,11 @@
   sweeper().NotifyDoneIfNeeded();
 }
 
-void CppHeap::RunMinorGCIfNeeded(StackState stack_state) {
+void CppHeap::RunMinorGCIfNeeded() {
   if (!generational_gc_supported()) return;
   if (in_no_gc_scope()) return;
   // Minor GC does not support nesting in full GCs.
   if (IsMarking()) return;
-  // Minor GCs with the stack are currently not supported.
-  if (stack_state == StackState::kMayContainHeapPointers) return;
   // Run only when the limit is reached.
   if (!minor_gc_heap_growing_->LimitReached()) return;
 
@@ -1055,7 +1079,12 @@
       stats_collector_->GetMetricRecorder());
 }
 
-void CppHeap::FinishSweepingIfRunning() { sweeper_.FinishIfRunning(); }
+void CppHeap::FinishSweepingIfRunning() {
+  sweeper_.FinishIfRunning();
+  if (isolate_) {
+    isolate_->traced_handles()->DeleteEmptyBlocks();
+  }
+}
 
 void CppHeap::FinishSweepingIfOutOfWork() { sweeper_.FinishIfOutOfWork(); }
 
diff -r -u --color up/v8/src/heap/cppgc-js/cpp-heap.h nw/v8/src/heap/cppgc-js/cpp-heap.h
--- up/v8/src/heap/cppgc-js/cpp-heap.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/cppgc-js/cpp-heap.h	2023-01-19 16:46:36.211442904 -0500
@@ -147,7 +147,7 @@
   void EnterFinalPause(cppgc::EmbedderStackState stack_state);
   bool FinishConcurrentMarkingIfNeeded();
 
-  void RunMinorGCIfNeeded(StackState);
+  void RunMinorGCIfNeeded();
 
   // StatsCollector::AllocationObserver interface.
   void AllocatedObjectSizeIncreased(size_t) final;
@@ -162,6 +162,8 @@
 
   Isolate* isolate() const { return isolate_; }
 
+  ::heap::base::Stack* stack() final;
+
   std::unique_ptr<CppMarkingState> CreateCppMarkingState();
   std::unique_ptr<CppMarkingState> CreateCppMarkingStateForMutatorThread();
 
diff -r -u --color up/v8/src/heap/cppgc-js/unified-heap-marking-state-inl.h nw/v8/src/heap/cppgc-js/unified-heap-marking-state-inl.h
--- up/v8/src/heap/cppgc-js/unified-heap-marking-state-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/cppgc-js/unified-heap-marking-state-inl.h	2023-01-19 16:46:36.211442904 -0500
@@ -9,8 +9,7 @@
 
 #include "include/v8-traced-handle.h"
 #include "src/base/logging.h"
-#include "src/handles/global-handles-inl.h"
-#include "src/handles/global-handles.h"
+#include "src/handles/traced-handles.h"
 #include "src/heap/cppgc-js/unified-heap-marking-state.h"
 #include "src/heap/heap.h"
 #include "src/heap/mark-compact.h"
@@ -23,17 +22,17 @@
 class BasicTracedReferenceExtractor {
  public:
   static Object GetObjectForMarking(const TracedReferenceBase& ref) {
-    Address* global_handle_location = const_cast<Address*>(
+    Address* traced_handle_location = const_cast<Address*>(
         reinterpret_cast<const Address*>(ref.GetSlotThreadSafe()));
     // We cannot assume that the reference is non-null as we may get here by
     // tracing an ephemeron which doesn't have early bailouts, see
     // `cppgc::Visitor::TraceEphemeron()` for non-Member values.
-    if (!global_handle_location) return Object();
+    if (!traced_handle_location) return Object();
 
     // The load synchronizes internal bitfields that are also read atomically
     // from the concurrent marker.
-    Object object = GlobalHandles::Acquire(global_handle_location);
-    GlobalHandles::MarkTraced(global_handle_location);
+    Object object = TracedHandles::Acquire(traced_handle_location);
+    TracedHandles::Mark(traced_handle_location);
     return object;
   }
 };
diff -r -u --color up/v8/src/heap/evacuation-allocator-inl.h nw/v8/src/heap/evacuation-allocator-inl.h
--- up/v8/src/heap/evacuation-allocator-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/evacuation-allocator-inl.h	2023-01-19 16:46:36.222276234 -0500
@@ -23,9 +23,6 @@
     case OLD_SPACE:
       return compaction_spaces_.Get(OLD_SPACE)->AllocateRaw(object_size,
                                                             alignment, origin);
-    case MAP_SPACE:
-      return compaction_spaces_.Get(MAP_SPACE)->AllocateRaw(object_size,
-                                                            alignment, origin);
     case CODE_SPACE:
       return compaction_spaces_.Get(CODE_SPACE)
           ->AllocateRaw(object_size, alignment, origin);
@@ -47,9 +44,6 @@
     case OLD_SPACE:
       FreeLastInCompactionSpace(OLD_SPACE, object, object_size);
       return;
-    case MAP_SPACE:
-      FreeLastInCompactionSpace(MAP_SPACE, object, object_size);
-      return;
     case SHARED_SPACE:
       FreeLastInCompactionSpace(SHARED_SPACE, object, object_size);
       return;
diff -r -u --color up/v8/src/heap/evacuation-allocator.h nw/v8/src/heap/evacuation-allocator.h
--- up/v8/src/heap/evacuation-allocator.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/evacuation-allocator.h	2023-01-19 16:46:36.222276234 -0500
@@ -35,10 +35,6 @@
     heap_->old_space()->MergeCompactionSpace(compaction_spaces_.Get(OLD_SPACE));
     heap_->code_space()->MergeCompactionSpace(
         compaction_spaces_.Get(CODE_SPACE));
-    if (heap_->map_space()) {
-      heap_->map_space()->MergeCompactionSpace(
-          compaction_spaces_.Get(MAP_SPACE));
-    }
     if (heap_->shared_space()) {
       heap_->shared_space()->MergeCompactionSpace(
           compaction_spaces_.Get(SHARED_SPACE));
diff -r -u --color up/v8/src/heap/evacuation-verifier.cc nw/v8/src/heap/evacuation-verifier.cc
--- up/v8/src/heap/evacuation-verifier.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/evacuation-verifier.cc	2023-01-19 16:46:36.222276234 -0500
@@ -41,8 +41,9 @@
   VerifyMap(object.map(cage_base()));
 }
 void EvacuationVerifier::VerifyRoots() {
-  heap_->IterateRootsIncludingClients(this,
-                                      base::EnumSet<SkipRoot>{SkipRoot::kWeak});
+  heap_->IterateRootsIncludingClients(
+      this,
+      base::EnumSet<SkipRoot>{SkipRoot::kWeak, SkipRoot::kConservativeStack});
 }
 
 void EvacuationVerifier::VerifyEvacuationOnPage(Address start, Address end) {
@@ -94,7 +95,6 @@
   VerifyEvacuation(heap_->old_space());
   VerifyEvacuation(heap_->code_space());
   if (heap_->shared_space()) VerifyEvacuation(heap_->shared_space());
-  if (heap_->map_space()) VerifyEvacuation(heap_->map_space());
 }
 
 void FullEvacuationVerifier::VerifyMap(Map map) { VerifyHeapObjectImpl(map); }
@@ -136,7 +136,6 @@
   VerifyEvacuation(heap_->new_space());
   VerifyEvacuation(heap_->old_space());
   VerifyEvacuation(heap_->code_space());
-  if (heap_->map_space()) VerifyEvacuation(heap_->map_space());
 }
 
 void YoungGenerationEvacuationVerifier::VerifyMap(Map map) {
diff -r -u --color up/v8/src/heap/factory-base.cc nw/v8/src/heap/factory-base.cc
--- up/v8/src/heap/factory-base.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/factory-base.cc	2023-01-19 16:46:36.222276234 -0500
@@ -86,8 +86,6 @@
                                     SKIP_WRITE_BARRIER);
   data_container.set_kind_specific_flags(flags, kRelaxedStore);
   if (V8_EXTERNAL_CODE_SPACE_BOOL) {
-    data_container.set_code_cage_base(impl()->isolate()->code_cage_base(),
-                                      kRelaxedStore);
     Isolate* isolate_for_sandbox = impl()->isolate_for_sandbox();
     data_container.set_raw_code(Smi::zero(), SKIP_WRITE_BARRIER);
     data_container.init_code_entry_point(isolate_for_sandbox, kNullAddress);
@@ -1128,7 +1126,7 @@
     int size, AllocationType allocation, Map map,
     AllocationAlignment alignment) {
   // TODO(delphick): Potentially you could also pass a immortal immovable Map
-  // from MAP_SPACE here, like external_map or message_object_map, but currently
+  // from OLD_SPACE here, like external_map or message_object_map, but currently
   // no one does so this check is sufficient.
   DCHECK(ReadOnlyHeap::Contains(map));
   HeapObject result = AllocateRaw(size, allocation, alignment);
diff -r -u --color up/v8/src/heap/factory.cc nw/v8/src/heap/factory.cc
--- up/v8/src/heap/factory.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/factory.cc	2023-01-19 16:46:36.222276234 -0500
@@ -415,9 +415,13 @@
 }
 
 Handle<EnumCache> Factory::NewEnumCache(Handle<FixedArray> keys,
-                                        Handle<FixedArray> indices) {
-  auto result =
-      NewStructInternal<EnumCache>(ENUM_CACHE_TYPE, AllocationType::kOld);
+                                        Handle<FixedArray> indices,
+                                        AllocationType allocation) {
+  DCHECK(allocation == AllocationType::kOld ||
+         allocation == AllocationType::kSharedOld);
+  DCHECK_EQ(allocation == AllocationType::kSharedOld,
+            keys->InSharedHeap() && indices->InSharedHeap());
+  auto result = NewStructInternal<EnumCache>(ENUM_CACHE_TYPE, allocation);
   DisallowGarbageCollection no_gc;
   result.set_keys(*keys);
   result.set_indices(*indices);
@@ -2490,7 +2494,7 @@
   CHECK(Builtins::IsIsolateIndependentBuiltin(*code));
 
 #ifdef V8_EXTERNAL_CODE_SPACE
-  if (V8_REMOVE_BUILTINS_CODE_OBJECTS) {
+  if (V8_EXTERNAL_CODE_SPACE_BOOL) {
     const int no_flags = 0;
     Handle<CodeDataContainer> code_data_container =
         NewCodeDataContainer(no_flags, AllocationType::kOld);
@@ -2838,19 +2842,26 @@
 }
 
 Handle<JSArray> Factory::NewJSArrayForTemplateLiteralArray(
-    Handle<FixedArray> cooked_strings, Handle<FixedArray> raw_strings) {
+    Handle<FixedArray> cooked_strings, Handle<FixedArray> raw_strings,
+    int function_literal_id, int slot_id) {
   Handle<JSArray> raw_object =
       NewJSArrayWithElements(raw_strings, PACKED_ELEMENTS,
                              raw_strings->length(), AllocationType::kOld);
   JSObject::SetIntegrityLevel(raw_object, FROZEN, kThrowOnError).ToChecked();
 
   Handle<NativeContext> native_context = isolate()->native_context();
-  Handle<JSArray> template_object = NewJSArrayWithUnverifiedElements(
-      handle(native_context->js_array_template_literal_object_map(), isolate()),
-      cooked_strings, cooked_strings->length(), AllocationType::kOld);
-  TemplateLiteralObject::SetRaw(template_object, raw_object);
-  DCHECK_EQ(template_object->map(),
+  Handle<TemplateLiteralObject> template_object =
+      Handle<TemplateLiteralObject>::cast(NewJSArrayWithUnverifiedElements(
+          handle(native_context->js_array_template_literal_object_map(),
+                 isolate()),
+          cooked_strings, cooked_strings->length(), AllocationType::kOld));
+  DisallowGarbageCollection no_gc;
+  TemplateLiteralObject raw_template_object = *template_object;
+  DCHECK_EQ(raw_template_object.map(),
             native_context->js_array_template_literal_object_map());
+  raw_template_object.set_raw(*raw_object);
+  raw_template_object.set_function_literal_id(function_literal_id);
+  raw_template_object.set_slot_id(slot_id);
   return template_object;
 }
 
@@ -3029,7 +3040,7 @@
   auto result =
       Handle<JSArrayBuffer>::cast(NewJSObjectFromMap(map, allocation));
   result->Setup(SharedFlag::kNotShared, ResizableFlag::kNotResizable,
-                std::move(backing_store));
+                std::move(backing_store), isolate());
   return result;
 }
 
@@ -3048,7 +3059,7 @@
   auto array_buffer =
       Handle<JSArrayBuffer>::cast(NewJSObjectFromMap(map, allocation));
   array_buffer->Setup(SharedFlag::kNotShared, ResizableFlag::kNotResizable,
-                      std::move(backing_store));
+                      std::move(backing_store), isolate());
   return array_buffer;
 }
 
@@ -3064,7 +3075,8 @@
   ResizableFlag resizable = backing_store->is_resizable_by_js()
                                 ? ResizableFlag::kResizable
                                 : ResizableFlag::kNotResizable;
-  result->Setup(SharedFlag::kShared, resizable, std::move(backing_store));
+  result->Setup(SharedFlag::kShared, resizable, std::move(backing_store),
+                isolate());
   return result;
 }
 
diff -r -u --color up/v8/src/heap/factory.h nw/v8/src/heap/factory.h
--- up/v8/src/heap/factory.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/factory.h	2023-01-19 16:46:36.222276234 -0500
@@ -183,8 +183,9 @@
   Handle<PrototypeInfo> NewPrototypeInfo();
 
   // Create a new EnumCache struct.
-  Handle<EnumCache> NewEnumCache(Handle<FixedArray> keys,
-                                 Handle<FixedArray> indices);
+  Handle<EnumCache> NewEnumCache(
+      Handle<FixedArray> keys, Handle<FixedArray> indices,
+      AllocationType allocation = AllocationType::kOld);
 
   // Create a new Tuple2 struct.
   Handle<Tuple2> NewTuple2(Handle<Object> value1, Handle<Object> value2,
@@ -607,7 +608,8 @@
       AllocationType allocation = AllocationType::kYoung);
 
   Handle<JSArray> NewJSArrayForTemplateLiteralArray(
-      Handle<FixedArray> cooked_strings, Handle<FixedArray> raw_strings);
+      Handle<FixedArray> cooked_strings, Handle<FixedArray> raw_strings,
+      int function_literal_id, int slot_id);
 
   void NewJSArrayStorage(
       Handle<JSArray> array, int length, int capacity,
diff -r -u --color up/v8/src/heap/free-list.cc nw/v8/src/heap/free-list.cc
--- up/v8/src/heap/free-list.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/free-list.cc	2023-01-19 16:46:36.222276234 -0500
@@ -112,7 +112,7 @@
 FreeList* FreeList::CreateFreeList() { return new FreeListManyCachedOrigin(); }
 
 FreeList* FreeList::CreateFreeListForNewSpace() {
-  return new FreeListManyCachedOriginForNewSpace();
+  return new FreeListManyCachedFastPathForNewSpace();
 }
 
 FreeSpace FreeList::TryFindNodeIn(FreeListCategoryType type,
@@ -343,11 +343,11 @@
 }
 
 // ------------------------------------------------
-// FreeListManyCachedFastPath implementation
+// FreeListManyCachedFastPathBase implementation
 
-FreeSpace FreeListManyCachedFastPath::Allocate(size_t size_in_bytes,
-                                               size_t* node_size,
-                                               AllocationOrigin origin) {
+FreeSpace FreeListManyCachedFastPathBase::Allocate(size_t size_in_bytes,
+                                                   size_t* node_size,
+                                                   AllocationOrigin origin) {
   USE(origin);
   DCHECK_GE(kMaxBlockSize, size_in_bytes);
   FreeSpace node;
@@ -363,16 +363,18 @@
   }
 
   // Fast path part 2: searching the medium categories for tiny objects
-  if (node.is_null()) {
-    if (size_in_bytes <= kTinyObjectMaxSize) {
-      DCHECK_EQ(kFastPathFirstCategory, first_category);
-      for (type = next_nonempty_category[kFastPathFallBackTiny];
-           type < kFastPathFirstCategory;
-           type = next_nonempty_category[type + 1]) {
-        node = TryFindNodeIn(type, size_in_bytes, node_size);
-        if (!node.is_null()) break;
+  if (small_blocks_mode_ == SmallBlocksMode::kAllow) {
+    if (node.is_null()) {
+      if (size_in_bytes <= kTinyObjectMaxSize) {
+        DCHECK_EQ(kFastPathFirstCategory, first_category);
+        for (type = next_nonempty_category[kFastPathFallBackTiny];
+             type < kFastPathFirstCategory;
+             type = next_nonempty_category[type + 1]) {
+          node = TryFindNodeIn(type, size_in_bytes, node_size);
+          if (!node.is_null()) break;
+        }
+        first_category = kFastPathFallBackTiny;
       }
-      first_category = kFastPathFallBackTiny;
     }
   }
 
@@ -407,32 +409,6 @@
 }
 
 // ------------------------------------------------
-// FreeListManyCachedFastPathForNewSpace implementation
-
-FreeSpace FreeListManyCachedFastPathForNewSpace::Allocate(
-    size_t size_in_bytes, size_t* node_size, AllocationOrigin origin) {
-  FreeSpace node =
-      FreeListManyCachedFastPath::Allocate(size_in_bytes, node_size, origin);
-  if (!node.is_null()) return node;
-
-  // Search through the precise category for a fit
-  FreeListCategoryType type = SelectFreeListCategoryType(size_in_bytes);
-  node = SearchForNodeInList(type, size_in_bytes, node_size);
-
-  if (!node.is_null()) {
-    if (categories_[type] == nullptr) UpdateCacheAfterRemoval(type);
-    Page::FromHeapObject(node)->IncreaseAllocatedBytes(*node_size);
-  }
-
-#ifdef DEBUG
-  CheckCacheIntegrity();
-#endif
-
-  DCHECK(IsVeryLong() || Available() == SumFreeLists());
-  return node;
-}
-
-// ------------------------------------------------
 // FreeListManyCachedOrigin implementation
 
 FreeSpace FreeListManyCachedOrigin::Allocate(size_t size_in_bytes,
@@ -446,19 +422,6 @@
   }
 }
 
-// ------------------------------------------------
-// FreeListManyCachedOriginForNewSpace implementation
-
-FreeSpace FreeListManyCachedOriginForNewSpace::Allocate(
-    size_t size_in_bytes, size_t* node_size, AllocationOrigin origin) {
-  if (origin == AllocationOrigin::kGC) {
-    return FreeListManyCached::Allocate(size_in_bytes, node_size, origin);
-  } else {
-    return FreeListManyCachedFastPathForNewSpace::Allocate(size_in_bytes,
-                                                           node_size, origin);
-  }
-}
-
 // ------------------------------------------------
 // Generic FreeList methods (non alloc/free related)
 
diff -r -u --color up/v8/src/heap/free-list.h nw/v8/src/heap/free-list.h
--- up/v8/src/heap/free-list.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/free-list.h	2023-01-19 16:46:36.222276234 -0500
@@ -446,8 +446,18 @@
 // FreeListMany), which makes its fast path less fast in the Scavenger. This is
 // done on purpose, since this class's only purpose is to be used by
 // FreeListManyCachedOrigin, which is precise for the scavenger.
-class V8_EXPORT_PRIVATE FreeListManyCachedFastPath : public FreeListManyCached {
+class V8_EXPORT_PRIVATE FreeListManyCachedFastPathBase
+    : public FreeListManyCached {
  public:
+  enum class SmallBlocksMode { kAllow, kProhibit };
+
+  FreeListManyCachedFastPathBase(SmallBlocksMode small_blocks_mode)
+      : small_blocks_mode_(small_blocks_mode) {
+    if (small_blocks_mode_ == SmallBlocksMode::kProhibit) {
+      min_block_size_ = kFastPathStart;
+    }
+  }
+
   V8_WARN_UNUSED_RESULT FreeSpace Allocate(size_t size_in_bytes,
                                            size_t* node_size,
                                            AllocationOrigin origin) override;
@@ -480,24 +490,25 @@
     return last_category_;
   }
 
+ private:
+  SmallBlocksMode small_blocks_mode_;
+
   FRIEND_TEST(
       SpacesTest,
       FreeListManyCachedFastPathSelectFastAllocationFreeListCategoryType);
 };
 
-// Same as FreeListManyCachedFastPath but falls back to a precise search of the
-// precise category in case allocation fails. Because new space is relatively
-// small, the free list is also relatively small and larger categories are more
-// likely to be empty. The precise search is meant to avoid an allocation
-// failure and thus avoid GCs.
-class V8_EXPORT_PRIVATE FreeListManyCachedFastPathForNewSpace
-    : public FreeListManyCachedFastPath {
+class FreeListManyCachedFastPath : public FreeListManyCachedFastPathBase {
  public:
-  V8_WARN_UNUSED_RESULT FreeSpace Allocate(size_t size_in_bytes,
-                                           size_t* node_size,
-                                           AllocationOrigin origin) override;
+  FreeListManyCachedFastPath()
+      : FreeListManyCachedFastPathBase(SmallBlocksMode::kAllow) {}
+};
 
- protected:
+class FreeListManyCachedFastPathForNewSpace
+    : public FreeListManyCachedFastPathBase {
+ public:
+  FreeListManyCachedFastPathForNewSpace()
+      : FreeListManyCachedFastPathBase(SmallBlocksMode::kProhibit) {}
 };
 
 // Uses FreeListManyCached if in the GC; FreeListManyCachedFastPath otherwise.
@@ -513,16 +524,6 @@
  public:
   V8_WARN_UNUSED_RESULT FreeSpace Allocate(size_t size_in_bytes,
                                            size_t* node_size,
-                                           AllocationOrigin origin) override;
-};
-
-// Similar to FreeListManyCachedOrigin but uses
-// FreeListManyCachedFastPathForNewSpace for allocations outside the GC.
-class V8_EXPORT_PRIVATE FreeListManyCachedOriginForNewSpace
-    : public FreeListManyCachedFastPathForNewSpace {
- public:
-  V8_WARN_UNUSED_RESULT FreeSpace Allocate(size_t size_in_bytes,
-                                           size_t* node_size,
                                            AllocationOrigin origin) override;
 };
 
diff -r -u --color up/v8/src/heap/gc-tracer-inl.h nw/v8/src/heap/gc-tracer-inl.h
--- up/v8/src/heap/gc-tracer-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/gc-tracer-inl.h	2023-01-19 16:46:36.222276234 -0500
@@ -138,7 +138,9 @@
 
 bool GCTracer::IsSweepingInProgress() const {
   return (current_.type == Event::MARK_COMPACTOR ||
-          current_.type == Event::INCREMENTAL_MARK_COMPACTOR) &&
+          current_.type == Event::INCREMENTAL_MARK_COMPACTOR ||
+          current_.type == Event::MINOR_MARK_COMPACTOR ||
+          current_.type == Event::INCREMENTAL_MINOR_MARK_COMPACTOR) &&
          current_.state == Event::State::SWEEPING;
 }
 #endif
diff -r -u --color up/v8/src/heap/gc-tracer.cc nw/v8/src/heap/gc-tracer.cc
--- up/v8/src/heap/gc-tracer.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/gc-tracer.cc	2023-01-19 16:46:36.233109563 -0500
@@ -190,8 +190,9 @@
   current_.end_time = MonotonicallyIncreasingTimeInMs();
   previous_ = current_;
   start_of_observable_pause_ = 0.0;
-  notified_sweeping_completed_ = false;
+  notified_full_sweeping_completed_ = false;
   notified_full_cppgc_completed_ = false;
+  notified_young_sweeping_completed_ = false;
   notified_young_cppgc_completed_ = false;
   notified_young_cppgc_running_ = false;
   young_gc_while_full_gc_ = false;
@@ -261,8 +262,6 @@
   DCHECK_IMPLIES(young_gc_while_full_gc_,
                  Heap::IsYoungGenerationCollector(collector) &&
                      !Event::IsYoungGenerationEvent(current_.type));
-  DCHECK_IMPLIES(collector != GarbageCollector::SCAVENGER,
-                 !young_gc_while_full_gc_);
 
   Event::Type type;
   switch (collector) {
@@ -462,7 +461,12 @@
     // If a young generation GC interrupted an unfinished full GC cycle, restore
     // the event corresponding to the full GC cycle.
     if (young_gc_while_full_gc_) {
-      DCHECK_EQ(current_.type, Event::Type::SCAVENGER);
+      // Sweeping for full GC could have occured during the young GC. Copy over
+      // any sweeping scope values to the previous_ event. The full GC sweeping
+      // scopes are never reported by young cycles.
+      previous_.scopes[Scope::MC_SWEEP] += current_.scopes[Scope::MC_SWEEP];
+      previous_.scopes[Scope::MC_BACKGROUND_SWEEPING] +=
+          current_.scopes[Scope::MC_BACKGROUND_SWEEPING];
       std::swap(current_, previous_);
       young_gc_while_full_gc_ = false;
     }
@@ -482,37 +486,48 @@
 
 void GCTracer::StopFullCycleIfNeeded() {
   if (current_.state != Event::State::SWEEPING) return;
-  if (!notified_sweeping_completed_) return;
+  if (!notified_full_sweeping_completed_) return;
   if (heap_->cpp_heap() && !notified_full_cppgc_completed_) return;
   StopCycle(GarbageCollector::MARK_COMPACTOR);
-  notified_sweeping_completed_ = false;
+  notified_full_sweeping_completed_ = false;
   notified_full_cppgc_completed_ = false;
 }
 
 void GCTracer::StopYoungCycleIfNeeded() {
-  // We rely here on the fact that young GCs in V8 are atomic and by the time
-  // this is called, the Scavenger or Minor MC has already finished.
   DCHECK(Event::IsYoungGenerationEvent(current_.type));
   if (current_.state != Event::State::SWEEPING) return;
+  if (current_.type == Event::MINOR_MARK_COMPACTOR &&
+      !notified_young_sweeping_completed_)
+    return;
   // Check if young cppgc was scheduled but hasn't completed yet.
   if (heap_->cpp_heap() && notified_young_cppgc_running_ &&
       !notified_young_cppgc_completed_)
     return;
+  bool was_young_gc_while_full_gc_ = young_gc_while_full_gc_;
   StopCycle(current_.type == Event::SCAVENGER
                 ? GarbageCollector::SCAVENGER
                 : GarbageCollector::MINOR_MARK_COMPACTOR);
+  notified_young_sweeping_completed_ = false;
   notified_young_cppgc_running_ = false;
   notified_young_cppgc_completed_ = false;
+  if (was_young_gc_while_full_gc_) {
+    // Check if the full gc cycle is ready to be stopped.
+    StopFullCycleIfNeeded();
+  }
 }
 
-void GCTracer::NotifySweepingCompleted() {
+void GCTracer::NotifyFullSweepingCompleted() {
+  if (Event::IsYoungGenerationEvent(current_.type)) {
+    bool was_young_gc_while_full_gc_ = young_gc_while_full_gc_;
+    NotifyYoungSweepingCompleted();
+    if (!was_young_gc_while_full_gc_) return;
+  }
+  DCHECK(!Event::IsYoungGenerationEvent(current_.type));
   if (v8_flags.verify_heap) {
     // If heap verification is enabled, sweeping finalization can also be
     // triggered from inside a full GC cycle's atomic pause.
-    DCHECK((current_.type == Event::MARK_COMPACTOR ||
-            current_.type == Event::INCREMENTAL_MARK_COMPACTOR) &&
-           (current_.state == Event::State::SWEEPING ||
-            current_.state == Event::State::ATOMIC));
+    DCHECK(current_.state == Event::State::SWEEPING ||
+           current_.state == Event::State::ATOMIC);
   } else {
     DCHECK(IsSweepingInProgress());
   }
@@ -528,13 +543,33 @@
     heap_->new_space()->PrintAllocationsOrigins();
     heap_->old_space()->PrintAllocationsOrigins();
     heap_->code_space()->PrintAllocationsOrigins();
-    heap_->map_space()->PrintAllocationsOrigins();
   }
-  DCHECK(!notified_sweeping_completed_);
-  notified_sweeping_completed_ = true;
+  // Notifying twice that V8 sweeping is finished for the same cycle is possible
+  // only if Oilpan sweeping is still in progress.
+  DCHECK_IMPLIES(notified_full_sweeping_completed_,
+                 notified_full_cppgc_completed_);
+  notified_full_sweeping_completed_ = true;
   StopFullCycleIfNeeded();
 }
 
+void GCTracer::NotifyYoungSweepingCompleted() {
+  if (!Event::IsYoungGenerationEvent(current_.type)) return;
+  if (v8_flags.verify_heap) {
+    // If heap verification is enabled, sweeping finalization can also be
+    // triggered from inside a full GC cycle's atomic pause.
+    DCHECK(current_.type == Event::MINOR_MARK_COMPACTOR ||
+           current_.type == Event::INCREMENTAL_MINOR_MARK_COMPACTOR);
+    DCHECK(current_.state == Event::State::SWEEPING ||
+           current_.state == Event::State::ATOMIC);
+  } else {
+    DCHECK(IsSweepingInProgress());
+  }
+
+  DCHECK(!notified_young_sweeping_completed_);
+  notified_young_sweeping_completed_ = true;
+  StopYoungCycleIfNeeded();
+}
+
 void GCTracer::NotifyFullCppGCCompleted() {
   // Stop a full GC cycle only when both v8 and cppgc (if available) GCs have
   // finished sweeping. This method is invoked by cppgc.
@@ -545,6 +580,12 @@
   DCHECK(metric_recorder->FullGCMetricsReportPending());
   DCHECK(!notified_full_cppgc_completed_);
   notified_full_cppgc_completed_ = true;
+  // Cppgc sweeping may finalize during MinorMC sweeping. In that case, delay
+  // stopping the cycle until the nested MinorMC cycle is stopped.
+  if (Event::IsYoungGenerationEvent(current_.type)) {
+    DCHECK(young_gc_while_full_gc_);
+    return;
+  }
   StopFullCycleIfNeeded();
 }
 
@@ -771,7 +812,7 @@
           current_.reduce_memory, current_.scopes[Scope::TIME_TO_SAFEPOINT],
           current_scope(Scope::HEAP_PROLOGUE),
           current_scope(Scope::HEAP_EPILOGUE),
-          current_scope(Scope::HEAP_EPILOGUE_REDUCE_NEW_SPACE),
+          current_scope(Scope::HEAP_EPILOGUE_ADJUST_NEW_SPACE),
           current_scope(Scope::HEAP_EXTERNAL_PROLOGUE),
           current_scope(Scope::HEAP_EXTERNAL_EPILOGUE),
           current_scope(Scope::HEAP_EXTERNAL_WEAK_GLOBAL_HANDLES),
@@ -998,7 +1039,7 @@
           current_scope(Scope::HEAP_PROLOGUE),
           current_scope(Scope::HEAP_EMBEDDER_TRACING_EPILOGUE),
           current_scope(Scope::HEAP_EPILOGUE),
-          current_scope(Scope::HEAP_EPILOGUE_REDUCE_NEW_SPACE),
+          current_scope(Scope::HEAP_EPILOGUE_ADJUST_NEW_SPACE),
           current_scope(Scope::HEAP_EXTERNAL_PROLOGUE),
           current_scope(Scope::HEAP_EXTERNAL_EPILOGUE),
           current_scope(Scope::HEAP_EXTERNAL_WEAK_GLOBAL_HANDLES),
diff -r -u --color up/v8/src/heap/gc-tracer.h nw/v8/src/heap/gc-tracer.h
--- up/v8/src/heap/gc-tracer.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/gc-tracer.h	2023-01-19 16:46:36.233109563 -0500
@@ -29,7 +29,12 @@
 #define TRACE_GC_CATEGORIES \
   "devtools.timeline," TRACE_DISABLED_BY_DEFAULT("v8.gc")
 
+// Sweeping for full GC may be interleaved with sweeping for minor
+// gc. The below scopes should use TRACE_GC_EPOCH to associate them
+// with the right cycle.
 #define TRACE_GC(tracer, scope_id)                                    \
+  DCHECK_NE(GCTracer::Scope::MC_SWEEP, scope_id);                     \
+  DCHECK_NE(GCTracer::Scope::MC_BACKGROUND_SWEEPING, scope_id);       \
   GCTracer::Scope UNIQUE_IDENTIFIER(gc_tracer_scope)(                 \
       tracer, GCTracer::Scope::ScopeId(scope_id), ThreadKind::kMain); \
   TRACE_EVENT0(TRACE_GC_CATEGORIES,                                   \
@@ -263,9 +268,10 @@
   void StartInSafepoint();
   void StopInSafepoint();
 
-  void NotifySweepingCompleted();
-  void NotifyFullCppGCCompleted();
+  void NotifyFullSweepingCompleted();
+  void NotifyYoungSweepingCompleted();
 
+  void NotifyFullCppGCCompleted();
   void NotifyYoungCppGCRunning();
   void NotifyYoungCppGCCompleted();
 
@@ -555,8 +561,10 @@
 
   // A full GC cycle stops only when both v8 and cppgc (if available) GCs have
   // finished sweeping.
-  bool notified_sweeping_completed_ = false;
+  bool notified_full_sweeping_completed_ = false;
   bool notified_full_cppgc_completed_ = false;
+
+  bool notified_young_sweeping_completed_ = false;
   // Similar to full GCs, a young GC cycle stops only when both v8 and cppgc GCs
   // have finished sweeping.
   bool notified_young_cppgc_completed_ = false;
diff -r -u --color up/v8/src/heap/global-handle-marking-visitor.cc nw/v8/src/heap/global-handle-marking-visitor.cc
--- up/v8/src/heap/global-handle-marking-visitor.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/global-handle-marking-visitor.cc	2023-01-19 16:46:36.233109563 -0500
@@ -15,8 +15,7 @@
     : heap_(heap),
       marking_state_(*heap_.marking_state()),
       local_marking_worklist_(local_marking_worklist),
-      traced_node_bounds_(
-          heap.isolate()->global_handles()->GetTracedNodeBounds()) {}
+      traced_node_bounds_(heap.isolate()->traced_handles()->GetNodeBounds()) {}
 
 void GlobalHandleMarkingVisitor::VisitPointer(const void* address) {
   const auto upper_it = std::upper_bound(
@@ -27,7 +26,7 @@
 
   const auto bounds = std::next(upper_it, -1);
   if (address < bounds->second) {
-    auto object = GlobalHandles::MarkTracedConservatively(
+    auto object = TracedHandles::MarkConservatively(
         const_cast<Address*>(reinterpret_cast<const Address*>(address)),
         const_cast<Address*>(reinterpret_cast<const Address*>(bounds->first)));
     if (!object.IsHeapObject()) {
diff -r -u --color up/v8/src/heap/global-handle-marking-visitor.h nw/v8/src/heap/global-handle-marking-visitor.h
--- up/v8/src/heap/global-handle-marking-visitor.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/global-handle-marking-visitor.h	2023-01-19 16:46:36.233109563 -0500
@@ -5,7 +5,7 @@
 #ifndef V8_HEAP_GLOBAL_HANDLE_MARKING_VISITOR_H_
 #define V8_HEAP_GLOBAL_HANDLE_MARKING_VISITOR_H_
 
-#include "src/handles/global-handles.h"
+#include "src/handles/traced-handles.h"
 #include "src/heap/base/stack.h"
 #include "src/heap/heap.h"
 #include "src/heap/mark-compact.h"
@@ -27,7 +27,7 @@
   Heap& heap_;
   MarkingState& marking_state_;
   MarkingWorklists::Local& local_marking_worklist_;
-  GlobalHandles::NodeBounds traced_node_bounds_;
+  const TracedHandles::NodeBounds traced_node_bounds_;
 };
 
 #endif  // V8_HEAP_GLOBAL_HANDLE_MARKING_VISITOR_H_
diff -r -u --color up/v8/src/heap/heap-allocator-inl.h nw/v8/src/heap/heap-allocator-inl.h
--- up/v8/src/heap/heap-allocator-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/heap-allocator-inl.h	2023-01-19 16:46:36.233109563 -0500
@@ -38,8 +38,6 @@
   return shared_lo_space_;
 }
 
-PagedSpace* HeapAllocator::space_for_maps() const { return space_for_maps_; }
-
 NewSpace* HeapAllocator::new_space() const {
   return static_cast<NewSpace*>(spaces_[NEW_SPACE]);
 }
@@ -106,6 +104,7 @@
           allocation =
               new_space()->AllocateRaw(size_in_bytes, alignment, origin);
           break;
+        case AllocationType::kMap:
         case AllocationType::kOld:
           allocation =
               old_space()->AllocateRaw(size_in_bytes, alignment, origin);
@@ -116,20 +115,12 @@
           allocation = code_space()->AllocateRaw(
               size_in_bytes, AllocationAlignment::kTaggedAligned);
           break;
-        case AllocationType::kMap:
-          DCHECK_EQ(alignment, AllocationAlignment::kTaggedAligned);
-          allocation = space_for_maps()->AllocateRaw(
-              size_in_bytes, AllocationAlignment::kTaggedAligned);
-          break;
         case AllocationType::kReadOnly:
           DCHECK(read_only_space()->writable());
           DCHECK_EQ(AllocationOrigin::kRuntime, origin);
           allocation = read_only_space()->AllocateRaw(size_in_bytes, alignment);
           break;
         case AllocationType::kSharedMap:
-          allocation = shared_map_allocator_->AllocateRaw(size_in_bytes,
-                                                          alignment, origin);
-          break;
         case AllocationType::kSharedOld:
           allocation = shared_old_allocator_->AllocateRaw(size_in_bytes,
                                                           alignment, origin);
diff -r -u --color up/v8/src/heap/heap-allocator.cc nw/v8/src/heap/heap-allocator.cc
--- up/v8/src/heap/heap-allocator.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/heap-allocator.cc	2023-01-19 16:46:36.233109563 -0500
@@ -23,14 +23,7 @@
     spaces_[i] = heap_->space(i);
   }
 
-  space_for_maps_ = spaces_[MAP_SPACE]
-                        ? static_cast<PagedSpace*>(spaces_[MAP_SPACE])
-                        : static_cast<PagedSpace*>(spaces_[OLD_SPACE]);
-
   shared_old_allocator_ = heap_->shared_space_allocator_.get();
-  shared_map_allocator_ = heap_->shared_map_allocator_
-                              ? heap_->shared_map_allocator_.get()
-                              : shared_old_allocator_;
   shared_lo_space_ = heap_->shared_lo_allocation_space();
 }
 
diff -r -u --color up/v8/src/heap/heap-allocator.h nw/v8/src/heap/heap-allocator.h
--- up/v8/src/heap/heap-allocator.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/heap-allocator.h	2023-01-19 16:46:36.233109563 -0500
@@ -79,7 +79,6 @@
  private:
   V8_INLINE PagedSpace* code_space() const;
   V8_INLINE CodeLargeObjectSpace* code_lo_space() const;
-  V8_INLINE PagedSpace* space_for_maps() const;
   V8_INLINE NewSpace* new_space() const;
   V8_INLINE NewLargeObjectSpace* new_lo_space() const;
   V8_INLINE OldLargeObjectSpace* lo_space() const;
@@ -105,11 +104,9 @@
 
   Heap* const heap_;
   Space* spaces_[LAST_SPACE + 1];
-  PagedSpace* space_for_maps_;
   ReadOnlySpace* read_only_space_;
 
   ConcurrentAllocator* shared_old_allocator_;
-  ConcurrentAllocator* shared_map_allocator_;
   OldLargeObjectSpace* shared_lo_space_;
 
 #ifdef V8_ENABLE_ALLOCATION_TIMEOUT
diff -r -u --color up/v8/src/heap/heap-inl.h nw/v8/src/heap/heap-inl.h
--- up/v8/src/heap/heap-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/heap-inl.h	2023-01-19 16:46:36.233109563 -0500
@@ -115,16 +115,6 @@
   return external_memory_.Update(delta);
 }
 
-PagedSpace* Heap::space_for_maps() {
-  return V8_LIKELY(map_space_) ? static_cast<PagedSpace*>(map_space_)
-                               : static_cast<PagedSpace*>(old_space_);
-}
-
-ConcurrentAllocator* Heap::concurrent_allocator_for_maps() {
-  return V8_LIKELY(shared_map_allocator_) ? shared_map_allocator_.get()
-                                          : shared_space_allocator_.get();
-}
-
 RootsTable& Heap::roots_table() { return isolate()->roots_table(); }
 
 #define ROOT_ACCESSOR(Type, name, CamelName)                           \
@@ -164,14 +154,14 @@
   roots_table()[RootIndex::kMessageListeners] = value.ptr();
 }
 
-void Heap::SetPendingOptimizeForTestBytecode(Object hash_table) {
+void Heap::SetFunctionsMarkedForManualOptimization(Object hash_table) {
   DCHECK(hash_table.IsObjectHashTable() || hash_table.IsUndefined(isolate()));
-  roots_table()[RootIndex::kPendingOptimizeForTestBytecode] = hash_table.ptr();
+  roots_table()[RootIndex::kFunctionsMarkedForManualOptimization] =
+      hash_table.ptr();
 }
 
 PagedSpace* Heap::paged_space(int idx) {
-  DCHECK(idx == OLD_SPACE || idx == CODE_SPACE || idx == MAP_SPACE ||
-         idx == SHARED_SPACE);
+  DCHECK(idx == OLD_SPACE || idx == CODE_SPACE || idx == SHARED_SPACE);
   return static_cast<PagedSpace*>(space_[idx].get());
 }
 
@@ -377,8 +367,7 @@
     }
 
     case OLD_SPACE:
-    case CODE_SPACE:
-    case MAP_SPACE: {
+    case CODE_SPACE: {
       PagedSpace* paged_space = static_cast<PagedSpace*>(base_space);
       base::SharedMutexGuard<base::kShared> guard(
           paged_space->linear_area_lock());
diff -r -u --color up/v8/src/heap/heap-verifier.cc nw/v8/src/heap/heap-verifier.cc
--- up/v8/src/heap/heap-verifier.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/heap-verifier.cc	2023-01-19 16:46:36.233109563 -0500
@@ -29,6 +29,10 @@
 namespace v8 {
 namespace internal {
 
+namespace {
+thread_local HeapObject pending_layout_change_object = HeapObject();
+}  // namespace
+
 // Verify that all objects are Smis.
 class VerifySmisVisitor final : public RootVisitor {
  public:
@@ -54,7 +58,6 @@
   ReadOnlySpace* read_only_space() const { return heap_->read_only_space(); }
   NewSpace* new_space() const { return heap_->new_space(); }
   OldSpace* old_space() const { return heap_->old_space(); }
-  MapSpace* map_space() const { return heap_->map_space(); }
   CodeSpace* code_space() const { return heap_->code_space(); }
   LargeObjectSpace* lo_space() const { return heap_->lo_space(); }
   CodeLargeObjectSpace* code_lo_space() const { return heap_->code_lo_space(); }
@@ -70,7 +73,7 @@
   CHECK(heap()->HasBeenSetUp());
   AllowGarbageCollection allow_gc;
   IgnoreLocalGCRequests ignore_gc_requests(heap());
-  SafepointScope safepoint_scope(heap());
+  IsolateSafepointScope safepoint_scope(heap());
   HandleScope scope(isolate());
 
   heap()->MakeHeapIterable();
@@ -78,7 +81,8 @@
   heap()->array_buffer_sweeper()->EnsureFinished();
 
   VerifyPointersVisitor visitor(heap());
-  heap()->IterateRoots(&visitor, {});
+  heap()->IterateRoots(&visitor,
+                       base::EnumSet<SkipRoot>{SkipRoot::kConservativeStack});
 
   if (!isolate()->context().is_null() &&
       !isolate()->raw_native_context().is_null()) {
@@ -103,9 +107,6 @@
   if (new_space()) new_space()->Verify(isolate());
 
   old_space()->Verify(isolate(), &visitor);
-  if (map_space()) {
-    map_space()->Verify(isolate(), &visitor);
-  }
 
   VerifyPointersVisitor no_dirty_regions_visitor(heap());
   code_space()->Verify(isolate(), &no_dirty_regions_visitor);
@@ -412,6 +413,32 @@
 }
 
 // static
+void HeapVerifier::VerifyObjectLayoutChangeIsAllowed(Heap* heap,
+                                                     HeapObject object) {
+  if (object.InSharedWritableHeap()) {
+    // Out of objects in the shared heap, only strings can change layout.
+    DCHECK(object.IsString());
+    // Shared strings only change layout under GC, never concurrently.
+    if (object.IsShared()) {
+      Isolate* isolate = heap->isolate();
+      Isolate* shared_heap_isolate = isolate->is_shared_heap_isolate()
+                                         ? isolate
+                                         : isolate->shared_heap_isolate();
+      shared_heap_isolate->global_safepoint()->AssertActive();
+    }
+    // Non-shared strings in the shared heap are allowed to change layout
+    // outside of GC like strings in non-shared heaps.
+  }
+}
+
+// static
+void HeapVerifier::SetPendingLayoutChangeObject(Heap* heap, HeapObject object) {
+  VerifyObjectLayoutChangeIsAllowed(heap, object);
+  DCHECK(pending_layout_change_object.is_null());
+  pending_layout_change_object = object;
+}
+
+// static
 void HeapVerifier::VerifyObjectLayoutChange(Heap* heap, HeapObject object,
                                             Map new_map) {
   // Object layout changes are currently not supported on background threads.
@@ -419,17 +446,19 @@
 
   if (!v8_flags.verify_heap) return;
 
+  VerifyObjectLayoutChangeIsAllowed(heap, object);
+
   PtrComprCageBase cage_base(heap->isolate());
 
   // Check that Heap::NotifyObjectLayoutChange was called for object transitions
   // that are not safe for concurrent marking.
   // If you see this check triggering for a freshly allocated object,
   // use object->set_map_after_allocation() to initialize its map.
-  if (heap->pending_layout_change_object_.is_null()) {
+  if (pending_layout_change_object.is_null()) {
     VerifySafeMapTransition(heap, object, new_map);
   } else {
-    DCHECK_EQ(heap->pending_layout_change_object_, object);
-    heap->pending_layout_change_object_ = HeapObject();
+    DCHECK_EQ(pending_layout_change_object, object);
+    pending_layout_change_object = HeapObject();
   }
 }
 
diff -r -u --color up/v8/src/heap/heap-verifier.h nw/v8/src/heap/heap-verifier.h
--- up/v8/src/heap/heap-verifier.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/heap-verifier.h	2023-01-19 16:46:36.233109563 -0500
@@ -45,6 +45,13 @@
                                                          HeapObject object,
                                                          Map new_map);
 
+  // Verifies that that the object is allowed to change layout. Checks that if
+  // the object is in shared space, it must be a string as no other objects in
+  // shared space change layouts.
+  static void VerifyObjectLayoutChangeIsAllowed(Heap* heap, HeapObject object);
+
+  static void SetPendingLayoutChangeObject(Heap* heap, HeapObject object);
+
 #else
   static void VerifyHeap(Heap* heap) {}
   static void VerifyReadOnlyHeap(Heap* heap) {}
@@ -54,6 +61,8 @@
                                       Map new_map) {}
   static void VerifyObjectLayoutChange(Heap* heap, HeapObject object,
                                        Map new_map) {}
+  static void VerifyObjectLayoutChangeIsAllowed(Heap* heap, HeapObject object) {
+  }
 #endif
 
   V8_INLINE static void VerifyHeapIfEnabled(Heap* heap) {
diff -r -u --color up/v8/src/heap/heap.cc nw/v8/src/heap/heap.cc
--- up/v8/src/heap/heap.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/heap.cc	2023-01-19 16:46:36.233109563 -0500
@@ -36,6 +36,7 @@
 #include "src/execution/vm-state-inl.h"
 #include "src/flags/flags.h"
 #include "src/handles/global-handles-inl.h"
+#include "src/handles/traced-handles.h"
 #include "src/heap/array-buffer-sweeper.h"
 #include "src/heap/base/stack.h"
 #include "src/heap/basic-memory-chunk.h"
@@ -114,6 +115,11 @@
 #include "src/tracing/trace-event.h"
 #include "src/utils/utils-inl.h"
 #include "src/utils/utils.h"
+
+#ifdef V8_ENABLE_CONSERVATIVE_STACK_SCANNING
+#include "src/heap/conservative-stack-visitor.h"
+#endif  // V8_ENABLE_CONSERVATIVE_STACK_SCANNING
+
 // Has to be the last include (doesn't have include guards):
 #include "src/objects/object-macros.h"
 
@@ -449,6 +455,12 @@
   return memory_allocator()->Size() + size <= MaxReserved();
 }
 
+namespace {
+bool IsIsolateDeserializationActive(LocalHeap* local_heap) {
+  return local_heap && !local_heap->heap()->deserialization_complete();
+}
+}  // anonymous namespace
+
 bool Heap::CanExpandOldGenerationBackground(LocalHeap* local_heap,
                                             size_t size) {
   if (force_oom_) return false;
@@ -456,6 +468,7 @@
   // When the heap is tearing down, then GC requests from background threads
   // are not served and the threads are allowed to expand the heap to avoid OOM.
   return gc_state() == TEAR_DOWN || IsMainThreadParked(local_heap) ||
+         IsIsolateDeserializationActive(local_heap) ||
          memory_allocator()->Size() + size <= MaxReserved();
 }
 
@@ -569,14 +582,6 @@
                ", committed: %6zu KB\n",
                code_space_->SizeOfObjects() / KB, code_space_->Available() / KB,
                code_space_->CommittedMemory() / KB);
-  if (map_space()) {
-    PrintIsolate(isolate_,
-                 "Map space,              used: %6zu KB"
-                 ", available: %6zu KB"
-                 ", committed: %6zu KB\n",
-                 map_space_->SizeOfObjects() / KB, map_space_->Available() / KB,
-                 map_space_->CommittedMemory() / KB);
-  }
   PrintIsolate(isolate_,
                "Large object space,     used: %6zu KB"
                ", available: %6zu KB"
@@ -733,7 +738,6 @@
       SpaceStatistics(NEW_SPACE)     << "," <<
       SpaceStatistics(OLD_SPACE)     << "," <<
       SpaceStatistics(CODE_SPACE)    << "," <<
-      SpaceStatistics(MAP_SPACE)     << "," <<
       SpaceStatistics(LO_SPACE)      << "," <<
       SpaceStatistics(CODE_LO_SPACE) << "," <<
       SpaceStatistics(NEW_LO_SPACE)));
@@ -956,12 +960,11 @@
   PrintF("-------------------------------------------------\n");
 }
 
-void UpdateRetainersMapAfterScavenge(
-    std::unordered_map<HeapObject, HeapObject, Object::Hasher>* map) {
+void UpdateRetainersMapAfterScavenge(UnorderedHeapObjectMap<HeapObject>* map) {
   // This is only used for Scavenger.
   DCHECK(!v8_flags.minor_mc);
 
-  std::unordered_map<HeapObject, HeapObject, Object::Hasher> updated_map;
+  UnorderedHeapObjectMap<HeapObject> updated_map;
 
   for (auto pair : *map) {
     HeapObject object = pair.first;
@@ -994,7 +997,7 @@
   UpdateRetainersMapAfterScavenge(&retainer_);
   UpdateRetainersMapAfterScavenge(&ephemeron_retainer_);
 
-  std::unordered_map<HeapObject, Root, Object::Hasher> updated_retaining_root;
+  UnorderedHeapObjectMap<Root> updated_retaining_root;
 
   for (auto pair : retaining_root_) {
     HeapObject object = pair.first;
@@ -1104,7 +1107,6 @@
 
   if (new_space_) {
     UpdateNewSpaceAllocationCounter();
-    CheckNewSpaceExpansionCriteria();
     new_space_->ResetParkedAllocationBuffers();
   }
 }
@@ -1128,11 +1130,13 @@
 }
 
 size_t Heap::TotalGlobalHandlesSize() {
-  return isolate_->global_handles()->TotalSize();
+  return isolate_->global_handles()->TotalSize() +
+         isolate_->traced_handles()->total_size_bytes();
 }
 
 size_t Heap::UsedGlobalHandlesSize() {
-  return isolate_->global_handles()->UsedSize();
+  return isolate_->global_handles()->UsedSize() +
+         isolate_->traced_handles()->used_size_bytes();
 }
 
 void Heap::AddAllocationObserversToAllSpaces(
@@ -1246,10 +1250,6 @@
   UPDATE_COUNTERS_AND_FRAGMENTATION_FOR_SPACE(old_space)
   UPDATE_COUNTERS_AND_FRAGMENTATION_FOR_SPACE(code_space)
 
-  if (map_space()) {
-    UPDATE_COUNTERS_AND_FRAGMENTATION_FOR_SPACE(map_space)
-  }
-
   UPDATE_COUNTERS_AND_FRAGMENTATION_FOR_SPACE(lo_space)
 #undef UPDATE_COUNTERS_FOR_SPACE
 #undef UPDATE_FRAGMENTATION_FOR_SPACE
@@ -1275,10 +1275,20 @@
     if (Heap::ShouldZapGarbage() || v8_flags.clear_free_memory) {
       new_space()->ZapUnusedMemory();
     }
-    TRACE_GC(tracer(), GCTracer::Scope::HEAP_EPILOGUE_REDUCE_NEW_SPACE);
-    ReduceNewSpaceSize();
 
     if (!v8_flags.minor_mc) {
+      {
+        TRACE_GC(tracer(), GCTracer::Scope::HEAP_EPILOGUE_ADJUST_NEW_SPACE);
+        ResizeNewSpaceMode resize_new_space = ShouldResizeNewSpace();
+        if (resize_new_space == ResizeNewSpaceMode::kGrow) {
+          ExpandNewSpaceSize();
+        }
+
+        if (resize_new_space == ResizeNewSpaceMode::kShrink) {
+          ReduceNewSpaceSize();
+        }
+      }
+
       SemiSpaceNewSpace::From(new_space())->MakeAllPagesInFromSpaceIterable();
     }
 
@@ -1331,10 +1341,6 @@
         static_cast<int>(CommittedMemory() / KB));
     isolate_->counters()->heap_sample_total_used()->AddSample(
         static_cast<int>(SizeOfObjects() / KB));
-    if (map_space()) {
-      isolate_->counters()->heap_sample_map_space_committed()->AddSample(
-          static_cast<int>(map_space()->CommittedMemory() / KB));
-    }
     isolate_->counters()->heap_sample_code_space_committed()->AddSample(
         static_cast<int>(code_space()->CommittedMemory() / KB));
 
@@ -1658,6 +1664,10 @@
         this, IsYoungGenerationCollector(collector) ? "MinorGC" : "MajorGC",
         GarbageCollectionReasonToString(gc_reason));
 
+    auto stack_marker = v8::base::Stack::GetCurrentStackPosition();
+#ifdef V8_ENABLE_CONSERVATIVE_STACK_SCANNING
+    stack().set_marker(stack_marker);
+#endif
     if (collector == GarbageCollector::MARK_COMPACTOR && cpp_heap()) {
       // CppHeap needs a stack marker at the top of all entry points to allow
       // deterministic passes over the stack. E.g., a verifier that should only
@@ -1666,7 +1676,7 @@
       // TODO(chromium:1056170): Consider adding a component that keeps track
       // of relevant GC stack regions where interesting pointers can be found.
       static_cast<v8::internal::CppHeap*>(cpp_heap())
-          ->SetStackEndOfCurrentGC(v8::base::Stack::GetCurrentStackPosition());
+          ->SetStackEndOfCurrentGC(stack_marker);
     }
 
     GarbageCollectionPrologue(gc_reason, gc_callback_flags);
@@ -1692,8 +1702,8 @@
       if (V8_ENABLE_THIRD_PARTY_HEAP_BOOL) {
         tp_heap_->CollectGarbage();
       } else {
-        freed_global_handles += PerformGarbageCollection(
-            collector, gc_reason, collector_reason, gc_callback_flags);
+        freed_global_handles +=
+            PerformGarbageCollection(collector, gc_reason, collector_reason);
       }
       // Clear flags describing the current GC now that the current GC is
       // complete. Do this before GarbageCollectionEpilogue() since that could
@@ -1751,6 +1761,10 @@
     } else {
       tracer()->StopFullCycleIfNeeded();
     }
+
+#ifdef V8_ENABLE_CONSERVATIVE_STACK_SCANNING
+    stack().clear_marker();
+#endif
   }
 
   // Part 3: Invoke all callbacks which should happen after the actual garbage
@@ -1840,24 +1854,32 @@
     CompleteSweepingFull();
   }
 
-  base::Optional<GlobalSafepointScope> global_safepoint_scope;
   base::Optional<SafepointScope> safepoint_scope;
 
   {
     AllowGarbageCollection allow_shared_gc;
     IgnoreLocalGCRequests ignore_gc_requests(this);
 
-    if (isolate()->is_shared_heap_isolate()) {
-      global_safepoint_scope.emplace(isolate());
-    } else {
-      safepoint_scope.emplace(this);
-    }
+    SafepointKind safepoint_kind = isolate()->is_shared_heap_isolate()
+                                       ? SafepointKind::kGlobal
+                                       : SafepointKind::kIsolate;
+    safepoint_scope.emplace(isolate(), safepoint_kind);
   }
 
 #ifdef DEBUG
   VerifyCountersAfterSweeping();
 #endif
 
+  if (isolate()->is_shared_heap_isolate()) {
+    isolate()->global_safepoint()->IterateClientIsolates([](Isolate* client) {
+      if (client->is_shared_heap_isolate()) return;
+
+      if (v8_flags.concurrent_marking) {
+        client->heap()->concurrent_marking()->Pause();
+      }
+    });
+  }
+
   // Now that sweeping is completed, we can start the next full GC cycle.
   tracer()->StartCycle(collector, gc_reason, nullptr,
                        GCTracer::MarkingType::kIncremental);
@@ -1866,6 +1888,17 @@
   current_gc_callback_flags_ = gc_callback_flags;
 
   incremental_marking()->Start(collector, gc_reason);
+
+  if (isolate()->is_shared_heap_isolate()) {
+    isolate()->global_safepoint()->IterateClientIsolates([](Isolate* client) {
+      if (client->is_shared_heap_isolate()) return;
+
+      if (v8_flags.concurrent_marking &&
+          client->heap()->incremental_marking()->IsMarking()) {
+        client->heap()->concurrent_marking()->Resume();
+      }
+    });
+  }
 }
 
 void Heap::CompleteSweepingFull() {
@@ -2098,9 +2131,9 @@
 }
 }  // namespace
 
-size_t Heap::PerformGarbageCollection(
-    GarbageCollector collector, GarbageCollectionReason gc_reason,
-    const char* collector_reason, const v8::GCCallbackFlags gc_callback_flags) {
+size_t Heap::PerformGarbageCollection(GarbageCollector collector,
+                                      GarbageCollectionReason gc_reason,
+                                      const char* collector_reason) {
   DisallowJavascriptExecution no_js(isolate());
 
   if (IsYoungGenerationCollector(collector)) {
@@ -2132,6 +2165,7 @@
                            GCTracer::MarkingType::kAtomic);
     }
   }
+  if (v8_flags.minor_mc) pretenuring_handler_.ProcessPretenuringFeedback();
 
   tracer()->StartAtomicPause();
   if (!Heap::IsYoungGenerationCollector(collector) &&
@@ -2142,18 +2176,17 @@
   DCHECK(tracer()->IsConsistentWithCollector(collector));
   TRACE_GC_EPOCH(tracer(), CollectorScopeId(collector), ThreadKind::kMain);
 
-  base::Optional<GlobalSafepointScope> global_safepoint_scope;
-  base::Optional<SafepointScope> isolate_safepoint_scope;
+  base::Optional<SafepointScope> safepoint_scope;
 
   {
     AllowGarbageCollection allow_shared_gc;
     IgnoreLocalGCRequests ignore_gc_requests(this);
 
-    if (isolate()->is_shared_heap_isolate()) {
-      global_safepoint_scope.emplace(isolate());
-    } else {
-      isolate_safepoint_scope.emplace(this);
-    }
+    SafepointKind safepoint_kind =
+        v8_flags.shared_space && isolate()->is_shared_heap_isolate()
+            ? SafepointKind::kGlobal
+            : SafepointKind::kIsolate;
+    safepoint_scope.emplace(isolate(), safepoint_kind);
   }
 
   collection_barrier_->StopTimeToCollectionTimer();
@@ -2163,6 +2196,11 @@
   if (isolate()->is_shared_heap_isolate()) {
     isolate()->global_safepoint()->IterateClientIsolates([](Isolate* client) {
       if (client->is_shared_heap_isolate()) return;
+
+      if (v8_flags.concurrent_marking) {
+        client->heap()->concurrent_marking()->Pause();
+      }
+
       HeapVerifier::VerifyHeapIfEnabled(client->heap());
     });
   }
@@ -2229,11 +2267,7 @@
   // stack scanning, do it only when Scavenger runs from task, which is
   // non-nestable.
   if (cpp_heap() && IsYoungGenerationCollector(collector)) {
-    const bool with_stack = (gc_reason != GarbageCollectionReason::kTask);
-    CppHeap::From(cpp_heap())
-        ->RunMinorGCIfNeeded(with_stack
-                                 ? CppHeap::StackState::kMayContainHeapPointers
-                                 : CppHeap::StackState::kNoHeapPointers);
+    CppHeap::From(cpp_heap())->RunMinorGCIfNeeded();
   }
 #endif  // defined(CPPGC_YOUNG_GENERATION)
 
@@ -2248,6 +2282,12 @@
   if (isolate()->is_shared_heap_isolate()) {
     isolate()->global_safepoint()->IterateClientIsolates([](Isolate* client) {
       if (client->is_shared_heap_isolate()) return;
+
+      if (v8_flags.concurrent_marking &&
+          client->heap()->incremental_marking()->IsMarking()) {
+        client->heap()->concurrent_marking()->Resume();
+      }
+
       HeapVerifier::VerifyHeapIfEnabled(client->heap());
     });
   }
@@ -2264,7 +2304,6 @@
     Isolate* shared_space_isolate = isolate()->shared_space_isolate();
     return shared_space_isolate->heap()->CollectGarbageFromAnyThread(local_heap,
                                                                      gc_reason);
-
   } else {
     DCHECK(!IsShared());
     DCHECK_NOT_NULL(isolate()->shared_isolate());
@@ -2315,6 +2354,10 @@
   DCHECK(incremental_marking_->IsStopped());
   DCHECK_NOT_NULL(isolate()->global_safepoint());
 
+#ifdef V8_ENABLE_CONSERVATIVE_STACK_SCANNING
+  stack().set_marker(v8::base::Stack::GetCurrentStackPosition());
+#endif
+
   isolate()->global_safepoint()->IterateClientIsolates([](Isolate* client) {
     client->heap()->FreeSharedLinearAllocationAreas();
 
@@ -2345,6 +2388,10 @@
   tracer()->StopObservablePause();
   tracer()->UpdateStatistics(collector);
   tracer()->StopFullCycleIfNeeded();
+
+#ifdef V8_ENABLE_CONSERVATIVE_STACK_SCANNING
+  stack().clear_marker();
+#endif
 }
 
 void Heap::CompleteSweepingYoung(GarbageCollector collector) {
@@ -2366,15 +2413,13 @@
     array_buffer_sweeper()->EnsureFinished();
   }
 
-  if (v8_flags.minor_mc) {
-    DCHECK(v8_flags.separate_gc_phases);
-    // Do not interleave sweeping.
-    EnsureSweepingCompleted(SweepingForcedFinalizationMode::kV8Only);
-  } else {
-    // If sweeping is in progress and there are no sweeper tasks running, finish
-    // the sweeping here, to avoid having to pause and resume during the young
-    // generation GC.
-    FinishSweepingIfOutOfWork();
+  // If sweeping is in progress and there are no sweeper tasks running, finish
+  // the sweeping here, to avoid having to pause and resume during the young
+  // generation GC.
+  FinishSweepingIfOutOfWork();
+
+  if (v8_flags.minor_mc && sweeping_in_progress()) {
+    PauseSweepingAndEnsureYoungSweepingCompleted();
   }
 
 #if defined(CPPGC_YOUNG_GENERATION)
@@ -2568,17 +2613,6 @@
   FlushNumberStringCache();
 }
 
-void Heap::CheckNewSpaceExpansionCriteria() {
-  if (new_space_->TotalCapacity() < new_space_->MaximumCapacity() &&
-      survived_since_last_expansion_ > new_space_->TotalCapacity()) {
-    // Grow the size of new space if there is room to grow, and enough data
-    // has survived scavenge since the last expansion.
-    new_space_->Grow();
-    survived_since_last_expansion_ = 0;
-  }
-  new_lo_space()->SetCapacity(new_space()->Capacity());
-}
-
 void Heap::Scavenge() {
   DCHECK_NOT_NULL(new_space());
   DCHECK_IMPLIES(v8_flags.separate_gc_phases,
@@ -3393,7 +3427,7 @@
   if (v8_flags.enable_slow_asserts) {
     // Make sure the stack or other roots (e.g., Handles) don't contain pointers
     // to the original FixedArray (which is now the filler object).
-    base::Optional<SafepointScope> safepoint_scope;
+    base::Optional<IsolateSafepointScope> safepoint_scope;
 
     {
       AllowGarbageCollection allow_gc;
@@ -3403,7 +3437,9 @@
 
     LeftTrimmerVerifierRootVisitor root_visitor(object);
     ReadOnlyRoots(this).Iterate(&root_visitor);
-    IterateRoots(&root_visitor, {});
+
+    IterateRoots(&root_visitor,
+                 base::EnumSet<SkipRoot>{SkipRoot::kConservativeStack});
   }
 #endif  // ENABLE_SLOW_DCHECKS
 
@@ -3517,6 +3553,12 @@
     local_heap->MakeLinearAllocationAreaIterable();
   });
 
+  if (isolate()->is_shared_space_isolate()) {
+    isolate()->global_safepoint()->IterateClientIsolates([](Isolate* client) {
+      client->heap()->MakeSharedLinearAllocationAreasIterable();
+    });
+  }
+
   PagedSpaceIterator spaces(this);
   for (PagedSpace* space = spaces.Next(); space != nullptr;
        space = spaces.Next()) {
@@ -3562,10 +3604,23 @@
 void Heap::FreeMainThreadSharedLinearAllocationAreas() {
   if (!isolate()->has_shared_heap()) return;
   shared_space_allocator_->FreeLinearAllocationArea();
-  if (shared_map_allocator_) shared_map_allocator_->FreeLinearAllocationArea();
   main_thread_local_heap()->FreeSharedLinearAllocationArea();
 }
 
+void Heap::MakeSharedLinearAllocationAreasIterable() {
+  if (!isolate()->has_shared_heap()) return;
+
+  safepoint()->IterateLocalHeaps([](LocalHeap* local_heap) {
+    local_heap->MakeSharedLinearAllocationAreaIterable();
+  });
+
+  if (v8_flags.shared_space && shared_space_allocator_) {
+    shared_space_allocator_->MakeLinearAllocationAreaIterable();
+  }
+
+  main_thread_local_heap()->MakeSharedLinearAllocationAreaIterable();
+}
+
 void Heap::MarkSharedLinearAllocationAreasBlack() {
   DCHECK(v8_flags.shared_space);
   if (shared_space_allocator_) {
@@ -3720,25 +3775,44 @@
   }
 }
 
-bool Heap::ShouldReduceNewSpaceSize() const {
-  static const size_t kLowAllocationThroughput = 1000;
-
-  if (v8_flags.predictable) return false;
+Heap::ResizeNewSpaceMode Heap::ShouldResizeNewSpace() {
+  if (ShouldReduceMemory()) {
+    return (v8_flags.predictable) ? ResizeNewSpaceMode::kNone
+                                  : ResizeNewSpaceMode::kShrink;
+  }
 
+  static const size_t kLowAllocationThroughput = 1000;
   const double allocation_throughput =
       tracer_->CurrentAllocationThroughputInBytesPerMillisecond();
+  const bool should_shrink = !v8_flags.predictable &&
+                             (allocation_throughput != 0) &&
+                             (allocation_throughput < kLowAllocationThroughput);
+
+  const bool should_grow =
+      (new_space_->TotalCapacity() < new_space_->MaximumCapacity()) &&
+      (survived_since_last_expansion_ > new_space_->TotalCapacity());
 
-  return ShouldReduceMemory() ||
-         ((allocation_throughput != 0) &&
-          (allocation_throughput < kLowAllocationThroughput));
+  if (should_grow) survived_since_last_expansion_ = 0;
+
+  if (should_grow == should_shrink) return ResizeNewSpaceMode::kNone;
+  return should_grow ? ResizeNewSpaceMode::kGrow : ResizeNewSpaceMode::kShrink;
 }
 
-void Heap::ReduceNewSpaceSize() {
-  if (!ShouldReduceNewSpaceSize()) return;
+void Heap::ExpandNewSpaceSize() {
+  // Grow the size of new space if there is room to grow, and enough data
+  // has survived scavenge since the last expansion.
+  new_space_->Grow();
+  new_lo_space()->SetCapacity(new_space()->TotalCapacity());
+}
 
+void Heap::ReduceNewSpaceSize() {
   // MinorMC shrinks new space as part of sweeping.
-  if (!v8_flags.minor_mc) new_space_->Shrink();
-  new_lo_space_->SetCapacity(new_space_->Capacity());
+  if (!v8_flags.minor_mc) {
+    new_space()->Shrink();
+  } else {
+    paged_new_space()->FinishShrinking();
+  }
+  new_lo_space_->SetCapacity(new_space()->TotalCapacity());
 }
 
 size_t Heap::NewSpaceSize() { return new_space() ? new_space()->Size() : 0; }
@@ -3805,8 +3879,7 @@
   }
 #ifdef VERIFY_HEAP
   if (v8_flags.verify_heap) {
-    DCHECK(pending_layout_change_object_.is_null());
-    pending_layout_change_object_ = object;
+    HeapVerifier::SetPendingLayoutChangeObject(this, object);
   }
 #endif
 }
@@ -4163,7 +4236,7 @@
 void Heap::CollectCodeStatistics() {
   TRACE_EVENT0("v8", "Heap::CollectCodeStatistics");
   IgnoreLocalGCRequests ignore_gc_requests(this);
-  SafepointScope safepoint_scope(this);
+  IsolateSafepointScope safepoint_scope(this);
   MakeHeapIterable();
   CodeStatistics::ResetCodeAndMetadataStatistics(isolate());
   // We do not look for code in new space, or map space.  If code
@@ -4269,7 +4342,6 @@
 
   return (new_space_ && new_space_->Contains(value)) ||
          old_space_->Contains(value) || code_space_->Contains(value) ||
-         (map_space_ && map_space_->Contains(value)) ||
          (shared_space_ && shared_space_->Contains(value)) ||
          lo_space_->Contains(value) || code_lo_space_->Contains(value) ||
          (new_lo_space_ && new_lo_space_->Contains(value)) ||
@@ -4292,9 +4364,6 @@
   if (shared_allocation_space_) {
     if (shared_allocation_space_->Contains(value)) return true;
     if (shared_lo_allocation_space_->Contains(value)) return true;
-    if (shared_map_allocation_space_ &&
-        shared_map_allocation_space_->Contains(value))
-      return true;
   }
 
   return false;
@@ -4324,9 +4393,6 @@
       return old_space_->Contains(value);
     case CODE_SPACE:
       return code_space_->Contains(value);
-    case MAP_SPACE:
-      DCHECK(map_space_);
-      return map_space_->Contains(value);
     case SHARED_SPACE:
       return shared_space_->Contains(value);
     case LO_SPACE:
@@ -4362,9 +4428,6 @@
       return old_space_->ContainsSlow(addr);
     case CODE_SPACE:
       return code_space_->ContainsSlow(addr);
-    case MAP_SPACE:
-      DCHECK(map_space_);
-      return map_space_->ContainsSlow(addr);
     case SHARED_SPACE:
       return shared_space_->ContainsSlow(addr);
     case LO_SPACE:
@@ -4386,7 +4449,6 @@
     case NEW_SPACE:
     case OLD_SPACE:
     case CODE_SPACE:
-    case MAP_SPACE:
     case SHARED_SPACE:
     case LO_SPACE:
     case NEW_LO_SPACE:
@@ -4401,6 +4463,7 @@
 
 #ifdef DEBUG
 void Heap::VerifyCountersAfterSweeping() {
+  MakeHeapIterable();
   PagedSpaceIterator spaces(this);
   for (PagedSpace* space = spaces.Next(); space != nullptr;
        space = spaces.Next()) {
@@ -4408,7 +4471,13 @@
   }
 }
 
-void Heap::VerifyCountersBeforeConcurrentSweeping() {
+void Heap::VerifyCountersBeforeConcurrentSweeping(GarbageCollector collector) {
+  if (v8_flags.minor_mc && new_space()) {
+    PagedSpaceBase* space = paged_new_space()->paged_space();
+    space->RefillFreeList();
+    space->VerifyCountersBeforeConcurrentSweeping();
+  }
+  if (collector != GarbageCollector::MARK_COMPACTOR) return;
   PagedSpaceIterator spaces(this);
   for (PagedSpace* space = spaces.Next(); space != nullptr;
        space = spaces.Next()) {
@@ -4606,6 +4675,7 @@
         if (options.contains(SkipRoot::kOldGeneration)) {
           // Skip handles that are either weak or old.
           isolate_->global_handles()->IterateYoungStrongAndDependentRoots(v);
+          isolate_->traced_handles()->IterateYoungRoots(v);
         } else {
           // Skip handles that are weak.
           isolate_->global_handles()->IterateStrongRoots(v);
@@ -4615,16 +4685,22 @@
         if (options.contains(SkipRoot::kOldGeneration)) {
           // Skip handles that are old.
           isolate_->global_handles()->IterateAllYoungRoots(v);
+          isolate_->traced_handles()->IterateYoung(v);
         } else {
           // Do not skip any handles.
           isolate_->global_handles()->IterateAllRoots(v);
+          isolate_->traced_handles()->Iterate(v);
         }
       }
     }
     v->Synchronize(VisitorSynchronization::kGlobalHandles);
 
     if (!options.contains(SkipRoot::kStack)) {
-      IterateStackRoots(v);
+      ScanStackMode mode =
+          options.contains(SkipRoot::kConservativeStack) ? ScanStackMode::kNone
+          : options.contains(SkipRoot::kTopOfStack) ? ScanStackMode::kFromMarker
+                                                    : ScanStackMode::kComplete;
+      IterateStackRoots(v, mode);
       v->Synchronize(VisitorSynchronization::kStackRoots);
     }
 
@@ -4745,6 +4821,9 @@
 
   if (isolate()->is_shared_heap_isolate()) {
     ClientRootVisitor client_root_visitor(v);
+    // TODO(v8:13257): We cannot run CSS on client isolates now, as the
+    // stack markers will not be correct.
+    options.Add(SkipRoot::kConservativeStack);
     isolate()->global_safepoint()->IterateClientIsolates(
         [v = &client_root_visitor, options](Isolate* client) {
           client->heap()->IterateRoots(v, options);
@@ -4752,19 +4831,24 @@
   }
 }
 
-void Heap::IterateRootsFromStackIncludingClient(RootVisitor* v) {
-  IterateStackRoots(v);
+void Heap::IterateRootsFromStackIncludingClient(RootVisitor* v,
+                                                ScanStackMode mode) {
+  IterateStackRoots(v, mode);
+
   if (isolate()->is_shared_heap_isolate()) {
     ClientRootVisitor client_root_visitor(v);
     isolate()->global_safepoint()->IterateClientIsolates(
         [v = &client_root_visitor](Isolate* client) {
-          client->heap()->IterateStackRoots(v);
+          // TODO(v8:13257): We cannot run CSS on client isolates now, as the
+          // stack markers will not be correct.
+          client->heap()->IterateStackRoots(v, ScanStackMode::kNone);
         });
   }
 }
 
 void Heap::IterateWeakGlobalHandles(RootVisitor* v) {
   isolate_->global_handles()->IterateWeakRoots(v);
+  isolate_->traced_handles()->Iterate(v);
 }
 
 void Heap::IterateBuiltins(RootVisitor* v) {
@@ -4785,7 +4869,27 @@
   static_assert(Builtins::AllBuiltinsAreIsolateIndependent());
 }
 
-void Heap::IterateStackRoots(RootVisitor* v) { isolate_->Iterate(v); }
+void Heap::IterateStackRoots(RootVisitor* v, ScanStackMode mode) {
+  isolate_->Iterate(v);
+
+#ifdef V8_ENABLE_CONSERVATIVE_STACK_SCANNING
+  switch (std::min(mode, scan_stack_mode_for_testing_)) {
+    case ScanStackMode::kNone: {
+      break;
+    }
+    case ScanStackMode::kComplete: {
+      ConservativeStackVisitor stack_visitor(isolate(), v);
+      stack().IteratePointers(&stack_visitor);
+      break;
+    }
+    case ScanStackMode::kFromMarker: {
+      ConservativeStackVisitor stack_visitor(isolate(), v);
+      stack().IteratePointersUnsafe(&stack_visitor, stack().get_marker());
+      break;
+    }
+  }
+#endif  // V8_ENABLE_CONSERVATIVE_STACK_SCANNING
+}
 
 namespace {
 size_t GlobalMemorySizeFromV8Size(size_t v8_size) {
@@ -4997,8 +5101,8 @@
   *stats->old_space_capacity = old_space_->Capacity();
   *stats->code_space_size = code_space_->SizeOfObjects();
   *stats->code_space_capacity = code_space_->Capacity();
-  *stats->map_space_size = map_space_ ? map_space_->SizeOfObjects() : 0;
-  *stats->map_space_capacity = map_space_ ? map_space_->Capacity() : 0;
+  *stats->map_space_size = 0;
+  *stats->map_space_capacity = 0;
   *stats->lo_space_size = lo_space_->Size();
   *stats->code_lo_space_size = code_lo_space_->Size();
   isolate_->global_handles()->RecordStats(stats);
@@ -5105,6 +5209,10 @@
   // allowing the allocation.
   if (IsMainThreadParked(local_heap)) return true;
 
+  // If allocating isolate is deserialized at the moment then always allow
+  // allocation.
+  if (IsIsolateDeserializationActive(local_heap)) return true;
+
   // Make it more likely that retry of allocation on background thread succeeds
   if (IsRetryOfFailedAllocation(local_heap)) return true;
 
@@ -5481,11 +5589,6 @@
   space_[CODE_SPACE] = std::make_unique<CodeSpace>(this);
   code_space_ = static_cast<CodeSpace*>(space_[CODE_SPACE].get());
 
-  if (v8_flags.use_map_space) {
-    space_[MAP_SPACE] = std::make_unique<MapSpace>(this);
-    map_space_ = static_cast<MapSpace*>(space_[MAP_SPACE].get());
-  }
-
   if (isolate()->is_shared_space_isolate()) {
     space_[SHARED_SPACE] = std::make_unique<SharedSpace>(this);
     shared_space_ = static_cast<SharedSpace*>(space_[SHARED_SPACE].get());
@@ -5512,7 +5615,6 @@
   tracer_.reset(new GCTracer(this));
   array_buffer_sweeper_.reset(new ArrayBufferSweeper(this));
   gc_idle_time_handler_.reset(new GCIdleTimeHandler());
-  stack_ = std::make_unique<::heap::base::Stack>(base::Stack::GetStackStart());
   memory_measurement_.reset(new MemoryMeasurement(isolate()));
   if (!IsShared()) memory_reducer_.reset(new MemoryReducer(this));
   if (V8_UNLIKELY(TracingFlags::is_gc_stats_enabled())) {
@@ -5583,12 +5685,8 @@
     shared_space_allocator_ = std::make_unique<ConcurrentAllocator>(
         main_thread_local_heap(), heap->shared_space_);
 
-    DCHECK_NULL(shared_map_allocator_.get());
-
     shared_allocation_space_ = heap->shared_space_;
     shared_lo_allocation_space_ = heap->shared_lo_space_;
-    DCHECK(!v8_flags.use_map_space);
-    DCHECK_NULL(shared_map_allocation_space_);
 
   } else if (isolate()->shared_isolate()) {
     Heap* shared_heap = isolate()->shared_isolate()->heap();
@@ -5596,14 +5694,8 @@
     shared_space_allocator_ = std::make_unique<ConcurrentAllocator>(
         main_thread_local_heap(), shared_heap->old_space());
 
-    if (shared_heap->map_space()) {
-      shared_map_allocator_ = std::make_unique<ConcurrentAllocator>(
-          main_thread_local_heap(), shared_heap->map_space());
-    }
-
     shared_allocation_space_ = shared_heap->old_space();
     shared_lo_allocation_space_ = shared_heap->lo_space();
-    shared_map_allocation_space_ = shared_heap->map_space();
   }
 
   main_thread_local_heap()->SetUpMainThread();
@@ -5651,6 +5743,8 @@
 void Heap::NotifyDeserializationComplete() {
   PagedSpaceIterator spaces(this);
   for (PagedSpace* s = spaces.Next(); s != nullptr; s = spaces.Next()) {
+    // Shared space is used concurrently and cannot be shrunk.
+    if (s->identity() == SHARED_SPACE) continue;
     if (isolate()->snapshot_available()) s->ShrinkImmortalImmovablePages();
 #ifdef DEBUG
     // All pages right after bootstrapping must be marked as never-evacuate.
@@ -5753,13 +5847,15 @@
 }
 
 void Heap::SetStackStart(void* stack_start) {
-  stack_->SetStackStart(stack_start);
+  stack().SetStackStart(stack_start);
 }
 
-::heap::base::Stack& Heap::stack() { return *stack_.get(); }
+::heap::base::Stack& Heap::stack() {
+  return isolate_->thread_local_top()->stack_;
+}
 
 void Heap::RegisterExternallyReferencedObject(Address* location) {
-  GlobalHandles::MarkTraced(location);
+  TracedHandles::Mark(location);
   Object object(*location);
   if (!object.IsHeapObject()) {
     // The embedder is not aware of whether numbers are materialized as heap
@@ -5901,7 +5997,6 @@
   concurrent_marking_.reset();
 
   gc_idle_time_handler_.reset();
-  stack_.reset();
   memory_measurement_.reset();
   allocation_tracker_for_debugging_.reset();
 
@@ -5926,7 +6021,6 @@
   pretenuring_handler_.reset();
 
   shared_space_allocator_.reset();
-  shared_map_allocator_.reset();
 
   {
     CodePageHeaderModificationScope rwx_write_scope(
@@ -6186,7 +6280,10 @@
   Page* page = Page::FromAddress(start);
   DCHECK(!page->IsLargePage());
   if (!page->InYoungGeneration()) {
-    DCHECK_EQ(page->owner_identity(), OLD_SPACE);
+    // This method will be invoked on objects in shared space for
+    // internalization and string forwarding during GC.
+    DCHECK(page->owner_identity() == OLD_SPACE ||
+           page->owner_identity() == SHARED_SPACE);
 
     if (!page->SweepingDone()) {
       RememberedSet<OLD_TO_NEW>::RemoveRange(page, start, end,
@@ -6200,8 +6297,11 @@
 
 PagedSpace* PagedSpaceIterator::Next() {
   DCHECK_GE(counter_, FIRST_GROWABLE_PAGED_SPACE);
-  if (counter_ > LAST_GROWABLE_PAGED_SPACE) return nullptr;
-  return heap_->paged_space(counter_++);
+  while (counter_ <= LAST_GROWABLE_PAGED_SPACE) {
+    PagedSpace* space = heap_->paged_space(counter_++);
+    if (space) return space;
+  }
+  return nullptr;
 }
 
 SpaceIterator::SpaceIterator(Heap* heap)
@@ -6367,7 +6467,10 @@
 HeapObjectIterator::HeapObjectIterator(
     Heap* heap, HeapObjectIterator::HeapObjectsFiltering filtering)
     : heap_(heap),
-      safepoint_scope_(std::make_unique<SafepointScope>(heap)),
+      safepoint_scope_(std::make_unique<SafepointScope>(
+          heap->isolate(), heap->isolate()->is_shared_heap_isolate()
+                               ? SafepointKind::kGlobal
+                               : SafepointKind::kIsolate)),
       filtering_(filtering),
       filter_(nullptr),
       space_iterator_(nullptr),
@@ -6382,8 +6485,7 @@
     default:
       break;
   }
-  // By not calling |space_iterator_->HasNext()|, we assume that the old
-  // space is first returned and that it has been set up.
+  CHECK(space_iterator_->HasNext());
   object_iterator_ = space_iterator_->Next()->GetObjectIterator(heap_);
   if (V8_ENABLE_THIRD_PARTY_HEAP_BOOL) heap_->tp_heap_->ResetIterator();
 }
@@ -6860,8 +6962,6 @@
       return dst == OLD_SPACE;
     case CODE_SPACE:
       return dst == CODE_SPACE && type == CODE_TYPE;
-    case MAP_SPACE:
-      return dst == MAP_SPACE && type == MAP_TYPE;
     case SHARED_SPACE:
       return dst == SHARED_SPACE;
     case LO_SPACE:
@@ -7337,12 +7437,12 @@
     if (shared_space()) {
       shared_space()->RefillFreeList();
     }
-    if (map_space()) {
-      map_space()->RefillFreeList();
-      map_space()->SortFreeList();
+
+    if (v8_flags.minor_mc && new_space()) {
+      paged_new_space()->paged_space()->RefillFreeList();
     }
 
-    tracer()->NotifySweepingCompleted();
+    tracer()->NotifyFullSweepingCompleted();
 
 #ifdef VERIFY_HEAP
     if (v8_flags.verify_heap && !evacuation()) {
@@ -7364,6 +7464,25 @@
       !tracer()->IsSweepingInProgress());
 }
 
+void Heap::PauseSweepingAndEnsureYoungSweepingCompleted() {
+  if (sweeper()->sweeping_in_progress()) {
+    TRACE_GC_EPOCH(tracer(), sweeper()->GetTracingScopeForCompleteYoungSweep(),
+                   ThreadKind::kMain);
+
+    sweeper()->PauseAndEnsureNewSpaceCompleted();
+    paged_new_space()->paged_space()->RefillFreeList();
+
+    tracer()->NotifyYoungSweepingCompleted();
+
+#ifdef VERIFY_HEAP
+    if (v8_flags.verify_heap && !evacuation()) {
+      YoungGenerationEvacuationVerifier verifier(this);
+      verifier.Run();
+    }
+#endif
+  }
+}
+
 void Heap::DrainSweepingWorklistForSpace(AllocationSpace space) {
   if (!sweeper()->sweeping_in_progress()) return;
   sweeper()->DrainSweepingWorklistForSpace(space);
diff -r -u --color up/v8/src/heap/heap.h nw/v8/src/heap/heap.h
--- up/v8/src/heap/heap.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/heap.h	2023-01-19 16:46:36.233109563 -0500
@@ -28,7 +28,6 @@
 #include "src/common/globals.h"
 #include "src/heap/allocation-observer.h"
 #include "src/heap/allocation-result.h"
-#include "src/heap/base/stack.h"
 #include "src/heap/gc-callbacks.h"
 #include "src/heap/heap-allocator.h"
 #include "src/heap/marking-state.h"
@@ -173,7 +172,9 @@
   kStack,
   kMainThreadHandles,
   kUnserializable,
-  kWeak
+  kWeak,
+  kTopOfStack,
+  kConservativeStack,
 };
 
 enum UnprotectMemoryOrigin {
@@ -213,6 +214,12 @@
     std::unordered_map<EphemeronHashTable, std::unordered_set<int>,
                        Object::Hasher>;
 
+// An alias for std::unordered_map<HeapObject, T> which also sets proper
+// Hash and KeyEqual functions.
+template <typename T>
+using UnorderedHeapObjectMap =
+    std::unordered_map<HeapObject, T, Object::Hasher, Object::KeyEqualSafe>;
+
 class Heap {
  public:
   // Stores ephemeron entries where the EphemeronHashTable is in old-space,
@@ -702,9 +709,6 @@
       Handle<NativeContext> context, Handle<JSPromise> promise,
       v8::MeasureMemoryMode mode);
 
-  // Check new space expansion criteria and expand semispaces if it was hit.
-  void CheckNewSpaceExpansionCriteria();
-
   void VisitExternalResources(v8::ExternalResourceVisitor* visitor);
 
   void IncrementDeferredCount(v8::Isolate::UseCounterFeature feature);
@@ -837,8 +841,6 @@
   OldSpace* old_space() const { return old_space_; }
   CodeSpace* code_space() const { return code_space_; }
   SharedSpace* shared_space() const { return shared_space_; }
-  MapSpace* map_space() const { return map_space_; }
-  inline PagedSpace* space_for_maps();
   OldLargeObjectSpace* lo_space() const { return lo_space_; }
   CodeLargeObjectSpace* code_lo_space() const { return code_lo_space_; }
   SharedLargeObjectSpace* shared_lo_space() const { return shared_lo_space_; }
@@ -866,8 +868,6 @@
     return memory_allocator_.get();
   }
 
-  inline ConcurrentAllocator* concurrent_allocator_for_maps();
-
   inline Isolate* isolate() const;
 
   // Check if we run on isolate's main thread.
@@ -921,7 +921,7 @@
   V8_INLINE void SetRootScriptList(Object value);
   V8_INLINE void SetRootNoScriptSharedFunctionInfos(Object value);
   V8_INLINE void SetMessageListeners(TemplateList value);
-  V8_INLINE void SetPendingOptimizeForTestBytecode(Object bytecode);
+  V8_INLINE void SetFunctionsMarkedForManualOptimization(Object bytecode);
 
   StrongRootsEntry* RegisterStrongRoots(const char* label, FullObjectSlot start,
                                         FullObjectSlot end);
@@ -1029,11 +1029,16 @@
   // garbage collection and is usually only performed as part of
   // (de)serialization or heap verification.
 
+  // The order of this enumeration's elements is important: they should go from
+  // more precise to more conservative modes for stack scanning, so that we can
+  // use std::min to override for testing purposes.
+  enum class ScanStackMode { kNone, kFromMarker, kComplete };
+
   // Iterates over the strong roots and the weak roots.
   void IterateRoots(RootVisitor* v, base::EnumSet<SkipRoot> options);
   void IterateRootsIncludingClients(RootVisitor* v,
                                     base::EnumSet<SkipRoot> options);
-  void IterateRootsFromStackIncludingClient(RootVisitor* v);
+  void IterateRootsFromStackIncludingClient(RootVisitor* v, ScanStackMode mode);
 
   // Iterates over entries in the smi roots list.  Only interesting to the
   // serializer/deserializer, since GC does not care about smis.
@@ -1042,7 +1047,7 @@
   void IterateWeakRoots(RootVisitor* v, base::EnumSet<SkipRoot> options);
   void IterateWeakGlobalHandles(RootVisitor* v);
   void IterateBuiltins(RootVisitor* v);
-  void IterateStackRoots(RootVisitor* v);
+  void IterateStackRoots(RootVisitor* v, ScanStackMode mode);
 
   // ===========================================================================
   // Remembered set API. =======================================================
@@ -1594,6 +1599,7 @@
   // Note: Can only be called safely from main thread.
   V8_EXPORT_PRIVATE void EnsureSweepingCompleted(
       SweepingForcedFinalizationMode mode);
+  void PauseSweepingAndEnsureYoungSweepingCompleted();
 
   void DrainSweepingWorklistForSpace(AllocationSpace space);
 
@@ -1609,7 +1615,7 @@
 
 #ifdef DEBUG
   void VerifyCountersAfterSweeping();
-  void VerifyCountersBeforeConcurrentSweeping();
+  void VerifyCountersBeforeConcurrentSweeping(GarbageCollector collector);
   void VerifyCommittedPhysicalMemory();
 
   void Print();
@@ -1648,7 +1654,7 @@
 
   // Ensure that we have swept all spaces in such a way that we can iterate
   // over all objects.
-  void MakeHeapIterable();
+  V8_EXPORT_PRIVATE void MakeHeapIterable();
 
   V8_EXPORT_PRIVATE bool CanPromoteYoungAndExpandOldGeneration(size_t size);
   V8_EXPORT_PRIVATE bool CanExpandOldGeneration(size_t size);
@@ -1781,6 +1787,9 @@
   // Free all shared LABs.
   void FreeSharedLinearAllocationAreas();
 
+  // Makes all shared LABs iterable.
+  void MakeSharedLinearAllocationAreasIterable();
+
   // Free all shared LABs of main thread.
   void FreeMainThreadSharedLinearAllocationAreas();
 
@@ -1790,10 +1799,9 @@
 
   // Performs garbage collection in a safepoint.
   // Returns the number of freed global handles.
-  size_t PerformGarbageCollection(
-      GarbageCollector collector, GarbageCollectionReason gc_reason,
-      const char* collector_reason,
-      const GCCallbackFlags gc_callback_flags = kNoGCCallbackFlags);
+  size_t PerformGarbageCollection(GarbageCollector collector,
+                                  GarbageCollectionReason gc_reason,
+                                  const char* collector_reason);
 
   // Performs garbage collection in the shared heap.
   void PerformSharedGarbageCollection(Isolate* initiator,
@@ -1855,7 +1863,9 @@
   bool HasLowOldGenerationAllocationRate();
   bool HasLowEmbedderAllocationRate();
 
-  bool ShouldReduceNewSpaceSize() const;
+  enum class ResizeNewSpaceMode { kShrink, kGrow, kNone };
+  ResizeNewSpaceMode ShouldResizeNewSpace();
+  void ExpandNewSpaceSize();
   void ReduceNewSpaceSize();
 
   GCIdleTimeHeapState ComputeHeapState();
@@ -2166,7 +2176,6 @@
   NewSpace* new_space_ = nullptr;
   OldSpace* old_space_ = nullptr;
   CodeSpace* code_space_ = nullptr;
-  MapSpace* map_space_ = nullptr;
   SharedSpace* shared_space_ = nullptr;
   OldLargeObjectSpace* lo_space_ = nullptr;
   CodeLargeObjectSpace* code_lo_space_ = nullptr;
@@ -2178,11 +2187,9 @@
   // in another isolate.
   PagedSpace* shared_allocation_space_ = nullptr;
   OldLargeObjectSpace* shared_lo_allocation_space_ = nullptr;
-  PagedSpace* shared_map_allocation_space_ = nullptr;
 
   // Allocators for the shared spaces.
   std::unique_ptr<ConcurrentAllocator> shared_space_allocator_;
-  std::unique_ptr<ConcurrentAllocator> shared_map_allocator_;
 
   // Map from the space id to the space.
   std::unique_ptr<Space> space_[LAST_SPACE + 1];
@@ -2303,7 +2310,6 @@
   std::unique_ptr<LocalEmbedderHeapTracer> local_embedder_heap_tracer_;
   std::unique_ptr<AllocationTrackerForDebugging>
       allocation_tracker_for_debugging_;
-  std::unique_ptr<::heap::base::Stack> stack_;
 
   // This object controls virtual space reserved for code on the V8 heap. This
   // is only valid for 64-bit architectures where kRequiresCodeRange.
@@ -2385,15 +2391,13 @@
   bool force_oom_ = false;
   bool force_gc_on_next_allocation_ = false;
   bool delay_sweeper_tasks_for_testing_ = false;
+  ScanStackMode scan_stack_mode_for_testing_ = ScanStackMode::kComplete;
 
-  HeapObject pending_layout_change_object_;
-
-  std::unordered_map<HeapObject, HeapObject, Object::Hasher> retainer_;
-  std::unordered_map<HeapObject, Root, Object::Hasher> retaining_root_;
+  UnorderedHeapObjectMap<HeapObject> retainer_;
+  UnorderedHeapObjectMap<Root> retaining_root_;
   // If an object is retained by an ephemeron, then the retaining key of the
   // ephemeron is stored in this map.
-  std::unordered_map<HeapObject, HeapObject, Object::Hasher>
-      ephemeron_retainer_;
+  UnorderedHeapObjectMap<HeapObject> ephemeron_retainer_;
   // For each index in the retaining_path_targets_ array this map
   // stores the option of the corresponding target.
   std::unordered_map<int, RetainingPathOption> retaining_path_target_option_;
@@ -2449,6 +2453,7 @@
   friend class PagedSpaceBase;
   friend class PretenturingHandler;
   friend class ReadOnlyRoots;
+  friend class ScanStackModeScopeForTesting;
   friend class Scavenger;
   friend class ScavengerCollector;
   friend class StressConcurrentAllocationObserver;
@@ -2668,6 +2673,23 @@
   Heap* heap_;
 };
 
+class V8_NODISCARD ScanStackModeScopeForTesting {
+ public:
+  explicit inline ScanStackModeScopeForTesting(Heap* heap,
+                                               Heap::ScanStackMode mode)
+      : heap_(heap), old_value_(heap_->scan_stack_mode_for_testing_) {
+    heap_->scan_stack_mode_for_testing_ = mode;
+  }
+
+  inline ~ScanStackModeScopeForTesting() {
+    heap_->scan_stack_mode_for_testing_ = old_value_;
+  }
+
+ protected:
+  Heap* heap_;
+  Heap::ScanStackMode old_value_;
+};
+
 // Visitor class to verify interior pointers in spaces that do not contain
 // or care about inter-generational references. All heap object pointers have to
 // point into the heap to a location that has a map pointer at its first word.
diff -r -u --color up/v8/src/heap/incremental-marking.cc nw/v8/src/heap/incremental-marking.cc
--- up/v8/src/heap/incremental-marking.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/incremental-marking.cc	2023-01-19 16:46:36.233109563 -0500
@@ -272,6 +272,7 @@
 
     heap()->isolate()->global_handles()->IterateYoungStrongAndDependentRoots(
         &visitor);
+    heap()->isolate()->traced_handles()->IterateYoungRoots(&visitor);
 
     std::vector<PageMarkingItem> marking_items;
     RememberedSet<OLD_TO_NEW>::IterateMemoryChunks(
@@ -333,7 +334,7 @@
 
   MarkingBarrier::ActivateAll(heap(), is_compacting_,
                               MarkingBarrierType::kMajor);
-  GlobalHandles::EnableMarkingBarrier(heap()->isolate());
+  heap()->isolate()->traced_handles()->SetIsMarking(true);
 
   heap_->isolate()->compilation_cache()->MarkCompactPrologue();
 
@@ -395,6 +396,8 @@
     heap()->isolate()->PrintWithTimestamp(
         "[IncrementalMarking] (MinorMC) Running\n");
   }
+
+  DCHECK(!is_compacting_);
 }
 
 void IncrementalMarking::StartBlackAllocation() {
@@ -402,7 +405,6 @@
   DCHECK(IsMarking());
   black_allocation_ = true;
   heap()->old_space()->MarkLinearAllocationAreaBlack();
-  if (heap()->map_space()) heap()->map_space()->MarkLinearAllocationAreaBlack();
   {
     CodePageHeaderModificationScope rwx_write_scope(
         "Marking Code objects requires write access to the Code page header");
@@ -427,7 +429,6 @@
 void IncrementalMarking::PauseBlackAllocation() {
   DCHECK(IsMarking());
   heap()->old_space()->UnmarkLinearAllocationArea();
-  if (heap()->map_space()) heap()->map_space()->UnmarkLinearAllocationArea();
   {
     CodePageHeaderModificationScope rwx_write_scope(
         "Marking Code objects requires write access to the Code page header");
diff -r -u --color up/v8/src/heap/large-spaces.cc nw/v8/src/heap/large-spaces.cc
--- up/v8/src/heap/large-spaces.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/large-spaces.cc	2023-01-19 16:46:36.233109563 -0500
@@ -274,6 +274,7 @@
     IncrementExternalBackingStoreBytes(t, page->ExternalBackingStoreBytes(t));
   }
 }
+
 void LargeObjectSpace::RemovePage(LargePage* page) {
   size_ -= static_cast<int>(page->size());
   AccountUncommitted(page->size());
@@ -373,7 +374,7 @@
     Map map = object.map(cage_base);
     CHECK(map.IsMap(cage_base));
     CHECK(ReadOnlyHeap::Contains(map) ||
-          isolate->heap()->space_for_maps()->Contains(map));
+          isolate->heap()->old_space()->Contains(map));
 
     // We have only the following types in the large object space:
     const bool is_valid_lo_space_object =                         //
diff -r -u --color up/v8/src/heap/local-heap.cc nw/v8/src/heap/local-heap.cc
--- up/v8/src/heap/local-heap.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/local-heap.cc	2023-01-19 16:46:36.233109563 -0500
@@ -358,6 +358,12 @@
   code_space_allocator_->MakeLinearAllocationAreaIterable();
 }
 
+void LocalHeap::MakeSharedLinearAllocationAreaIterable() {
+  if (shared_old_space_allocator_) {
+    shared_old_space_allocator_->MakeLinearAllocationAreaIterable();
+  }
+}
+
 void LocalHeap::MarkLinearAllocationAreaBlack() {
   old_space_allocator_->MarkLinearAllocationAreaBlack();
   code_space_allocator_->MarkLinearAllocationAreaBlack();
@@ -420,14 +426,14 @@
 
 void LocalHeap::AddGCEpilogueCallback(GCEpilogueCallback* callback, void* data,
                                       GCType gc_type) {
-  DCHECK(!IsParked());
+  DCHECK(IsRunning());
   gc_epilogue_callbacks_.Add(callback, LocalIsolate::FromHeap(this), gc_type,
                              data);
 }
 
 void LocalHeap::RemoveGCEpilogueCallback(GCEpilogueCallback* callback,
                                          void* data) {
-  DCHECK(!IsParked());
+  DCHECK(IsRunning());
   gc_epilogue_callbacks_.Remove(callback, data);
 }
 
diff -r -u --color up/v8/src/heap/local-heap.h nw/v8/src/heap/local-heap.h
--- up/v8/src/heap/local-heap.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/local-heap.h	2023-01-19 16:46:36.233109563 -0500
@@ -130,6 +130,9 @@
   // iterable heap.
   void MakeLinearAllocationAreaIterable();
 
+  // Makes the shared LAB iterable.
+  void MakeSharedLinearAllocationAreaIterable();
+
   // Fetches a pointer to the local heap from the thread local storage.
   // It is intended to be used in handle and write barrier code where it is
   // difficult to get a pointer to the current instance of local heap otherwise.
@@ -335,11 +338,11 @@
   friend class CollectionBarrier;
   friend class ConcurrentAllocator;
   friend class GlobalSafepoint;
-  friend class IsolateSafepoint;
   friend class Heap;
   friend class Isolate;
+  friend class IsolateSafepoint;
+  friend class IsolateSafepointScope;
   friend class ParkedScope;
-  friend class SafepointScope;
   friend class UnparkedScope;
 };
 
diff -r -u --color up/v8/src/heap/mark-compact-inl.h nw/v8/src/heap/mark-compact-inl.h
--- up/v8/src/heap/mark-compact-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/mark-compact-inl.h	2023-01-19 16:46:36.233109563 -0500
@@ -58,11 +58,13 @@
   }
 }
 
+// static
 void MarkCompactCollector::RecordSlot(HeapObject object, ObjectSlot slot,
                                       HeapObject target) {
   RecordSlot(object, HeapObjectSlot(slot), target);
 }
 
+// static
 void MarkCompactCollector::RecordSlot(HeapObject object, HeapObjectSlot slot,
                                       HeapObject target) {
   MemoryChunk* source_page = MemoryChunk::FromHeapObject(object);
@@ -71,6 +73,7 @@
   }
 }
 
+// static
 void MarkCompactCollector::RecordSlot(MemoryChunk* source_page,
                                       HeapObjectSlot slot, HeapObject target) {
   BasicMemoryChunk* target_page = BasicMemoryChunk::FromHeapObject(target);
diff -r -u --color up/v8/src/heap/mark-compact.cc nw/v8/src/heap/mark-compact.cc
--- up/v8/src/heap/mark-compact.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/mark-compact.cc	2023-01-19 16:46:36.243942896 -0500
@@ -10,7 +10,9 @@
 
 #include "src/base/logging.h"
 #include "src/base/optional.h"
+#include "src/base/platform/platform.h"
 #include "src/base/utils/random-number-generator.h"
+#include "src/base/v8-fallthrough.h"
 #include "src/codegen/compilation-cache.h"
 #include "src/common/globals.h"
 #include "src/deoptimizer/deoptimizer.h"
@@ -151,8 +153,10 @@
 };
 
 void MarkingVerifier::VerifyRoots() {
-  heap_->IterateRootsIncludingClients(this,
-                                      base::EnumSet<SkipRoot>{SkipRoot::kWeak});
+  // When verifying marking, we never want to scan conservatively the top of the
+  // stack.
+  heap_->IterateRootsIncludingClients(
+      this, base::EnumSet<SkipRoot>{SkipRoot::kWeak, SkipRoot::kTopOfStack});
 }
 
 void MarkingVerifier::VerifyMarkingOnPage(const Page* page, Address start,
@@ -232,7 +236,6 @@
     VerifyMarking(heap_->old_space());
     VerifyMarking(heap_->code_space());
     if (heap_->shared_space()) VerifyMarking(heap_->shared_space());
-    if (heap_->map_space()) VerifyMarking(heap_->map_space());
     VerifyMarking(heap_->lo_space());
     VerifyMarking(heap_->code_lo_space());
     if (heap_->shared_lo_space()) VerifyMarking(heap_->shared_lo_space());
@@ -405,9 +408,10 @@
 
   int will_be_swept = 0;
 
-  if (heap()->ShouldReduceNewSpaceSize()) {
+  DCHECK_EQ(Heap::ResizeNewSpaceMode::kNone, resize_new_space_);
+  resize_new_space_ = heap()->ShouldResizeNewSpace();
+  if (resize_new_space_ == Heap::ResizeNewSpaceMode::kShrink) {
     paged_space->StartShrinking();
-    is_new_space_shrinking_ = true;
   }
 
   Sweeper* sweeper = heap()->sweeper();
@@ -421,7 +425,8 @@
       continue;
     }
 
-    if (is_new_space_shrinking_ && paged_space->ShouldReleasePage()) {
+    if ((resize_new_space_ == ResizeNewSpaceMode::kShrink) &&
+        paged_space->ShouldReleasePage()) {
       paged_space->ReleasePage(p);
     } else {
       sweeper->AddNewSpacePage(p);
@@ -539,10 +544,6 @@
 
   CollectEvacuationCandidates(heap()->old_space());
 
-  if (heap()->map_space() && v8_flags.compact_maps) {
-    CollectEvacuationCandidates(heap()->map_space());
-  }
-
   if (heap()->shared_space()) {
     CollectEvacuationCandidates(heap()->shared_space());
   }
@@ -554,10 +555,6 @@
     TraceFragmentation(heap()->code_space());
   }
 
-  if (v8_flags.trace_fragmentation && heap()->map_space()) {
-    TraceFragmentation(heap()->map_space());
-  }
-
   compacting_ = !evacuation_candidates_.empty();
   return compacting_;
 }
@@ -650,9 +647,6 @@
 void MarkCompactCollector::VerifyMarkbitsAreClean() {
   VerifyMarkbitsAreClean(heap_->old_space());
   VerifyMarkbitsAreClean(heap_->code_space());
-  if (heap_->map_space()) {
-    VerifyMarkbitsAreClean(heap_->map_space());
-  }
   VerifyMarkbitsAreClean(heap_->new_space());
   // Read-only space should always be black since we never collect any objects
   // in it or linked from it.
@@ -714,7 +708,7 @@
 
 void MarkCompactCollector::CollectEvacuationCandidates(PagedSpace* space) {
   DCHECK(space->identity() == OLD_SPACE || space->identity() == CODE_SPACE ||
-         space->identity() == MAP_SPACE || space->identity() == SHARED_SPACE);
+         space->identity() == SHARED_SPACE);
 
   int number_of_pages = space->CountTotalPages();
   size_t area_size = space->AreaSize();
@@ -959,7 +953,6 @@
 #ifdef VERIFY_HEAP
   if (v8_flags.verify_heap) {
     heap()->old_space()->VerifyLiveBytes();
-    if (heap()->map_space()) heap()->map_space()->VerifyLiveBytes();
     heap()->code_space()->VerifyLiveBytes();
     if (heap()->shared_space()) heap()->shared_space()->VerifyLiveBytes();
     if (v8_flags.minor_mc && heap()->paged_new_space())
@@ -987,30 +980,44 @@
 
 void MarkCompactCollector::Finish() {
   {
-    TRACE_GC(heap()->tracer(), GCTracer::Scope::MC_SWEEP);
+    TRACE_GC_EPOCH(heap()->tracer(), GCTracer::Scope::MC_SWEEP,
+                   ThreadKind::kMain);
     if (heap()->new_lo_space()) {
       TRACE_GC(heap()->tracer(), GCTracer::Scope::MC_SWEEP_NEW_LO);
       SweepLargeSpace(heap()->new_lo_space());
     }
 
-    if (v8_flags.minor_mc && heap()->new_space()) {
-      // Keep new space sweeping atomic.
-      GCTracer::Scope sweep_scope(heap()->tracer(),
-                                  GCTracer::Scope::MC_SWEEP_FINISH_NEW,
-                                  ThreadKind::kMain);
-      sweeper()->ParallelSweepSpace(NEW_SPACE,
-                                    Sweeper::SweepingMode::kEagerDuringGC, 0);
-      heap()->paged_new_space()->paged_space()->RefillFreeList();
-    }
-
 #ifdef DEBUG
-    heap()->VerifyCountersBeforeConcurrentSweeping();
-#endif
+    heap()->VerifyCountersBeforeConcurrentSweeping(garbage_collector_);
+#endif  // DEBUG
+  }
+
+  if (heap()->new_space()) {
+    if (v8_flags.minor_mc) {
+      switch (resize_new_space_) {
+        case ResizeNewSpaceMode::kShrink:
+          heap()->ReduceNewSpaceSize();
+          break;
+        case ResizeNewSpaceMode::kGrow:
+          heap()->ExpandNewSpaceSize();
+          break;
+        case ResizeNewSpaceMode::kNone:
+          break;
+      }
+      resize_new_space_ = ResizeNewSpaceMode::kNone;
+    }
+    TRACE_GC(heap()->tracer(), GCTracer::Scope::MC_EVACUATE);
+    TRACE_GC(heap()->tracer(), GCTracer::Scope::MC_EVACUATE_REBALANCE);
+    if (!heap()->new_space()->EnsureCurrentCapacity()) {
+      heap()->FatalProcessOutOfMemory("NewSpace::EnsureCurrentCapacity");
+    }
   }
 
   TRACE_GC(heap()->tracer(), GCTracer::Scope::MC_FINISH);
 
-  heap()->isolate()->global_handles()->ClearListOfYoungNodes();
+  auto* isolate = heap()->isolate();
+  isolate->global_handles()->ClearListOfYoungNodes();
+  isolate->traced_handles()->ClearListOfYoungNodes();
 
   SweepArrayBufferExtensions();
 
@@ -1043,7 +1050,7 @@
 
   if (have_code_to_deoptimize_) {
     // Some code objects were marked for deoptimization during the GC.
-    Deoptimizer::DeoptimizeMarkedCode(isolate());
+    Deoptimizer::DeoptimizeMarkedCode(isolate);
     have_code_to_deoptimize_ = false;
   }
 }
@@ -1649,7 +1656,7 @@
       if (V8_UNLIKELY(v8_flags.minor_mc)) {
         base->record_visitor_->MarkArrayBufferExtensionPromoted(dst);
       }
-    } else if (dest == MAP_SPACE || dest == SHARED_SPACE) {
+    } else if (dest == SHARED_SPACE) {
       DCHECK_OBJECT_SIZE(size);
       DCHECK(IsAligned(size, kTaggedSize));
       base->heap_->CopyBlock(dst_addr, src_addr, size);
@@ -1683,7 +1690,8 @@
         local_allocator_(local_allocator),
         shared_old_allocator_(shared_old_allocator),
         record_visitor_(record_visitor),
-        shared_string_table_(shared_old_allocator != nullptr) {
+        shared_string_table_(v8_flags.shared_string_table &&
+                             heap->isolate()->has_shared_heap()) {
     migration_function_ = RawMigrateObject<MigrationMode::kFast>;
   }
 
@@ -1704,9 +1712,14 @@
     AllocationAlignment alignment = HeapObject::RequiredAlignment(map);
     AllocationResult allocation;
     if (target_space == OLD_SPACE && ShouldPromoteIntoSharedHeap(map)) {
-      DCHECK_NOT_NULL(shared_old_allocator_);
-      allocation = shared_old_allocator_->AllocateRaw(size, alignment,
-                                                      AllocationOrigin::kGC);
+      if (heap_->isolate()->is_shared_heap_isolate()) {
+        DCHECK_NULL(shared_old_allocator_);
+        allocation = local_allocator_->Allocate(
+            SHARED_SPACE, size, AllocationOrigin::kGC, alignment);
+      } else {
+        allocation = shared_old_allocator_->AllocateRaw(size, alignment,
+                                                        AllocationOrigin::kGC);
+      }
     } else {
       allocation = local_allocator_->Allocate(target_space, size,
                                               AllocationOrigin::kGC, alignment);
@@ -1748,7 +1761,7 @@
   RecordMigratedSlotVisitor* record_visitor_;
   std::vector<MigrationObserver*> observers_;
   MigrateFunction migration_function_;
-  bool shared_string_table_ = false;
+  const bool shared_string_table_;
 #if DEBUG
   Address abort_evacuation_at_address_{kNullAddress};
 #endif  // DEBUG
@@ -2146,7 +2159,8 @@
 #endif  // V8_ENABLE_INNER_POINTER_RESOLUTION_MB
 
 void MarkCompactCollector::MarkRootsFromStack(RootVisitor* root_visitor) {
-  heap()->IterateRootsFromStackIncludingClient(root_visitor);
+  heap()->IterateRootsFromStackIncludingClient(root_visitor,
+                                               Heap::ScanStackMode::kComplete);
 }
 
 void MarkCompactCollector::MarkObjectsFromClientHeaps() {
@@ -2755,7 +2769,7 @@
     // finished as it will reset page flags that share the same bitmap as
     // the evacuation candidate bit.
     MarkingBarrier::DeactivateAll(heap());
-    GlobalHandles::DisableMarkingBarrier(heap()->isolate());
+    heap()->isolate()->traced_handles()->SetIsMarking(false);
   }
 
   epoch_++;
@@ -2951,6 +2965,7 @@
     // CPU profiler.
     heap()->isolate()->global_handles()->IterateWeakRootsForPhantomHandles(
         &IsUnmarkedHeapObject);
+    heap()->isolate()->traced_handles()->ResetDeadNodes(&IsUnmarkedHeapObject);
   }
 
   {
@@ -3144,13 +3159,22 @@
   SharedFunctionInfo flushing_candidate;
   while (local_weak_objects()->code_flushing_candidates_local.Pop(
       &flushing_candidate)) {
-    bool is_bytecode_live = non_atomic_marking_state()->IsBlackOrGrey(
-        flushing_candidate.GetBytecodeArray(isolate()));
+    // During flushing a BytecodeArray is transformed into an UncompiledData in
+    // place. Seeing an UncompiledData here implies that another
+    // SharedFunctionInfo had a reference to the same ByteCodeArray and flushed
+    // it before processing this candidate. This can happen when using
+    // CloneSharedFunctionInfo().
+    bool bytecode_already_decompiled =
+        flushing_candidate.function_data(isolate(), kAcquireLoad)
+            .IsUncompiledData(isolate());
+    bool is_bytecode_live = !bytecode_already_decompiled &&
+                            non_atomic_marking_state()->IsBlackOrGrey(
+                                flushing_candidate.GetBytecodeArray(isolate()));
     if (v8_flags.flush_baseline_code && flushing_candidate.HasBaselineCode()) {
       CodeT baseline_codet =
           CodeT::cast(flushing_candidate.function_data(kAcquireLoad));
       // Safe to do a relaxed load here since the CodeT was acquire-loaded.
-      Code baseline_code = FromCodeT(baseline_codet, kRelaxedLoad);
+      Code baseline_code = FromCodeT(baseline_codet, isolate(), kRelaxedLoad);
       if (non_atomic_marking_state()->IsBlackOrGrey(baseline_code)) {
         // Currently baseline code holds bytecode array strongly and it is
         // always ensured that bytecode is live if baseline code is live. Hence
@@ -3178,9 +3202,17 @@
       DCHECK(v8_flags.flush_baseline_code ||
              !flushing_candidate.HasBaselineCode());
 
-      // If the BytecodeArray is dead, flush it, which will replace the field
-      // with an uncompiled data object.
-      FlushBytecodeFromSFI(flushing_candidate);
+      if (bytecode_already_decompiled) {
+        flushing_candidate.DiscardCompiledMetadata(
+            isolate(),
+            [](HeapObject object, ObjectSlot slot, HeapObject target) {
+              RecordSlot(object, slot, target);
+            });
+      } else {
+        // If the BytecodeArray is dead, flush it, which will replace the field
+        // with an uncompiled data object.
+        FlushBytecodeFromSFI(flushing_candidate);
+      }
     }
 
     // Now record the slot, which has either been updated to an uncompiled data,
@@ -3922,21 +3954,18 @@
   DCHECK(evacuation_candidates_.empty());
 }
 
-void MarkCompactCollector::EvacuateEpilogue() {
-  aborted_evacuation_candidates_due_to_oom_.clear();
-  aborted_evacuation_candidates_due_to_flags_.clear();
-
-  // New space.
-  if (heap()->new_space()) {
-    heap()->new_space()->EvacuateEpilogue();
-    DCHECK_EQ(0, heap()->new_space()->Size());
-  }
+#if DEBUG
+namespace {
 
-  // Old generation. Deallocate evacuated candidate pages.
-  ReleaseEvacuationCandidates();
+void VerifyRememberedSetsAfterEvacuation(Heap* heap,
+                                         GarbageCollector collector) {
+  // Old-to-old slot sets must be empty after evacuation.
+  bool new_space_is_empty =
+      !heap->new_space() || heap->new_space()->Size() == 0;
+  DCHECK_IMPLIES(collector == GarbageCollector::MARK_COMPACTOR,
+                 new_space_is_empty);
 
-#ifdef DEBUG
-  MemoryChunkIterator chunk_iterator(heap());
+  MemoryChunkIterator chunk_iterator(heap);
 
   while (chunk_iterator.HasNext()) {
     MemoryChunk* chunk = chunk_iterator.Next();
@@ -3945,15 +3974,17 @@
     DCHECK_NULL((chunk->slot_set<OLD_TO_OLD, AccessMode::ATOMIC>()));
     DCHECK_NULL((chunk->typed_slot_set<OLD_TO_OLD, AccessMode::ATOMIC>()));
 
-    // Old-to-new slot sets must be empty after evacuation.
-    DCHECK_NULL((chunk->slot_set<OLD_TO_NEW, AccessMode::ATOMIC>()));
-    DCHECK_NULL((chunk->typed_slot_set<OLD_TO_NEW, AccessMode::ATOMIC>()));
+    if (new_space_is_empty) {
+      // Old-to-new slot sets must be empty after evacuation.
+      DCHECK_NULL((chunk->slot_set<OLD_TO_NEW, AccessMode::ATOMIC>()));
+      DCHECK_NULL((chunk->typed_slot_set<OLD_TO_NEW, AccessMode::ATOMIC>()));
+    }
 
     // Old-to-shared slots may survive GC but there should never be any slots in
     // new or shared spaces.
     AllocationSpace id = chunk->owner_identity();
     if (id == SHARED_SPACE || id == SHARED_LO_SPACE || id == NEW_SPACE ||
-        id == NEW_LO_SPACE || isolate()->is_shared()) {
+        id == NEW_LO_SPACE || heap->isolate()->is_shared()) {
       DCHECK_NULL((chunk->slot_set<OLD_TO_SHARED, AccessMode::ATOMIC>()));
       DCHECK_NULL((chunk->typed_slot_set<OLD_TO_SHARED, AccessMode::ATOMIC>()));
     }
@@ -3961,14 +3992,37 @@
     // GCs need to filter invalidated slots.
     DCHECK_NULL(chunk->invalidated_slots<OLD_TO_OLD>());
     DCHECK_NULL(chunk->invalidated_slots<OLD_TO_NEW>());
-    DCHECK_NULL(chunk->invalidated_slots<OLD_TO_SHARED>());
+    if (collector == GarbageCollector::MARK_COMPACTOR) {
+      DCHECK_NULL(chunk->invalidated_slots<OLD_TO_SHARED>());
+    }
   }
-#endif
+}
+
+}  // namespace
+#endif  // DEBUG
+
+void MarkCompactCollector::EvacuateEpilogue() {
+  aborted_evacuation_candidates_due_to_oom_.clear();
+  aborted_evacuation_candidates_due_to_flags_.clear();
+
+  // New space.
+  if (heap()->new_space()) {
+    heap()->new_space()->EvacuateEpilogue();
+    DCHECK_EQ(0, heap()->new_space()->Size());
+  }
+
+  // Old generation. Deallocate evacuated candidate pages.
+  ReleaseEvacuationCandidates();
+
+#ifdef DEBUG
+  VerifyRememberedSetsAfterEvacuation(heap(), GarbageCollector::MARK_COMPACTOR);
+#endif  // DEBUG
 }
 
 namespace {
 ConcurrentAllocator* CreateSharedOldAllocator(Heap* heap) {
-  if (v8_flags.shared_string_table && heap->isolate()->has_shared_heap()) {
+  if (v8_flags.shared_string_table && heap->isolate()->has_shared_heap() &&
+      !heap->isolate()->is_shared_heap_isolate()) {
     return new ConcurrentAllocator(nullptr, heap->shared_allocation_space());
   }
 
@@ -4568,7 +4622,8 @@
         DCHECK_EQ(0, non_atomic_marking_state()->live_bytes(p));
         DCHECK(p->SweepingDone());
         PagedNewSpace* space = heap()->paged_new_space();
-        if (is_new_space_shrinking_ && space->ShouldReleasePage()) {
+        if ((resize_new_space_ == ResizeNewSpaceMode::kShrink) &&
+            space->ShouldReleasePage()) {
           space->ReleasePage(p);
         } else {
           sweeper()->AddNewSpacePage(p);
@@ -4577,19 +4632,6 @@
     }
     new_space_evacuation_pages_.clear();
 
-    if (is_new_space_shrinking_) {
-      DCHECK(v8_flags.minor_mc);
-      heap()->paged_new_space()->FinishShrinking();
-      is_new_space_shrinking_ = false;
-    }
-
-    if (heap()->new_space()) {
-      TRACE_GC(heap()->tracer(), GCTracer::Scope::MC_EVACUATE_REBALANCE);
-      if (!heap()->new_space()->EnsureCurrentCapacity()) {
-        heap()->FatalProcessOutOfMemory("NewSpace::Rebalance");
-      }
-    }
-
     for (LargePage* p : promoted_large_pages_) {
       DCHECK(p->IsFlagSet(Page::PAGE_NEW_OLD_PROMOTION));
       p->ClearFlag(Page::PAGE_NEW_OLD_PROMOTION);
@@ -4741,16 +4783,15 @@
   MarkingState* marking_state_;
 };
 
-template <typename MarkingState, GarbageCollector collector>
+namespace {
+
+template <GarbageCollector collector>
 class RememberedSetUpdatingItem : public UpdatingItem {
  public:
-  explicit RememberedSetUpdatingItem(Heap* heap, MarkingState* marking_state,
-                                     MemoryChunk* chunk,
-                                     RememberedSetUpdatingMode updating_mode)
+  explicit RememberedSetUpdatingItem(Heap* heap, MemoryChunk* chunk)
       : heap_(heap),
-        marking_state_(marking_state),
+        marking_state_(heap_->non_atomic_marking_state()),
         chunk_(chunk),
-        updating_mode_(updating_mode),
         record_old_to_shared_slots_(heap->isolate()->has_shared_heap() &&
                                     !chunk->InSharedHeap()) {}
   ~RememberedSetUpdatingItem() override = default;
@@ -4758,7 +4799,6 @@
   void Process() override {
     TRACE_EVENT0(TRACE_DISABLED_BY_DEFAULT("v8.gc"),
                  "RememberedSetUpdatingItem::Process");
-    base::MutexGuard guard(chunk_->mutex());
     CodePageMemoryModificationScope memory_modification_scope(chunk_);
     UpdateUntypedPointers();
     UpdateTypedPointers();
@@ -4808,14 +4848,39 @@
         std::is_same<TSlot, FullMaybeObjectSlot>::value ||
             std::is_same<TSlot, MaybeObjectSlot>::value,
         "Only FullMaybeObjectSlot and MaybeObjectSlot are expected here");
-    using THeapObjectSlot = typename TSlot::THeapObjectSlot;
     HeapObject heap_object;
     if (!(*slot).GetHeapObject(&heap_object)) {
       return REMOVE_SLOT;
     }
+    if (!Heap::InYoungGeneration(heap_object)) return REMOVE_SLOT;
+    if (collector == GarbageCollector::MINOR_MARK_COMPACTOR) {
+      return CheckAndUpdateOldToNewSlotMinor(heap_object);
+    } else {
+      return CheckAndUpdateOldToNewSlotMajor(slot, heap_object);
+    }
+  }
+
+  inline SlotCallbackResult CheckAndUpdateOldToNewSlotMinor(
+      HeapObject heap_object) {
+    DCHECK_EQ(GarbageCollector::MINOR_MARK_COMPACTOR, collector);
+    DCHECK(!heap_object.map_word(kRelaxedLoad).IsForwardingAddress());
+    DCHECK(!Heap::InFromPage(heap_object));
+    if (marking_state_->IsBlack(heap_object)) return KEEP_SLOT;
+    return REMOVE_SLOT;
+  }
+
+  template <typename TSlot>
+  inline SlotCallbackResult CheckAndUpdateOldToNewSlotMajor(
+      TSlot slot, HeapObject heap_object) {
+    DCHECK_EQ(GarbageCollector::MARK_COMPACTOR, collector);
+    using THeapObjectSlot = typename TSlot::THeapObjectSlot;
     if (Heap::InFromPage(heap_object)) {
-      DCHECK_IMPLIES(v8_flags.minor_mc,
-                     Page::FromHeapObject(heap_object)->IsLargePage());
+      if (v8_flags.minor_mc) {
+        DCHECK(!heap_object.map_word(kRelaxedLoad).IsForwardingAddress());
+        DCHECK(Page::FromHeapObject(heap_object)->IsLargePage());
+        DCHECK(!marking_state_->IsBlack(heap_object));
+        return REMOVE_SLOT;
+      }
       MapWord map_word = heap_object.map_word(kRelaxedLoad);
       if (map_word.IsForwardingAddress()) {
         HeapObjectReference::Update(THeapObjectSlot(slot),
@@ -4831,7 +4896,9 @@
       if (Heap::InToPage(heap_object)) {
         return KEEP_SLOT;
       }
-    } else if (Heap::InToPage(heap_object)) {
+      return REMOVE_SLOT;
+    } else {
+      DCHECK(Heap::InToPage(heap_object));
       // Slots can point to "to" space if the page has been moved, or if the
       // slot has been recorded multiple times in the remembered set, or
       // if the slot was already updated during old->old updating.
@@ -4859,23 +4926,26 @@
         }
       }
       return KEEP_SLOT;
-    } else {
-      DCHECK(!Heap::InYoungGeneration(heap_object));
     }
-    return REMOVE_SLOT;
   }
 
   void UpdateUntypedPointers() {
-    const PtrComprCageBase cage_base = heap_->isolate();
-    if (chunk_->slot_set<OLD_TO_NEW, AccessMode::NON_ATOMIC>() != nullptr) {
+    UpdateUntypedOldToNewPointers();
+    if (collector == GarbageCollector::MINOR_MARK_COMPACTOR) return;
+    UpdateUntypedOldToOldPointers();
+    UpdateUntypedOldToCodePointers();
+    UpdateUntypedOldToSharedPointers();
+  }
+
+  void UpdateUntypedOldToNewPointers() {
+    if (chunk_->slot_set<OLD_TO_NEW, AccessMode::NON_ATOMIC>()) {
+      const PtrComprCageBase cage_base = heap_->isolate();
       // Marking bits are cleared already when the page is already swept. This
       // is fine since in that case the sweeper has already removed dead invalid
       // objects as well.
       InvalidatedSlotsFilter::LivenessCheck liveness_check =
-          updating_mode_ == RememberedSetUpdatingMode::ALL &&
-                  !chunk_->SweepingDone()
-              ? InvalidatedSlotsFilter::LivenessCheck::kYes
-              : InvalidatedSlotsFilter::LivenessCheck::kNo;
+          !chunk_->SweepingDone() ? InvalidatedSlotsFilter::LivenessCheck::kYes
+                                  : InvalidatedSlotsFilter::LivenessCheck::kNo;
       InvalidatedSlotsFilter filter =
           InvalidatedSlotsFilter::OldToNew(chunk_, liveness_check);
       int slots = RememberedSet<OLD_TO_NEW>::Iterate(
@@ -4899,14 +4969,14 @@
       }
     }
 
-    if (chunk_->invalidated_slots<OLD_TO_NEW>() != nullptr) {
-      // The invalidated slots are not needed after old-to-new slots were
-      // processed.
-      chunk_->ReleaseInvalidatedSlots<OLD_TO_NEW>();
-    }
+    // The invalidated slots are not needed after old-to-new slots were
+    // processed.
+    chunk_->ReleaseInvalidatedSlots<OLD_TO_NEW>();
+  }
 
-    if ((updating_mode_ == RememberedSetUpdatingMode::ALL) &&
-        (chunk_->slot_set<OLD_TO_OLD, AccessMode::NON_ATOMIC>() != nullptr)) {
+  void UpdateUntypedOldToOldPointers() {
+    if (chunk_->slot_set<OLD_TO_OLD, AccessMode::NON_ATOMIC>()) {
+      const PtrComprCageBase cage_base = heap_->isolate();
       InvalidatedSlotsFilter filter = InvalidatedSlotsFilter::OldToOld(
           chunk_, InvalidatedSlotsFilter::LivenessCheck::kNo);
       RememberedSet<OLD_TO_OLD>::Iterate(
@@ -4927,164 +4997,145 @@
           SlotSet::KEEP_EMPTY_BUCKETS);
       chunk_->ReleaseSlotSet<OLD_TO_OLD>();
     }
-    if ((updating_mode_ == RememberedSetUpdatingMode::ALL) &&
-        chunk_->invalidated_slots<OLD_TO_OLD>() != nullptr) {
-      // The invalidated slots are not needed after old-to-old slots were
-      // processed.
-      chunk_->ReleaseInvalidatedSlots<OLD_TO_OLD>();
-    }
-    if (V8_EXTERNAL_CODE_SPACE_BOOL) {
-      if ((updating_mode_ == RememberedSetUpdatingMode::ALL) &&
-          (chunk_->slot_set<OLD_TO_CODE, AccessMode::NON_ATOMIC>() !=
-           nullptr)) {
-        PtrComprCageBase cage_base = heap_->isolate();
+
+    // The invalidated slots are not needed after old-to-old slots were
+    // processed.
+    chunk_->ReleaseInvalidatedSlots<OLD_TO_OLD>();
+  }
+
+  void UpdateUntypedOldToCodePointers() {
+    if (!V8_EXTERNAL_CODE_SPACE_BOOL) return;
+
+    if (chunk_->slot_set<OLD_TO_CODE, AccessMode::NON_ATOMIC>()) {
+      const PtrComprCageBase cage_base = heap_->isolate();
 #ifdef V8_EXTERNAL_CODE_SPACE
-        PtrComprCageBase code_cage_base(heap_->isolate()->code_cage_base());
+      const PtrComprCageBase code_cage_base(heap_->isolate()->code_cage_base());
 #else
-        PtrComprCageBase code_cage_base = cage_base;
+      const PtrComprCageBase code_cage_base = cage_base;
 #endif
-        RememberedSet<OLD_TO_CODE>::Iterate(
-            chunk_,
-            [=](MaybeObjectSlot slot) {
-              HeapObject host = HeapObject::FromAddress(
-                  slot.address() - CodeDataContainer::kCodeOffset);
-              DCHECK(host.IsCodeDataContainer(cage_base));
-              UpdateStrongCodeSlot<AccessMode::NON_ATOMIC>(
-                  host, cage_base, code_cage_base,
-                  CodeObjectSlot(slot.address()));
-              // Always keep slot since all slots are dropped at once after
-              // iteration.
-              return KEEP_SLOT;
-            },
-            SlotSet::FREE_EMPTY_BUCKETS);
-        chunk_->ReleaseSlotSet<OLD_TO_CODE>();
-      }
-      // The invalidated slots are not needed after old-to-code slots were
-      // processed, but since there are no invalidated OLD_TO_CODE slots,
-      // there's nothing to clear.
-    }
-    if (updating_mode_ == RememberedSetUpdatingMode::ALL) {
-      if (chunk_->slot_set<OLD_TO_SHARED, AccessMode::NON_ATOMIC>()) {
-        // Client GCs need to remove invalidated OLD_TO_SHARED slots.
-        DCHECK(!heap_->IsShared());
-        InvalidatedSlotsFilter filter = InvalidatedSlotsFilter::OldToShared(
-            chunk_, InvalidatedSlotsFilter::LivenessCheck::kNo);
-        RememberedSet<OLD_TO_SHARED>::Iterate(
-            chunk_,
-            [&filter](MaybeObjectSlot slot) {
-              return filter.IsValid(slot.address()) ? KEEP_SLOT : REMOVE_SLOT;
-            },
-            SlotSet::FREE_EMPTY_BUCKETS);
-      }
-      chunk_->ReleaseInvalidatedSlots<OLD_TO_SHARED>();
+      RememberedSet<OLD_TO_CODE>::Iterate(
+          chunk_,
+          [=](MaybeObjectSlot slot) {
+            HeapObject host = HeapObject::FromAddress(
+                slot.address() - CodeDataContainer::kCodeOffset);
+            DCHECK(host.IsCodeDataContainer(cage_base));
+            UpdateStrongCodeSlot<AccessMode::NON_ATOMIC>(
+                host, cage_base, code_cage_base,
+                CodeObjectSlot(slot.address()));
+            // Always keep slot since all slots are dropped at once after
+            // iteration.
+            return KEEP_SLOT;
+          },
+          SlotSet::FREE_EMPTY_BUCKETS);
+      chunk_->ReleaseSlotSet<OLD_TO_CODE>();
     }
+
+    // The invalidated slots are not needed after old-to-code slots were
+    // processed, but since there are no invalidated OLD_TO_CODE slots,
+    // there's nothing to clear.
+    DCHECK_NULL(chunk_->invalidated_slots<OLD_TO_CODE>());
   }
 
-  void UpdateTypedPointers() {
-    if (chunk_->typed_slot_set<OLD_TO_NEW, AccessMode::NON_ATOMIC>() !=
-        nullptr) {
-      CHECK_NE(chunk_->owner(), heap_->map_space());
-      const auto check_and_update_old_to_new_slot_fn =
-          [this](FullMaybeObjectSlot slot) {
-            return CheckAndUpdateOldToNewSlot(slot);
-          };
-      RememberedSet<OLD_TO_NEW>::IterateTyped(
-          chunk_, [this, &check_and_update_old_to_new_slot_fn](
-                      SlotType slot_type, Address slot) {
-            SlotCallbackResult result = UpdateTypedSlotHelper::UpdateTypedSlot(
-                heap_, slot_type, slot, check_and_update_old_to_new_slot_fn);
-            // A new space string might have been promoted into the shared heap
-            // during GC.
-            if (record_old_to_shared_slots_) {
-              CheckSlotForOldToSharedTyped(chunk_, slot_type, slot);
-            }
-            return result;
-          });
-    }
-    if ((updating_mode_ == RememberedSetUpdatingMode::ALL) &&
-        (chunk_->typed_slot_set<OLD_TO_OLD, AccessMode::NON_ATOMIC>() !=
-         nullptr)) {
-      CHECK_NE(chunk_->owner(), heap_->map_space());
-      RememberedSet<OLD_TO_OLD>::IterateTyped(
-          chunk_, [this](SlotType slot_type, Address slot) {
-            // Using UpdateStrongSlot is OK here, because there are no weak
-            // typed slots.
-            PtrComprCageBase cage_base = heap_->isolate();
-            SlotCallbackResult result = UpdateTypedSlotHelper::UpdateTypedSlot(
-                heap_, slot_type, slot, [cage_base](FullMaybeObjectSlot slot) {
-                  UpdateStrongSlot<AccessMode::NON_ATOMIC>(cage_base, slot);
-                  // Always keep slot since all slots are dropped at once after
-                  // iteration.
-                  return KEEP_SLOT;
-                });
-            // A string might have been promoted into the shared heap during GC.
-            if (record_old_to_shared_slots_) {
-              CheckSlotForOldToSharedTyped(chunk_, slot_type, slot);
-            }
-            return result;
-          });
-      chunk_->ReleaseTypedSlotSet<OLD_TO_OLD>();
+  void UpdateUntypedOldToSharedPointers() {
+    if (chunk_->slot_set<OLD_TO_SHARED, AccessMode::NON_ATOMIC>()) {
+      // Client GCs need to remove invalidated OLD_TO_SHARED slots.
+      DCHECK(!heap_->IsShared());
+      InvalidatedSlotsFilter filter = InvalidatedSlotsFilter::OldToShared(
+          chunk_, InvalidatedSlotsFilter::LivenessCheck::kNo);
+      RememberedSet<OLD_TO_SHARED>::Iterate(
+          chunk_,
+          [&filter](MaybeObjectSlot slot) {
+            return filter.IsValid(slot.address()) ? KEEP_SLOT : REMOVE_SLOT;
+          },
+          SlotSet::FREE_EMPTY_BUCKETS);
     }
+
+    // The invalidated slots are not needed after old-to-shared slots were
+    // processed.
+    chunk_->ReleaseInvalidatedSlots<OLD_TO_SHARED>();
+  }
+
+  void UpdateTypedPointers() {
+    UpdateTypedOldToNewPointers();
+    if (collector == GarbageCollector::MINOR_MARK_COMPACTOR) return;
+    UpdateTypedOldToOldPointers();
+  }
+
+  void UpdateTypedOldToNewPointers() {
+    if (chunk_->typed_slot_set<OLD_TO_NEW, AccessMode::NON_ATOMIC>() == nullptr)
+      return;
+    const auto check_and_update_old_to_new_slot_fn =
+        [this](FullMaybeObjectSlot slot) {
+          return CheckAndUpdateOldToNewSlot(slot);
+        };
+    RememberedSet<OLD_TO_NEW>::IterateTyped(
+        chunk_, [this, &check_and_update_old_to_new_slot_fn](SlotType slot_type,
+                                                             Address slot) {
+          SlotCallbackResult result = UpdateTypedSlotHelper::UpdateTypedSlot(
+              heap_, slot_type, slot, check_and_update_old_to_new_slot_fn);
+          // A new space string might have been promoted into the shared heap
+          // during GC.
+          if (record_old_to_shared_slots_) {
+            CheckSlotForOldToSharedTyped(chunk_, slot_type, slot);
+          }
+          return result;
+        });
+  }
+
+  void UpdateTypedOldToOldPointers() {
+    if (chunk_->typed_slot_set<OLD_TO_OLD, AccessMode::NON_ATOMIC>() == nullptr)
+      return;
+    RememberedSet<OLD_TO_OLD>::IterateTyped(
+        chunk_, [this](SlotType slot_type, Address slot) {
+          // Using UpdateStrongSlot is OK here, because there are no weak
+          // typed slots.
+          PtrComprCageBase cage_base = heap_->isolate();
+          SlotCallbackResult result = UpdateTypedSlotHelper::UpdateTypedSlot(
+              heap_, slot_type, slot, [cage_base](FullMaybeObjectSlot slot) {
+                UpdateStrongSlot<AccessMode::NON_ATOMIC>(cage_base, slot);
+                // Always keep slot since all slots are dropped at once after
+                // iteration.
+                return KEEP_SLOT;
+              });
+          // A string might have been promoted into the shared heap during GC.
+          if (record_old_to_shared_slots_) {
+            CheckSlotForOldToSharedTyped(chunk_, slot_type, slot);
+          }
+          return result;
+        });
+    chunk_->ReleaseTypedSlotSet<OLD_TO_OLD>();
   }
 
   Heap* heap_;
-  MarkingState* marking_state_;
+  NonAtomicMarkingState* marking_state_;
   MemoryChunk* chunk_;
-  RememberedSetUpdatingMode updating_mode_;
   const bool record_old_to_shared_slots_;
 };
 
+}  // namespace
+
 std::unique_ptr<UpdatingItem>
-MarkCompactCollector::CreateRememberedSetUpdatingItem(
-    MemoryChunk* chunk, RememberedSetUpdatingMode updating_mode) {
-  return std::make_unique<RememberedSetUpdatingItem<
-      NonAtomicMarkingState, GarbageCollector::MARK_COMPACTOR>>(
-      heap(), non_atomic_marking_state(), chunk, updating_mode);
+MarkCompactCollector::CreateRememberedSetUpdatingItem(MemoryChunk* chunk) {
+  return std::make_unique<
+      RememberedSetUpdatingItem<GarbageCollector::MARK_COMPACTOR>>(heap(),
+                                                                   chunk);
 }
 
 namespace {
 template <typename IterateableSpace, typename Collector>
-int CollectRememberedSetUpdatingItems(
+void CollectRememberedSetUpdatingItems(
     Collector* collector, std::vector<std::unique_ptr<UpdatingItem>>* items,
     IterateableSpace* space, RememberedSetUpdatingMode mode) {
-  int pages = 0;
   for (MemoryChunk* chunk : *space) {
     // No need to update pointers on evacuation candidates. Evacuated pages will
     // be released after this phase.
     if (chunk->IsEvacuationCandidate()) continue;
-    const bool contains_old_to_old_slots =
-        chunk->slot_set<OLD_TO_OLD>() != nullptr ||
-        chunk->typed_slot_set<OLD_TO_OLD>() != nullptr;
-    const bool contains_old_to_code_slots =
-        V8_EXTERNAL_CODE_SPACE_BOOL &&
-        chunk->slot_set<OLD_TO_CODE>() != nullptr;
-    const bool contains_old_to_new_slots =
-        chunk->slot_set<OLD_TO_NEW>() != nullptr ||
-        chunk->typed_slot_set<OLD_TO_NEW>() != nullptr;
-    const bool contains_old_to_shared_slots =
-        chunk->slot_set<OLD_TO_SHARED>() != nullptr ||
-        chunk->typed_slot_set<OLD_TO_SHARED>() != nullptr;
-    const bool contains_old_to_old_invalidated_slots =
-        chunk->invalidated_slots<OLD_TO_OLD>() != nullptr;
-    const bool contains_old_to_new_invalidated_slots =
-        chunk->invalidated_slots<OLD_TO_NEW>() != nullptr;
-    const bool contains_old_to_shared_invalidated_slots =
-        chunk->invalidated_slots<OLD_TO_SHARED>() != nullptr;
-    if (!contains_old_to_new_slots && !contains_old_to_old_slots &&
-        !contains_old_to_old_invalidated_slots &&
-        !contains_old_to_new_invalidated_slots && !contains_old_to_code_slots &&
-        !contains_old_to_shared_slots &&
-        !contains_old_to_shared_invalidated_slots)
-      continue;
-    if (mode == RememberedSetUpdatingMode::ALL || contains_old_to_new_slots ||
-        contains_old_to_old_invalidated_slots ||
-        contains_old_to_new_invalidated_slots) {
-      items->emplace_back(
-          collector->CreateRememberedSetUpdatingItem(chunk, mode));
-      pages++;
+    if (mode == RememberedSetUpdatingMode::ALL
+            ? chunk->HasRecordedSlots()
+            : chunk->HasRecordedOldToNewSlots()) {
+      items->emplace_back(collector->CreateRememberedSetUpdatingItem(chunk));
     }
   }
-  return pages;
 }
 }  // namespace
 
@@ -5150,7 +5201,8 @@
     PointersUpdatingVisitor updating_visitor(heap());
     heap_->IterateRootsIncludingClients(
         &updating_visitor,
-        base::EnumSet<SkipRoot>{SkipRoot::kExternalStringTable});
+        base::EnumSet<SkipRoot>{SkipRoot::kExternalStringTable,
+                                SkipRoot::kConservativeStack});
   }
 
   {
@@ -5185,11 +5237,6 @@
                                         heap()->shared_lo_space(),
                                         RememberedSetUpdatingMode::ALL);
     }
-    if (heap()->map_space()) {
-      CollectRememberedSetUpdatingItems(this, &updating_items,
-                                        heap()->map_space(),
-                                        RememberedSetUpdatingMode::ALL);
-    }
 
     // Iterating to space may require a valid body descriptor for e.g.
     // WasmStruct which races with updating a slot in Map. Since to space is
@@ -5375,7 +5422,8 @@
 
 void MarkCompactCollector::Sweep() {
   DCHECK(!sweeper()->sweeping_in_progress());
-  TRACE_GC(heap()->tracer(), GCTracer::Scope::MC_SWEEP);
+  TRACE_GC_EPOCH(heap()->tracer(), GCTracer::Scope::MC_SWEEP,
+                 ThreadKind::kMain);
 #ifdef DEBUG
   state_ = SWEEP_SPACES;
 #endif
@@ -5400,14 +5448,9 @@
         heap()->tracer(), GCTracer::Scope::MC_SWEEP_CODE, ThreadKind::kMain);
     StartSweepSpace(heap()->code_space());
   }
-  if (heap()->map_space()) {
-    GCTracer::Scope sweep_scope(heap()->tracer(), GCTracer::Scope::MC_SWEEP_MAP,
-                                ThreadKind::kMain);
-    StartSweepSpace(heap()->map_space());
-  }
   if (heap()->shared_space()) {
-    GCTracer::Scope sweep_scope(heap()->tracer(), GCTracer::Scope::MC_SWEEP_MAP,
-                                ThreadKind::kMain);
+    GCTracer::Scope sweep_scope(
+        heap()->tracer(), GCTracer::Scope::MC_SWEEP_SHARED, ThreadKind::kMain);
     StartSweepSpace(heap()->shared_space());
   }
   if (v8_flags.minor_mc && heap()->new_space()) {
@@ -5658,42 +5701,59 @@
   }
 };
 
+namespace {
+
+template <typename IterateableSpace>
+void DropOldToNewRememberedSets(IterateableSpace* space) {
+  for (MemoryChunk* chunk : *space) {
+    DCHECK(!chunk->IsEvacuationCandidate());
+    chunk->ReleaseSlotSet<OLD_TO_NEW>();
+    chunk->ReleaseTypedSlotSet<OLD_TO_NEW>();
+    chunk->ReleaseInvalidatedSlots<OLD_TO_NEW>();
+  }
+}
+
+}  // namespace
+
 void MinorMarkCompactCollector::UpdatePointersAfterEvacuation() {
   TRACE_GC(heap()->tracer(),
            GCTracer::Scope::MINOR_MC_EVACUATE_UPDATE_POINTERS);
 
-  std::vector<std::unique_ptr<UpdatingItem>> updating_items;
-
   {
     TRACE_GC(heap()->tracer(),
              GCTracer::Scope::MINOR_MC_EVACUATE_UPDATE_POINTERS_SLOTS);
-    // Create batches of global handles.
-    CollectRememberedSetUpdatingItems(
-        this, &updating_items, heap()->old_space(),
-        RememberedSetUpdatingMode::OLD_TO_NEW_ONLY);
-    CollectRememberedSetUpdatingItems(
-        this, &updating_items, heap()->code_space(),
-        RememberedSetUpdatingMode::OLD_TO_NEW_ONLY);
-    if (heap()->map_space()) {
+    if (heap()->paged_new_space()->Size() == 0) {
+      DropOldToNewRememberedSets(heap()->old_space());
+      DropOldToNewRememberedSets(heap()->code_space());
+      DropOldToNewRememberedSets(heap()->lo_space());
+      DropOldToNewRememberedSets(heap()->code_lo_space());
+    } else {
+      std::vector<std::unique_ptr<UpdatingItem>> updating_items;
+
+      // Create batches of global handles.
       CollectRememberedSetUpdatingItems(
-          this, &updating_items, heap()->map_space(),
+          this, &updating_items, heap()->old_space(),
+          RememberedSetUpdatingMode::OLD_TO_NEW_ONLY);
+      CollectRememberedSetUpdatingItems(
+          this, &updating_items, heap()->code_space(),
+          RememberedSetUpdatingMode::OLD_TO_NEW_ONLY);
+      CollectRememberedSetUpdatingItems(
+          this, &updating_items, heap()->lo_space(),
+          RememberedSetUpdatingMode::OLD_TO_NEW_ONLY);
+      CollectRememberedSetUpdatingItems(
+          this, &updating_items, heap()->code_lo_space(),
           RememberedSetUpdatingMode::OLD_TO_NEW_ONLY);
-    }
-    CollectRememberedSetUpdatingItems(
-        this, &updating_items, heap()->lo_space(),
-        RememberedSetUpdatingMode::OLD_TO_NEW_ONLY);
-    CollectRememberedSetUpdatingItems(
-        this, &updating_items, heap()->code_lo_space(),
-        RememberedSetUpdatingMode::OLD_TO_NEW_ONLY);
 
-    V8::GetCurrentPlatform()
-        ->CreateJob(
-            v8::TaskPriority::kUserBlocking,
-            std::make_unique<PointersUpdatingJob>(
-                isolate(), std::move(updating_items),
-                GCTracer::Scope::MINOR_MC_EVACUATE_UPDATE_POINTERS_PARALLEL,
-                GCTracer::Scope::MINOR_MC_BACKGROUND_EVACUATE_UPDATE_POINTERS))
-        ->Join();
+      V8::GetCurrentPlatform()
+          ->CreateJob(
+              v8::TaskPriority::kUserBlocking,
+              std::make_unique<PointersUpdatingJob>(
+                  isolate(), std::move(updating_items),
+                  GCTracer::Scope::MINOR_MC_EVACUATE_UPDATE_POINTERS_PARALLEL,
+                  GCTracer::Scope::
+                      MINOR_MC_BACKGROUND_EVACUATE_UPDATE_POINTERS))
+          ->Join();
+    }
   }
 
   {
@@ -5701,8 +5761,12 @@
              GCTracer::Scope::MINOR_MC_EVACUATE_UPDATE_POINTERS_WEAK);
 
     // Update pointers from external string table.
-    heap()->UpdateYoungReferencesInExternalStringTable(
-        &UpdateReferenceInExternalStringTableEntry);
+    heap()->UpdateYoungReferencesInExternalStringTable([](Heap* heap,
+                                                          FullObjectSlot p) {
+      DCHECK(
+          !HeapObject::cast(*p).map_word(kRelaxedLoad).IsForwardingAddress());
+      return String::cast(*p);
+    });
   }
 }
 
@@ -5733,6 +5797,8 @@
 };
 
 void MinorMarkCompactCollector::Prepare() {
+  DCHECK(sweeper()->IsSweepingDoneForSpace(NEW_SPACE));
+
   // Probably requires more.
   if (!heap()->incremental_marking()->IsMarking()) {
     StartMarking();
@@ -5763,13 +5829,28 @@
       SweepLargeSpace(heap()->new_lo_space());
     }
 
-    {
-      // Keep new space sweeping atomic.
-      GCTracer::Scope sweep_scope(heap()->tracer(),
-                                  GCTracer::Scope::MINOR_MC_SWEEP_FINISH_NEW,
-                                  ThreadKind::kMain);
-      sweeper_->EnsureCompleted(Sweeper::SweepingMode::kEagerDuringGC);
-      heap()->paged_new_space()->paged_space()->RefillFreeList();
+#ifdef DEBUG
+    heap()->VerifyCountersBeforeConcurrentSweeping(garbage_collector_);
+#endif
+  }
+
+  switch (resize_new_space_) {
+    case ResizeNewSpaceMode::kShrink:
+      heap()->ReduceNewSpaceSize();
+      break;
+    case ResizeNewSpaceMode::kGrow:
+      heap()->ExpandNewSpaceSize();
+      break;
+    case ResizeNewSpaceMode::kNone:
+      break;
+  }
+  resize_new_space_ = ResizeNewSpaceMode::kNone;
+
+  {
+    TRACE_GC(heap()->tracer(), GCTracer::Scope::MC_EVACUATE);
+    TRACE_GC(heap()->tracer(), GCTracer::Scope::MC_EVACUATE_REBALANCE);
+    if (!heap()->new_space()->EnsureCurrentCapacity()) {
+      heap()->FatalProcessOutOfMemory("NewSpace::EnsureCurrentCapacity");
     }
   }
 
@@ -5777,6 +5858,8 @@
 
   local_marking_worklists_.reset();
   main_marking_visitor_.reset();
+
+  sweeper()->StartSweeperTasks();
 }
 
 void MinorMarkCompactCollector::CollectGarbage() {
@@ -5801,7 +5884,9 @@
   Finish();
 
 #ifdef VERIFY_HEAP
-  if (v8_flags.verify_heap) {
+  // If concurrent sweeping is active, evacuation will be verified once sweeping
+  // is done using the FullEvacuationVerifier.
+  if (v8_flags.verify_heap && !sweeper()->sweeping_in_progress()) {
     YoungGenerationEvacuationVerifier verifier(heap());
     verifier.Run();
   }
@@ -5811,7 +5896,9 @@
 
   SweepArrayBufferExtensions();
 
-  heap()->isolate()->global_handles()->UpdateListOfYoungNodes();
+  auto* isolate = heap()->isolate();
+  isolate->global_handles()->UpdateListOfYoungNodes();
+  isolate->traced_handles()->UpdateListOfYoungNodes();
 }
 
 void MinorMarkCompactCollector::MakeIterable(
@@ -5923,14 +6010,18 @@
 
 void MinorMarkCompactCollector::EvacuateEpilogue() {
   heap()->new_space()->EvacuateEpilogue();
+
+#ifdef DEBUG
+  VerifyRememberedSetsAfterEvacuation(heap(),
+                                      GarbageCollector::MINOR_MARK_COMPACTOR);
+#endif  // DEBUG
 }
 
 std::unique_ptr<UpdatingItem>
-MinorMarkCompactCollector::CreateRememberedSetUpdatingItem(
-    MemoryChunk* chunk, RememberedSetUpdatingMode updating_mode) {
-  return std::make_unique<RememberedSetUpdatingItem<
-      NonAtomicMarkingState, GarbageCollector::MINOR_MARK_COMPACTOR>>(
-      heap(), non_atomic_marking_state(), chunk, updating_mode);
+MinorMarkCompactCollector::CreateRememberedSetUpdatingItem(MemoryChunk* chunk) {
+  return std::make_unique<
+      RememberedSetUpdatingItem<GarbageCollector::MINOR_MARK_COMPACTOR>>(heap(),
+                                                                         chunk);
 }
 
 class PageMarkingItem;
@@ -5996,7 +6087,7 @@
 
 void PageMarkingItem::MarkTypedPointers(YoungGenerationMarkingTask* task) {
   RememberedSet<OLD_TO_NEW>::IterateTyped(
-      chunk_, [=](SlotType slot_type, Address slot) {
+      chunk_, [this, task](SlotType slot_type, Address slot) {
         return UpdateTypedSlotHelper::UpdateTypedSlot(
             heap(), slot_type, slot, [this, task](FullMaybeObjectSlot slot) {
               return CheckAndMarkObject(task, slot);
@@ -6113,7 +6204,7 @@
     // Seed the root set (roots + old->new set).
     {
       TRACE_GC(heap()->tracer(), GCTracer::Scope::MINOR_MC_MARK_SEED);
-      isolate()->global_handles()->ComputeWeaknessForYoungObjects(
+      isolate()->traced_handles()->ComputeWeaknessForYoungObjects(
           &JSObject::IsUnmodifiedApiObject);
       // MinorMC treats all weak roots except for global handles as strong.
       // That is why we don't set skip_weak = true here and instead visit
@@ -6124,6 +6215,7 @@
                                                 SkipRoot::kOldGeneration});
       isolate()->global_handles()->IterateYoungStrongAndDependentRoots(
           root_visitor);
+      isolate()->traced_handles()->IterateYoungRoots(root_visitor);
 
       if (!was_marked_incrementally) {
         // Create items for each page.
@@ -6191,6 +6283,8 @@
     TRACE_GC(heap()->tracer(), GCTracer::Scope::MINOR_MC_MARK_GLOBAL_HANDLES);
     isolate()->global_handles()->ProcessWeakYoungObjects(
         &root_visitor, &IsUnmarkedObjectForYoungGeneration);
+    isolate()->traced_handles()->ProcessYoungObjects(
+        &root_visitor, &IsUnmarkedObjectForYoungGeneration);
     DrainMarkingWorklist();
   }
 
@@ -6293,27 +6387,11 @@
       DCHECK(!p->IsFlagSet(Page::PAGE_NEW_NEW_PROMOTION));
       if (p->IsFlagSet(Page::PAGE_NEW_OLD_PROMOTION)) {
         promoted_pages_.push_back(p);
-      } else {
-        // Page was not promoted. Sweep it instead.
-        DCHECK_EQ(NEW_SPACE, p->owner_identity());
-        sweeper()->AddNewSpacePage(p);
       }
     }
     new_space_evacuation_pages_.clear();
   }
 
-  if (is_new_space_shrinking_) {
-    heap()->paged_new_space()->FinishShrinking();
-    is_new_space_shrinking_ = false;
-  }
-
-  {
-    TRACE_GC(heap()->tracer(), GCTracer::Scope::MINOR_MC_EVACUATE_REBALANCE);
-    if (!heap()->new_space()->EnsureCurrentCapacity()) {
-      heap()->FatalProcessOutOfMemory("NewSpace::Rebalance");
-    }
-  }
-
   {
     TRACE_GC(heap()->tracer(), GCTracer::Scope::MINOR_MC_EVACUATE_EPILOGUE);
     EvacuateEpilogue();
@@ -6393,6 +6471,9 @@
                            : PromoteUnusablePages::kNo)) {
       EvacuateNewSpacePageVisitor<NEW_TO_OLD>::Move(page);
       evacuation_items.emplace_back(ParallelWorkItem{}, page);
+    } else {
+      // Page is not promoted. Sweep it instead.
+      sweeper()->AddNewSpacePage(page);
     }
   }
 
@@ -6425,7 +6506,7 @@
 }
 
 void MinorMarkCompactCollector::Sweep() {
-  DCHECK(!sweeper()->sweeping_in_progress());
+  DCHECK(!sweeper()->AreSweeperTasksRunning());
   TRACE_GC(heap()->tracer(), GCTracer::Scope::MINOR_MC_SWEEP);
   {
     GCTracer::Scope sweep_scope(heap()->tracer(),
diff -r -u --color up/v8/src/heap/mark-compact.h nw/v8/src/heap/mark-compact.h
--- up/v8/src/heap/mark-compact.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/mark-compact.h	2023-01-19 16:46:36.243942896 -0500
@@ -285,6 +285,8 @@
   std::vector<LargePage*> promoted_large_pages_;
 
  protected:
+  using ResizeNewSpaceMode = Heap::ResizeNewSpaceMode;
+
   inline Heap* heap() const { return heap_; }
   inline Isolate* isolate();
 
@@ -307,7 +309,7 @@
   MarkingState* const marking_state_;
   NonAtomicMarkingState* const non_atomic_marking_state_;
 
-  bool is_new_space_shrinking_ = false;
+  ResizeNewSpaceMode resize_new_space_ = ResizeNewSpaceMode::kNone;
 
   explicit CollectorBase(Heap* heap, GarbageCollector collector);
   virtual ~CollectorBase() = default;
@@ -448,7 +450,7 @@
   V8_INLINE void MarkExternallyReferencedObject(HeapObject obj);
 
   std::unique_ptr<UpdatingItem> CreateRememberedSetUpdatingItem(
-      MemoryChunk* chunk, RememberedSetUpdatingMode updating_mode);
+      MemoryChunk* chunk);
 
 #ifdef V8_ENABLE_INNER_POINTER_RESOLUTION_MB
   // Finds an object header based on a `maybe_inner_ptr`. It returns
@@ -699,7 +701,7 @@
   void CleanupPromotedPages();
 
   std::unique_ptr<UpdatingItem> CreateRememberedSetUpdatingItem(
-      MemoryChunk* chunk, RememberedSetUpdatingMode updating_mode);
+      MemoryChunk* chunk);
 
   void Finish() final;
 
diff -r -u --color up/v8/src/heap/marking-barrier.cc nw/v8/src/heap/marking-barrier.cc
--- up/v8/src/heap/marking-barrier.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/marking-barrier.cc	2023-01-19 16:46:36.243942896 -0500
@@ -144,9 +144,97 @@
   typed_slots->Insert(info.slot_type, info.offset);
 }
 
+namespace {
+void ActivateSpace(PagedSpace* space) {
+  for (Page* p : *space) {
+    p->SetOldGenerationPageFlags(true);
+  }
+}
+
+void ActivateSpace(NewSpace* space) {
+  for (Page* p : *space) {
+    p->SetYoungGenerationPageFlags(true);
+  }
+}
+
+void ActivateSpaces(Heap* heap) {
+  ActivateSpace(heap->old_space());
+  {
+    CodePageHeaderModificationScope rwx_write_scope(
+        "Modification of Code page header flags requires write access");
+    ActivateSpace(heap->code_space());
+  }
+  ActivateSpace(heap->new_space());
+  if (heap->shared_space()) {
+    ActivateSpace(heap->shared_space());
+  }
+
+  for (LargePage* p : *heap->new_lo_space()) {
+    p->SetYoungGenerationPageFlags(true);
+    DCHECK(p->IsLargePage());
+  }
+
+  for (LargePage* p : *heap->lo_space()) {
+    p->SetOldGenerationPageFlags(true);
+  }
+
+  {
+    CodePageHeaderModificationScope rwx_write_scope(
+        "Modification of Code page header flags requires write access");
+    for (LargePage* p : *heap->code_lo_space()) {
+      p->SetOldGenerationPageFlags(true);
+    }
+  }
+
+  if (heap->shared_lo_space()) {
+    for (LargePage* p : *heap->shared_lo_space()) {
+      p->SetOldGenerationPageFlags(true);
+    }
+  }
+}
+
+void DeactivateSpace(PagedSpace* space) {
+  for (Page* p : *space) {
+    p->SetOldGenerationPageFlags(false);
+  }
+}
+
+void DeactivateSpace(NewSpace* space) {
+  for (Page* p : *space) {
+    p->SetYoungGenerationPageFlags(false);
+  }
+}
+
+void DeactivateSpaces(Heap* heap) {
+  DeactivateSpace(heap->old_space());
+  DeactivateSpace(heap->code_space());
+  DeactivateSpace(heap->new_space());
+  if (heap->shared_space()) {
+    DeactivateSpace(heap->shared_space());
+  }
+  for (LargePage* p : *heap->new_lo_space()) {
+    p->SetYoungGenerationPageFlags(false);
+    DCHECK(p->IsLargePage());
+  }
+  for (LargePage* p : *heap->lo_space()) {
+    p->SetOldGenerationPageFlags(false);
+  }
+  for (LargePage* p : *heap->code_lo_space()) {
+    p->SetOldGenerationPageFlags(false);
+  }
+  if (heap->shared_lo_space()) {
+    for (LargePage* p : *heap->shared_lo_space()) {
+      p->SetOldGenerationPageFlags(false);
+    }
+  }
+}
+}  // namespace
+
 // static
 void MarkingBarrier::ActivateAll(Heap* heap, bool is_compacting,
                                  MarkingBarrierType marking_barrier_type) {
+  ActivateSpaces(heap);
+
   heap->safepoint()->IterateLocalHeaps(
       [is_compacting, marking_barrier_type](LocalHeap* local_heap) {
         local_heap->marking_barrier()->Activate(is_compacting,
@@ -154,13 +242,33 @@
       });
 }
 
+void MarkingBarrier::Activate(bool is_compacting,
+                              MarkingBarrierType marking_barrier_type) {
+  DCHECK(!is_activated_);
+  DCHECK(major_worklist_.IsLocalEmpty());
+  DCHECK(minor_worklist_.IsLocalEmpty());
+  is_compacting_ = is_compacting;
+  marking_barrier_type_ = marking_barrier_type;
+  current_worklist_ = is_minor() ? &minor_worklist_ : &major_worklist_;
+  is_activated_ = true;
+}
+
 // static
 void MarkingBarrier::DeactivateAll(Heap* heap) {
+  DeactivateSpaces(heap);
+
   heap->safepoint()->IterateLocalHeaps([](LocalHeap* local_heap) {
     local_heap->marking_barrier()->Deactivate();
   });
 }
 
+void MarkingBarrier::Deactivate() {
+  is_activated_ = false;
+  is_compacting_ = false;
+  DCHECK(typed_slots_map_.empty());
+  DCHECK(current_worklist_->IsLocalEmpty());
+}
+
 // static
 void MarkingBarrier::PublishAll(Heap* heap) {
   heap->safepoint()->IterateLocalHeaps(
@@ -191,112 +299,6 @@
   }
 }
 
-void MarkingBarrier::DeactivateSpace(PagedSpace* space) {
-  DCHECK(is_main_thread_barrier_);
-  for (Page* p : *space) {
-    p->SetOldGenerationPageFlags(false);
-  }
-}
-
-void MarkingBarrier::DeactivateSpace(NewSpace* space) {
-  DCHECK(is_main_thread_barrier_);
-  for (Page* p : *space) {
-    p->SetYoungGenerationPageFlags(false);
-  }
-}
-
-void MarkingBarrier::Deactivate() {
-  is_activated_ = false;
-  is_compacting_ = false;
-  if (is_main_thread_barrier_) {
-    DeactivateSpace(heap_->old_space());
-    if (heap_->map_space()) DeactivateSpace(heap_->map_space());
-    DeactivateSpace(heap_->code_space());
-    DeactivateSpace(heap_->new_space());
-    if (heap_->shared_space()) {
-      DeactivateSpace(heap_->shared_space());
-    }
-    for (LargePage* p : *heap_->new_lo_space()) {
-      p->SetYoungGenerationPageFlags(false);
-      DCHECK(p->IsLargePage());
-    }
-    for (LargePage* p : *heap_->lo_space()) {
-      p->SetOldGenerationPageFlags(false);
-    }
-    for (LargePage* p : *heap_->code_lo_space()) {
-      p->SetOldGenerationPageFlags(false);
-    }
-    if (heap_->shared_lo_space()) {
-      for (LargePage* p : *heap_->shared_lo_space()) {
-        p->SetOldGenerationPageFlags(false);
-      }
-    }
-  }
-  DCHECK(typed_slots_map_.empty());
-  DCHECK(current_worklist_->IsLocalEmpty());
-}
-
-void MarkingBarrier::ActivateSpace(PagedSpace* space) {
-  DCHECK(is_main_thread_barrier_);
-  for (Page* p : *space) {
-    p->SetOldGenerationPageFlags(true);
-  }
-}
-
-void MarkingBarrier::ActivateSpace(NewSpace* space) {
-  DCHECK(is_main_thread_barrier_);
-  for (Page* p : *space) {
-    p->SetYoungGenerationPageFlags(true);
-  }
-}
-
-void MarkingBarrier::Activate(bool is_compacting,
-                              MarkingBarrierType marking_barrier_type) {
-  DCHECK(!is_activated_);
-  DCHECK(major_worklist_.IsLocalEmpty());
-  DCHECK(minor_worklist_.IsLocalEmpty());
-  is_compacting_ = is_compacting;
-  marking_barrier_type_ = marking_barrier_type;
-  current_worklist_ = is_minor() ? &minor_worklist_ : &major_worklist_;
-  is_activated_ = true;
-  if (is_main_thread_barrier_) {
-    ActivateSpace(heap_->old_space());
-    if (heap_->map_space()) ActivateSpace(heap_->map_space());
-    {
-      CodePageHeaderModificationScope rwx_write_scope(
-          "Modification of Code page header flags requires write access");
-      ActivateSpace(heap_->code_space());
-    }
-    ActivateSpace(heap_->new_space());
-    if (heap_->shared_space()) {
-      ActivateSpace(heap_->shared_space());
-    }
-
-    for (LargePage* p : *heap_->new_lo_space()) {
-      p->SetYoungGenerationPageFlags(true);
-      DCHECK(p->IsLargePage());
-    }
-
-    for (LargePage* p : *heap_->lo_space()) {
-      p->SetOldGenerationPageFlags(true);
-    }
-
-    {
-      CodePageHeaderModificationScope rwx_write_scope(
-          "Modification of Code page header flags requires write access");
-      for (LargePage* p : *heap_->code_lo_space()) {
-        p->SetOldGenerationPageFlags(true);
-      }
-    }
-
-    if (heap_->shared_lo_space()) {
-      for (LargePage* p : *heap_->shared_lo_space()) {
-        p->SetOldGenerationPageFlags(true);
-      }
-    }
-  }
-}
-
 bool MarkingBarrier::IsCurrentMarkingBarrier() {
   return WriteBarrier::CurrentMarkingBarrier(heap_) == this;
 }
diff -r -u --color up/v8/src/heap/marking-barrier.h nw/v8/src/heap/marking-barrier.h
--- up/v8/src/heap/marking-barrier.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/marking-barrier.h	2023-01-19 16:46:36.243942896 -0500
@@ -55,12 +55,6 @@
 
   void RecordRelocSlot(Code host, RelocInfo* rinfo, HeapObject target);
 
-  void ActivateSpace(PagedSpace*);
-  void ActivateSpace(NewSpace*);
-
-  void DeactivateSpace(PagedSpace*);
-  void DeactivateSpace(NewSpace*);
-
   bool IsCurrentMarkingBarrier();
 
   template <typename TSlot>
diff -r -u --color up/v8/src/heap/marking-visitor-inl.h nw/v8/src/heap/marking-visitor-inl.h
--- up/v8/src/heap/marking-visitor-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/marking-visitor-inl.h	2023-01-19 16:46:36.243942896 -0500
@@ -7,7 +7,7 @@
 
 #include "src/heap/marking-state-inl.h"
 #include "src/heap/marking-visitor.h"
-#include "src/heap/marking-worklist.h"
+#include "src/heap/marking-worklist-inl.h"
 #include "src/heap/objects-visiting-inl.h"
 #include "src/heap/objects-visiting.h"
 #include "src/heap/progress-bar.h"
@@ -213,7 +213,9 @@
     DCHECK(IsBaselineCodeFlushingEnabled(code_flush_mode_));
     CodeT baseline_codet = CodeT::cast(shared_info.function_data(kAcquireLoad));
     // Safe to do a relaxed load here since the CodeT was acquire-loaded.
-    Code baseline_code = FromCodeT(baseline_codet, kRelaxedLoad);
+    Code baseline_code =
+        FromCodeT(baseline_codet, ObjectVisitorWithCageBases::code_cage_base(),
+                  kRelaxedLoad);
     // Visit the bytecode hanging off baseline code.
     VisitPointer(baseline_code,
                  baseline_code.RawField(
diff -r -u --color up/v8/src/heap/memory-chunk-layout.h nw/v8/src/heap/memory-chunk-layout.h
--- up/v8/src/heap/memory-chunk-layout.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/memory-chunk-layout.h	2023-01-19 16:46:36.243942896 -0500
@@ -37,8 +37,13 @@
 
 class V8_EXPORT_PRIVATE MemoryChunkLayout {
  public:
-  static const int kNumSets = NUMBER_OF_REMEMBERED_SET_TYPES;
-  static const int kNumTypes = ExternalBackingStoreType::kNumTypes;
+  static constexpr int kNumSets = NUMBER_OF_REMEMBERED_SET_TYPES;
+  static constexpr int kNumTypes = ExternalBackingStoreType::kNumTypes;
+#if V8_CC_MSVC && V8_TARGET_ARCH_IA32
+  static constexpr int kMemoryChunkAlignment = 8;
+#else
+  static constexpr int kMemoryChunkAlignment = sizeof(size_t);
+#endif  // V8_CC_MSVC && V8_TARGET_ARCH_IA32
 #define FIELD(Type, Name) \
   k##Name##Offset, k##Name##End = k##Name##Offset + sizeof(Type) - 1
   enum Header {
@@ -74,11 +79,17 @@
 #endif  // V8_ENABLE_INNER_POINTER_RESOLUTION_OSB
     FIELD(size_t, WasUsedForAllocation),
     kMarkingBitmapOffset,
-    kMemoryChunkHeaderSize = kMarkingBitmapOffset,
+    kMemoryChunkHeaderSize =
+        kMarkingBitmapOffset +
+        ((kMarkingBitmapOffset % kMemoryChunkAlignment) == 0
+             ? 0
+             : kMemoryChunkAlignment -
+                   (kMarkingBitmapOffset % kMemoryChunkAlignment)),
     kMemoryChunkHeaderStart = kSlotSetOffset,
     kBasicMemoryChunkHeaderSize = kMemoryChunkHeaderStart,
     kBasicMemoryChunkHeaderStart = 0,
   };
+#undef FIELD
   static size_t CodePageGuardStartOffset();
   static size_t CodePageGuardSize();
   static intptr_t ObjectStartOffsetInCodePage();
diff -r -u --color up/v8/src/heap/memory-chunk.cc nw/v8/src/heap/memory-chunk.cc
--- up/v8/src/heap/memory-chunk.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/memory-chunk.cc	2023-01-19 16:46:36.243942896 -0500
@@ -456,6 +456,22 @@
          invalidated_slots<type>()->end();
 }
 
+bool MemoryChunk::HasRecordedSlots() const {
+  for (int rs_type = 0; rs_type < NUMBER_OF_REMEMBERED_SET_TYPES; rs_type++) {
+    if (slot_set_[rs_type] || typed_slot_set_[rs_type] ||
+        invalidated_slots_[rs_type]) {
+      return true;
+    }
+  }
+
+  return false;
+}
+
+bool MemoryChunk::HasRecordedOldToNewSlots() const {
+  return slot_set_[OLD_TO_NEW] || typed_slot_set_[OLD_TO_NEW] ||
+         invalidated_slots_[OLD_TO_NEW];
+}
+
 #ifdef DEBUG
 void MemoryChunk::ValidateOffsets(MemoryChunk* chunk) {
   // Note that we cannot use offsetof because MemoryChunk is not a POD.
@@ -496,6 +512,17 @@
   DCHECK_EQ(reinterpret_cast<Address>(&chunk->possibly_empty_buckets_) -
                 chunk->address(),
             MemoryChunkLayout::kPossiblyEmptyBucketsOffset);
+  DCHECK_EQ(reinterpret_cast<Address>(&chunk->active_system_pages_) -
+                chunk->address(),
+            MemoryChunkLayout::kActiveSystemPagesOffset);
+#ifdef V8_ENABLE_INNER_POINTER_RESOLUTION_OSB
+  DCHECK_EQ(reinterpret_cast<Address>(&chunk->object_start_bitmap_) -
+                chunk->address(),
+            MemoryChunkLayout::kObjectStartBitmapOffset);
+#endif  // V8_ENABLE_INNER_POINTER_RESOLUTION_OSB
+  DCHECK_EQ(reinterpret_cast<Address>(&chunk->was_used_for_allocation_) -
+                chunk->address(),
+            MemoryChunkLayout::kWasUsedForAllocationOffset);
 }
 #endif
 
diff -r -u --color up/v8/src/heap/memory-chunk.h nw/v8/src/heap/memory-chunk.h
--- up/v8/src/heap/memory-chunk.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/memory-chunk.h	2023-01-19 16:46:36.243942896 -0500
@@ -152,6 +152,9 @@
     return invalidated_slots_[type];
   }
 
+  bool HasRecordedSlots() const;
+  bool HasRecordedOldToNewSlots() const;
+
   int FreeListsLength();
 
   // Approximate amount of physical memory committed for this chunk.
diff -r -u --color up/v8/src/heap/new-spaces.cc nw/v8/src/heap/new-spaces.cc
--- up/v8/src/heap/new-spaces.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/new-spaces.cc	2023-01-19 16:46:36.243942896 -0500
@@ -492,7 +492,7 @@
       Map map = object.map(cage_base);
       CHECK(map.IsMap(cage_base));
       CHECK(ReadOnlyHeap::Contains(map) ||
-            isolate->heap()->space_for_maps()->Contains(map));
+            isolate->heap()->old_space()->Contains(map));
 
       // The object should not be code or a map.
       CHECK(!object.IsMap(cage_base));
@@ -958,7 +958,6 @@
                RoundUp(static_cast<size_t>(v8_flags.semi_space_growth_factor) *
                            TotalCapacity(),
                        Page::kPageSize));
-  CHECK(EnsureCurrentCapacity());
 }
 
 bool PagedSpaceForNewSpace::StartShrinking() {
diff -r -u --color up/v8/src/heap/new-spaces.h nw/v8/src/heap/new-spaces.h
--- up/v8/src/heap/new-spaces.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/new-spaces.h	2023-01-19 16:46:36.243942896 -0500
@@ -578,7 +578,7 @@
   // Return the maximum capacity of the space.
   size_t MaximumCapacity() const { return max_capacity_; }
 
-  size_t TotalCapacity() const { return current_capacity_; }
+  size_t TotalCapacity() const { return target_capacity_; }
 
   // Return the address of the first allocatable address in the active
   // semispace. This may be the address where the first object resides.
diff -r -u --color up/v8/src/heap/paged-spaces-inl.h nw/v8/src/heap/paged-spaces-inl.h
--- up/v8/src/heap/paged-spaces-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/paged-spaces-inl.h	2023-01-19 16:46:36.243942896 -0500
@@ -16,7 +16,55 @@
 namespace internal {
 
 // -----------------------------------------------------------------------------
-// PagedSpaceObjectIterator
+// Heap object iterator in paged spaces.
+//
+// A PagedSpaceObjectIterator iterates objects from the bottom of the given
+// space to its top or from the bottom of the given page to its top.
+//
+// If objects are allocated in the page during iteration the iterator may
+// or may not iterate over those objects.  The caller must create a new
+// iterator in order to be sure to visit these new objects.
+class V8_EXPORT_PRIVATE PagedSpaceObjectIterator : public ObjectIterator {
+ public:
+  // Creates a new object iterator in a given space.
+  PagedSpaceObjectIterator(Heap* heap, const PagedSpaceBase* space);
+  PagedSpaceObjectIterator(Heap* heap, const PagedSpaceBase* space,
+                           const Page* page);
+  PagedSpaceObjectIterator(Heap* heap, const PagedSpace* space,
+                           const Page* page, Address start_address);
+
+  // Advance to the next object, skipping free spaces and other fillers and
+  // skipping the special garbage section of which there is one per space.
+  // Returns nullptr when the iteration has ended.
+  inline HeapObject Next() override;
+
+  // The pointer compression cage base value used for decompression of all
+  // tagged values except references to Code objects.
+  PtrComprCageBase cage_base() const {
+#if V8_COMPRESS_POINTERS
+    return cage_base_;
+#else
+    return PtrComprCageBase{};
+#endif  // V8_COMPRESS_POINTERS
+  }
+
+ private:
+  // Fast (inlined) path of next().
+  inline HeapObject FromCurrentPage();
+
+  // Slow path of next(), goes into the next page.  Returns false if the
+  // iteration has ended.
+  bool AdvanceToNextPage();
+
+  Address cur_addr_;  // Current iteration point.
+  Address cur_end_;   // End iteration point.
+  const PagedSpaceBase* const space_;
+  ConstPageRange page_range_;
+  ConstPageRange::iterator current_page_;
+#if V8_COMPRESS_POINTERS
+  const PtrComprCageBase cage_base_;
+#endif  // V8_COMPRESS_POINTERS
+};
 
 HeapObject PagedSpaceObjectIterator::Next() {
   do {
diff -r -u --color up/v8/src/heap/paged-spaces.cc nw/v8/src/heap/paged-spaces.cc
--- up/v8/src/heap/paged-spaces.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/paged-spaces.cc	2023-01-19 16:46:36.243942896 -0500
@@ -146,8 +146,7 @@
   // Any PagedSpace might invoke RefillFreeList. We filter all but our old
   // generation spaces out.
   DCHECK(identity() == OLD_SPACE || identity() == CODE_SPACE ||
-         identity() == MAP_SPACE || identity() == NEW_SPACE ||
-         identity() == SHARED_SPACE);
+         identity() == NEW_SPACE || identity() == SHARED_SPACE);
 
   Sweeper* sweeper = heap()->sweeper();
   size_t added = 0;
@@ -338,7 +337,8 @@
   // Pages are only removed from new space when they are promoted to old space
   // during a GC. This happens after sweeping as started and the allocation
   // counters have been reset.
-  DCHECK_IMPLIES(identity() == NEW_SPACE, Size() == 0);
+  DCHECK_IMPLIES(identity() == NEW_SPACE,
+                 heap()->gc_state() != Heap::NOT_IN_GC);
   if (identity() != NEW_SPACE) {
     DecreaseAllocatedBytes(page->allocated_bytes(), page);
   }
@@ -665,7 +665,7 @@
   base::MutexGuard lock(&space_mutex_);
   DCHECK_LE(min_size_in_bytes, max_size_in_bytes);
   DCHECK(identity() == OLD_SPACE || identity() == CODE_SPACE ||
-         identity() == MAP_SPACE || identity() == SHARED_SPACE);
+         identity() == SHARED_SPACE);
 
   size_t new_node_size = 0;
   FreeSpace new_node =
@@ -742,7 +742,7 @@
       Map map = object.map(cage_base);
       CHECK(map.IsMap(cage_base));
       CHECK(ReadOnlyHeap::Contains(map) ||
-            isolate->heap()->space_for_maps()->Contains(map));
+            isolate->heap()->old_space()->Contains(map));
 
       // Perform space-specific object verification.
       VerifyObject(object);
@@ -910,19 +910,12 @@
 
   if (TryAllocationFromFreeListMain(size_in_bytes, origin)) return true;
 
-  if (identity() == NEW_SPACE) {
-    // New space should not allocate new pages when running out of space and it
-    // is not currently swept.
-    return false;
-  }
-
   const bool is_main_thread =
       heap()->IsMainThread() || heap()->IsSharedMainThread();
-  const auto sweeping_scope_id = is_main_thread
-                                     ? GCTracer::Scope::MC_SWEEP
-                                     : GCTracer::Scope::MC_BACKGROUND_SWEEPING;
   const auto sweeping_scope_kind =
       is_main_thread ? ThreadKind::kMain : ThreadKind::kBackground;
+  const auto sweeping_scope_id =
+      heap()->sweeper()->GetTracingScope(identity(), is_main_thread);
   // Sweeping is still in progress.
   if (heap()->sweeping_in_progress()) {
     // First try to refill the free-list, concurrent sweeper threads
@@ -959,7 +952,8 @@
     }
   }
 
-  if (heap()->ShouldExpandOldGenerationOnSlowAllocation(
+  if (identity() != NEW_SPACE &&
+      heap()->ShouldExpandOldGenerationOnSlowAllocation(
           heap()->main_thread_local_heap()) &&
       heap()->CanExpandOldGeneration(AreaSize())) {
     if (TryExpand(size_in_bytes, origin)) {
@@ -973,7 +967,8 @@
     if (ContributeToSweepingMain(0, 0, size_in_bytes, origin)) return true;
   }
 
-  if (heap()->gc_state() != Heap::NOT_IN_GC && !heap()->force_oom()) {
+  if (identity() != NEW_SPACE && heap()->gc_state() != Heap::NOT_IN_GC &&
+      !heap()->force_oom()) {
     // Avoid OOM crash in the GC in order to invoke NearHeapLimitCallback after
     // GC and give it a chance to increase the heap limit.
     return TryExpand(size_in_bytes, origin);
@@ -984,10 +979,6 @@
 bool PagedSpaceBase::ContributeToSweepingMain(int required_freed_bytes,
                                               int max_pages, int size_in_bytes,
                                               AllocationOrigin origin) {
-  // TODO(v8:12612): New space is not currently swept so new space allocation
-  // shoudl not contribute to sweeping, Revisit this once sweeping for young gen
-  // is implemented.
-  DCHECK_NE(NEW_SPACE, identity());
   // Cleanup invalidated old-to-new refs for compaction space in the
   // final atomic pause.
   Sweeper::SweepingMode sweeping_mode =
@@ -1046,44 +1037,5 @@
   return added;
 }
 
-// -----------------------------------------------------------------------------
-// MapSpace implementation
-
-// TODO(dmercadier): use a heap instead of sorting like that.
-// Using a heap will have multiple benefits:
-//   - for now, SortFreeList is only called after sweeping, which is somewhat
-//   late. Using a heap, sorting could be done online: FreeListCategories would
-//   be inserted in a heap (ie, in a sorted manner).
-//   - SortFreeList is a bit fragile: any change to FreeListMap (or to
-//   MapSpace::free_list_) could break it.
-void MapSpace::SortFreeList() {
-  using LiveBytesPagePair = std::pair<size_t, Page*>;
-  std::vector<LiveBytesPagePair> pages;
-  pages.reserve(CountTotalPages());
-
-  for (Page* p : *this) {
-    free_list()->RemoveCategory(p->free_list_category(kFirstCategory));
-    pages.push_back(std::make_pair(p->allocated_bytes(), p));
-  }
-
-  // Sorting by least-allocated-bytes first.
-  std::sort(pages.begin(), pages.end(),
-            [](const LiveBytesPagePair& a, const LiveBytesPagePair& b) {
-              return a.first < b.first;
-            });
-
-  for (LiveBytesPagePair const& p : pages) {
-    // Since AddCategory inserts in head position, it reverts the order produced
-    // by the sort above: least-allocated-bytes will be Added first, and will
-    // therefore be the last element (and the first one will be
-    // most-allocated-bytes).
-    free_list()->AddCategory(p.second->free_list_category(kFirstCategory));
-  }
-}
-
-#ifdef VERIFY_HEAP
-void MapSpace::VerifyObject(HeapObject object) const { CHECK(object.IsMap()); }
-#endif
-
 }  // namespace internal
 }  // namespace v8
diff -r -u --color up/v8/src/heap/paged-spaces.h nw/v8/src/heap/paged-spaces.h
--- up/v8/src/heap/paged-spaces.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/paged-spaces.h	2023-01-19 16:46:36.243942896 -0500
@@ -33,57 +33,6 @@
 class PagedSpaceBase;
 class Sweeper;
 
-// -----------------------------------------------------------------------------
-// Heap object iterator in paged spaces.
-//
-// A PagedSpaceObjectIterator iterates objects from the bottom of the given
-// space to its top or from the bottom of the given page to its top.
-//
-// If objects are allocated in the page during iteration the iterator may
-// or may not iterate over those objects.  The caller must create a new
-// iterator in order to be sure to visit these new objects.
-class V8_EXPORT_PRIVATE PagedSpaceObjectIterator : public ObjectIterator {
- public:
-  // Creates a new object iterator in a given space.
-  PagedSpaceObjectIterator(Heap* heap, const PagedSpaceBase* space);
-  PagedSpaceObjectIterator(Heap* heap, const PagedSpaceBase* space,
-                           const Page* page);
-  PagedSpaceObjectIterator(Heap* heap, const PagedSpace* space,
-                           const Page* page, Address start_address);
-
-  // Advance to the next object, skipping free spaces and other fillers and
-  // skipping the special garbage section of which there is one per space.
-  // Returns nullptr when the iteration has ended.
-  inline HeapObject Next() override;
-
-  // The pointer compression cage base value used for decompression of all
-  // tagged values except references to Code objects.
-  PtrComprCageBase cage_base() const {
-#if V8_COMPRESS_POINTERS
-    return cage_base_;
-#else
-    return PtrComprCageBase{};
-#endif  // V8_COMPRESS_POINTERS
-  }
-
- private:
-  // Fast (inlined) path of next().
-  inline HeapObject FromCurrentPage();
-
-  // Slow path of next(), goes into the next page.  Returns false if the
-  // iteration has ended.
-  bool AdvanceToNextPage();
-
-  Address cur_addr_;  // Current iteration point.
-  Address cur_end_;   // End iteration point.
-  const PagedSpaceBase* const space_;
-  ConstPageRange page_range_;
-  ConstPageRange::iterator current_page_;
-#if V8_COMPRESS_POINTERS
-  const PtrComprCageBase cage_base_;
-#endif  // V8_COMPRESS_POINTERS
-};
-
 class V8_EXPORT_PRIVATE PagedSpaceBase
     : NON_EXPORTED_BASE(public SpaceWithLinearArea) {
  public:
@@ -475,8 +424,6 @@
                                      CompactionSpaceKind compaction_space_kind)
       : old_space_(heap, OLD_SPACE, Executability::NOT_EXECUTABLE,
                    compaction_space_kind),
-        map_space_(heap, MAP_SPACE, Executability::NOT_EXECUTABLE,
-                   compaction_space_kind),
         code_space_(heap, CODE_SPACE, Executability::EXECUTABLE,
                     compaction_space_kind),
         shared_space_(heap, SHARED_SPACE, Executability::NOT_EXECUTABLE,
@@ -486,8 +433,6 @@
     switch (space) {
       case OLD_SPACE:
         return &old_space_;
-      case MAP_SPACE:
-        return &map_space_;
       case CODE_SPACE:
         return &code_space_;
       case SHARED_SPACE:
@@ -500,7 +445,6 @@
 
  private:
   CompactionSpace old_space_;
-  CompactionSpace map_space_;
   CompactionSpace code_space_;
   CompactionSpace shared_space_;
 };
@@ -544,36 +488,6 @@
 };
 
 // -----------------------------------------------------------------------------
-// Old space for all map objects
-
-class MapSpace final : public PagedSpace {
- public:
-  // Creates a map space object.
-  explicit MapSpace(Heap* heap)
-      : PagedSpace(heap, MAP_SPACE, NOT_EXECUTABLE, FreeList::CreateFreeList(),
-                   paged_allocation_info_) {}
-
-  int RoundSizeDownToObjectAlignment(int size) const override {
-    if (V8_COMPRESS_POINTERS_8GB_BOOL) {
-      return RoundDown(size, kObjectAlignment8GbHeap);
-    } else if (base::bits::IsPowerOfTwo(Map::kSize)) {
-      return RoundDown(size, Map::kSize);
-    } else {
-      return (size / Map::kSize) * Map::kSize;
-    }
-  }
-
-  void SortFreeList();
-
-#ifdef VERIFY_HEAP
-  void VerifyObject(HeapObject obj) const override;
-#endif
-
- private:
-  LinearAllocationArea paged_allocation_info_;
-};
-
-// -----------------------------------------------------------------------------
 // Shared space regular object space.
 
 class SharedSpace final : public PagedSpace {
@@ -611,7 +525,6 @@
  private:
   enum State {
     kOldSpaceState,
-    kMapState,
     kCodeState,
     kLargeObjectState,
     kCodeLargeObjectState,
@@ -621,8 +534,6 @@
   State state_;
   PageIterator old_iterator_;
   PageIterator code_iterator_;
-  PageIterator map_iterator_;
-  const PageIterator map_iterator_end_;
   LargePageIterator lo_iterator_;
   LargePageIterator code_lo_iterator_;
 };
diff -r -u --color up/v8/src/heap/pretenuring-handler-inl.h nw/v8/src/heap/pretenuring-handler-inl.h
--- up/v8/src/heap/pretenuring-handler-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/pretenuring-handler-inl.h	2023-01-19 16:46:36.243942896 -0500
@@ -11,6 +11,7 @@
 #include "src/heap/pretenuring-handler.h"
 #include "src/heap/spaces.h"
 #include "src/objects/allocation-site-inl.h"
+#include "src/objects/allocation-site.h"
 
 namespace v8 {
 namespace internal {
@@ -52,6 +53,13 @@
   if (!Page::OnSamePage(object_address, last_memento_word_address)) {
     return AllocationMemento();
   }
+
+  Page* object_page = Page::FromAddress(object_address);
+  // If the page is being swept, treat it as if the memento was already swept
+  // and bail out.
+  if (mode != FindMementoMode::kForGC && !object_page->SweepingDone())
+    return AllocationMemento();
+
   HeapObject candidate = HeapObject::FromAddress(memento_address);
   ObjectSlot candidate_map_slot = candidate.map_slot();
   // This fast check may peek at an uninitialized word. However, the slow check
@@ -65,7 +73,6 @@
 
   // Bail out if the memento is below the age mark, which can happen when
   // mementos survived because a page got moved within new space.
-  Page* object_page = Page::FromAddress(object_address);
   if (object_page->IsFlagSet(Page::NEW_SPACE_BELOW_AGE_MARK)) {
     Address age_mark =
         reinterpret_cast<SemiSpace*>(object_page->owner())->age_mark();
diff -r -u --color up/v8/src/heap/pretenuring-handler.cc nw/v8/src/heap/pretenuring-handler.cc
--- up/v8/src/heap/pretenuring-handler.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/pretenuring-handler.cc	2023-01-19 16:46:36.254776226 -0500
@@ -42,12 +42,6 @@
   }
 }
 
-bool PretenturingHandler::DeoptMaybeTenuredAllocationSites() const {
-  NewSpace* new_space = heap_->new_space();
-  return new_space && new_space->IsAtMaximumCapacity() &&
-         !heap_->MaximumSizeMinorGC();
-}
-
 namespace {
 
 inline bool MakePretenureDecision(
@@ -193,20 +187,17 @@
       allocation_sites_to_pretenure_.reset();
     }
 
-    // Step 3: Deopt maybe tenured allocation sites if necessary.
-    bool deopt_maybe_tenured = DeoptMaybeTenuredAllocationSites();
-    if (deopt_maybe_tenured) {
-      heap_->ForeachAllocationSite(
-          heap_->allocation_sites_list(),
-          [&allocation_sites, &trigger_deoptimization](AllocationSite site) {
-            DCHECK(site.IsAllocationSite());
-            allocation_sites++;
-            if (site.IsMaybeTenure()) {
-              site.set_deopt_dependent_code(true);
-              trigger_deoptimization = true;
-            }
-          });
-    }
+    // Step 3: Deopt maybe tenured allocation sites.
+    heap_->ForeachAllocationSite(
+        heap_->allocation_sites_list(),
+        [&allocation_sites, &trigger_deoptimization](AllocationSite site) {
+          DCHECK(site.IsAllocationSite());
+          allocation_sites++;
+          if (site.IsMaybeTenure()) {
+            site.set_deopt_dependent_code(true);
+            trigger_deoptimization = true;
+          }
+        });
 
     if (trigger_deoptimization) {
       heap_->isolate()->stack_guard()->RequestDeoptMarkedAllocationSites();
@@ -216,12 +207,12 @@
         (allocation_mementos_found > 0 || tenure_decisions > 0 ||
          dont_tenure_decisions > 0)) {
       PrintIsolate(heap_->isolate(),
-                   "pretenuring: deopt_maybe_tenured=%d visited_sites=%d "
+                   "pretenuring: visited_sites=%d "
                    "active_sites=%d "
                    "mementos=%d tenured=%d not_tenured=%d\n",
-                   deopt_maybe_tenured ? 1 : 0, allocation_sites,
-                   active_allocation_sites, allocation_mementos_found,
-                   tenure_decisions, dont_tenure_decisions);
+                   allocation_sites, active_allocation_sites,
+                   allocation_mementos_found, tenure_decisions,
+                   dont_tenure_decisions);
     }
 
     global_pretenuring_feedback_.clear();
diff -r -u --color up/v8/src/heap/pretenuring-handler.h nw/v8/src/heap/pretenuring-handler.h
--- up/v8/src/heap/pretenuring-handler.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/pretenuring-handler.h	2023-01-19 16:46:36.254776226 -0500
@@ -69,8 +69,6 @@
   void RemoveAllocationSitePretenuringFeedback(AllocationSite site);
 
  private:
-  bool DeoptMaybeTenuredAllocationSites() const;
-
   Heap* const heap_;
 
   // The feedback storage is used to store allocation sites (keys) and how often
diff -r -u --color up/v8/src/heap/safepoint.cc nw/v8/src/heap/safepoint.cc
--- up/v8/src/heap/safepoint.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/safepoint.cc	2023-01-19 16:46:36.254776226 -0500
@@ -282,11 +282,14 @@
   return isolate()->shared_heap_isolate();
 }
 
-SafepointScope::SafepointScope(Heap* heap) : safepoint_(heap->safepoint()) {
+IsolateSafepointScope::IsolateSafepointScope(Heap* heap)
+    : safepoint_(heap->safepoint()) {
   safepoint_->EnterLocalSafepointScope();
 }
 
-SafepointScope::~SafepointScope() { safepoint_->LeaveLocalSafepointScope(); }
+IsolateSafepointScope::~IsolateSafepointScope() {
+  safepoint_->LeaveLocalSafepointScope();
+}
 
 GlobalSafepoint::GlobalSafepoint(Isolate* isolate)
     : shared_heap_isolate_(isolate) {}
@@ -415,23 +418,22 @@
 
 GlobalSafepointScope::GlobalSafepointScope(Isolate* initiator)
     : initiator_(initiator),
-      shared_heap_isolate_(initiator->has_shared_heap()
-                               ? initiator->shared_heap_isolate()
-                               : nullptr) {
-  if (shared_heap_isolate_) {
-    shared_heap_isolate_->global_safepoint()->EnterGlobalSafepointScope(
-        initiator_);
-  } else {
-    initiator_->heap()->safepoint()->EnterLocalSafepointScope();
-  }
+      shared_heap_isolate_(initiator->shared_heap_isolate()) {
+  shared_heap_isolate_->global_safepoint()->EnterGlobalSafepointScope(
+      initiator_);
 }
 
 GlobalSafepointScope::~GlobalSafepointScope() {
-  if (shared_heap_isolate_) {
-    shared_heap_isolate_->global_safepoint()->LeaveGlobalSafepointScope(
-        initiator_);
+  shared_heap_isolate_->global_safepoint()->LeaveGlobalSafepointScope(
+      initiator_);
+}
+
+SafepointScope::SafepointScope(Isolate* initiator, SafepointKind kind) {
+  if (kind == SafepointKind::kIsolate) {
+    isolate_safepoint_.emplace(initiator->heap());
   } else {
-    initiator_->heap()->safepoint()->LeaveLocalSafepointScope();
+    DCHECK_EQ(kind, SafepointKind::kGlobal);
+    global_safepoint_.emplace(initiator);
   }
 }
 
diff -r -u --color up/v8/src/heap/safepoint.h nw/v8/src/heap/safepoint.h
--- up/v8/src/heap/safepoint.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/safepoint.h	2023-01-19 16:46:36.254776226 -0500
@@ -148,14 +148,14 @@
   friend class GlobalSafepoint;
   friend class GlobalSafepointScope;
   friend class Isolate;
+  friend class IsolateSafepointScope;
   friend class LocalHeap;
-  friend class SafepointScope;
 };
 
-class V8_NODISCARD SafepointScope {
+class V8_NODISCARD IsolateSafepointScope {
  public:
-  V8_EXPORT_PRIVATE explicit SafepointScope(Heap* heap);
-  V8_EXPORT_PRIVATE ~SafepointScope();
+  V8_EXPORT_PRIVATE explicit IsolateSafepointScope(Heap* heap);
+  V8_EXPORT_PRIVATE ~IsolateSafepointScope();
 
  private:
   IsolateSafepoint* safepoint_;
@@ -204,6 +204,18 @@
   Isolate* const shared_heap_isolate_;
 };
 
+enum class SafepointKind { kIsolate, kGlobal };
+
+class V8_NODISCARD SafepointScope {
+ public:
+  V8_EXPORT_PRIVATE explicit SafepointScope(Isolate* initiator,
+                                            SafepointKind kind);
+
+ private:
+  base::Optional<IsolateSafepointScope> isolate_safepoint_;
+  base::Optional<GlobalSafepointScope> global_safepoint_;
+};
+
 }  // namespace internal
 }  // namespace v8
 
diff -r -u --color up/v8/src/heap/scavenger-inl.h nw/v8/src/heap/scavenger-inl.h
--- up/v8/src/heap/scavenger-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/scavenger-inl.h	2023-01-19 16:46:36.254776226 -0500
@@ -199,8 +199,7 @@
 
     // During incremental marking we want to push every object in order to
     // record slots for map words. Necessary for map space compaction.
-    if (object_fields == ObjectFields::kMaybePointers ||
-        is_compacting_including_map_space_) {
+    if (object_fields == ObjectFields::kMaybePointers || is_compacting_) {
       promotion_list_local_.PushRegularObject(target, object_size);
     }
     promoted_size_ += object_size;
@@ -467,6 +466,32 @@
   return REMOVE_SLOT;
 }
 
+class ScavengeVisitor final : public NewSpaceVisitor<ScavengeVisitor> {
+ public:
+  explicit ScavengeVisitor(Scavenger* scavenger);
+
+  V8_INLINE void VisitPointers(HeapObject host, ObjectSlot start,
+                               ObjectSlot end) final;
+
+  V8_INLINE void VisitPointers(HeapObject host, MaybeObjectSlot start,
+                               MaybeObjectSlot end) final;
+  V8_INLINE void VisitCodePointer(HeapObject host, CodeObjectSlot slot) final;
+
+  V8_INLINE void VisitCodeTarget(Code host, RelocInfo* rinfo) final;
+  V8_INLINE void VisitEmbeddedPointer(Code host, RelocInfo* rinfo) final;
+  V8_INLINE int VisitEphemeronHashTable(Map map, EphemeronHashTable object);
+  V8_INLINE int VisitJSArrayBuffer(Map map, JSArrayBuffer object);
+
+ private:
+  template <typename TSlot>
+  V8_INLINE void VisitHeapObjectImpl(TSlot slot, HeapObject heap_object);
+
+  template <typename TSlot>
+  V8_INLINE void VisitPointersImpl(HeapObject host, TSlot start, TSlot end);
+
+  Scavenger* const scavenger_;
+};
+
 void ScavengeVisitor::VisitPointers(HeapObject host, ObjectSlot start,
                                     ObjectSlot end) {
   return VisitPointersImpl(host, start, end);
diff -r -u --color up/v8/src/heap/scavenger.cc nw/v8/src/heap/scavenger.cc
--- up/v8/src/heap/scavenger.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/scavenger.cc	2023-01-19 16:46:36.254776226 -0500
@@ -145,7 +145,7 @@
                          MemoryChunk::IS_EXECUTABLE));
       // Shared heap pages do not have evacuation candidates outside an atomic
       // shared GC pause.
-      DCHECK(!target.InSharedWritableHeap());
+      DCHECK_IMPLIES(!v8_flags.shared_space, !target.InSharedWritableHeap());
 
       // We cannot call MarkCompactCollector::RecordSlot because that checks
       // that the host page is not in young generation, which does not hold
@@ -364,7 +364,7 @@
       TRACE_GC(
           heap_->tracer(),
           GCTracer::Scope::SCAVENGER_SCAVENGE_WEAK_GLOBAL_HANDLES_IDENTIFY);
-      isolate_->global_handles()->ComputeWeaknessForYoungObjects(
+      isolate_->traced_handles()->ComputeWeaknessForYoungObjects(
           &JSObject::IsUnmodifiedApiObject);
     }
     {
@@ -373,15 +373,16 @@
       // Scavenger treats all weak roots except for global handles as strong.
       // That is why we don't set skip_weak = true here and instead visit
       // global handles separately.
-      base::EnumSet<SkipRoot> options({SkipRoot::kExternalStringTable,
-                                       SkipRoot::kGlobalHandles,
-                                       SkipRoot::kOldGeneration});
+      base::EnumSet<SkipRoot> options(
+          {SkipRoot::kExternalStringTable, SkipRoot::kGlobalHandles,
+           SkipRoot::kOldGeneration, SkipRoot::kConservativeStack});
       if (V8_UNLIKELY(v8_flags.scavenge_separate_stack_scanning)) {
         options.Add(SkipRoot::kStack);
       }
       heap_->IterateRoots(&root_scavenge_visitor, options);
       isolate_->global_handles()->IterateYoungStrongAndDependentRoots(
           &root_scavenge_visitor);
+      isolate_->traced_handles()->IterateYoungRoots(&root_scavenge_visitor);
       scavengers[kMainThreadId]->Publish();
     }
     {
@@ -411,6 +412,8 @@
       GlobalHandlesWeakRootsUpdatingVisitor visitor;
       isolate_->global_handles()->ProcessWeakYoungObjects(
           &visitor, &IsUnscavengedHeapObjectSlot);
+      isolate_->traced_handles()->ProcessYoungObjects(
+          &visitor, &IsUnscavengedHeapObjectSlot);
     }
 
     {
@@ -495,13 +498,13 @@
   }
 
   isolate_->global_handles()->UpdateListOfYoungNodes();
+  isolate_->traced_handles()->UpdateListOfYoungNodes();
 
   // Update how much has survived scavenge.
   heap_->IncrementYoungSurvivorsCounter(heap_->SurvivedYoungObjectSize());
 }
 
 void ScavengerCollector::IterateStackAndScavenge(
-
     RootScavengeVisitor* root_scavenge_visitor,
     std::vector<std::unique_ptr<Scavenger>>* scavengers, int main_thread_id) {
   // Scan the stack, scavenge the newly discovered objects, and report
@@ -513,7 +516,7 @@
     survived_bytes_before +=
         scavenger->bytes_copied() + scavenger->bytes_promoted();
   }
-  heap_->IterateStackRoots(root_scavenge_visitor);
+  heap_->IterateStackRoots(root_scavenge_visitor, Heap::ScanStackMode::kNone);
   (*scavengers)[main_thread_id]->Process();
   size_t survived_bytes_after = 0;
   for (auto& scavenger : *scavengers) {
@@ -625,8 +628,6 @@
       is_logging_(is_logging),
       is_incremental_marking_(heap->incremental_marking()->IsMarking()),
       is_compacting_(heap->incremental_marking()->IsCompacting()),
-      is_compacting_including_map_space_(is_compacting_ &&
-                                         v8_flags.compact_maps),
       shared_string_table_(shared_old_allocator_.get() != nullptr),
       mark_shared_heap_(heap->isolate()->is_shared_space_isolate()) {}
 
@@ -643,12 +644,8 @@
 
   IterateAndScavengePromotedObjectsVisitor visitor(this, record_slots);
 
-  if (is_compacting_including_map_space_) {
-    // When we compact map space, we also want to visit the map word.
-    target.IterateFast(map, size, &visitor);
-  } else {
-    target.IterateBodyFast(map, size, &visitor);
-  }
+  // Iterate all outgoing pointers including map word.
+  target.IterateFast(map, size, &visitor);
 
   if (map.IsJSArrayBufferMap()) {
     DCHECK(!BasicMemoryChunk::FromHeapObject(target)->IsLargePage());
@@ -700,7 +697,8 @@
   }
 
   RememberedSet<OLD_TO_NEW>::IterateTyped(
-      page, [=](SlotType slot_type, Address slot_address) {
+      page, [this, page, record_old_to_shared_slots](SlotType slot_type,
+                                                     Address slot_address) {
         return UpdateTypedSlotHelper::UpdateTypedSlot(
             heap_, slot_type, slot_address,
             [this, page, slot_type, slot_address,
diff -r -u --color up/v8/src/heap/scavenger.h nw/v8/src/heap/scavenger.h
--- up/v8/src/heap/scavenger.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/scavenger.h	2023-01-19 16:46:36.254776226 -0500
@@ -20,6 +20,7 @@
 
 class RootScavengeVisitor;
 class Scavenger;
+class ScavengeVisitor;
 
 enum class CopyAndForwardResult {
   SUCCESS_YOUNG_GENERATION,
@@ -211,7 +212,6 @@
   const bool is_logging_;
   const bool is_incremental_marking_;
   const bool is_compacting_;
-  const bool is_compacting_including_map_space_;
   const bool shared_string_table_;
   const bool mark_shared_heap_;
 
@@ -236,32 +236,6 @@
 
   Scavenger* const scavenger_;
 };
-
-class ScavengeVisitor final : public NewSpaceVisitor<ScavengeVisitor> {
- public:
-  explicit ScavengeVisitor(Scavenger* scavenger);
-
-  V8_INLINE void VisitPointers(HeapObject host, ObjectSlot start,
-                               ObjectSlot end) final;
-
-  V8_INLINE void VisitPointers(HeapObject host, MaybeObjectSlot start,
-                               MaybeObjectSlot end) final;
-  V8_INLINE void VisitCodePointer(HeapObject host, CodeObjectSlot slot) final;
-
-  V8_INLINE void VisitCodeTarget(Code host, RelocInfo* rinfo) final;
-  V8_INLINE void VisitEmbeddedPointer(Code host, RelocInfo* rinfo) final;
-  V8_INLINE int VisitEphemeronHashTable(Map map, EphemeronHashTable object);
-  V8_INLINE int VisitJSArrayBuffer(Map map, JSArrayBuffer object);
-
- private:
-  template <typename TSlot>
-  V8_INLINE void VisitHeapObjectImpl(TSlot slot, HeapObject heap_object);
-
-  template <typename TSlot>
-  V8_INLINE void VisitPointersImpl(HeapObject host, TSlot start, TSlot end);
-
-  Scavenger* const scavenger_;
-};
 
 class ScavengerCollector {
  public:
diff -r -u --color up/v8/src/heap/setup-heap-internal.cc nw/v8/src/heap/setup-heap-internal.cc
--- up/v8/src/heap/setup-heap-internal.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/setup-heap-internal.cc	2023-01-19 16:46:36.254776226 -0500
@@ -876,7 +876,7 @@
   set_retaining_path_targets(roots.empty_weak_array_list());
 
   set_feedback_vectors_for_profiling_tools(roots.undefined_value());
-  set_pending_optimize_for_test_bytecode(roots.undefined_value());
+  set_functions_marked_for_manual_optimization(roots.undefined_value());
   set_shared_wasm_memories(roots.empty_weak_array_list());
   set_locals_block_list_cache(roots.undefined_value());
 #ifdef V8_ENABLE_WEBASSEMBLY
diff -r -u --color up/v8/src/heap/spaces-inl.h nw/v8/src/heap/spaces-inl.h
--- up/v8/src/heap/spaces-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/spaces-inl.h	2023-01-19 16:46:36.254776226 -0500
@@ -103,10 +103,6 @@
       state_(kOldSpaceState),
       old_iterator_(heap->old_space()->begin()),
       code_iterator_(heap->code_space()->begin()),
-      map_iterator_(heap->map_space() ? heap->map_space()->begin()
-                                      : PageRange::iterator(nullptr)),
-      map_iterator_end_(heap->map_space() ? heap->map_space()->end()
-                                          : PageRange::iterator(nullptr)),
       lo_iterator_(heap->lo_space()->begin()),
       code_lo_iterator_(heap->code_lo_space()->begin()) {}
 
@@ -114,11 +110,6 @@
   switch (state_) {
     case kOldSpaceState: {
       if (old_iterator_ != heap_->old_space()->end()) return *(old_iterator_++);
-      state_ = kMapState;
-      V8_FALLTHROUGH;
-    }
-    case kMapState: {
-      if (map_iterator_ != map_iterator_end_) return *(map_iterator_++);
       state_ = kCodeState;
       V8_FALLTHROUGH;
     }
diff -r -u --color up/v8/src/heap/sweeper.cc nw/v8/src/heap/sweeper.cc
--- up/v8/src/heap/sweeper.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/sweeper.cc	2023-01-19 16:46:36.254776226 -0500
@@ -36,6 +36,7 @@
             PretenturingHandler::kInitialFeedbackCapacity) {}
 
   bool ConcurrentSweepSpace(AllocationSpace identity, JobDelegate* delegate) {
+    DCHECK(IsValidSweepingSpace(identity));
     while (!delegate->ShouldYield()) {
       Page* page = sweeper_->GetSweepingPageSafe(identity);
       if (page == nullptr) return true;
@@ -69,22 +70,8 @@
 
   void Run(JobDelegate* delegate) final {
     RwxMemoryWriteScope::SetDefaultPermissionsForNewThread();
-    DCHECK(sweeper_->current_collector_.has_value());
-    if (delegate->IsJoiningThread()) {
-      TRACE_GC(tracer_, sweeper_->current_collector_ ==
-                                GarbageCollector::MINOR_MARK_COMPACTOR
-                            ? GCTracer::Scope::MINOR_MC_SWEEP
-                            : GCTracer::Scope::MC_SWEEP);
-      RunImpl(delegate);
-    } else {
-      TRACE_GC_EPOCH(
-          tracer_,
-          sweeper_->current_collector_ == GarbageCollector::MINOR_MARK_COMPACTOR
-              ? GCTracer::Scope::MINOR_MC_BACKGROUND_SWEEPING
-              : GCTracer::Scope::MC_BACKGROUND_SWEEPING,
-          ThreadKind::kBackground);
-      RunImpl(delegate);
-    }
+    DCHECK(sweeper_->current_new_space_collector_.has_value());
+    RunImpl(delegate, delegate->IsJoiningThread());
   }
 
   size_t GetMaxConcurrency(size_t worker_count) const override {
@@ -97,16 +84,42 @@
   }
 
  private:
-  void RunImpl(JobDelegate* delegate) {
+  void RunImpl(JobDelegate* delegate, bool is_joining_thread) {
+    static_assert(NEW_SPACE == FIRST_SWEEPABLE_SPACE);
     const int offset = delegate->GetTaskId();
     DCHECK_LT(offset, concurrent_sweepers_->size());
-    ConcurrentSweeper& sweeper = (*concurrent_sweepers_)[offset];
-    for (int i = 0; i < kNumberOfSweepingSpaces; i++) {
-      const AllocationSpace space_id = static_cast<AllocationSpace>(
-          FIRST_SWEEPABLE_SPACE + ((i + offset) % kNumberOfSweepingSpaces));
-      DCHECK(IsValidSweepingSpace(space_id));
-      if (!sweeper.ConcurrentSweepSpace(space_id, delegate)) return;
+    ConcurrentSweeper& concurrent_sweeper = (*concurrent_sweepers_)[offset];
+    if (offset > 0) {
+      if (!SweepNonNewSpaces(concurrent_sweeper, delegate, is_joining_thread,
+                             offset, kNumberOfSweepingSpaces))
+        return;
     }
+    {
+      TRACE_GC_EPOCH(
+          tracer_, sweeper_->GetTracingScope(NEW_SPACE, is_joining_thread),
+          is_joining_thread ? ThreadKind::kMain : ThreadKind::kBackground);
+      if (!concurrent_sweeper.ConcurrentSweepSpace(NEW_SPACE, delegate)) return;
+    }
+    if (!SweepNonNewSpaces(concurrent_sweeper, delegate, is_joining_thread, 1,
+                           offset == 0 ? kNumberOfSweepingSpaces : offset))
+      return;
+  }
+
+  bool SweepNonNewSpaces(ConcurrentSweeper& concurrent_sweeper,
+                         JobDelegate* delegate, bool is_joining_thread,
+                         int first_space_index, int last_space_index) {
+    if (!sweeper_->should_sweep_non_new_spaces_) return true;
+    TRACE_GC_EPOCH(
+        tracer_, sweeper_->GetTracingScope(OLD_SPACE, is_joining_thread),
+        is_joining_thread ? ThreadKind::kMain : ThreadKind::kBackground);
+    for (int i = first_space_index; i < last_space_index; i++) {
+      const AllocationSpace space_id =
+          static_cast<AllocationSpace>(FIRST_SWEEPABLE_SPACE + i);
+      DCHECK_NE(NEW_SPACE, space_id);
+      if (!concurrent_sweeper.ConcurrentSweepSpace(space_id, delegate))
+        return false;
+    }
+    return true;
   }
 
   Sweeper* const sweeper_;
@@ -173,7 +186,9 @@
 void Sweeper::StartSweeping(GarbageCollector collector) {
   DCHECK(local_pretenuring_feedback_.empty());
   sweeping_in_progress_ = true;
-  current_collector_ = collector;
+  if (collector == GarbageCollector::MARK_COMPACTOR)
+    should_sweep_non_new_spaces_ = true;
+  current_new_space_collector_ = collector;
   should_reduce_memory_ = heap_->ShouldReduceMemory();
   ForAllSweepingSpaces([this](AllocationSpace space) {
     // Sorting is done in order to make compaction more efficient: by sweeping
@@ -199,7 +214,7 @@
 }
 
 void Sweeper::StartSweeperTasks() {
-  DCHECK(current_collector_.has_value());
+  DCHECK(current_new_space_collector_.has_value());
   DCHECK(!job_handle_ || !job_handle_->IsValid());
   if (v8_flags.concurrent_sweeping && sweeping_in_progress_ &&
       !heap_->delay_sweeper_tasks_for_testing_) {
@@ -227,13 +242,13 @@
   return nullptr;
 }
 
-void Sweeper::EnsureCompleted(SweepingMode sweeping_mode) {
+void Sweeper::EnsureCompleted() {
   if (!sweeping_in_progress_) return;
 
   // If sweeping is not completed or not running at all, we try to complete it
   // here.
-  ForAllSweepingSpaces([this, sweeping_mode](AllocationSpace space) {
-    ParallelSweepSpace(space, sweeping_mode, 0);
+  ForAllSweepingSpaces([this](AllocationSpace space) {
+    ParallelSweepSpace(space, SweepingMode::kLazyOrConcurrent, 0);
   });
 
   if (job_handle_ && job_handle_->IsValid()) job_handle_->Join();
@@ -244,17 +259,41 @@
 
   pretenuring_handler_->MergeAllocationSitePretenuringFeedback(
       local_pretenuring_feedback_);
+  local_pretenuring_feedback_.clear();
   for (ConcurrentSweeper& concurrent_sweeper : concurrent_sweepers_) {
     pretenuring_handler_->MergeAllocationSitePretenuringFeedback(
         *concurrent_sweeper.local_pretenuring_feedback());
+    // No need to clear the concurrent feedback map since the concurrent sweeper
+    // goes away.
   }
-  local_pretenuring_feedback_.clear();
   concurrent_sweepers_.clear();
 
-  current_collector_.reset();
+  current_new_space_collector_.reset();
+  should_sweep_non_new_spaces_ = false;
   sweeping_in_progress_ = false;
 }
 
+void Sweeper::PauseAndEnsureNewSpaceCompleted() {
+  if (!sweeping_in_progress_) return;
+
+  ParallelSweepSpace(NEW_SPACE, SweepingMode::kLazyOrConcurrent, 0);
+
+  if (job_handle_ && job_handle_->IsValid()) job_handle_->Cancel();
+
+  CHECK(sweeping_list_[GetSweepSpaceIndex(NEW_SPACE)].empty());
+
+  pretenuring_handler_->MergeAllocationSitePretenuringFeedback(
+      local_pretenuring_feedback_);
+  local_pretenuring_feedback_.clear();
+  for (ConcurrentSweeper& concurrent_sweeper : concurrent_sweepers_) {
+    pretenuring_handler_->MergeAllocationSitePretenuringFeedback(
+        *concurrent_sweeper.local_pretenuring_feedback());
+    concurrent_sweeper.local_pretenuring_feedback()->clear();
+  }
+
+  current_new_space_collector_.reset();
+}
+
 void Sweeper::DrainSweepingWorklistForSpace(AllocationSpace space) {
   if (!sweeping_in_progress_) return;
   ParallelSweepSpace(space, SweepingMode::kLazyOrConcurrent, 0);
@@ -359,10 +398,8 @@
   Space* space = p->owner();
   DCHECK_NOT_NULL(space);
   DCHECK(space->identity() == OLD_SPACE || space->identity() == CODE_SPACE ||
-         space->identity() == MAP_SPACE || space->identity() == SHARED_SPACE ||
+         space->identity() == SHARED_SPACE ||
          (space->identity() == NEW_SPACE && v8_flags.minor_mc));
-  DCHECK_IMPLIES(space->identity() == NEW_SPACE,
-                 sweeping_mode == SweepingMode::kEagerDuringGC);
   DCHECK(!p->IsEvacuationCandidate() && !p->SweepingDone());
 
   // Phase 1: Prepare the page for sweeping.
@@ -494,11 +531,11 @@
 
 size_t Sweeper::ConcurrentSweepingPageCount() {
   base::MutexGuard guard(&mutex_);
-  return sweeping_list_[GetSweepSpaceIndex(OLD_SPACE)].size() +
-         sweeping_list_[GetSweepSpaceIndex(MAP_SPACE)].size() +
-         (v8_flags.minor_mc
-              ? sweeping_list_[GetSweepSpaceIndex(NEW_SPACE)].size()
-              : 0);
+  size_t count = 0;
+  for (int i = 0; i < kNumberOfSweepingSpaces; i++) {
+    count += sweeping_list_[i].size();
+  }
+  return count;
 }
 
 int Sweeper::ParallelSweepSpace(AllocationSpace identity,
@@ -616,8 +653,8 @@
                           Sweeper::AddPageMode mode) {
   base::MutexGuard guard(&mutex_);
   DCHECK(IsValidSweepingSpace(space));
-  DCHECK(!v8_flags.concurrent_sweeping || !job_handle_ ||
-         !job_handle_->IsValid());
+  DCHECK_IMPLIES(v8_flags.concurrent_sweeping,
+                 !job_handle_ || !job_handle_->IsValid());
   if (mode == Sweeper::REGULAR) {
     PrepareToBeSweptPage(space, page);
   } else {
@@ -663,5 +700,27 @@
   return page;
 }
 
+GCTracer::Scope::ScopeId Sweeper::GetTracingScope(AllocationSpace space,
+                                                  bool is_joining_thread) {
+  if (space == NEW_SPACE &&
+      current_new_space_collector_ == GarbageCollector::MINOR_MARK_COMPACTOR) {
+    return is_joining_thread ? GCTracer::Scope::MINOR_MC_SWEEP
+                             : GCTracer::Scope::MINOR_MC_BACKGROUND_SWEEPING;
+  }
+  return is_joining_thread ? GCTracer::Scope::MC_SWEEP
+                           : GCTracer::Scope::MC_BACKGROUND_SWEEPING;
+}
+
+GCTracer::Scope::ScopeId Sweeper::GetTracingScopeForCompleteYoungSweep() {
+  return current_new_space_collector_ == GarbageCollector::MINOR_MARK_COMPACTOR
+             ? GCTracer::Scope::MINOR_MC_COMPLETE_SWEEPING
+             : GCTracer::Scope::MC_COMPLETE_SWEEPING;
+}
+
+bool Sweeper::IsSweepingDoneForSpace(AllocationSpace space) {
+  DCHECK(!AreSweeperTasksRunning());
+  return sweeping_list_[GetSweepSpaceIndex(space)].empty();
+}
+
 }  // namespace internal
 }  // namespace v8
diff -r -u --color up/v8/src/heap/sweeper.h nw/v8/src/heap/sweeper.h
--- up/v8/src/heap/sweeper.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/heap/sweeper.h	2023-01-19 16:46:36.254776226 -0500
@@ -13,6 +13,7 @@
 #include "src/base/platform/semaphore.h"
 #include "src/common/globals.h"
 #include "src/flags/flags.h"
+#include "src/heap/gc-tracer.h"
 #include "src/heap/pretenuring-handler.h"
 #include "src/heap/slot-set.h"
 #include "src/tasks/cancelable-task.h"
@@ -106,13 +107,19 @@
   // are not running yet.
   void StartSweeping(GarbageCollector collector);
   V8_EXPORT_PRIVATE void StartSweeperTasks();
-  void EnsureCompleted(
-      SweepingMode sweeping_mode = SweepingMode::kLazyOrConcurrent);
+  void EnsureCompleted();
+  void PauseAndEnsureNewSpaceCompleted();
   void DrainSweepingWorklistForSpace(AllocationSpace space);
   bool AreSweeperTasksRunning();
 
   Page* GetSweptPageSafe(PagedSpaceBase* space);
 
+  bool IsSweepingDoneForSpace(AllocationSpace space);
+
+  GCTracer::Scope::ScopeId GetTracingScope(AllocationSpace space,
+                                           bool is_joining_thread);
+  GCTracer::Scope::ScopeId GetTracingScopeForCompleteYoungSweep();
+
  private:
   NonAtomicMarkingState* marking_state() const { return marking_state_; }
 
@@ -123,7 +130,7 @@
 
   static const int kNumberOfSweepingSpaces =
       LAST_SWEEPABLE_SPACE - FIRST_SWEEPABLE_SPACE + 1;
-  static constexpr int kMaxSweeperTasks = 3;
+  static constexpr int kMaxSweeperTasks = kNumberOfSweepingSpaces;
 
   template <typename Callback>
   void ForAllSweepingSpaces(Callback callback) const {
@@ -132,7 +139,6 @@
     }
     callback(OLD_SPACE);
     callback(CODE_SPACE);
-    callback(MAP_SPACE);
     callback(SHARED_SPACE);
   }
 
@@ -201,9 +207,10 @@
   // path checks this flag to see whether it could support concurrent sweeping.
   std::atomic<bool> sweeping_in_progress_;
   bool should_reduce_memory_;
+  bool should_sweep_non_new_spaces_ = false;
   PretenturingHandler* const pretenuring_handler_;
   PretenturingHandler::PretenuringFeedbackMap local_pretenuring_feedback_;
-  base::Optional<GarbageCollector> current_collector_;
+  base::Optional<GarbageCollector> current_new_space_collector_;
 };
 
 }  // namespace internal
diff -r -u --color up/v8/src/ic/accessor-assembler.cc nw/v8/src/ic/accessor-assembler.cc
--- up/v8/src/ic/accessor-assembler.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/ic/accessor-assembler.cc	2023-01-19 16:46:36.254776226 -0500
@@ -1231,8 +1231,7 @@
       DecodeWordFromWord32<StoreHandler::FieldIndexBits>(handler_word);
   TNode<IntPtrT> offset = Signed(TimesTaggedSize(index));
 
-  StoreJSSharedStructInObjectField(property_storage, offset,
-                                   shared_value.value());
+  StoreSharedObjectField(property_storage, offset, shared_value.value());
 
   // Return the original value.
   Return(value);
@@ -1758,8 +1757,7 @@
   BIND(&inobject);
   {
     TNode<IntPtrT> field_offset = Signed(TimesTaggedSize(field_index));
-    StoreJSSharedStructInObjectField(shared_struct, field_offset,
-                                     shared_value.value());
+    StoreSharedObjectField(shared_struct, field_offset, shared_value.value());
     Goto(&done);
   }
 
@@ -1768,7 +1766,10 @@
     TNode<IntPtrT> backing_store_index =
         Signed(IntPtrSub(field_index, instance_size_in_words));
 
-    Label tagged_rep(this), double_rep(this);
+    CSA_DCHECK(
+        this,
+        Word32Equal(DecodeWord32<PropertyDetails::RepresentationField>(details),
+                    Int32Constant(Representation::kTagged)));
     TNode<PropertyArray> properties =
         CAST(LoadFastProperties(CAST(shared_struct)));
     StoreJSSharedStructPropertyArrayElement(properties, backing_store_index,
diff -r -u --color up/v8/src/ic/binary-op-assembler.cc nw/v8/src/ic/binary-op-assembler.cc
--- up/v8/src/ic/binary-op-assembler.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/ic/binary-op-assembler.cc	2023-01-19 16:46:36.265609556 -0500
@@ -437,37 +437,72 @@
 
   if (Is64()) {
     BIND(&if_both_bigint64);
-    // TODO(panq): Remove the condition when all the operations are supported.
-    if (op == Operation::kSubtract || op == Operation::kMultiply) {
-      var_type_feedback = SmiConstant(BinaryOperationFeedback::kBigInt64);
-      UpdateFeedback(var_type_feedback.value(), maybe_feedback_vector(),
-                     slot_id, update_feedback_mode);
-
-      TVARIABLE(UintPtrT, lhs_raw);
-      TVARIABLE(UintPtrT, rhs_raw);
-      BigIntToRawBytes(CAST(lhs), &lhs_raw, &lhs_raw);
-      BigIntToRawBytes(CAST(rhs), &rhs_raw, &rhs_raw);
-
-      switch (op) {
-        case Operation::kSubtract: {
-          var_result = BigIntFromInt64(TryIntPtrSub(
-              UncheckedCast<IntPtrT>(lhs_raw.value()),
-              UncheckedCast<IntPtrT>(rhs_raw.value()), &if_both_bigint));
-          Goto(&end);
-          break;
+    var_type_feedback = SmiConstant(BinaryOperationFeedback::kBigInt64);
+    UpdateFeedback(var_type_feedback.value(), maybe_feedback_vector(), slot_id,
+                   update_feedback_mode);
+
+    TVARIABLE(UintPtrT, lhs_raw);
+    TVARIABLE(UintPtrT, rhs_raw);
+    BigIntToRawBytes(CAST(lhs), &lhs_raw, &lhs_raw);
+    BigIntToRawBytes(CAST(rhs), &rhs_raw, &rhs_raw);
+
+    switch (op) {
+      case Operation::kSubtract: {
+        var_result = BigIntFromInt64(TryIntPtrSub(
+            UncheckedCast<IntPtrT>(lhs_raw.value()),
+            UncheckedCast<IntPtrT>(rhs_raw.value()), &if_both_bigint));
+        Goto(&end);
+        break;
+      }
+      case Operation::kMultiply: {
+        var_result = BigIntFromInt64(TryIntPtrMul(
+            UncheckedCast<IntPtrT>(lhs_raw.value()),
+            UncheckedCast<IntPtrT>(rhs_raw.value()), &if_both_bigint));
+        Goto(&end);
+        break;
+      }
+      case Operation::kDivide: {
+        // No need to check overflow because INT_MIN is excluded
+        // from the range of small BigInts.
+        Label if_div_zero(this);
+        var_result = BigIntFromInt64(TryIntPtrDiv(
+            UncheckedCast<IntPtrT>(lhs_raw.value()),
+            UncheckedCast<IntPtrT>(rhs_raw.value()), &if_div_zero));
+        Goto(&end);
+
+        BIND(&if_div_zero);
+        {
+          // Update feedback to prevent deopt loop.
+          UpdateFeedback(SmiConstant(BinaryOperationFeedback::kAny),
+                         maybe_feedback_vector(), slot_id,
+                         update_feedback_mode);
+          ThrowRangeError(context(), MessageTemplate::kBigIntDivZero);
         }
-        case Operation::kMultiply: {
-          var_result = BigIntFromInt64(TryIntPtrMul(
-              UncheckedCast<IntPtrT>(lhs_raw.value()),
-              UncheckedCast<IntPtrT>(rhs_raw.value()), &if_both_bigint));
-          Goto(&end);
-          break;
+        break;
+      }
+      case Operation::kModulus: {
+        Label if_div_zero(this);
+        var_result = BigIntFromInt64(TryIntPtrMod(
+            UncheckedCast<IntPtrT>(lhs_raw.value()),
+            UncheckedCast<IntPtrT>(rhs_raw.value()), &if_div_zero));
+        Goto(&end);
+
+        BIND(&if_div_zero);
+        {
+          // Update feedback to prevent deopt loop.
+          UpdateFeedback(SmiConstant(BinaryOperationFeedback::kAny),
+                         maybe_feedback_vector(), slot_id,
+                         update_feedback_mode);
+          ThrowRangeError(context(), MessageTemplate::kBigIntDivZero);
         }
-        default:
-          UNREACHABLE();
+        break;
+      }
+      case Operation::kExponentiate: {
+        Goto(&if_both_bigint);
+        break;
       }
-    } else {
-      Goto(&if_both_bigint);
+      default:
+        UNREACHABLE();
     }
   }
 
@@ -499,7 +534,7 @@
 
         GotoIfNot(TaggedIsSmi(var_result.value()), &end);
 
-        // Check for sentinel that signals TerminationReqeusted exception.
+        // Check for sentinel that signals TerminationRequested exception.
         GotoIf(TaggedEqual(var_result.value(), SmiConstant(1)),
                &termination_requested);
 
@@ -521,7 +556,7 @@
 
         GotoIfNot(TaggedIsSmi(var_result.value()), &end);
 
-        // Check for sentinel that signals TerminationReqeusted exception.
+        // Check for sentinel that signals TerminationRequested exception.
         GotoIf(TaggedEqual(var_result.value(), SmiConstant(1)),
                &termination_requested);
 
@@ -535,30 +570,37 @@
         TerminateExecution(context());
         break;
       }
-      case Operation::kBitwiseAnd: {
-        Label bigint_too_big(this);
+      case Operation::kModulus: {
+        Label bigint_div_zero(this),
+            termination_requested(this, Label::kDeferred);
         var_result =
-            CallBuiltin(Builtin::kBigIntBitwiseAndNoThrow, context(), lhs, rhs);
+            CallBuiltin(Builtin::kBigIntModulusNoThrow, context(), lhs, rhs);
 
-        // Check for sentinel that signals BigIntTooBig exception.
-        GotoIf(TaggedIsSmi(var_result.value()), &bigint_too_big);
-        Goto(&end);
+        GotoIfNot(TaggedIsSmi(var_result.value()), &end);
 
-        BIND(&bigint_too_big);
-        {
-          // Update feedback to prevent deopt loop.
-          UpdateFeedback(SmiConstant(BinaryOperationFeedback::kAny),
-                         maybe_feedback_vector(), slot_id,
-                         update_feedback_mode);
-          ThrowRangeError(context(), MessageTemplate::kBigIntTooBig);
-        }
+        // Check for sentinel that signals TerminationRequested exception.
+        GotoIf(TaggedEqual(var_result.value(), SmiConstant(1)),
+               &termination_requested);
+
+        // Handles BigIntDivZero exception.
+        // Update feedback to prevent deopt loop.
+        UpdateFeedback(SmiConstant(BinaryOperationFeedback::kAny),
+                       maybe_feedback_vector(), slot_id, update_feedback_mode);
+        ThrowRangeError(context(), MessageTemplate::kBigIntDivZero);
+
+        BIND(&termination_requested);
+        TerminateExecution(context());
         break;
       }
-      default: {
+      case Operation::kExponentiate: {
+        // TODO(panq): replace the runtime with builtin once it is implemented.
         var_result = CallRuntime(Runtime::kBigIntBinaryOp, context(), lhs, rhs,
                                  SmiConstant(op));
         Goto(&end);
+        break;
       }
+      default:
+        UNREACHABLE();
     }
   }
 
diff -r -u --color up/v8/src/init/bootstrapper.cc nw/v8/src/init/bootstrapper.cc
--- up/v8/src/init/bootstrapper.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/init/bootstrapper.cc	2023-01-19 16:46:36.265609556 -0500
@@ -23,6 +23,7 @@
 #include "src/extensions/trigger-failure-extension.h"
 #include "src/logging/runtime-call-stats-scope.h"
 #include "src/objects/instance-type.h"
+#include "src/objects/js-array.h"
 #include "src/objects/objects.h"
 #include "src/sandbox/testing.h"
 #ifdef ENABLE_VTUNE_TRACEMARK
@@ -5584,7 +5585,7 @@
 
 #ifdef V8_INTL_SUPPORT
 void Genesis::InitializeGlobal_harmony_intl_duration_format() {
-  if (!FLAG_harmony_intl_duration_format) return;
+  if (!v8_flags.harmony_intl_duration_format) return;
   Handle<JSObject> intl = Handle<JSObject>::cast(
       JSReceiver::GetProperty(
           isolate(),
@@ -5868,9 +5869,10 @@
     Handle<Map> template_map(array_function->initial_map(), isolate_);
     template_map = Map::CopyAsElementsKind(isolate_, template_map,
                                            PACKED_ELEMENTS, OMIT_TRANSITION);
-    template_map->set_instance_size(template_map->instance_size() +
-                                    kTaggedSize);
-    // Temporarily instantiate full template_literal_object to get the final
+    DCHECK_GE(TemplateLiteralObject::kHeaderSize,
+              template_map->instance_size());
+    template_map->set_instance_size(TemplateLiteralObject::kHeaderSize);
+    // Temporarily instantiate a full template_literal_object to get the final
     // map.
     auto template_object =
         Handle<JSArray>::cast(factory()->NewJSObjectFromMap(template_map));
@@ -5893,20 +5895,56 @@
                                factory()->raw_string(), &raw_desc,
                                Just(kThrowOnError))
         .ToChecked();
+    // Install private symbol fields for function_literal_id and slot_id.
+    raw_desc.set_value(handle(Smi::zero(), isolate()));
+    JSArray::DefineOwnProperty(
+        isolate(), template_object,
+        factory()->template_literal_function_literal_id_symbol(), &raw_desc,
+        Just(kThrowOnError))
+        .ToChecked();
+    JSArray::DefineOwnProperty(isolate(), template_object,
+                               factory()->template_literal_slot_id_symbol(),
+                               &raw_desc, Just(kThrowOnError))
+        .ToChecked();
 
     // Freeze the {template_object} as well.
     JSObject::SetIntegrityLevel(template_object, FROZEN, kThrowOnError)
         .ToChecked();
     {
       DisallowGarbageCollection no_gc;
-      // Verify TemplateLiteralObject::kRawFieldOffset
       DescriptorArray desc = template_object->map().instance_descriptors();
-      InternalIndex descriptor_index =
-          desc.Search(*factory()->raw_string(), desc.number_of_descriptors());
-      FieldIndex index =
-          FieldIndex::ForDescriptor(template_object->map(), descriptor_index);
-      CHECK(index.is_inobject());
-      CHECK_EQ(index.offset(), TemplateLiteralObject::kRawFieldOffset);
+      {
+        // Verify TemplateLiteralObject::kRawOffset
+        InternalIndex descriptor_index =
+            desc.Search(*factory()->raw_string(), desc.number_of_descriptors());
+        FieldIndex index =
+            FieldIndex::ForDescriptor(template_object->map(), descriptor_index);
+        CHECK(index.is_inobject());
+        CHECK_EQ(index.offset(), TemplateLiteralObject::kRawOffset);
+      }
+
+      {
+        // Verify TemplateLiteralObject::kFunctionLiteralIdOffset
+        InternalIndex descriptor_index = desc.Search(
+            *factory()->template_literal_function_literal_id_symbol(),
+            desc.number_of_descriptors());
+        FieldIndex index =
+            FieldIndex::ForDescriptor(template_object->map(), descriptor_index);
+        CHECK(index.is_inobject());
+        CHECK_EQ(index.offset(),
+                 TemplateLiteralObject::kFunctionLiteralIdOffset);
+      }
+
+      {
+        // Verify TemplateLiteralObject::kSlotIdOffset
+        InternalIndex descriptor_index =
+            desc.Search(*factory()->template_literal_slot_id_symbol(),
+                        desc.number_of_descriptors());
+        FieldIndex index =
+            FieldIndex::ForDescriptor(template_object->map(), descriptor_index);
+        CHECK(index.is_inobject());
+        CHECK_EQ(index.offset(), TemplateLiteralObject::kSlotIdOffset);
+      }
     }
 
     native_context()->set_js_array_template_literal_object_map(
diff -r -u --color up/v8/src/init/heap-symbols.h nw/v8/src/init/heap-symbols.h
--- up/v8/src/init/heap-symbols.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/init/heap-symbols.h	2023-01-19 16:46:36.265609556 -0500
@@ -226,6 +226,7 @@
   V(_, dot_for_string, ".for")                                       \
   V(_, dot_generator_object_string, ".generator_object")             \
   V(_, dot_home_object_string, ".home_object")                       \
+  V(_, dot_new_target_string, ".new.target")                         \
   V(_, dot_result_string, ".result")                                 \
   V(_, dot_repl_result_string, ".repl_result")                       \
   V(_, dot_static_home_object_string, ".static_home_object")         \
@@ -456,41 +457,43 @@
   V(_, years_string, "years")                                        \
   V(_, zero_string, "0")
 
-#define PRIVATE_SYMBOL_LIST_GENERATOR(V, _)    \
-  V(_, array_buffer_wasm_memory_symbol)        \
-  V(_, call_site_info_symbol)                  \
-  V(_, console_context_id_symbol)              \
-  V(_, console_context_name_symbol)            \
-  V(_, class_fields_symbol)                    \
-  V(_, class_positions_symbol)                 \
-  V(_, elements_transition_symbol)             \
-  V(_, error_end_pos_symbol)                   \
-  V(_, error_script_symbol)                    \
-  V(_, error_stack_symbol)                     \
-  V(_, error_start_pos_symbol)                 \
-  V(_, frozen_symbol)                          \
-  V(_, interpreter_trampoline_symbol)          \
-  V(_, mega_dom_symbol)                        \
-  V(_, megamorphic_symbol)                     \
-  V(_, native_context_index_symbol)            \
-  V(_, nonextensible_symbol)                   \
-  V(_, not_mapped_symbol)                      \
-  V(_, promise_debug_marker_symbol)            \
-  V(_, promise_debug_message_symbol)           \
-  V(_, promise_forwarding_handler_symbol)      \
-  V(_, promise_handled_by_symbol)              \
-  V(_, promise_awaited_by_symbol)              \
-  V(_, regexp_result_names_symbol)             \
-  V(_, regexp_result_regexp_input_symbol)      \
-  V(_, regexp_result_regexp_last_index_symbol) \
-  V(_, sealed_symbol)                          \
-  V(_, strict_function_transition_symbol)      \
-  V(_, wasm_exception_tag_symbol)              \
-  V(_, wasm_exception_values_symbol)           \
-  V(_, wasm_uncatchable_symbol)                \
-  V(_, wasm_wrapped_object_symbol)             \
-  V(_, wasm_debug_proxy_cache_symbol)          \
-  V(_, wasm_debug_proxy_names_symbol)          \
+#define PRIVATE_SYMBOL_LIST_GENERATOR(V, _)         \
+  V(_, array_buffer_wasm_memory_symbol)             \
+  V(_, call_site_info_symbol)                       \
+  V(_, console_context_id_symbol)                   \
+  V(_, console_context_name_symbol)                 \
+  V(_, class_fields_symbol)                         \
+  V(_, class_positions_symbol)                      \
+  V(_, elements_transition_symbol)                  \
+  V(_, error_end_pos_symbol)                        \
+  V(_, error_script_symbol)                         \
+  V(_, error_stack_symbol)                          \
+  V(_, error_start_pos_symbol)                      \
+  V(_, frozen_symbol)                               \
+  V(_, interpreter_trampoline_symbol)               \
+  V(_, mega_dom_symbol)                             \
+  V(_, megamorphic_symbol)                          \
+  V(_, native_context_index_symbol)                 \
+  V(_, nonextensible_symbol)                        \
+  V(_, not_mapped_symbol)                           \
+  V(_, promise_debug_marker_symbol)                 \
+  V(_, promise_debug_message_symbol)                \
+  V(_, promise_forwarding_handler_symbol)           \
+  V(_, promise_handled_by_symbol)                   \
+  V(_, promise_awaited_by_symbol)                   \
+  V(_, regexp_result_names_symbol)                  \
+  V(_, regexp_result_regexp_input_symbol)           \
+  V(_, regexp_result_regexp_last_index_symbol)      \
+  V(_, sealed_symbol)                               \
+  V(_, strict_function_transition_symbol)           \
+  V(_, template_literal_function_literal_id_symbol) \
+  V(_, template_literal_slot_id_symbol)             \
+  V(_, wasm_exception_tag_symbol)                   \
+  V(_, wasm_exception_values_symbol)                \
+  V(_, wasm_uncatchable_symbol)                     \
+  V(_, wasm_wrapped_object_symbol)                  \
+  V(_, wasm_debug_proxy_cache_symbol)               \
+  V(_, wasm_debug_proxy_names_symbol)               \
   V(_, uninitialized_symbol)
 
 #define PUBLIC_SYMBOL_LIST_GENERATOR(V, _)                \
@@ -565,7 +568,7 @@
   MINOR_INCREMENTAL_SCOPES(F)                        \
   F(HEAP_EMBEDDER_TRACING_EPILOGUE)                  \
   F(HEAP_EPILOGUE)                                   \
-  F(HEAP_EPILOGUE_REDUCE_NEW_SPACE)                  \
+  F(HEAP_EPILOGUE_ADJUST_NEW_SPACE)                  \
   F(HEAP_EPILOGUE_SAFEPOINT)                         \
   F(HEAP_EXTERNAL_EPILOGUE)                          \
   F(HEAP_EXTERNAL_NEAR_HEAP_LIMIT)                   \
@@ -618,7 +621,6 @@
   F(MC_MARK_WEAK_CLOSURE_EPHEMERON_LINEAR)           \
   F(MC_SWEEP_CODE)                                   \
   F(MC_SWEEP_CODE_LO)                                \
-  F(MC_SWEEP_FINISH_NEW)                             \
   F(MC_SWEEP_LO)                                     \
   F(MC_SWEEP_MAP)                                    \
   F(MC_SWEEP_NEW)                                    \
@@ -630,6 +632,7 @@
   TOP_MINOR_MC_SCOPES(F)                             \
   F(MINOR_MC_CLEAR_STRING_TABLE)                     \
   F(MINOR_MC_COMPLETE_SWEEP_ARRAY_BUFFERS)           \
+  F(MINOR_MC_COMPLETE_SWEEPING)                      \
   F(MINOR_MC_EVACUATE_CLEAN_UP)                      \
   F(MINOR_MC_EVACUATE_COPY)                          \
   F(MINOR_MC_EVACUATE_COPY_PARALLEL)                 \
@@ -650,7 +653,6 @@
   F(MINOR_MC_MARK_CLOSURE)                           \
   F(MINOR_MC_SWEEP_NEW)                              \
   F(MINOR_MC_SWEEP_NEW_LO)                           \
-  F(MINOR_MC_SWEEP_FINISH_NEW)                       \
   F(SAFEPOINT)                                       \
   F(SCAVENGER)                                       \
   F(SCAVENGER_COMPLETE_SWEEP_ARRAY_BUFFERS)          \
diff -r -u --color up/v8/src/init/icu_util.cc nw/v8/src/init/icu_util.cc
--- up/v8/src/init/icu_util.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/init/icu_util.cc	2023-01-19 16:46:36.265609556 -0500
@@ -36,6 +36,10 @@
 }  // namespace
 #endif
 
+void* RawICUData() {
+  return (void*)g_icu_data_ptr;
+}
+
 bool InitializeICUDefaultLocation(const char* exec_path,
                                   const char* icu_data_file) {
 #if !defined(V8_INTL_SUPPORT)
diff -r -u --color up/v8/src/init/icu_util.h nw/v8/src/init/icu_util.h
--- up/v8/src/init/icu_util.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/init/icu_util.h	2023-01-19 16:46:36.265609556 -0500
@@ -18,6 +18,8 @@
 bool InitializeICUDefaultLocation(const char* exec_path,
                                   const char* icu_data_file);
 
+void* RawICUData();
+
 }  // namespace internal
 }  // namespace v8
 
diff -r -u --color up/v8/src/init/startup-data-util.cc nw/v8/src/init/startup-data-util.cc
--- up/v8/src/init/startup-data-util.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/init/startup-data-util.cc	2023-01-19 16:46:36.265609556 -0500
@@ -77,10 +77,23 @@
 
 void InitializeExternalStartupData(const char* directory_path) {
 #ifdef V8_USE_EXTERNAL_STARTUP_DATA
+#if 0
   const char* snapshot_name = "snapshot_blob.bin";
   std::unique_ptr<char[]> snapshot =
       base::RelativePath(directory_path, snapshot_name);
   LoadFromFile(snapshot.get());
+#endif
+#ifdef __APPLE__
+#if V8_TARGET_ARCH_X64
+  const char* snapshot_name = "v8_context_snapshot.x86_64.bin";
+#else
+  const char* snapshot_name = "v8_context_snapshot.arm64.bin";
+#endif
+#else
+  const char* snapshot_name = "v8_context_snapshot.bin";
+#endif
+  std::unique_ptr<char[]> snapshot = base::RelativePath(directory_path, snapshot_name);
+  LoadFromFile(snapshot.get());
 #endif  // V8_USE_EXTERNAL_STARTUP_DATA
 }
 
diff -r -u --color up/v8/src/init/v8.cc nw/v8/src/init/v8.cc
--- up/v8/src/init/v8.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/init/v8.cc	2023-01-19 16:46:36.265609556 -0500
@@ -73,8 +73,9 @@
     // isolate->Dispose();
     // v8::V8::Dispose();
     // v8::V8::DisposePlatform();
-    FATAL("Wrong initialization order: got %d expected %d!",
-          static_cast<int>(current_state), static_cast<int>(next_state));
+    FATAL("Wrong initialization order: from %d to %d, expected to %d!",
+          static_cast<int>(current_state), static_cast<int>(next_state),
+          static_cast<int>(expected_next_state));
   }
   if (!v8_startup_state_.compare_exchange_strong(current_state, next_state)) {
     FATAL(
@@ -91,11 +92,20 @@
 V8_DECLARE_ONCE(init_snapshot_once);
 #endif
 
+base::Thread::LocalStorageKey platform_tls_key_;
+void V8::SetTLSPlatform(v8::Platform* platform) {
+  base::Thread::SetThreadLocal(platform_tls_key_, platform);
+}
+
 void V8::InitializePlatform(v8::Platform* platform) {
   AdvanceStartupState(V8StartupState::kPlatformInitializing);
   CHECK(!platform_);
   CHECK_NOT_NULL(platform);
   platform_ = platform;
+
+  platform_tls_key_ = base::Thread::CreateThreadLocalKey();
+  base::Thread::SetThreadLocal(platform_tls_key_, platform);
+
   v8::base::SetPrintStackTrace(platform_->GetStackTracePrinter());
   v8::tracing::TracingCategoryObserver::SetUp();
 #if defined(V8_OS_WIN) && defined(V8_ENABLE_ETW_STACK_WALKING)
@@ -243,7 +253,6 @@
 #endif
   IsolateAllocator::InitializeOncePerProcess();
   Isolate::InitializeOncePerProcess();
-
 #if defined(USE_SIMULATOR)
   Simulator::InitializeOncePerProcess();
 #endif
@@ -280,7 +289,6 @@
   CallDescriptors::TearDown();
   ElementsAccessor::TearDown();
   RegisteredExtension::UnregisterAll();
-  Isolate::DisposeOncePerProcess();
   FlagList::ReleaseDynamicAllocations();
   AdvanceStartupState(V8StartupState::kV8Disposed);
 }
@@ -307,7 +315,11 @@
 }
 
 v8::Platform* V8::GetCurrentPlatform() {
-  v8::Platform* platform = reinterpret_cast<v8::Platform*>(
+  v8::Platform* platform;
+  platform = reinterpret_cast<v8::Platform*>(base::Thread::GetThreadLocal(platform_tls_key_));
+  if (platform)
+    return platform;
+  platform = reinterpret_cast<v8::Platform*>(
       base::Relaxed_Load(reinterpret_cast<base::AtomicWord*>(&platform_)));
   DCHECK(platform);
   return platform;
diff -r -u --color up/v8/src/init/v8.h nw/v8/src/init/v8.h
--- up/v8/src/init/v8.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/init/v8.h	2023-01-19 16:46:36.265609556 -0500
@@ -42,6 +42,7 @@
   [[noreturn]] V8_EXPORT_PRIVATE static void FatalProcessOutOfMemory(
       Isolate* isolate, const char* location, const char* detail);
 
+  static void SetTLSPlatform(v8::Platform* platform);
   static void InitializePlatform(v8::Platform* platform);
   static void DisposePlatform();
   V8_EXPORT_PRIVATE static v8::Platform* GetCurrentPlatform();
diff -r -u --color up/v8/src/inspector/custom-preview.cc nw/v8/src/inspector/custom-preview.cc
--- up/v8/src/inspector/custom-preview.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/inspector/custom-preview.cc	2023-01-19 16:46:36.265609556 -0500
@@ -260,7 +260,7 @@
   }
 
   v8::Isolate* isolate = context->GetIsolate();
-  v8::MicrotasksScope microtasksScope(isolate,
+  v8::MicrotasksScope microtasksScope(context,
                                       v8::MicrotasksScope::kDoNotRunMicrotasks);
   v8::TryCatch tryCatch(isolate);
 
diff -r -u --color up/v8/src/inspector/injected-script.cc nw/v8/src/inspector/injected-script.cc
--- up/v8/src/inspector/injected-script.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/inspector/injected-script.cc	2023-01-19 16:46:36.265609556 -0500
@@ -713,7 +713,7 @@
     return;
   }
 
-  v8::MicrotasksScope microtasksScope(m_context->isolate(),
+  v8::MicrotasksScope microtasksScope(m_context->context(),
                                       v8::MicrotasksScope::kRunMicrotasks);
   ProtocolPromiseHandler::add(session, m_context->context(),
                               value.ToLocalChecked(), m_context->contextId(),
diff -r -u --color up/v8/src/inspector/inspected-context.cc nw/v8/src/inspector/inspected-context.cc
--- up/v8/src/inspector/inspected-context.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/inspector/inspected-context.cc	2023-01-19 16:46:36.276442886 -0500
@@ -76,10 +76,8 @@
     return;
   }
 
-  if (v8::debug::isExperimentalAsyncStackTaggingApiEnabled()) {
-    m_inspector->console()->installAsyncStackTaggingAPI(
-        info.context, console.As<v8::Object>());
-  }
+  m_inspector->console()->installAsyncStackTaggingAPI(info.context,
+                                                      console.As<v8::Object>());
 
   if (info.hasMemoryOnConsole) {
     m_inspector->console()->installMemoryGetter(info.context,
diff -r -u --color up/v8/src/inspector/v8-console.cc nw/v8/src/inspector/v8-console.cc
--- up/v8/src/inspector/v8-console.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/inspector/v8-console.cc	2023-01-19 16:46:36.276442886 -0500
@@ -824,7 +824,7 @@
   v8::Isolate* isolate = context->GetIsolate();
   v8::Local<v8::External> data = v8::External::New(isolate, this);
 
-  v8::MicrotasksScope microtasksScope(isolate,
+  v8::MicrotasksScope microtasksScope(context,
                                       v8::MicrotasksScope::kDoNotRunMicrotasks);
 
   createBoundFunctionProperty(context, console, data, "createTask",
@@ -834,7 +834,7 @@
 v8::Local<v8::Object> V8Console::createCommandLineAPI(
     v8::Local<v8::Context> context, int sessionId) {
   v8::Isolate* isolate = context->GetIsolate();
-  v8::MicrotasksScope microtasksScope(isolate,
+  v8::MicrotasksScope microtasksScope(context,
                                       v8::MicrotasksScope::kDoNotRunMicrotasks);
 
   v8::Local<v8::Object> commandLineAPI = v8::Object::New(isolate);
@@ -933,7 +933,7 @@
   if (isCommandLineAPIGetter(
           toProtocolStringWithTypeCheck(info.GetIsolate(), name))) {
     DCHECK(value->IsFunction());
-    v8::MicrotasksScope microtasks(info.GetIsolate(),
+    v8::MicrotasksScope microtasks(context,
                                    v8::MicrotasksScope::kDoNotRunMicrotasks);
     if (value.As<v8::Function>()
             ->Call(context, commandLineAPI, 0, nullptr)
@@ -964,7 +964,7 @@
       m_commandLineAPI(commandLineAPI),
       m_global(global),
       m_installedMethods(v8::Set::New(context->GetIsolate())) {
-  v8::MicrotasksScope microtasksScope(context->GetIsolate(),
+  v8::MicrotasksScope microtasksScope(context,
                                       v8::MicrotasksScope::kDoNotRunMicrotasks);
   v8::Local<v8::Array> names;
   if (!m_commandLineAPI->GetOwnPropertyNames(context).ToLocal(&names)) return;
@@ -996,7 +996,7 @@
 V8Console::CommandLineAPIScope::~CommandLineAPIScope() {
   auto isolate = m_context->GetIsolate();
   if (isolate->IsExecutionTerminating()) return;
-  v8::MicrotasksScope microtasksScope(isolate,
+  v8::MicrotasksScope microtasksScope(m_context,
                                       v8::MicrotasksScope::kDoNotRunMicrotasks);
   *static_cast<CommandLineAPIScope**>(
       m_thisReference->GetBackingStore()->Data()) = nullptr;
diff -r -u --color up/v8/src/inspector/v8-debugger-agent-impl.cc nw/v8/src/inspector/v8-debugger-agent-impl.cc
--- up/v8/src/inspector/v8-debugger-agent-impl.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/inspector/v8-debugger-agent-impl.cc	2023-01-19 16:46:36.276442886 -0500
@@ -793,7 +793,7 @@
       return Response::ServerError("Cannot retrive script context");
     }
     v8::Context::Scope contextScope(inspected->context());
-    v8::MicrotasksScope microtasks(m_isolate,
+    v8::MicrotasksScope microtasks(inspected->context(),
                                    v8::MicrotasksScope::kDoNotRunMicrotasks);
     v8::TryCatch tryCatch(m_isolate);
     it->second->getPossibleBreakpoints(
@@ -1328,9 +1328,16 @@
 
 Response V8DebuggerAgentImpl::pause() {
   if (!enabled()) return Response::ServerError(kDebuggerNotEnabled);
-  if (isPaused()) return Response::Success();
 
-  if (m_debugger->canBreakProgram()) {
+  if (m_debugger->isInInstrumentationPause()) {
+    // If we are inside an instrumentation pause, remember the pause request
+    // so that we can enter the requested pause once we are done
+    // with the instrumentation.
+    m_debugger->requestPauseAfterInstrumentation();
+  } else if (isPaused()) {
+    // Ignore the pause request if we are already paused.
+    return Response::Success();
+  } else if (m_debugger->canBreakProgram()) {
     m_debugger->interruptAndBreak(m_session->contextGroupId());
   } else {
     pushBreakDetails(protocol::Debugger::Paused::ReasonEnum::Other, nullptr);
@@ -1403,6 +1410,8 @@
     pauseState = v8::debug::NoBreakOnException;
   } else if (stringPauseState == "all") {
     pauseState = v8::debug::BreakOnAnyException;
+  } else if (stringPauseState == "caught") {
+    pauseState = v8::debug::BreakOnCaughtException;
   } else if (stringPauseState == "uncaught") {
     pauseState = v8::debug::BreakOnUncaughtException;
   } else {
@@ -2061,6 +2070,7 @@
   if (!response.IsSuccess())
     protocolCallFrames = std::make_unique<Array<CallFrame>>();
 
+  v8::debug::NotifyDebuggerPausedEventSent(m_debugger->isolate());
   m_frontend.paused(std::move(protocolCallFrames), breakReason,
                     std::move(breakAuxData), std::move(hitBreakpointIds),
                     currentAsyncStackTrace(), currentExternalStackTrace());
diff -r -u --color up/v8/src/inspector/v8-debugger-agent-impl.h nw/v8/src/inspector/v8-debugger-agent-impl.h
--- up/v8/src/inspector/v8-debugger-agent-impl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/inspector/v8-debugger-agent-impl.h	2023-01-19 16:46:36.276442886 -0500
@@ -232,9 +232,6 @@
   ScriptsMap m_scripts;
   BreakpointIdToDebuggerBreakpointIdsMap m_breakpointIdToDebuggerBreakpointIds;
   DebuggerBreakpointIdToBreakpointIdMap m_debuggerBreakpointIdToBreakpointId;
-  std::unordered_map<v8::debug::BreakpointId,
-                     std::unique_ptr<protocol::DictionaryValue>>
-      m_breakpointsOnScriptRun;
   std::map<String16, std::unique_ptr<DisassemblyCollectorImpl>>
       m_wasmDisassemblies;
   size_t m_nextWasmDisassemblyStreamId = 0;
diff -r -u --color up/v8/src/inspector/v8-debugger.cc nw/v8/src/inspector/v8-debugger.cc
--- up/v8/src/inspector/v8-debugger.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/inspector/v8-debugger.cc	2023-01-19 16:46:36.276442886 -0500
@@ -202,6 +202,10 @@
   return v8::debug::CanBreakProgram(m_isolate);
 }
 
+bool V8Debugger::isInInstrumentationPause() const {
+  return m_instrumentationPause;
+}
+
 void V8Debugger::breakProgram(int targetContextGroupId) {
   DCHECK(canBreakProgram());
   // Don't allow nested breaks.
@@ -225,6 +229,10 @@
       nullptr);
 }
 
+void V8Debugger::requestPauseAfterInstrumentation() {
+  m_requestedPauseAfterInstrumentation = true;
+}
+
 void V8Debugger::continueProgram(int targetContextGroupId,
                                  bool terminateOnResume) {
   if (m_pausedContextGroupId != targetContextGroupId) return;
@@ -510,11 +518,11 @@
       });
 }
 
-void V8Debugger::BreakOnInstrumentation(
+V8Debugger::PauseAfterInstrumentation V8Debugger::BreakOnInstrumentation(
     v8::Local<v8::Context> pausedContext,
     v8::debug::BreakpointId instrumentationId) {
   // Don't allow nested breaks.
-  if (isPaused()) return;
+  if (isPaused()) return kNoPauseAfterInstrumentationRequested;
 
   int contextGroupId = m_inspector->contextGroupId(pausedContext);
   bool hasAgents = false;
@@ -523,9 +531,10 @@
         if (session->debuggerAgent()->acceptsPause(false /* isOOMBreak */))
           hasAgents = true;
       });
-  if (!hasAgents) return;
+  if (!hasAgents) return kNoPauseAfterInstrumentationRequested;
 
   m_pausedContextGroupId = contextGroupId;
+  m_instrumentationPause = true;
   m_inspector->forEachSession(
       contextGroupId, [instrumentationId](V8InspectorSessionImpl* session) {
         if (session->debuggerAgent()->acceptsPause(false /* isOOMBreak */)) {
@@ -536,14 +545,22 @@
   {
     v8::Context::Scope scope(pausedContext);
     m_inspector->client()->runMessageLoopOnInstrumentationPause(contextGroupId);
-    m_pausedContextGroupId = 0;
   }
+  bool requestedPauseAfterInstrumentation =
+      m_requestedPauseAfterInstrumentation;
+
+  m_requestedPauseAfterInstrumentation = false;
+  m_pausedContextGroupId = 0;
+  m_instrumentationPause = false;
 
   m_inspector->forEachSession(contextGroupId,
                               [](V8InspectorSessionImpl* session) {
                                 if (session->debuggerAgent()->enabled())
                                   session->debuggerAgent()->didContinue();
                               });
+  return requestedPauseAfterInstrumentation
+             ? kPauseAfterInstrumentationRequested
+             : kNoPauseAfterInstrumentationRequested;
 }
 
 void V8Debugger::BreakProgramRequested(
@@ -818,7 +835,7 @@
   MatchPrototypePredicate predicate(m_inspector, context, prototype);
   v8::debug::QueryObjects(context, &predicate, &v8_objects);
 
-  v8::MicrotasksScope microtasksScope(isolate,
+  v8::MicrotasksScope microtasksScope(context,
                                       v8::MicrotasksScope::kDoNotRunMicrotasks);
   v8::Local<v8::Array> resultArray = v8::Array::New(
       m_inspector->isolate(), static_cast<int>(v8_objects.size()));
diff -r -u --color up/v8/src/inspector/v8-debugger.h nw/v8/src/inspector/v8-debugger.h
--- up/v8/src/inspector/v8-debugger.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/inspector/v8-debugger.h	2023-01-19 16:46:36.276442886 -0500
@@ -58,8 +58,10 @@
   v8::debug::ExceptionBreakState getPauseOnExceptionsState();
   void setPauseOnExceptionsState(v8::debug::ExceptionBreakState);
   bool canBreakProgram();
+  bool isInInstrumentationPause() const;
   void breakProgram(int targetContextGroupId);
   void interruptAndBreak(int targetContextGroupId);
+  void requestPauseAfterInstrumentation();
   void continueProgram(int targetContextGroupId,
                        bool terminateOnResume = false);
   void breakProgramOnAssert(int targetContextGroupId);
@@ -191,8 +193,8 @@
       v8::Local<v8::Context> paused_context,
       const std::vector<v8::debug::BreakpointId>& break_points_hit,
       v8::debug::BreakReasons break_reasons) override;
-  void BreakOnInstrumentation(v8::Local<v8::Context> paused_context,
-                              v8::debug::BreakpointId) override;
+  PauseAfterInstrumentation BreakOnInstrumentation(
+      v8::Local<v8::Context> paused_context, v8::debug::BreakpointId) override;
   void ExceptionThrown(v8::Local<v8::Context> paused_context,
                        v8::Local<v8::Value> exception,
                        v8::Local<v8::Value> promise, bool is_uncaught,
@@ -218,6 +220,8 @@
   bool m_scheduledOOMBreak = false;
   int m_targetContextGroupId = 0;
   int m_pausedContextGroupId = 0;
+  bool m_instrumentationPause = false;
+  bool m_requestedPauseAfterInstrumentation = false;
   int m_continueToLocationBreakpointId;
   String16 m_continueToLocationTargetCallFrames;
   std::unique_ptr<V8StackTraceImpl> m_continueToLocationStack;
diff -r -u --color up/v8/src/inspector/v8-heap-profiler-agent-impl.cc nw/v8/src/inspector/v8-heap-profiler-agent-impl.cc
--- up/v8/src/inspector/v8-heap-profiler-agent-impl.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/inspector/v8-heap-profiler-agent-impl.cc	2023-01-19 16:46:36.276442886 -0500
@@ -281,6 +281,15 @@
     progress.reset(new HeapSnapshotProgress(&m_frontend));
 
   GlobalObjectNameResolver resolver(m_session);
+#ifdef __APPLE__
+    // exit the context we entered in g_uv_runloop_once or taking
+    // snapshot will fail.
+    v8::Isolate* isolate = v8::Isolate::GetCurrent();
+    v8::HandleScope handle_scope(isolate);
+    v8::Local<v8::Context> context = isolate->GetEnteredOrMicrotaskContext();
+    if (!context.IsEmpty())
+      context->Exit();
+#endif
   v8::HeapProfiler::HeapSnapshotOptions options;
   options.global_object_name_resolver = &resolver;
   options.control = progress.get();
@@ -296,6 +305,10 @@
           ? v8::HeapProfiler::NumericsMode::kExposeNumericValues
           : v8::HeapProfiler::NumericsMode::kHideNumericValues;
   const v8::HeapSnapshot* snapshot = profiler->TakeHeapSnapshot(options);
+#ifdef __APPLE__
+    if (!context.IsEmpty())
+      context->Enter();
+#endif
   if (!snapshot) return Response::ServerError("Failed to take heap snapshot");
   HeapSnapshotOutputStream stream(&m_frontend);
   snapshot->Serialize(&stream);
diff -r -u --color up/v8/src/inspector/v8-inspector-impl.cc nw/v8/src/inspector/v8-inspector-impl.cc
--- up/v8/src/inspector/v8-inspector-impl.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/inspector/v8-inspector-impl.cc	2023-01-19 16:46:36.276442886 -0500
@@ -97,7 +97,7 @@
   if (!v8::debug::CompileInspectorScript(m_isolate, source)
            .ToLocal(&unboundScript))
     return v8::MaybeLocal<v8::Value>();
-  v8::MicrotasksScope microtasksScope(m_isolate,
+  v8::MicrotasksScope microtasksScope(context,
                                       v8::MicrotasksScope::kDoNotRunMicrotasks);
   v8::Context::Scope contextScope(context);
   v8::Isolate::SafeForTerminationScope allowTermination(m_isolate);
@@ -533,7 +533,7 @@
   if (!exceptionMetaDataContext().ToLocal(&context)) return nullptr;
 
   v8::TryCatch tryCatch(m_isolate);
-  v8::MicrotasksScope microtasksScope(m_isolate,
+  v8::MicrotasksScope microtasksScope(context,
                                       v8::MicrotasksScope::kDoNotRunMicrotasks);
   v8::Context::Scope contextScope(context);
   std::unique_ptr<protocol::DictionaryValue> jsonObject;
diff -r -u --color up/v8/src/inspector/v8-regex.cc nw/v8/src/inspector/v8-regex.cc
--- up/v8/src/inspector/v8-regex.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/inspector/v8-regex.cc	2023-01-19 16:46:36.276442886 -0500
@@ -63,7 +63,7 @@
     return -1;
   }
   v8::Context::Scope contextScope(context);
-  v8::MicrotasksScope microtasks(isolate,
+  v8::MicrotasksScope microtasks(context,
                                  v8::MicrotasksScope::kDoNotRunMicrotasks);
   v8::TryCatch tryCatch(isolate);
 
diff -r -u --color up/v8/src/inspector/v8-runtime-agent-impl.cc nw/v8/src/inspector/v8-runtime-agent-impl.cc
--- up/v8/src/inspector/v8-runtime-agent-impl.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/inspector/v8-runtime-agent-impl.cc	2023-01-19 16:46:36.276442886 -0500
@@ -152,7 +152,7 @@
   if (inspector
           ->compileScript(scope.context(), "(" + expression + ")", String16())
           .ToLocal(&functionScript)) {
-    v8::MicrotasksScope microtasksScope(inspector->isolate(),
+    v8::MicrotasksScope microtasksScope(scope.context(),
                                         v8::MicrotasksScope::kRunMicrotasks);
     maybeFunctionValue = functionScript->Run(scope.context());
   }
@@ -181,7 +181,7 @@
 
   v8::MaybeLocal<v8::Value> maybeResultValue;
   {
-    v8::MicrotasksScope microtasksScope(inspector->isolate(),
+    v8::MicrotasksScope microtasksScope(scope.context(),
                                         v8::MicrotasksScope::kRunMicrotasks);
     maybeResultValue = v8::debug::CallFunctionOn(
         scope.context(), functionValue.As<v8::Function>(), recv, argc,
@@ -299,7 +299,7 @@
         return;
       }
     }
-    v8::MicrotasksScope microtasksScope(m_inspector->isolate(),
+    v8::MicrotasksScope microtasksScope(scope.context(),
                                         v8::MicrotasksScope::kRunMicrotasks);
     v8::debug::EvaluateGlobalMode mode =
         v8::debug::EvaluateGlobalMode::kDefault;
@@ -447,7 +447,7 @@
   if (!response.IsSuccess()) return response;
 
   scope.ignoreExceptionsAndMuteConsole();
-  v8::MicrotasksScope microtasks_scope(m_inspector->isolate(),
+  v8::MicrotasksScope microtasks_scope(scope.context(),
                                        v8::MicrotasksScope::kRunMicrotasks);
   if (!scope.object()->IsObject())
     return Response::ServerError("Value with given id is not an object");
@@ -491,6 +491,11 @@
 }
 
 Response V8RuntimeAgentImpl::runIfWaitingForDebugger() {
+  if (m_runIfWaitingForDebuggerCalled) return Response::Success();
+  m_runIfWaitingForDebuggerCalled = true;
+  // The client implementation is resposible for checking if the session is
+  // actually waiting for debugger. m_runIfWaitingForDebuggerCalled only makes
+  // sure that the client implementation is invoked once per agent instance.
   m_inspector->client()->runIfWaitingForDebugger(m_session->contextGroupId());
   return Response::Success();
 }
@@ -615,7 +620,7 @@
 
   v8::MaybeLocal<v8::Value> maybeResultValue;
   {
-    v8::MicrotasksScope microtasksScope(m_inspector->isolate(),
+    v8::MicrotasksScope microtasksScope(scope.context(),
                                         v8::MicrotasksScope::kRunMicrotasks);
     maybeResultValue = script->Run(scope.context());
   }
@@ -794,7 +799,7 @@
   v8::Local<v8::Object> global = localContext->Global();
   v8::Local<v8::String> v8Name = toV8String(m_inspector->isolate(), name);
   v8::Local<v8::Value> functionValue;
-  v8::MicrotasksScope microtasks(m_inspector->isolate(),
+  v8::MicrotasksScope microtasks(localContext,
                                  v8::MicrotasksScope::kDoNotRunMicrotasks);
   if (v8::Function::New(localContext, bindingCallback, v8Name)
           .ToLocal(&functionValue)) {
diff -r -u --color up/v8/src/inspector/v8-runtime-agent-impl.h nw/v8/src/inspector/v8-runtime-agent-impl.h
--- up/v8/src/inspector/v8-runtime-agent-impl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/inspector/v8-runtime-agent-impl.h	2023-01-19 16:46:36.276442886 -0500
@@ -160,6 +160,7 @@
       m_compiledScripts;
   // Binding name -> executionContextIds mapping.
   std::unordered_map<String16, std::unordered_set<int>> m_activeBindings;
+  bool m_runIfWaitingForDebuggerCalled = false;
 };
 
 }  // namespace v8_inspector
diff -r -u --color up/v8/src/inspector/v8-stack-trace-impl.cc nw/v8/src/inspector/v8-stack-trace-impl.cc
--- up/v8/src/inspector/v8-stack-trace-impl.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/inspector/v8-stack-trace-impl.cc	2023-01-19 16:46:36.276442886 -0500
@@ -178,8 +178,8 @@
       m_lineNumber(lineNumber),
       m_columnNumber(columnNumber),
       m_hasSourceURLComment(hasSourceURLComment) {
-  DCHECK_NE(v8::Message::kNoLineNumberInfo, m_lineNumber + 1);
-  DCHECK_NE(v8::Message::kNoColumnInfo, m_columnNumber + 1);
+  //DCHECK_NE(v8::Message::kNoLineNumberInfo, m_lineNumber + 1);
+  //DCHECK_NE(v8::Message::kNoColumnInfo, m_columnNumber + 1);
 }
 
 const String16& StackFrame::functionName() const { return m_functionName; }
diff -r -u --color up/v8/src/inspector/value-mirror.cc nw/v8/src/inspector/value-mirror.cc
--- up/v8/src/inspector/value-mirror.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/inspector/value-mirror.cc	2023-01-19 16:46:36.276442886 -0500
@@ -819,7 +819,7 @@
   if (!value->IsObject()) return false;
   v8::Isolate* isolate = context->GetIsolate();
   v8::TryCatch tryCatch(isolate);
-  v8::MicrotasksScope microtasksScope(isolate,
+  v8::MicrotasksScope microtasksScope(context,
                                       v8::MicrotasksScope::kDoNotRunMicrotasks);
   v8::Local<v8::Object> object = value.As<v8::Object>();
   v8::Local<v8::Value> spliceValue;
@@ -974,6 +974,8 @@
     allowlist.emplace_back("[[PromiseResult]]");
   } else if (object->IsGeneratorObject()) {
     allowlist.emplace_back("[[GeneratorState]]");
+  } else if (object->IsWeakRef()) {
+    allowlist.emplace_back("[[WeakRefTarget]]");
   }
   for (auto& mirror : mirrors) {
     if (std::find(allowlist.begin(), allowlist.end(), mirror.name) ==
@@ -1362,7 +1364,7 @@
   v8::TryCatch tryCatch(isolate);
   v8::Local<v8::Set> set = v8::Set::New(isolate);
 
-  v8::MicrotasksScope microtasksScope(isolate,
+  v8::MicrotasksScope microtasksScope(context,
                                       v8::MicrotasksScope::kDoNotRunMicrotasks);
   V8InternalValueType internalType = v8InternalValueTypeFrom(context, object);
   if (internalType == V8InternalValueType::kScope) {
@@ -1510,7 +1512,7 @@
     v8::Local<v8::Context> context, v8::Local<v8::Object> object,
     std::vector<InternalPropertyMirror>* mirrors) {
   v8::Isolate* isolate = context->GetIsolate();
-  v8::MicrotasksScope microtasksScope(isolate,
+  v8::MicrotasksScope microtasksScope(context,
                                       v8::MicrotasksScope::kDoNotRunMicrotasks);
   v8::TryCatch tryCatch(isolate);
   if (object->IsFunction()) {
@@ -1565,7 +1567,7 @@
     bool accessorPropertiesOnly) {
   std::vector<PrivatePropertyMirror> mirrors;
   v8::Isolate* isolate = context->GetIsolate();
-  v8::MicrotasksScope microtasksScope(isolate,
+  v8::MicrotasksScope microtasksScope(context,
                                       v8::MicrotasksScope::kDoNotRunMicrotasks);
   v8::TryCatch tryCatch(isolate);
   v8::Local<v8::Array> privateProperties;
diff -r -u --color up/v8/src/interpreter/bytecode-register.h nw/v8/src/interpreter/bytecode-register.h
--- up/v8/src/interpreter/bytecode-register.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/interpreter/bytecode-register.h	2023-01-19 16:46:36.287276216 -0500
@@ -135,7 +135,7 @@
     DCHECK_LT(new_count, register_count_);
     return RegisterList(first_reg_index_, new_count);
   }
-  const RegisterList PopLeft() {
+  const RegisterList PopLeft() const {
     DCHECK_GE(register_count_, 0);
     return RegisterList(first_reg_index_ + 1, register_count_ - 1);
   }
@@ -161,6 +161,7 @@
   friend class InterpreterTester;
   friend class BytecodeUtils;
   friend class BytecodeArrayIterator;
+  friend class CallArguments;
 
   RegisterList(int first_reg_index, int register_count)
       : first_reg_index_(first_reg_index), register_count_(register_count) {}
diff -r -u --color up/v8/src/logging/counters-definitions.h nw/v8/src/logging/counters-definitions.h
--- up/v8/src/logging/counters-definitions.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/logging/counters-definitions.h	2023-01-19 16:46:36.287276216 -0500
@@ -270,7 +270,10 @@
   HT(wasm_sum_lazy_compilation_time_60sec,                                     \
      V8.WasmSumLazyCompilationTime60SecMilliSeconds, 20000, MILLISECOND)       \
   HT(wasm_sum_lazy_compilation_time_120sec,                                    \
-     V8.WasmSumLazyCompilationTime120SecMilliSeconds, 20000, MILLISECOND)
+     V8.WasmSumLazyCompilationTime120SecMilliSeconds, 20000, MILLISECOND)      \
+  /* Debugger timers */                                                        \
+  HT(debug_pause_to_paused_event, V8.DebugPauseToPausedEventMilliSeconds,      \
+     1000000, MILLISECOND)
 
 #define AGGREGATABLE_HISTOGRAM_TIMER_LIST(AHT) \
   AHT(compile_lazy, V8.CompileLazyMicroSeconds)
diff -r -u --color up/v8/src/logging/log.cc nw/v8/src/logging/log.cc
--- up/v8/src/logging/log.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/logging/log.cc	2023-01-19 16:46:36.298109548 -0500
@@ -1372,7 +1372,7 @@
 #ifdef ENABLE_DISASSEMBLER
       Code::cast(*code).Disassemble(nullptr, stream, isolate_);
 #endif
-    } else if (V8_REMOVE_BUILTINS_CODE_OBJECTS &&
+    } else if (V8_EXTERNAL_CODE_SPACE_BOOL &&
                code->IsCodeDataContainer(cage_base)) {
 #ifdef ENABLE_DISASSEMBLER
       CodeT::cast(*code).Disassemble(nullptr, stream, isolate_);
@@ -1952,6 +1952,7 @@
       // only on a type feedback vector. We should make this mroe precise.
       if (function.HasAttachedOptimizedCode() &&
           Script::cast(function.shared().script()).HasValidSource()) {
+        // TODO(v8:13261): use ToAbstractCode() here.
         record(function.shared(),
                AbstractCode::cast(FromCodeT(function.code())));
       }
@@ -2330,7 +2331,7 @@
   for (HeapObject obj = iterator.Next(); !obj.is_null();
        obj = iterator.Next()) {
     InstanceType instance_type = obj.map(cage_base).instance_type();
-    if (V8_REMOVE_BUILTINS_CODE_OBJECTS) {
+    if (V8_EXTERNAL_CODE_SPACE_BOOL) {
       // In this case AbstactCode is Code|CodeDataContainer|BytecodeArray but
       // we want to log code objects only once, thus we ignore Code objects
       // which will be logged via corresponding CodeDataContainer.
@@ -2369,6 +2370,7 @@
     Handle<SharedFunctionInfo> shared = pair.first;
     SharedFunctionInfo::EnsureSourcePositionsAvailable(isolate_, shared);
     if (shared->HasInterpreterData()) {
+      // TODO(v8:13261): use ToAbstractCode() here.
       LogExistingFunction(
           shared,
           Handle<AbstractCode>(
@@ -2376,6 +2378,7 @@
               isolate_));
     }
     if (shared->HasBaselineCode()) {
+      // TODO(v8:13261): use ToAbstractCode() here.
       LogExistingFunction(shared, Handle<AbstractCode>(
                                       AbstractCode::cast(FromCodeT(
                                           shared->baseline_code(kAcquireLoad))),
diff -r -u --color up/v8/src/logging/runtime-call-stats.h nw/v8/src/logging/runtime-call-stats.h
--- up/v8/src/logging/runtime-call-stats.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/logging/runtime-call-stats.h	2023-01-19 16:46:36.298109548 -0500
@@ -323,6 +323,7 @@
   ADD_THREAD_SPECIFIC_COUNTER(V, Optimize, AllocateGeneralRegisters)        \
   ADD_THREAD_SPECIFIC_COUNTER(V, Optimize, AssembleCode)                    \
   ADD_THREAD_SPECIFIC_COUNTER(V, Optimize, AssignSpillSlots)                \
+  ADD_THREAD_SPECIFIC_COUNTER(V, Optimize, BitcastElision)                  \
   ADD_THREAD_SPECIFIC_COUNTER(V, Optimize, BranchConditionDuplication)      \
   ADD_THREAD_SPECIFIC_COUNTER(V, Optimize, BuildLiveRangeBundles)           \
   ADD_THREAD_SPECIFIC_COUNTER(V, Optimize, BuildLiveRanges)                 \
diff -r -u --color up/v8/src/maglev/maglev-assembler-inl.h nw/v8/src/maglev/maglev-assembler-inl.h
--- up/v8/src/maglev/maglev-assembler-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/maglev/maglev-assembler-inl.h	2023-01-19 16:46:36.298109548 -0500
@@ -18,6 +18,9 @@
 namespace internal {
 namespace maglev {
 
+ZoneLabelRef::ZoneLabelRef(MaglevAssembler* masm)
+    : ZoneLabelRef(masm->compilation_info()->zone()) {}
+
 void MaglevAssembler::Branch(Condition condition, BasicBlock* if_true,
                              BasicBlock* if_false, BasicBlock* next_block) {
   // We don't have any branch probability information, so try to jump
@@ -72,7 +75,7 @@
 }
 
 inline void MaglevAssembler::DefineLazyDeoptPoint(LazyDeoptInfo* info) {
-  info->deopting_call_return_pc = pc_offset_for_safepoint();
+  info->set_deopting_call_return_pc(pc_offset_for_safepoint());
   code_gen_state()->PushLazyDeopt(info);
   safepoint_table_builder()->DefineSafepoint(this);
 }
@@ -274,12 +277,12 @@
 
 inline void MaglevAssembler::RegisterEagerDeopt(EagerDeoptInfo* deopt_info,
                                                 DeoptimizeReason reason) {
-  if (deopt_info->reason != DeoptimizeReason::kUnknown) {
-    DCHECK_EQ(deopt_info->reason, reason);
+  if (deopt_info->reason() != DeoptimizeReason::kUnknown) {
+    DCHECK_EQ(deopt_info->reason(), reason);
   }
-  if (deopt_info->deopt_entry_label.is_unused()) {
+  if (deopt_info->deopt_entry_label()->is_unused()) {
     code_gen_state()->PushEagerDeopt(deopt_info);
-    deopt_info->reason = reason;
+    deopt_info->set_reason(reason);
   }
 }
 
@@ -289,7 +292,7 @@
   static_assert(NodeT::kProperties.can_eager_deopt());
   RegisterEagerDeopt(node->eager_deopt_info(), reason);
   RecordComment("-- Jump to eager deopt");
-  jmp(&node->eager_deopt_info()->deopt_entry_label);
+  jmp(node->eager_deopt_info()->deopt_entry_label());
 }
 
 template <typename NodeT>
@@ -299,7 +302,7 @@
   static_assert(NodeT::kProperties.can_eager_deopt());
   RegisterEagerDeopt(node->eager_deopt_info(), reason);
   RecordComment("-- Jump to eager deopt");
-  j(cond, &node->eager_deopt_info()->deopt_entry_label);
+  j(cond, node->eager_deopt_info()->deopt_entry_label());
 }
 
 }  // namespace maglev
Only in nw/v8/src/maglev: maglev-assembler.cc
diff -r -u --color up/v8/src/maglev/maglev-assembler.h nw/v8/src/maglev/maglev-assembler.h
--- up/v8/src/maglev/maglev-assembler.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/maglev/maglev-assembler.h	2023-01-19 16:46:36.298109548 -0500
@@ -12,10 +12,32 @@
 namespace internal {
 namespace maglev {
 
+class MaglevAssembler;
+
+// Label allowed to be passed to deferred code.
+class ZoneLabelRef {
+ public:
+  explicit ZoneLabelRef(Zone* zone) : label_(zone->New<Label>()) {}
+  explicit inline ZoneLabelRef(MaglevAssembler* masm);
+
+  static ZoneLabelRef UnsafeFromLabelPointer(Label* label) {
+    // This is an unsafe operation, {label} must be zone allocated.
+    return ZoneLabelRef(label);
+  }
+
+  Label* operator*() { return label_; }
+
+ private:
+  Label* label_;
+
+  // Unsafe constructor. {label} must be zone allocated.
+  explicit ZoneLabelRef(Label* label) : label_(label) {}
+};
+
 class MaglevAssembler : public MacroAssembler {
  public:
-  explicit MaglevAssembler(MaglevCodeGenState* code_gen_state)
-      : MacroAssembler(code_gen_state->isolate(), CodeObjectRequired::kNo),
+  explicit MaglevAssembler(Isolate* isolate, MaglevCodeGenState* code_gen_state)
+      : MacroAssembler(isolate, CodeObjectRequired::kNo),
         code_gen_state_(code_gen_state) {}
 
   inline MemOperand GetStackSlot(const compiler::AllocatedOperand& operand) {
@@ -39,11 +61,37 @@
     return GetFramePointerOffsetForStackSlot(index);
   }
 
+  void Allocate(RegisterSnapshot& register_snapshot, Register result,
+                int size_in_bytes,
+                AllocationType alloc_type = AllocationType::kYoung,
+                AllocationAlignment alignment = kTaggedAligned);
+
+  void AllocateTwoByteString(RegisterSnapshot register_snapshot,
+                             Register result, int length);
+
+  void LoadSingleCharacterString(Register result, int char_code);
+  void LoadSingleCharacterString(Register result, Register char_code,
+                                 Register scratch);
+
   inline void Branch(Condition condition, BasicBlock* if_true,
                      BasicBlock* if_false, BasicBlock* next_block);
   inline void PushInput(const Input& input);
   inline Register FromAnyToRegister(const Input& input, Register scratch);
 
+  // Warning: Input registers {string} and {index} will be scratched.
+  // {result} is allowed to alias with one the other 3 input registers.
+  // {result} is an int32.
+  void StringCharCodeAt(RegisterSnapshot& register_snapshot, Register result,
+                        Register string, Register index, Register scratch,
+                        Label* result_fits_one_byte);
+  // Warning: Input {char_code} will be scratched.
+  void StringFromCharCode(RegisterSnapshot register_snapshot,
+                          Label* char_code_fits_one_byte, Register result,
+                          Register char_code, Register scratch);
+
+  void ToBoolean(Register value, ZoneLabelRef is_true, ZoneLabelRef is_false,
+                 bool fallthrough_when_true);
+
   inline void DefineLazyDeoptPoint(LazyDeoptInfo* info);
   inline void DefineExceptionHandlerPoint(NodeBase* node);
   inline void DefineExceptionHandlerAndLazyDeoptPoint(NodeBase* node);
@@ -84,25 +132,46 @@
   MaglevCodeGenState* const code_gen_state_;
 };
 
-// Label allowed to be passed to deferred code.
-class ZoneLabelRef {
+class SaveRegisterStateForCall {
  public:
-  explicit ZoneLabelRef(Zone* zone) : label_(zone->New<Label>()) {}
-  explicit inline ZoneLabelRef(MaglevAssembler* masm)
-      : ZoneLabelRef(masm->compilation_info()->zone()) {}
-
-  static ZoneLabelRef UnsafeFromLabelPointer(Label* label) {
-    // This is an unsafe operation, {label} must be zone allocated.
-    return ZoneLabelRef(label);
+  SaveRegisterStateForCall(MaglevAssembler* masm, RegisterSnapshot snapshot)
+      : masm(masm), snapshot_(snapshot) {
+    masm->PushAll(snapshot_.live_registers);
+    masm->PushAll(snapshot_.live_double_registers, kDoubleSize);
+  }
+
+  ~SaveRegisterStateForCall() {
+    masm->PopAll(snapshot_.live_double_registers, kDoubleSize);
+    masm->PopAll(snapshot_.live_registers);
+  }
+
+  MaglevSafepointTableBuilder::Safepoint DefineSafepoint() {
+    // TODO(leszeks): Avoid emitting safepoints when there are no registers to
+    // save.
+    auto safepoint = masm->safepoint_table_builder()->DefineSafepoint(masm);
+    int pushed_reg_index = 0;
+    for (Register reg : snapshot_.live_registers) {
+      if (snapshot_.live_tagged_registers.has(reg)) {
+        safepoint.DefineTaggedRegister(pushed_reg_index);
+      }
+      pushed_reg_index++;
+    }
+    int num_pushed_double_reg = snapshot_.live_double_registers.Count();
+    safepoint.SetNumPushedRegisters(pushed_reg_index + num_pushed_double_reg);
+    return safepoint;
   }
 
-  Label* operator*() { return label_; }
+  MaglevSafepointTableBuilder::Safepoint DefineSafepointWithLazyDeopt(
+      LazyDeoptInfo* lazy_deopt_info) {
+    lazy_deopt_info->set_deopting_call_return_pc(
+        masm->pc_offset_for_safepoint());
+    masm->code_gen_state()->PushLazyDeopt(lazy_deopt_info);
+    return DefineSafepoint();
+  }
 
  private:
-  Label* label_;
-
-  // Unsafe constructor. {label} must be zone allocated.
-  explicit ZoneLabelRef(Label* label) : label_(label) {}
+  MaglevAssembler* masm;
+  RegisterSnapshot snapshot_;
 };
 
 }  // namespace maglev
diff -r -u --color up/v8/src/maglev/maglev-basic-block.h nw/v8/src/maglev/maglev-basic-block.h
--- up/v8/src/maglev/maglev-basic-block.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/maglev/maglev-basic-block.h	2023-01-19 16:46:36.298109548 -0500
@@ -50,24 +50,27 @@
 
   bool has_phi() const { return has_state() && state_->has_phi(); }
 
-  bool is_empty_block() const { return is_empty_block_; }
+  bool is_edge_split_block() const { return is_edge_split_block_; }
 
-  MergePointRegisterState& empty_block_register_state() {
-    DCHECK(is_empty_block());
-    return *empty_block_register_state_;
+  MergePointRegisterState& edge_split_block_register_state() {
+    DCHECK(is_edge_split_block());
+    return *edge_split_block_register_state_;
   }
 
-  void set_empty_block_register_state(MergePointRegisterState* register_state) {
-    DCHECK(is_empty_block());
-    empty_block_register_state_ = register_state;
+  void set_edge_split_block_register_state(
+      MergePointRegisterState* register_state) {
+    DCHECK(is_edge_split_block());
+    edge_split_block_register_state_ = register_state;
   }
 
-  void set_empty_block() {
-    DCHECK(nodes_.is_empty());
+  void set_edge_split_block() {
+    DCHECK_IMPLIES(!nodes_.is_empty(),
+                   nodes_.LengthForTest() == 1 &&
+                       nodes_.first()->Is<IncreaseInterruptBudget>());
     DCHECK(control_node()->Is<Jump>());
     DCHECK_NULL(state_);
-    is_empty_block_ = true;
-    empty_block_register_state_ = nullptr;
+    is_edge_split_block_ = true;
+    edge_split_block_register_state_ = nullptr;
   }
 
   Phi::List* phis() const {
@@ -92,19 +95,19 @@
     DCHECK(has_state());
     return state_;
   }
-  bool has_state() const { return !is_empty_block() && state_ != nullptr; }
+  bool has_state() const { return !is_edge_split_block() && state_ != nullptr; }
 
   bool is_exception_handler_block() const {
     return has_state() && state_->is_exception_handler();
   }
 
  private:
-  bool is_empty_block_ = false;
+  bool is_edge_split_block_ = false;
   Node::List nodes_;
   ControlNode* control_node_;
   union {
     MergePointInterpreterFrameState* state_;
-    MergePointRegisterState* empty_block_register_state_;
+    MergePointRegisterState* edge_split_block_register_state_;
   };
   Label label_;
 };
diff -r -u --color up/v8/src/maglev/maglev-code-gen-state.h nw/v8/src/maglev/maglev-code-gen-state.h
--- up/v8/src/maglev/maglev-code-gen-state.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/maglev/maglev-code-gen-state.h	2023-01-19 16:46:36.298109548 -0500
@@ -30,10 +30,9 @@
 
 class MaglevCodeGenState {
  public:
-  MaglevCodeGenState(Isolate* isolate, MaglevCompilationInfo* compilation_info,
+  MaglevCodeGenState(MaglevCompilationInfo* compilation_info,
                      MaglevSafepointTableBuilder* safepoint_table_builder)
-      : isolate_(isolate),
-        compilation_info_(compilation_info),
+      : compilation_info_(compilation_info),
         safepoint_table_builder_(safepoint_table_builder) {}
 
   void set_tagged_slots(int slots) { tagged_slots_ = slots; }
@@ -63,7 +62,6 @@
   compiler::NativeContextRef native_context() const {
     return broker()->target_native_context();
   }
-  Isolate* isolate() const { return isolate_; }
   compiler::JSHeapBroker* broker() const { return compilation_info_->broker(); }
   MaglevGraphLabeller* graph_labeller() const {
     return compilation_info_->graph_labeller();
@@ -76,7 +74,6 @@
   MaglevCompilationInfo* compilation_info() const { return compilation_info_; }
 
  private:
-  Isolate* const isolate_;
   MaglevCompilationInfo* const compilation_info_;
   MaglevSafepointTableBuilder* const safepoint_table_builder_;
 
diff -r -u --color up/v8/src/maglev/maglev-code-generator.cc nw/v8/src/maglev/maglev-code-generator.cc
--- up/v8/src/maglev/maglev-code-generator.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/maglev/maglev-code-generator.cc	2023-01-19 16:46:36.298109548 -0500
@@ -16,6 +16,7 @@
 #include "src/codegen/source-position.h"
 #include "src/common/globals.h"
 #include "src/compiler/backend/instruction.h"
+#include "src/deoptimizer/deoptimize-reason.h"
 #include "src/deoptimizer/translation-array.h"
 #include "src/execution/frame-constants.h"
 #include "src/interpreter/bytecode-register.h"
@@ -288,11 +289,11 @@
         Pop(kScratchRegT);
         scratch_has_cycle_start_ = true;
       }
-      EmitMovesFromSource(kScratchRegT, targets);
+      EmitMovesFromSource(kScratchRegT, std::move(targets));
       scratch_has_cycle_start_ = false;
       __ RecordComment("--   * End of cycle");
     } else {
-      EmitMovesFromSource(source, targets);
+      EmitMovesFromSource(source, std::move(targets));
       __ RecordComment("--   * Chain emitted with no cycles");
     }
   }
@@ -323,7 +324,7 @@
 
     bool has_cycle = RecursivelyEmitMoveChainTargets(chain_start, targets);
 
-    EmitMovesFromSource(source, targets);
+    EmitMovesFromSource(source, std::move(targets));
     return has_cycle;
   }
 
@@ -343,8 +344,7 @@
     return has_cycle;
   }
 
-  void EmitMovesFromSource(RegisterT source_reg,
-                           const GapMoveTargets& targets) {
+  void EmitMovesFromSource(RegisterT source_reg, GapMoveTargets&& targets) {
     DCHECK(moves_from_register_[source_reg.code()].is_empty());
     for (RegisterT target_reg : targets.registers) {
       DCHECK(moves_from_register_[target_reg.code()].is_empty());
@@ -357,24 +357,32 @@
     }
   }
 
-  void EmitMovesFromSource(uint32_t source_slot,
-                           const GapMoveTargets& targets) {
+  void EmitMovesFromSource(uint32_t source_slot, GapMoveTargets&& targets) {
     DCHECK_EQ(moves_from_stack_slot_.find(source_slot),
               moves_from_stack_slot_.end());
-    for (RegisterT target_reg : targets.registers) {
-      DCHECK(moves_from_register_[target_reg.code()].is_empty());
-      EmitStackMove(target_reg, source_slot);
-    }
-    if (scratch_has_cycle_start_ && !targets.stack_slots.empty()) {
-      Push(kScratchRegT);
-      scratch_has_cycle_start_ = false;
-    }
-    for (uint32_t target_slot : targets.stack_slots) {
-      DCHECK_EQ(moves_from_stack_slot_.find(target_slot),
-                moves_from_stack_slot_.end());
-      EmitStackMove(kScratchRegT, source_slot);
-      EmitStackMove(target_slot, kScratchRegT);
+
+    // Cache the slot value on a register.
+    RegisterT register_with_slot_value = RegisterT::no_reg();
+    if (!targets.registers.is_empty()) {
+      // If one of the targets is a register, we can move our value into it and
+      // optimize the moves from this stack slot to always be via that register.
+      register_with_slot_value = targets.registers.PopFirst();
+    } else {
+      DCHECK(!targets.stack_slots.empty());
+      // Otherwise, cache the slot value on the scratch register, clobbering it
+      // if necessary.
+      if (scratch_has_cycle_start_) {
+        Push(kScratchRegT);
+        scratch_has_cycle_start_ = false;
+      }
+      register_with_slot_value = kScratchRegT;
     }
+
+    // Now emit moves from that cached register instead of from the stack slot.
+    DCHECK(register_with_slot_value.is_valid());
+    DCHECK(moves_from_register_[register_with_slot_value.code()].is_empty());
+    EmitStackMove(register_with_slot_value, source_slot);
+    EmitMovesFromSource(register_with_slot_value, std::move(targets));
   }
 
   // The slot index used for representing slots in the move graph is the offset
@@ -478,12 +486,18 @@
     // values are tagged and b) the stack walk treats unknown stack slots as
     // tagged.
 
+    const InterpretedDeoptFrame& lazy_frame =
+        deopt_info->top_frame().type() ==
+                DeoptFrame::FrameType::kBuiltinContinuationFrame
+            ? deopt_info->top_frame().parent()->as_interpreted()
+            : deopt_info->top_frame().as_interpreted();
+
     // TODO(v8:7700): Handle inlining.
 
     ParallelMoveResolver<Register> direct_moves(masm_);
     MoveVector materialising_moves;
     bool save_accumulator = false;
-    RecordMoves(deopt_info->unit, catch_block, deopt_info->state.register_frame,
+    RecordMoves(lazy_frame.unit(), catch_block, lazy_frame.frame_state(),
                 &direct_moves, &materialising_moves, &save_accumulator);
 
     __ bind(&handler_info->trampoline_entry);
@@ -666,7 +680,8 @@
       // per Maglev code object on x64).
       {
         // Scratch registers. Don't clobber regs related to the calling
-        // convention (e.g. kJavaScriptCallArgCountRegister).
+        // convention (e.g. kJavaScriptCallArgCountRegister). Keep up-to-date
+        // with deferred flags code.
         Register flags = rcx;
         Register feedback_vector = r9;
 
@@ -678,20 +693,9 @@
             feedback_vector, FieldOperand(feedback_vector, Cell::kValueOffset));
         __ AssertFeedbackVector(feedback_vector);
 
-        Label flags_need_processing, next;
         __ LoadFeedbackVectorFlagsAndJumpIfNeedsProcessing(
-            flags, feedback_vector, CodeKind::MAGLEV, &flags_need_processing);
-        __ jmp(&next);
-
-        __ bind(&flags_need_processing);
-        {
-          ASM_CODE_COMMENT_STRING(masm(), "Optimized marker check");
-          __ OptimizeCodeOrTailCallOptimizedCodeSlot(
-              flags, feedback_vector, kJSFunctionRegister, JumpMode::kJump);
-          __ Trap();
-        }
-
-        __ bind(&next);
+            flags, feedback_vector, CodeKind::MAGLEV,
+            &deferred_flags_need_processing_);
       }
 
       __ EnterFrame(StackFrame::MAGLEV);
@@ -770,16 +774,30 @@
 
     if (!v8_flags.maglev_ool_prologue) {
       __ bind(&deferred_call_stack_guard_);
-      ASM_CODE_COMMENT_STRING(masm(), "Stack/interrupt call");
-      // Save any registers that can be referenced by RegisterInput.
-      // TODO(leszeks): Only push those that are used by the graph.
-      __ PushAll(RegisterInput::kAllowedRegisters);
-      // Push the frame size
-      __ Push(Immediate(
-          Smi::FromInt(code_gen_state()->stack_slots() * kSystemPointerSize)));
-      __ CallRuntime(Runtime::kStackGuardWithGap, 1);
-      __ PopAll(RegisterInput::kAllowedRegisters);
-      __ jmp(&deferred_call_stack_guard_return_);
+      {
+        ASM_CODE_COMMENT_STRING(masm(), "Stack/interrupt call");
+        // Save any registers that can be referenced by RegisterInput.
+        // TODO(leszeks): Only push those that are used by the graph.
+        __ PushAll(RegisterInput::kAllowedRegisters);
+        // Push the frame size
+        __ Push(Immediate(Smi::FromInt(code_gen_state()->stack_slots() *
+                                       kSystemPointerSize)));
+        __ CallRuntime(Runtime::kStackGuardWithGap, 1);
+        __ PopAll(RegisterInput::kAllowedRegisters);
+        __ jmp(&deferred_call_stack_guard_return_);
+      }
+
+      __ bind(&deferred_flags_need_processing_);
+      {
+        ASM_CODE_COMMENT_STRING(masm(), "Optimized marker check");
+        // See PreProcessGraph.
+        Register flags = rcx;
+        Register feedback_vector = r9;
+        // TODO(leszeks): This could definitely be a builtin that we tail-call.
+        __ OptimizeCodeOrTailCallOptimizedCodeSlot(
+            flags, feedback_vector, kJSFunctionRegister, JumpMode::kJump);
+        __ Trap();
+      }
     }
   }
 
@@ -949,254 +967,613 @@
   MaglevGraphLabeller* graph_labeller() const {
     return code_gen_state()->graph_labeller();
   }
-  MaglevSafepointTableBuilder* safepoint_table_builder() const {
-    return code_gen_state()->safepoint_table_builder();
-  }
 
  private:
   MaglevAssembler* const masm_;
   Label deferred_call_stack_guard_;
   Label deferred_call_stack_guard_return_;
+  Label deferred_flags_need_processing_;
 };
 
+class SafepointingNodeProcessor {
+ public:
+  explicit SafepointingNodeProcessor(LocalIsolate* local_isolate)
+      : local_isolate_(local_isolate) {}
+
+  void PreProcessGraph(Graph* graph) {}
+  void PostProcessGraph(Graph* graph) {}
+  void PreProcessBasicBlock(BasicBlock* block) {}
+  void Process(NodeBase* node, const ProcessingState& state) {
+    local_isolate_->heap()->Safepoint();
+  }
+
+ private:
+  LocalIsolate* local_isolate_;
+};
+
+namespace {
+int GetFrameCount(const DeoptFrame& deopt_frame) {
+  switch (deopt_frame.type()) {
+    case DeoptFrame::FrameType::kInterpretedFrame:
+      return 1 + deopt_frame.as_interpreted().unit().inlining_depth();
+    case DeoptFrame::FrameType::kBuiltinContinuationFrame:
+      return 1 + GetFrameCount(*deopt_frame.parent());
+  }
+}
+BytecodeOffset GetBytecodeOffset(const DeoptFrame& deopt_frame) {
+  switch (deopt_frame.type()) {
+    case DeoptFrame::FrameType::kInterpretedFrame:
+      return deopt_frame.as_interpreted().bytecode_position();
+    case DeoptFrame::FrameType::kBuiltinContinuationFrame:
+      return Builtins::GetContinuationBytecodeOffset(
+          deopt_frame.as_builtin_continuation().builtin_id());
+  }
+}
+SourcePosition GetSourcePosition(const DeoptFrame& deopt_frame) {
+  switch (deopt_frame.type()) {
+    case DeoptFrame::FrameType::kInterpretedFrame:
+      return deopt_frame.as_interpreted().source_position();
+    case DeoptFrame::FrameType::kBuiltinContinuationFrame:
+      return SourcePosition::Unknown();
+  }
+}
 }  // namespace
 
-class MaglevCodeGeneratorImpl final {
+class MaglevTranslationArrayBuilder {
  public:
-  static MaybeHandle<Code> Generate(Isolate* isolate,
-                                    MaglevCompilationInfo* compilation_info,
-                                    Graph* graph) {
-    return MaglevCodeGeneratorImpl(isolate, compilation_info, graph).Generate();
+  MaglevTranslationArrayBuilder(
+      LocalIsolate* local_isolate, MaglevAssembler* masm,
+      TranslationArrayBuilder* translation_array_builder,
+      IdentityMap<int, base::DefaultAllocationPolicy>* deopt_literals)
+      : local_isolate_(local_isolate),
+        masm_(masm),
+        translation_array_builder_(translation_array_builder),
+        deopt_literals_(deopt_literals) {}
+
+  void BuildEagerDeopt(EagerDeoptInfo* deopt_info) {
+    int frame_count = GetFrameCount(deopt_info->top_frame());
+    int jsframe_count = frame_count;
+    int update_feedback_count = 0;
+    deopt_info->set_translation_index(
+        translation_array_builder_->BeginTranslation(frame_count, jsframe_count,
+                                                     update_feedback_count));
+
+    const InputLocation* current_input_location = deopt_info->input_locations();
+    BuildDeoptFrame(deopt_info->top_frame(), current_input_location);
+  }
+
+  void BuildLazyDeopt(LazyDeoptInfo* deopt_info) {
+    int frame_count = GetFrameCount(deopt_info->top_frame());
+    int jsframe_count = frame_count;
+    int update_feedback_count = 0;
+    deopt_info->set_translation_index(
+        translation_array_builder_->BeginTranslation(frame_count, jsframe_count,
+                                                     update_feedback_count));
+
+    const InputLocation* current_input_location = deopt_info->input_locations();
+
+    if (deopt_info->top_frame().parent()) {
+      // Deopt input locations are in the order of deopt frame emission, so
+      // update the pointer after emitting the parent frame.
+      BuildDeoptFrame(*deopt_info->top_frame().parent(),
+                      current_input_location);
+    }
+
+    const DeoptFrame& top_frame = deopt_info->top_frame();
+    switch (top_frame.type()) {
+      case DeoptFrame::FrameType::kInterpretedFrame: {
+        const InterpretedDeoptFrame& interpreted_frame =
+            top_frame.as_interpreted();
+
+        // Return offsets are counted from the end of the translation frame,
+        // which is the array [parameters..., locals..., accumulator]. Since
+        // it's the end, we don't need to worry about earlier frames.
+        int return_offset;
+        if (deopt_info->result_location() ==
+            interpreter::Register::virtual_accumulator()) {
+          return_offset = 0;
+        } else if (deopt_info->result_location().is_parameter()) {
+          // This is slightly tricky to reason about because of zero indexing
+          // and fence post errors. As an example, consider a frame with 2
+          // locals and 2 parameters, where we want argument index 1 -- looking
+          // at the array in reverse order we have:
+          //   [acc, r1, r0, a1, a0]
+          //                  ^
+          // and this calculation gives, correctly:
+          //   2 + 2 - 1 = 3
+          return_offset = interpreted_frame.unit().register_count() +
+                          interpreted_frame.unit().parameter_count() -
+                          deopt_info->result_location().ToParameterIndex();
+        } else {
+          return_offset = interpreted_frame.unit().register_count() -
+                          deopt_info->result_location().index();
+        }
+        translation_array_builder_->BeginInterpretedFrame(
+            interpreted_frame.bytecode_position(),
+            GetDeoptLiteral(
+                *interpreted_frame.unit().shared_function_info().object()),
+            interpreted_frame.unit().register_count(), return_offset,
+            deopt_info->result_size());
+
+        BuildDeoptFrameValues(
+            interpreted_frame.unit(), interpreted_frame.frame_state(),
+            current_input_location, deopt_info->result_location(),
+            deopt_info->result_size());
+        break;
+      }
+      case DeoptFrame::FrameType::kBuiltinContinuationFrame: {
+        const BuiltinContinuationDeoptFrame& builtin_continuation_frame =
+            top_frame.as_builtin_continuation();
+
+        translation_array_builder_->BeginBuiltinContinuationFrame(
+            Builtins::GetContinuationBytecodeOffset(
+                builtin_continuation_frame.builtin_id()),
+            GetDeoptLiteral(*builtin_continuation_frame.parent()
+                                 ->as_interpreted()
+                                 .unit()
+                                 .shared_function_info()
+                                 .object()),
+            builtin_continuation_frame.parameters().length());
+
+        // Closure
+        translation_array_builder_->StoreOptimizedOut();
+
+        // Parameters
+        for (ValueNode* value : builtin_continuation_frame.parameters()) {
+          BuildDeoptFrameSingleValue(value, *current_input_location);
+          current_input_location++;
+        }
+
+        // Context
+        ValueNode* value = builtin_continuation_frame.context();
+        BuildDeoptFrameSingleValue(value, *current_input_location);
+        current_input_location++;
+      }
+    }
   }
 
  private:
-  MaglevCodeGeneratorImpl(Isolate* isolate,
-                          MaglevCompilationInfo* compilation_info, Graph* graph)
-      : safepoint_table_builder_(compilation_info->zone(),
-                                 graph->tagged_stack_slots(),
-                                 graph->untagged_stack_slots()),
-        code_gen_state_(isolate, compilation_info, safepoint_table_builder()),
-        masm_(&code_gen_state_),
-        processor_(&masm_),
-        graph_(graph) {}
-
-  MaybeHandle<Code> Generate() {
-    EmitCode();
-    EmitMetadata();
-    return BuildCodeObject();
-  }
-
-  void EmitCode() {
-    processor_.ProcessGraph(graph_);
-    EmitDeferredCode();
-    EmitDeopts();
-    EmitExceptionHandlerTrampolines();
-  }
-
-  void EmitDeferredCode() {
-    // Loop over deferred_code() multiple times, clearing the vector on each
-    // outer loop, so that deferred code can itself emit deferred code.
-    while (!code_gen_state_.deferred_code().empty()) {
-      for (DeferredCodeInfo* deferred_code :
-           code_gen_state_.TakeDeferredCode()) {
-        __ RecordComment("-- Deferred block");
-        __ bind(&deferred_code->deferred_code_label);
-        deferred_code->Generate(masm());
-        __ Trap();
+  constexpr int DeoptStackSlotIndexFromFPOffset(int offset) {
+    return 1 - offset / kSystemPointerSize;
+  }
+
+  int DeoptStackSlotFromStackSlot(const compiler::AllocatedOperand& operand) {
+    return DeoptStackSlotIndexFromFPOffset(
+        masm_->GetFramePointerOffsetForStackSlot(operand));
+  }
+
+  bool InReturnValues(interpreter::Register reg,
+                      interpreter::Register result_location, int result_size) {
+    if (result_size == 0 || !result_location.is_valid()) {
+      return false;
+    }
+    return base::IsInRange(reg.index(), result_location.index(),
+                           result_location.index() + result_size - 1);
+  }
+
+  void BuildDeoptFrame(const DeoptFrame& frame,
+                       const InputLocation*& current_input_location) {
+    if (frame.parent()) {
+      // Deopt input locations are in the order of deopt frame emission, so
+      // update the pointer after emitting the parent frame.
+      BuildDeoptFrame(*frame.parent(), current_input_location);
+    }
+
+    switch (frame.type()) {
+      case DeoptFrame::FrameType::kInterpretedFrame: {
+        const InterpretedDeoptFrame& interpreted_frame = frame.as_interpreted();
+        // Returns are used for updating an accumulator or register after a
+        // lazy deopt.
+        const int return_offset = 0;
+        const int return_count = 0;
+        translation_array_builder_->BeginInterpretedFrame(
+            interpreted_frame.bytecode_position(),
+            GetDeoptLiteral(
+                *interpreted_frame.unit().shared_function_info().object()),
+            interpreted_frame.unit().register_count(), return_offset,
+            return_count);
+
+        BuildDeoptFrameValues(
+            interpreted_frame.unit(), interpreted_frame.frame_state(),
+            current_input_location, interpreter::Register::invalid_value(),
+            return_count);
+        break;
+      }
+      case DeoptFrame::FrameType::kBuiltinContinuationFrame: {
+        const BuiltinContinuationDeoptFrame& builtin_continuation_frame =
+            frame.as_builtin_continuation();
+
+        translation_array_builder_->BeginBuiltinContinuationFrame(
+            Builtins::GetContinuationBytecodeOffset(
+                builtin_continuation_frame.builtin_id()),
+            GetDeoptLiteral(*builtin_continuation_frame.parent()
+                                 ->as_interpreted()
+                                 .unit()
+                                 .shared_function_info()
+                                 .object()),
+            builtin_continuation_frame.parameters().length());
+
+        // Closure
+        translation_array_builder_->StoreOptimizedOut();
+
+        // Parameters
+        for (ValueNode* value : builtin_continuation_frame.parameters()) {
+          BuildDeoptFrameSingleValue(value, *current_input_location);
+          current_input_location++;
+        }
+
+        // Context
+        ValueNode* value = builtin_continuation_frame.context();
+        BuildDeoptFrameSingleValue(value, *current_input_location);
+        current_input_location++;
+
+        break;
       }
     }
   }
 
-  void EmitDeopts() {
-    deopt_exit_start_offset_ = __ pc_offset();
+  void BuildDeoptStoreRegister(const compiler::AllocatedOperand& operand,
+                               ValueRepresentation repr) {
+    switch (repr) {
+      case ValueRepresentation::kTagged:
+        translation_array_builder_->StoreRegister(operand.GetRegister());
+        break;
+      case ValueRepresentation::kInt32:
+        translation_array_builder_->StoreInt32Register(operand.GetRegister());
+        break;
+      case ValueRepresentation::kFloat64:
+        translation_array_builder_->StoreDoubleRegister(
+            operand.GetDoubleRegister());
+        break;
+    }
+  }
 
-    int deopt_index = 0;
+  void BuildDeoptStoreStackSlot(const compiler::AllocatedOperand& operand,
+                                ValueRepresentation repr) {
+    int stack_slot = DeoptStackSlotFromStackSlot(operand);
+    switch (repr) {
+      case ValueRepresentation::kTagged:
+        translation_array_builder_->StoreStackSlot(stack_slot);
+        break;
+      case ValueRepresentation::kInt32:
+        translation_array_builder_->StoreInt32StackSlot(stack_slot);
+        break;
+      case ValueRepresentation::kFloat64:
+        translation_array_builder_->StoreDoubleStackSlot(stack_slot);
+        break;
+    }
+  }
 
-    __ RecordComment("-- Non-lazy deopts");
-    for (EagerDeoptInfo* deopt_info : code_gen_state_.eager_deopts()) {
-      // TODO(leszeks): Record source positions.
-      __ RecordDeoptReason(deopt_info->reason, 0, SourcePosition::Unknown(),
-                           deopt_index);
-      __ bind(&deopt_info->deopt_entry_label);
-      __ CallForDeoptimization(Builtin::kDeoptimizationEntry_Eager, deopt_index,
-                               &deopt_info->deopt_entry_label,
-                               DeoptimizeKind::kEager, nullptr, nullptr);
-      deopt_index++;
-    }
-
-    __ RecordComment("-- Lazy deopts");
-    int last_updated_safepoint = 0;
-    for (LazyDeoptInfo* deopt_info : code_gen_state_.lazy_deopts()) {
-      __ bind(&deopt_info->deopt_entry_label);
-      __ CallForDeoptimization(Builtin::kDeoptimizationEntry_Lazy, deopt_index,
-                               &deopt_info->deopt_entry_label,
-                               DeoptimizeKind::kLazy, nullptr, nullptr);
-
-      last_updated_safepoint =
-          safepoint_table_builder_.UpdateDeoptimizationInfo(
-              deopt_info->deopting_call_return_pc,
-              deopt_info->deopt_entry_label.pos(), last_updated_safepoint,
-              deopt_index);
-      deopt_index++;
-    }
-  }
-
-  void EmitExceptionHandlerTrampolines() {
-    if (code_gen_state_.handlers().size() == 0) return;
-    __ RecordComment("-- Exception handler trampolines");
-    for (NodeBase* node : code_gen_state_.handlers()) {
-      ExceptionHandlerTrampolineBuilder::Build(masm(), node);
-    }
-  }
-
-  void EmitMetadata() {
-    // Final alignment before starting on the metadata section.
-    masm()->Align(Code::kMetadataAlignment);
-
-    safepoint_table_builder()->Emit(masm());
-
-    // Exception handler table.
-    handler_table_offset_ = HandlerTable::EmitReturnTableStart(masm());
-    for (NodeBase* node : code_gen_state_.handlers()) {
-      ExceptionHandlerInfo* info = node->exception_handler_info();
-      HandlerTable::EmitReturnEntry(masm(), info->pc_offset,
-                                    info->trampoline_entry.pos());
-    }
-  }
-
-  MaybeHandle<Code> BuildCodeObject() {
-    CodeDesc desc;
-    masm()->GetCode(isolate(), &desc, safepoint_table_builder(),
-                    handler_table_offset_);
-    return Factory::CodeBuilder{isolate(), desc, CodeKind::MAGLEV}
-        .set_stack_slots(stack_slot_count_with_fixed_frame())
-        .set_deoptimization_data(GenerateDeoptimizationData())
-        .TryBuild();
-  }
-
-  Handle<DeoptimizationData> GenerateDeoptimizationData() {
-    int eager_deopt_count =
-        static_cast<int>(code_gen_state_.eager_deopts().size());
-    int lazy_deopt_count =
-        static_cast<int>(code_gen_state_.lazy_deopts().size());
-    int deopt_count = lazy_deopt_count + eager_deopt_count;
-    if (deopt_count == 0) {
-      return DeoptimizationData::Empty(isolate());
-    }
-    Handle<DeoptimizationData> data =
-        DeoptimizationData::New(isolate(), deopt_count, AllocationType::kOld);
-
-    Handle<TranslationArray> translation_array =
-        code_gen_state_.compilation_info()
-            ->translation_array_builder()
-            .ToTranslationArray(isolate()->factory());
+  void BuildDeoptFrameSingleValue(ValueNode* value,
+                                  const InputLocation& input_location) {
+    if (input_location.operand().IsConstant()) {
+      translation_array_builder_->StoreLiteral(
+          GetDeoptLiteral(*value->Reify(local_isolate_)));
+    } else {
+      const compiler::AllocatedOperand& operand =
+          compiler::AllocatedOperand::cast(input_location.operand());
+      ValueRepresentation repr = value->properties().value_representation();
+      if (operand.IsAnyRegister()) {
+        BuildDeoptStoreRegister(operand, repr);
+      } else {
+        BuildDeoptStoreStackSlot(operand, repr);
+      }
+    }
+  }
+
+  void BuildDeoptFrameValues(
+      const MaglevCompilationUnit& compilation_unit,
+      const CompactInterpreterFrameState* checkpoint_state,
+      const InputLocation*& input_location,
+      interpreter::Register result_location, int result_size) {
+    // Closure
+    if (compilation_unit.inlining_depth() == 0) {
+      int closure_index = DeoptStackSlotIndexFromFPOffset(
+          StandardFrameConstants::kFunctionOffset);
+      translation_array_builder_->StoreStackSlot(closure_index);
+    } else {
+      translation_array_builder_->StoreLiteral(
+          GetDeoptLiteral(*compilation_unit.function().object()));
+    }
+
+    // TODO(leszeks): The input locations array happens to be in the same order
+    // as parameters+context+locals+accumulator are accessed here. We should
+    // make this clearer and guard against this invariant failing.
+
+    // Parameters
     {
-      DisallowGarbageCollection no_gc;
-      auto raw_data = *data;
+      int i = 0;
+      checkpoint_state->ForEachParameter(
+          compilation_unit, [&](ValueNode* value, interpreter::Register reg) {
+            DCHECK_EQ(reg.ToParameterIndex(), i);
+            if (InReturnValues(reg, result_location, result_size)) {
+              translation_array_builder_->StoreOptimizedOut();
+            } else {
+              BuildDeoptFrameSingleValue(value, *input_location);
+              input_location++;
+            }
+            i++;
+          });
+    }
 
-      raw_data.SetTranslationByteArray(*translation_array);
-      // TODO(leszeks): Fix with the real inlined function count.
-      raw_data.SetInlinedFunctionCount(Smi::zero());
-      // TODO(leszeks): Support optimization IDs
-      raw_data.SetOptimizationId(Smi::zero());
-
-      DCHECK_NE(deopt_exit_start_offset_, -1);
-      raw_data.SetDeoptExitStart(Smi::FromInt(deopt_exit_start_offset_));
-      raw_data.SetEagerDeoptCount(Smi::FromInt(eager_deopt_count));
-      raw_data.SetLazyDeoptCount(Smi::FromInt(lazy_deopt_count));
-
-      raw_data.SetSharedFunctionInfo(*code_gen_state_.compilation_info()
-                                          ->toplevel_compilation_unit()
-                                          ->shared_function_info()
-                                          .object());
-    }
-
-    IdentityMap<int, base::DefaultAllocationPolicy>& deopt_literals =
-        code_gen_state_.compilation_info()->deopt_literals();
-    Handle<DeoptimizationLiteralArray> literals =
-        isolate()->factory()->NewDeoptimizationLiteralArray(
-            deopt_literals.size() + 1);
-    // TODO(leszeks): Fix with the real inlining positions.
-    Handle<PodArray<InliningPosition>> inlining_positions =
-        PodArray<InliningPosition>::New(isolate(), 0);
-    DisallowGarbageCollection no_gc;
+    // Context
+    ValueNode* value = checkpoint_state->context(compilation_unit);
+    BuildDeoptFrameSingleValue(value, *input_location);
+    input_location++;
 
-    auto raw_literals = *literals;
-    auto raw_data = *data;
-    IdentityMap<int, base::DefaultAllocationPolicy>::IteratableScope iterate(
-        &deopt_literals);
-    for (auto it = iterate.begin(); it != iterate.end(); ++it) {
-      raw_literals.set(*it.entry(), it.key());
-    }
-    // Add the bytecode to the deopt literals to make sure it's held strongly.
-    // TODO(leszeks): Do this for inlined functions too.
-    raw_literals.set(deopt_literals.size(), *code_gen_state_.compilation_info()
-                                                 ->toplevel_compilation_unit()
-                                                 ->bytecode()
-                                                 .object());
-    raw_data.SetLiteralArray(raw_literals);
-
-    // TODO(leszeks): Fix with the real inlining positions.
-    raw_data.SetInliningPositions(*inlining_positions);
-
-    // TODO(leszeks): Fix once we have OSR.
-    BytecodeOffset osr_offset = BytecodeOffset::None();
-    raw_data.SetOsrBytecodeOffset(Smi::FromInt(osr_offset.ToInt()));
-    raw_data.SetOsrPcOffset(Smi::FromInt(-1));
-
-    // Populate deoptimization entries.
-    int i = 0;
-    for (EagerDeoptInfo* deopt_info : code_gen_state_.eager_deopts()) {
-      DCHECK_NE(deopt_info->translation_index, -1);
-      raw_data.SetBytecodeOffset(i, deopt_info->state.bytecode_position);
-      raw_data.SetTranslationIndex(i,
-                                   Smi::FromInt(deopt_info->translation_index));
-      raw_data.SetPc(i, Smi::FromInt(deopt_info->deopt_entry_label.pos()));
-#ifdef DEBUG
-      raw_data.SetNodeId(i, Smi::FromInt(i));
-#endif  // DEBUG
-      i++;
+    // Locals
+    {
+      int i = 0;
+      checkpoint_state->ForEachLocal(
+          compilation_unit, [&](ValueNode* value, interpreter::Register reg) {
+            DCHECK_LE(i, reg.index());
+            if (InReturnValues(reg, result_location, result_size)) return;
+            while (i < reg.index()) {
+              translation_array_builder_->StoreOptimizedOut();
+              i++;
+            }
+            DCHECK_EQ(i, reg.index());
+            BuildDeoptFrameSingleValue(value, *input_location);
+            input_location++;
+            i++;
+          });
+      while (i < compilation_unit.register_count()) {
+        translation_array_builder_->StoreOptimizedOut();
+        i++;
+      }
     }
-    for (LazyDeoptInfo* deopt_info : code_gen_state_.lazy_deopts()) {
-      DCHECK_NE(deopt_info->translation_index, -1);
-      raw_data.SetBytecodeOffset(i, deopt_info->state.bytecode_position);
-      raw_data.SetTranslationIndex(i,
-                                   Smi::FromInt(deopt_info->translation_index));
-      raw_data.SetPc(i, Smi::FromInt(deopt_info->deopt_entry_label.pos()));
-#ifdef DEBUG
-      raw_data.SetNodeId(i, Smi::FromInt(i));
-#endif  // DEBUG
-      i++;
+
+    // Accumulator
+    {
+      if (checkpoint_state->liveness()->AccumulatorIsLive() &&
+          !InReturnValues(interpreter::Register::virtual_accumulator(),
+                          result_location, result_size)) {
+        ValueNode* value = checkpoint_state->accumulator(compilation_unit);
+        BuildDeoptFrameSingleValue(value, *input_location);
+        input_location++;
+      } else {
+        translation_array_builder_->StoreOptimizedOut();
+      }
+    }
+  }
+
+  int GetDeoptLiteral(Object obj) {
+    IdentityMapFindResult<int> res = deopt_literals_->FindOrInsert(obj);
+    if (!res.already_exists) {
+      DCHECK_EQ(0, *res.entry);
+      *res.entry = deopt_literals_->size() - 1;
     }
+    return *res.entry;
+  }
 
-    return data;
+  LocalIsolate* local_isolate_;
+  MaglevAssembler* masm_;
+  TranslationArrayBuilder* translation_array_builder_;
+  IdentityMap<int, base::DefaultAllocationPolicy>* deopt_literals_;
+};
+
+}  // namespace
+
+MaglevCodeGenerator::MaglevCodeGenerator(
+    LocalIsolate* isolate, MaglevCompilationInfo* compilation_info,
+    Graph* graph)
+    : local_isolate_(isolate),
+      safepoint_table_builder_(compilation_info->zone(),
+                               graph->tagged_stack_slots(),
+                               graph->untagged_stack_slots()),
+      translation_array_builder_(compilation_info->zone()),
+      code_gen_state_(compilation_info, &safepoint_table_builder_),
+      masm_(isolate->GetMainThreadIsolateUnsafe(), &code_gen_state_),
+      graph_(graph),
+      deopt_literals_(isolate->heap()->heap()) {}
+
+void MaglevCodeGenerator::Assemble() {
+  EmitCode();
+  EmitMetadata();
+}
+
+MaybeHandle<Code> MaglevCodeGenerator::Generate(Isolate* isolate) {
+  return BuildCodeObject(isolate);
+}
+
+void MaglevCodeGenerator::EmitCode() {
+  GraphProcessor<NodeMultiProcessor<SafepointingNodeProcessor,
+                                    MaglevCodeGeneratingNodeProcessor>>
+      processor(SafepointingNodeProcessor{local_isolate_},
+                MaglevCodeGeneratingNodeProcessor{masm()});
+  processor.ProcessGraph(graph_);
+  EmitDeferredCode();
+  EmitDeopts();
+  EmitExceptionHandlerTrampolines();
+}
+
+void MaglevCodeGenerator::EmitDeferredCode() {
+  // Loop over deferred_code() multiple times, clearing the vector on each
+  // outer loop, so that deferred code can itself emit deferred code.
+  while (!code_gen_state_.deferred_code().empty()) {
+    for (DeferredCodeInfo* deferred_code : code_gen_state_.TakeDeferredCode()) {
+      __ RecordComment("-- Deferred block");
+      __ bind(&deferred_code->deferred_code_label);
+      deferred_code->Generate(masm());
+      __ Trap();
+    }
+  }
+}
+
+void MaglevCodeGenerator::EmitDeopts() {
+  MaglevTranslationArrayBuilder translation_builder(
+      local_isolate_, &masm_, &translation_array_builder_, &deopt_literals_);
+
+  deopt_exit_start_offset_ = __ pc_offset();
+
+  int deopt_index = 0;
+
+  __ RecordComment("-- Non-lazy deopts");
+  for (EagerDeoptInfo* deopt_info : code_gen_state_.eager_deopts()) {
+    local_isolate_->heap()->Safepoint();
+    translation_builder.BuildEagerDeopt(deopt_info);
+
+    if (masm_.compilation_info()->collect_source_positions()) {
+      __ RecordDeoptReason(deopt_info->reason(), 0,
+                           GetSourcePosition(deopt_info->top_frame()),
+                           deopt_index);
+    }
+    __ bind(deopt_info->deopt_entry_label());
+    __ CallForDeoptimization(Builtin::kDeoptimizationEntry_Eager, deopt_index,
+                             deopt_info->deopt_entry_label(),
+                             DeoptimizeKind::kEager, nullptr, nullptr);
+    deopt_index++;
+  }
+
+  __ RecordComment("-- Lazy deopts");
+  int last_updated_safepoint = 0;
+  for (LazyDeoptInfo* deopt_info : code_gen_state_.lazy_deopts()) {
+    local_isolate_->heap()->Safepoint();
+    translation_builder.BuildLazyDeopt(deopt_info);
+
+    if (masm_.compilation_info()->collect_source_positions()) {
+      __ RecordDeoptReason(DeoptimizeReason::kUnknown, 0,
+                           GetSourcePosition(deopt_info->top_frame()),
+                           deopt_index);
+    }
+    __ bind(deopt_info->deopt_entry_label());
+    __ CallForDeoptimization(Builtin::kDeoptimizationEntry_Lazy, deopt_index,
+                             deopt_info->deopt_entry_label(),
+                             DeoptimizeKind::kLazy, nullptr, nullptr);
+
+    last_updated_safepoint = safepoint_table_builder_.UpdateDeoptimizationInfo(
+        deopt_info->deopting_call_return_pc(),
+        deopt_info->deopt_entry_label()->pos(), last_updated_safepoint,
+        deopt_index);
+    deopt_index++;
   }
+}
 
-  int stack_slot_count() const { return code_gen_state_.stack_slots(); }
-  int stack_slot_count_with_fixed_frame() const {
-    return stack_slot_count() + StandardFrameConstants::kFixedSlotCount;
+void MaglevCodeGenerator::EmitExceptionHandlerTrampolines() {
+  if (code_gen_state_.handlers().size() == 0) return;
+  __ RecordComment("-- Exception handler trampolines");
+  for (NodeBase* node : code_gen_state_.handlers()) {
+    ExceptionHandlerTrampolineBuilder::Build(masm(), node);
   }
+}
 
-  Isolate* isolate() const { return code_gen_state_.isolate(); }
-  MaglevAssembler* masm() { return &masm_; }
-  MaglevSafepointTableBuilder* safepoint_table_builder() {
-    return &safepoint_table_builder_;
+void MaglevCodeGenerator::EmitMetadata() {
+  // Final alignment before starting on the metadata section.
+  masm()->Align(Code::kMetadataAlignment);
+
+  safepoint_table_builder_.Emit(masm());
+
+  // Exception handler table.
+  handler_table_offset_ = HandlerTable::EmitReturnTableStart(masm());
+  for (NodeBase* node : code_gen_state_.handlers()) {
+    ExceptionHandlerInfo* info = node->exception_handler_info();
+    HandlerTable::EmitReturnEntry(masm(), info->pc_offset,
+                                  info->trampoline_entry.pos());
   }
+}
 
-  MaglevSafepointTableBuilder safepoint_table_builder_;
-  MaglevCodeGenState code_gen_state_;
-  MaglevAssembler masm_;
-  GraphProcessor<MaglevCodeGeneratingNodeProcessor> processor_;
-  Graph* const graph_;
+MaybeHandle<Code> MaglevCodeGenerator::BuildCodeObject(Isolate* isolate) {
+  CodeDesc desc;
+  masm()->GetCode(isolate, &desc, &safepoint_table_builder_,
+                  handler_table_offset_);
+  return Factory::CodeBuilder{isolate, desc, CodeKind::MAGLEV}
+      .set_stack_slots(stack_slot_count_with_fixed_frame())
+      .set_deoptimization_data(GenerateDeoptimizationData(isolate))
+      .TryBuild();
+}
 
-  int deopt_exit_start_offset_ = -1;
-  int handler_table_offset_ = 0;
-};
+Handle<DeoptimizationData> MaglevCodeGenerator::GenerateDeoptimizationData(
+    Isolate* isolate) {
+  int eager_deopt_count =
+      static_cast<int>(code_gen_state_.eager_deopts().size());
+  int lazy_deopt_count = static_cast<int>(code_gen_state_.lazy_deopts().size());
+  int deopt_count = lazy_deopt_count + eager_deopt_count;
+  if (deopt_count == 0) {
+    return DeoptimizationData::Empty(isolate);
+  }
+  Handle<DeoptimizationData> data =
+      DeoptimizationData::New(isolate, deopt_count, AllocationType::kOld);
+
+  Handle<TranslationArray> translation_array =
+      translation_array_builder_.ToTranslationArray(isolate->factory());
+  {
+    DisallowGarbageCollection no_gc;
+    auto raw_data = *data;
+
+    raw_data.SetTranslationByteArray(*translation_array);
+    // TODO(leszeks): Fix with the real inlined function count.
+    raw_data.SetInlinedFunctionCount(Smi::zero());
+    // TODO(leszeks): Support optimization IDs
+    raw_data.SetOptimizationId(Smi::zero());
+
+    DCHECK_NE(deopt_exit_start_offset_, -1);
+    raw_data.SetDeoptExitStart(Smi::FromInt(deopt_exit_start_offset_));
+    raw_data.SetEagerDeoptCount(Smi::FromInt(eager_deopt_count));
+    raw_data.SetLazyDeoptCount(Smi::FromInt(lazy_deopt_count));
+
+    raw_data.SetSharedFunctionInfo(*code_gen_state_.compilation_info()
+                                        ->toplevel_compilation_unit()
+                                        ->shared_function_info()
+                                        .object());
+  }
+
+  Handle<DeoptimizationLiteralArray> literals =
+      isolate->factory()->NewDeoptimizationLiteralArray(deopt_literals_.size() +
+                                                        1);
+  // TODO(leszeks): Fix with the real inlining positions.
+  Handle<PodArray<InliningPosition>> inlining_positions =
+      PodArray<InliningPosition>::New(isolate, 0);
+  DisallowGarbageCollection no_gc;
+
+  auto raw_literals = *literals;
+  auto raw_data = *data;
+  IdentityMap<int, base::DefaultAllocationPolicy>::IteratableScope iterate(
+      &deopt_literals_);
+  for (auto it = iterate.begin(); it != iterate.end(); ++it) {
+    raw_literals.set(*it.entry(), it.key());
+  }
+  // Add the bytecode to the deopt literals to make sure it's held strongly.
+  // TODO(leszeks): Do this for inlined functions too.
+  raw_literals.set(deopt_literals_.size(), *code_gen_state_.compilation_info()
+                                                ->toplevel_compilation_unit()
+                                                ->bytecode()
+                                                .object());
+  raw_data.SetLiteralArray(raw_literals);
+
+  // TODO(leszeks): Fix with the real inlining positions.
+  raw_data.SetInliningPositions(*inlining_positions);
+
+  // TODO(leszeks): Fix once we have OSR.
+  BytecodeOffset osr_offset = BytecodeOffset::None();
+  raw_data.SetOsrBytecodeOffset(Smi::FromInt(osr_offset.ToInt()));
+  raw_data.SetOsrPcOffset(Smi::FromInt(-1));
+
+  // Populate deoptimization entries.
+  int i = 0;
+  for (EagerDeoptInfo* deopt_info : code_gen_state_.eager_deopts()) {
+    DCHECK_NE(deopt_info->translation_index(), -1);
+    raw_data.SetBytecodeOffset(i, GetBytecodeOffset(deopt_info->top_frame()));
+    raw_data.SetTranslationIndex(i,
+                                 Smi::FromInt(deopt_info->translation_index()));
+    raw_data.SetPc(i, Smi::FromInt(deopt_info->deopt_entry_label()->pos()));
+#ifdef DEBUG
+    raw_data.SetNodeId(i, Smi::FromInt(i));
+#endif  // DEBUG
+    i++;
+  }
+  for (LazyDeoptInfo* deopt_info : code_gen_state_.lazy_deopts()) {
+    DCHECK_NE(deopt_info->translation_index(), -1);
+    raw_data.SetBytecodeOffset(i, GetBytecodeOffset(deopt_info->top_frame()));
+    raw_data.SetTranslationIndex(i,
+                                 Smi::FromInt(deopt_info->translation_index()));
+    raw_data.SetPc(i, Smi::FromInt(deopt_info->deopt_entry_label()->pos()));
+#ifdef DEBUG
+    raw_data.SetNodeId(i, Smi::FromInt(i));
+#endif  // DEBUG
+    i++;
+  }
 
-// static
-MaybeHandle<Code> MaglevCodeGenerator::Generate(
-    Isolate* isolate, MaglevCompilationInfo* compilation_info, Graph* graph) {
-  return MaglevCodeGeneratorImpl::Generate(isolate, compilation_info, graph);
+  return data;
 }
 
 }  // namespace maglev
diff -r -u --color up/v8/src/maglev/maglev-code-generator.h nw/v8/src/maglev/maglev-code-generator.h
--- up/v8/src/maglev/maglev-code-generator.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/maglev/maglev-code-generator.h	2023-01-19 16:46:36.298109548 -0500
@@ -5,7 +5,12 @@
 #ifndef V8_MAGLEV_MAGLEV_CODE_GENERATOR_H_
 #define V8_MAGLEV_MAGLEV_CODE_GENERATOR_H_
 
+#include "src/codegen/maglev-safepoint-table.h"
 #include "src/common/globals.h"
+#include "src/deoptimizer/translation-array.h"
+#include "src/maglev/maglev-assembler.h"
+#include "src/maglev/maglev-code-gen-state.h"
+#include "src/utils/identity-map.h"
 
 namespace v8 {
 namespace internal {
@@ -14,11 +19,42 @@
 class Graph;
 class MaglevCompilationInfo;
 
-class MaglevCodeGenerator : public AllStatic {
+class MaglevCodeGenerator final {
  public:
-  static MaybeHandle<Code> Generate(Isolate* isolate,
-                                    MaglevCompilationInfo* compilation_info,
-                                    Graph* graph);
+  MaglevCodeGenerator(LocalIsolate* isolate,
+                      MaglevCompilationInfo* compilation_info, Graph* graph);
+
+  void Assemble();
+
+  MaybeHandle<Code> Generate(Isolate* isolate);
+
+ private:
+  void EmitCode();
+  void EmitDeferredCode();
+  void EmitDeopts();
+  void EmitExceptionHandlerTrampolines();
+  void EmitMetadata();
+
+  MaybeHandle<Code> BuildCodeObject(Isolate* isolate);
+  Handle<DeoptimizationData> GenerateDeoptimizationData(Isolate* isolate);
+
+  int stack_slot_count() const { return code_gen_state_.stack_slots(); }
+  int stack_slot_count_with_fixed_frame() const {
+    return stack_slot_count() + StandardFrameConstants::kFixedSlotCount;
+  }
+
+  MaglevAssembler* masm() { return &masm_; }
+
+  LocalIsolate* local_isolate_;
+  MaglevSafepointTableBuilder safepoint_table_builder_;
+  TranslationArrayBuilder translation_array_builder_;
+  MaglevCodeGenState code_gen_state_;
+  MaglevAssembler masm_;
+  Graph* const graph_;
+
+  IdentityMap<int, base::DefaultAllocationPolicy> deopt_literals_;
+  int deopt_exit_start_offset_ = -1;
+  int handler_table_offset_ = 0;
 };
 
 }  // namespace maglev
diff -r -u --color up/v8/src/maglev/maglev-compilation-info.cc nw/v8/src/maglev/maglev-compilation-info.cc
--- up/v8/src/maglev/maglev-compilation-info.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/maglev/maglev-compilation-info.cc	2023-01-19 16:46:36.298109548 -0500
@@ -9,13 +9,12 @@
 #include "src/execution/isolate.h"
 #include "src/flags/flags.h"
 #include "src/handles/persistent-handles.h"
+#include "src/maglev/maglev-code-generator.h"
 #include "src/maglev/maglev-compilation-unit.h"
-#include "src/maglev/maglev-compiler.h"
 #include "src/maglev/maglev-concurrent-dispatcher.h"
 #include "src/maglev/maglev-graph-labeller.h"
 #include "src/objects/js-function-inl.h"
 #include "src/utils/identity-map.h"
-#include "src/utils/locked-queue-inl.h"
 
 namespace v8 {
 namespace internal {
@@ -64,6 +63,12 @@
               ReadOnlyRoots(isolate).one_closure_cell_map()) {
   DCHECK(v8_flags.maglev);
 
+  collect_source_positions_ = isolate->NeedsDetailedOptimizedCodeLineInfo();
+  if (collect_source_positions_) {
+    SharedFunctionInfo::EnsureSourcePositionsAvailable(
+        isolate, handle(function->shared(), isolate));
+  }
+
   MaglevCompilationHandleScope compilation(isolate, this);
 
   compiler::CompilationDependencies* deps =
@@ -92,12 +97,9 @@
   graph_labeller_.reset(graph_labeller);
 }
 
-void MaglevCompilationInfo::set_translation_array_builder(
-    std::unique_ptr<TranslationArrayBuilder> translation_array_builder,
-    std::unique_ptr<IdentityMap<int, base::DefaultAllocationPolicy>>
-        deopt_literals) {
-  translation_array_builder_ = std::move(translation_array_builder);
-  deopt_literals_ = std::move(deopt_literals);
+void MaglevCompilationInfo::set_code_generator(
+    std::unique_ptr<MaglevCodeGenerator> code_generator) {
+  code_generator_ = std::move(code_generator);
 }
 
 void MaglevCompilationInfo::ReopenHandlesInNewHandleScope(Isolate* isolate) {}
diff -r -u --color up/v8/src/maglev/maglev-compilation-info.h nw/v8/src/maglev/maglev-compilation-info.h
--- up/v8/src/maglev/maglev-compilation-info.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/maglev/maglev-compilation-info.h	2023-01-19 16:46:36.298109548 -0500
@@ -30,9 +30,9 @@
 
 namespace maglev {
 
-class Graph;
 class MaglevCompilationUnit;
 class MaglevGraphLabeller;
+class MaglevCodeGenerator;
 
 // A list of v8_flag values copied into the MaglevCompilationInfo for
 // guaranteed {immutable,threadsafe} access.
@@ -66,21 +66,8 @@
     return graph_labeller_.get();
   }
 
-  void set_graph(Graph* graph) { graph_ = graph; }
-  Graph* graph() const { return graph_; }
-
-  void set_translation_array_builder(
-      std::unique_ptr<TranslationArrayBuilder> translation_array_builder,
-      std::unique_ptr<IdentityMap<int, base::DefaultAllocationPolicy>>
-          deopt_literals);
-  TranslationArrayBuilder& translation_array_builder() const {
-    DCHECK(translation_array_builder_);
-    return *translation_array_builder_;
-  }
-  IdentityMap<int, base::DefaultAllocationPolicy>& deopt_literals() const {
-    DCHECK(deopt_literals_);
-    return *deopt_literals_;
-  }
+  void set_code_generator(std::unique_ptr<MaglevCodeGenerator> code_generator);
+  MaglevCodeGenerator* code_generator() const { return code_generator_.get(); }
 
   // Flag accessors (for thread-safe access to global flags).
   // TODO(v8:7700): Consider caching these.
@@ -88,6 +75,7 @@
   bool Name() const { return Name##_; }
   MAGLEV_COMPILATION_FLAG_LIST(V)
 #undef V
+  bool collect_source_positions() const { return collect_source_positions_; }
 
   bool specialize_to_function_context() const {
     return specialize_to_function_context_;
@@ -117,15 +105,12 @@
   std::unique_ptr<MaglevGraphLabeller> graph_labeller_;
 
   // Produced off-thread during ExecuteJobImpl.
-  Graph* graph_ = nullptr;
-
-  std::unique_ptr<TranslationArrayBuilder> translation_array_builder_;
-  std::unique_ptr<IdentityMap<int, base::DefaultAllocationPolicy>>
-      deopt_literals_;
+  std::unique_ptr<MaglevCodeGenerator> code_generator_;
 
 #define V(Name) const bool Name##_;
   MAGLEV_COMPILATION_FLAG_LIST(V)
 #undef V
+  bool collect_source_positions_;
 
   // If enabled, the generated code can rely on the function context to be a
   // constant (known at compile-time). This opens new optimization
diff -r -u --color up/v8/src/maglev/maglev-compilation-unit.cc nw/v8/src/maglev/maglev-compilation-unit.cc
--- up/v8/src/maglev/maglev-compilation-unit.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/maglev/maglev-compilation-unit.cc	2023-01-19 16:46:36.298109548 -0500
@@ -30,8 +30,6 @@
       bytecode_(shared_function_info_.GetBytecodeArray()),
       feedback_(
           function_.feedback_vector(info_->broker()->dependencies()).value()),
-      bytecode_analysis_(bytecode_.object(), zone(), BytecodeOffset::None(),
-                         true),
       register_count_(bytecode_.register_count()),
       parameter_count_(bytecode_.parameter_count()),
       inlining_depth_(caller == nullptr ? 0 : caller->inlining_depth_ + 1) {}
diff -r -u --color up/v8/src/maglev/maglev-compilation-unit.h nw/v8/src/maglev/maglev-compilation-unit.h
--- up/v8/src/maglev/maglev-compilation-unit.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/maglev/maglev-compilation-unit.h	2023-01-19 16:46:36.298109548 -0500
@@ -55,9 +55,6 @@
   const compiler::JSFunctionRef& function() const { return function_; }
   const compiler::BytecodeArrayRef& bytecode() const { return bytecode_; }
   const compiler::FeedbackVectorRef& feedback() const { return feedback_; }
-  const compiler::BytecodeAnalysis& bytecode_analysis() const {
-    return bytecode_analysis_;
-  }
 
   void RegisterNodeInGraphLabeller(const Node* node);
 
@@ -68,7 +65,6 @@
   const compiler::SharedFunctionInfoRef shared_function_info_;
   const compiler::BytecodeArrayRef bytecode_;
   const compiler::FeedbackVectorRef feedback_;
-  const compiler::BytecodeAnalysis bytecode_analysis_;
   const int register_count_;
   const int parameter_count_;
   const int inlining_depth_;
diff -r -u --color up/v8/src/maglev/maglev-compiler.cc nw/v8/src/maglev/maglev-compiler.cc
--- up/v8/src/maglev/maglev-compiler.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/maglev/maglev-compiler.cc	2023-01-19 16:46:36.298109548 -0500
@@ -180,21 +180,19 @@
                            LoopUsedNodes* loop_used_nodes,
                            const ProcessingState& state) {
     int use_id = node->id();
-    detail::DeepForEachInput(
-        deopt_info,
-        [&](ValueNode* node, interpreter::Register reg, InputLocation* input) {
-          MarkUse(node, use_id, input, loop_used_nodes);
-        });
+    detail::DeepForEachInput(deopt_info,
+                             [&](ValueNode* node, InputLocation* input) {
+                               MarkUse(node, use_id, input, loop_used_nodes);
+                             });
   }
   void MarkCheckpointNodes(NodeBase* node, const LazyDeoptInfo* deopt_info,
                            LoopUsedNodes* loop_used_nodes,
                            const ProcessingState& state) {
     int use_id = node->id();
-    detail::DeepForEachInput(
-        deopt_info,
-        [&](ValueNode* node, interpreter::Register reg, InputLocation* input) {
-          MarkUse(node, use_id, input, loop_used_nodes);
-        });
+    detail::DeepForEachInput(deopt_info,
+                             [&](ValueNode* node, InputLocation* input) {
+                               MarkUse(node, use_id, input, loop_used_nodes);
+                             });
   }
 
   MaglevCompilationInfo* compilation_info_;
@@ -202,303 +200,10 @@
   std::vector<LoopUsedNodes> loop_used_nodes_;
 };
 
-class TranslationArrayProcessor {
- public:
-  explicit TranslationArrayProcessor(LocalIsolate* local_isolate,
-                                     MaglevCompilationInfo* compilation_info)
-      : local_isolate_(local_isolate), compilation_info_(compilation_info) {}
-
-  void PreProcessGraph(Graph* graph) {
-    translation_array_builder_.reset(
-        new TranslationArrayBuilder(compilation_info_->zone()));
-    deopt_literals_.reset(new IdentityMap<int, base::DefaultAllocationPolicy>(
-        local_isolate_->heap()->heap()));
-
-    tagged_slots_ = graph->tagged_stack_slots();
-  }
-
-  void PostProcessGraph(Graph* graph) {
-    compilation_info_->set_translation_array_builder(
-        std::move(translation_array_builder_), std::move(deopt_literals_));
-  }
-  void PreProcessBasicBlock(BasicBlock* block) {}
-
-  void Process(NodeBase* node, const ProcessingState& state) {
-    if (node->properties().can_eager_deopt()) {
-      EmitEagerDeopt(node->eager_deopt_info());
-    }
-    if (node->properties().can_lazy_deopt()) {
-      EmitLazyDeopt(node->lazy_deopt_info());
-    }
-  }
-
- private:
-  void EmitDeoptFrame(const MaglevCompilationUnit& unit,
-                      const CheckpointedInterpreterState& state,
-                      const InputLocation* input_locations) {
-    if (state.parent) {
-      // Deopt input locations are in the order of deopt frame emission, so
-      // update the pointer after emitting the parent frame.
-      EmitDeoptFrame(*unit.caller(), *state.parent, input_locations);
-    }
-
-    // Returns are used for updating an accumulator or register after a lazy
-    // deopt.
-    const int return_offset = 0;
-    const int return_count = 0;
-    translation_array_builder().BeginInterpretedFrame(
-        state.bytecode_position,
-        GetDeoptLiteral(*unit.shared_function_info().object()),
-        unit.register_count(), return_offset, return_count);
-
-    EmitDeoptFrameValues(unit, state.register_frame, input_locations,
-                         interpreter::Register::invalid_value(), return_count);
-  }
-
-  void EmitEagerDeopt(EagerDeoptInfo* deopt_info) {
-    int frame_count = 1 + deopt_info->unit.inlining_depth();
-    int jsframe_count = frame_count;
-    int update_feedback_count = 0;
-    deopt_info->translation_index =
-        translation_array_builder().BeginTranslation(frame_count, jsframe_count,
-                                                     update_feedback_count);
-
-    EmitDeoptFrame(deopt_info->unit, deopt_info->state,
-                   deopt_info->input_locations);
-  }
-
-  void EmitLazyDeopt(LazyDeoptInfo* deopt_info) {
-    int frame_count = 1 + deopt_info->unit.inlining_depth();
-    int jsframe_count = frame_count;
-    int update_feedback_count = 0;
-    deopt_info->translation_index =
-        translation_array_builder().BeginTranslation(frame_count, jsframe_count,
-                                                     update_feedback_count);
-
-    const MaglevCompilationUnit& unit = deopt_info->unit;
-    const InputLocation* input_locations = deopt_info->input_locations;
-
-    if (deopt_info->state.parent) {
-      // Deopt input locations are in the order of deopt frame emission, so
-      // update the pointer after emitting the parent frame.
-      EmitDeoptFrame(*unit.caller(), *deopt_info->state.parent,
-                     input_locations);
-    }
-
-    // Return offsets are counted from the end of the translation frame, which
-    // is the array [parameters..., locals..., accumulator]. Since it's the end,
-    // we don't need to worry about earlier frames.
-    int return_offset;
-    if (deopt_info->result_location ==
-        interpreter::Register::virtual_accumulator()) {
-      return_offset = 0;
-    } else if (deopt_info->result_location.is_parameter()) {
-      // This is slightly tricky to reason about because of zero indexing and
-      // fence post errors. As an example, consider a frame with 2 locals and
-      // 2 parameters, where we want argument index 1 -- looking at the array
-      // in reverse order we have:
-      //   [acc, r1, r0, a1, a0]
-      //                  ^
-      // and this calculation gives, correctly:
-      //   2 + 2 - 1 = 3
-      return_offset = unit.register_count() + unit.parameter_count() -
-                      deopt_info->result_location.ToParameterIndex();
-    } else {
-      return_offset =
-          unit.register_count() - deopt_info->result_location.index();
-    }
-    translation_array_builder().BeginInterpretedFrame(
-        deopt_info->state.bytecode_position,
-        GetDeoptLiteral(*unit.shared_function_info().object()),
-        unit.register_count(), return_offset, deopt_info->result_size);
-
-    EmitDeoptFrameValues(unit, deopt_info->state.register_frame,
-                         input_locations, deopt_info->result_location,
-                         deopt_info->result_size);
-  }
-
-  void EmitDeoptStoreRegister(const compiler::AllocatedOperand& operand,
-                              ValueRepresentation repr) {
-    switch (repr) {
-      case ValueRepresentation::kTagged:
-        translation_array_builder().StoreRegister(operand.GetRegister());
-        break;
-      case ValueRepresentation::kInt32:
-        translation_array_builder().StoreInt32Register(operand.GetRegister());
-        break;
-      case ValueRepresentation::kFloat64:
-        translation_array_builder().StoreDoubleRegister(
-            operand.GetDoubleRegister());
-        break;
-    }
-  }
-
-  void EmitDeoptStoreStackSlot(const compiler::AllocatedOperand& operand,
-                               ValueRepresentation repr) {
-    int stack_slot = DeoptStackSlotFromStackSlot(operand);
-    switch (repr) {
-      case ValueRepresentation::kTagged:
-        translation_array_builder().StoreStackSlot(stack_slot);
-        break;
-      case ValueRepresentation::kInt32:
-        translation_array_builder().StoreInt32StackSlot(stack_slot);
-        break;
-      case ValueRepresentation::kFloat64:
-        translation_array_builder().StoreDoubleStackSlot(stack_slot);
-        break;
-    }
-  }
-
-  void EmitDeoptFrameSingleValue(ValueNode* value,
-                                 const InputLocation& input_location) {
-    if (input_location.operand().IsConstant()) {
-      translation_array_builder().StoreLiteral(
-          GetDeoptLiteral(*value->Reify(local_isolate_)));
-    } else {
-      const compiler::AllocatedOperand& operand =
-          compiler::AllocatedOperand::cast(input_location.operand());
-      ValueRepresentation repr = value->properties().value_representation();
-      if (operand.IsAnyRegister()) {
-        EmitDeoptStoreRegister(operand, repr);
-      } else {
-        EmitDeoptStoreStackSlot(operand, repr);
-      }
-    }
-  }
-
-  constexpr int DeoptStackSlotIndexFromFPOffset(int offset) {
-    return 1 - offset / kSystemPointerSize;
-  }
-
-  inline int GetFramePointerOffsetForStackSlot(
-      const compiler::AllocatedOperand& operand) {
-    int index = operand.index();
-    if (operand.representation() != MachineRepresentation::kTagged) {
-      index += tagged_slots_;
-    }
-    return GetFramePointerOffsetForStackSlot(index);
-  }
-
-  inline constexpr int GetFramePointerOffsetForStackSlot(int index) {
-    return StandardFrameConstants::kExpressionsOffset -
-           index * kSystemPointerSize;
-  }
-
-  int DeoptStackSlotFromStackSlot(const compiler::AllocatedOperand& operand) {
-    return DeoptStackSlotIndexFromFPOffset(
-        GetFramePointerOffsetForStackSlot(operand));
-  }
-
-  bool InReturnValues(interpreter::Register reg,
-                      interpreter::Register result_location, int result_size) {
-    if (result_size == 0 || !result_location.is_valid()) {
-      return false;
-    }
-    return base::IsInRange(reg.index(), result_location.index(),
-                           result_location.index() + result_size - 1);
-  }
-
-  void EmitDeoptFrameValues(
-      const MaglevCompilationUnit& compilation_unit,
-      const CompactInterpreterFrameState* checkpoint_state,
-      const InputLocation*& input_location,
-      interpreter::Register result_location, int result_size) {
-    // Closure
-    if (compilation_unit.inlining_depth() == 0) {
-      int closure_index = DeoptStackSlotIndexFromFPOffset(
-          StandardFrameConstants::kFunctionOffset);
-      translation_array_builder().StoreStackSlot(closure_index);
-    } else {
-      translation_array_builder().StoreLiteral(
-          GetDeoptLiteral(*compilation_unit.function().object()));
-    }
-
-    // TODO(leszeks): The input locations array happens to be in the same order
-    // as parameters+context+locals+accumulator are accessed here. We should
-    // make this clearer and guard against this invariant failing.
-
-    // Parameters
-    {
-      int i = 0;
-      checkpoint_state->ForEachParameter(
-          compilation_unit, [&](ValueNode* value, interpreter::Register reg) {
-            DCHECK_EQ(reg.ToParameterIndex(), i);
-            if (InReturnValues(reg, result_location, result_size)) {
-              translation_array_builder().StoreOptimizedOut();
-            } else {
-              EmitDeoptFrameSingleValue(value, *input_location);
-              input_location++;
-            }
-            i++;
-          });
-    }
-
-    // Context
-    ValueNode* value = checkpoint_state->context(compilation_unit);
-    EmitDeoptFrameSingleValue(value, *input_location);
-    input_location++;
-
-    // Locals
-    {
-      int i = 0;
-      checkpoint_state->ForEachLocal(
-          compilation_unit, [&](ValueNode* value, interpreter::Register reg) {
-            DCHECK_LE(i, reg.index());
-            if (InReturnValues(reg, result_location, result_size)) return;
-            while (i < reg.index()) {
-              translation_array_builder().StoreOptimizedOut();
-              i++;
-            }
-            DCHECK_EQ(i, reg.index());
-            EmitDeoptFrameSingleValue(value, *input_location);
-            input_location++;
-            i++;
-          });
-      while (i < compilation_unit.register_count()) {
-        translation_array_builder().StoreOptimizedOut();
-        i++;
-      }
-    }
-
-    // Accumulator
-    {
-      if (checkpoint_state->liveness()->AccumulatorIsLive() &&
-          !InReturnValues(interpreter::Register::virtual_accumulator(),
-                          result_location, result_size)) {
-        ValueNode* value = checkpoint_state->accumulator(compilation_unit);
-        EmitDeoptFrameSingleValue(value, *input_location);
-        input_location++;
-      } else {
-        translation_array_builder().StoreOptimizedOut();
-      }
-    }
-  }
-
-  int GetDeoptLiteral(Object obj) {
-    IdentityMapFindResult<int> res = deopt_literals_->FindOrInsert(obj);
-    if (!res.already_exists) {
-      DCHECK_EQ(0, *res.entry);
-      *res.entry = deopt_literals_->size() - 1;
-    }
-    return *res.entry;
-  }
-
-  TranslationArrayBuilder& translation_array_builder() {
-    return *translation_array_builder_;
-  }
-
-  LocalIsolate* local_isolate_;
-  MaglevCompilationInfo* compilation_info_;
-  std::unique_ptr<TranslationArrayBuilder> translation_array_builder_;
-  std::unique_ptr<IdentityMap<int, base::DefaultAllocationPolicy>>
-      deopt_literals_;
-  int tagged_slots_;
-};
-
 // static
 void MaglevCompiler::Compile(LocalIsolate* local_isolate,
                              MaglevCompilationInfo* compilation_info) {
-  compiler::UnparkedScopeIfNeeded unparked_scope(compilation_info->broker());
+  Graph* graph = Graph::New(compilation_info->zone());
 
   // Build graph.
   if (v8_flags.print_maglev_code || v8_flags.code_comments ||
@@ -507,78 +212,80 @@
     compilation_info->set_graph_labeller(new MaglevGraphLabeller());
   }
 
-  if (v8_flags.print_maglev_code || v8_flags.print_maglev_graph ||
-      v8_flags.trace_maglev_graph_building || v8_flags.trace_maglev_regalloc) {
-    MaglevCompilationUnit* top_level_unit =
-        compilation_info->toplevel_compilation_unit();
-    std::cout << "Compiling " << Brief(*top_level_unit->function().object())
-              << " with Maglev\n";
-    BytecodeArray::Disassemble(top_level_unit->bytecode().object(), std::cout);
-    top_level_unit->feedback().object()->Print(std::cout);
-  }
-
-  Graph* graph = Graph::New(compilation_info->zone());
-
-  MaglevGraphBuilder graph_builder(
-      local_isolate, compilation_info->toplevel_compilation_unit(), graph);
-
-  graph_builder.Build();
+  {
+    UnparkedScope unparked_scope(local_isolate->heap());
 
-  if (v8_flags.print_maglev_graph) {
-    std::cout << "\nAfter graph buiding" << std::endl;
-    PrintGraph(std::cout, compilation_info, graph_builder.graph());
+    if (v8_flags.print_maglev_code || v8_flags.print_maglev_graph ||
+        v8_flags.trace_maglev_graph_building ||
+        v8_flags.trace_maglev_regalloc) {
+      MaglevCompilationUnit* top_level_unit =
+          compilation_info->toplevel_compilation_unit();
+      std::cout << "Compiling " << Brief(*top_level_unit->function().object())
+                << " with Maglev\n";
+      BytecodeArray::Disassemble(top_level_unit->bytecode().object(),
+                                 std::cout);
+      top_level_unit->feedback().object()->Print(std::cout);
+    }
+
+    MaglevGraphBuilder graph_builder(
+        local_isolate, compilation_info->toplevel_compilation_unit(), graph);
+
+    graph_builder.Build();
+
+    if (v8_flags.print_maglev_graph) {
+      std::cout << "\nAfter graph buiding" << std::endl;
+      PrintGraph(std::cout, compilation_info, graph);
+    }
   }
 
 #ifdef DEBUG
   {
     GraphProcessor<MaglevGraphVerifier> verifier(compilation_info);
-    verifier.ProcessGraph(graph_builder.graph());
+    verifier.ProcessGraph(graph);
   }
 #endif
 
   {
     GraphMultiProcessor<UseMarkingProcessor, MaglevVregAllocator> processor(
         UseMarkingProcessor{compilation_info});
-    processor.ProcessGraph(graph_builder.graph());
+    processor.ProcessGraph(graph);
   }
 
   if (v8_flags.print_maglev_graph) {
+    UnparkedScope unparked_scope(local_isolate->heap());
     std::cout << "After node processor" << std::endl;
-    PrintGraph(std::cout, compilation_info, graph_builder.graph());
+    PrintGraph(std::cout, compilation_info, graph);
   }
 
-  StraightForwardRegisterAllocator allocator(compilation_info,
-                                             graph_builder.graph());
+  StraightForwardRegisterAllocator allocator(compilation_info, graph);
 
   if (v8_flags.print_maglev_graph) {
+    UnparkedScope unparked_scope(local_isolate->heap());
     std::cout << "After register allocation" << std::endl;
-    PrintGraph(std::cout, compilation_info, graph_builder.graph());
+    PrintGraph(std::cout, compilation_info, graph);
   }
 
-  GraphProcessor<TranslationArrayProcessor> build_translation_array(
-      local_isolate, compilation_info);
-  build_translation_array.ProcessGraph(graph_builder.graph());
+  {
+    UnparkedScope unparked_scope(local_isolate->heap());
+    std::unique_ptr<MaglevCodeGenerator> code_generator =
+        std::make_unique<MaglevCodeGenerator>(local_isolate, compilation_info,
+                                              graph);
+    code_generator->Assemble();
 
-  // Stash the compiled graph on the compilation info.
-  compilation_info->set_graph(graph_builder.graph());
+    // Stash the compiled code_generator on the compilation info.
+    compilation_info->set_code_generator(std::move(code_generator));
+  }
 }
 
 // static
 MaybeHandle<CodeT> MaglevCompiler::GenerateCode(
     Isolate* isolate, MaglevCompilationInfo* compilation_info) {
-  Graph* const graph = compilation_info->graph();
-  if (graph == nullptr) {
-    // Compilation failed.
-    compilation_info->toplevel_compilation_unit()
-        ->shared_function_info()
-        .object()
-        ->set_maglev_compilation_failed(true);
-    return {};
-  }
+  MaglevCodeGenerator* const code_generator =
+      compilation_info->code_generator();
+  DCHECK_NOT_NULL(code_generator);
 
   Handle<Code> code;
-  if (!MaglevCodeGenerator::Generate(isolate, compilation_info, graph)
-           .ToHandle(&code)) {
+  if (!code_generator->Generate(isolate).ToHandle(&code)) {
     compilation_info->toplevel_compilation_unit()
         ->shared_function_info()
         .object()
diff -r -u --color up/v8/src/maglev/maglev-concurrent-dispatcher.cc nw/v8/src/maglev/maglev-concurrent-dispatcher.cc
--- up/v8/src/maglev/maglev-concurrent-dispatcher.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/maglev/maglev-concurrent-dispatcher.cc	2023-01-19 16:46:36.298109548 -0500
@@ -94,6 +94,11 @@
 MaglevCompilationJob::~MaglevCompilationJob() = default;
 
 CompilationJob::Status MaglevCompilationJob::PrepareJobImpl(Isolate* isolate) {
+  if (info()->collect_source_positions()) {
+    SharedFunctionInfo::EnsureSourcePositionsAvailable(
+        isolate,
+        info()->toplevel_compilation_unit()->shared_function_info().object());
+  }
   // TODO(v8:7700): Actual return codes.
   return CompilationJob::SUCCEEDED;
 }
diff -r -u --color up/v8/src/maglev/maglev-graph-builder.cc nw/v8/src/maglev/maglev-graph-builder.cc
--- up/v8/src/maglev/maglev-graph-builder.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/maglev/maglev-graph-builder.cc	2023-01-19 16:46:36.298109548 -0500
@@ -21,6 +21,7 @@
 #include "src/interpreter/bytecodes.h"
 #include "src/maglev/maglev-compilation-info.h"
 #include "src/maglev/maglev-compilation-unit.h"
+#include "src/maglev/maglev-graph-printer.h"
 #include "src/maglev/maglev-interpreter-frame-state.h"
 #include "src/maglev/maglev-ir.h"
 #include "src/objects/elements-kind.h"
@@ -92,7 +93,10 @@
       compilation_unit_(compilation_unit),
       parent_(parent),
       graph_(graph),
+      bytecode_analysis_(bytecode().object(), zone(), BytecodeOffset::None(),
+                         true),
       iterator_(bytecode().object()),
+      source_position_iterator_(bytecode().SourcePositionTable()),
       // Add an extra jump_target slot for the inline exit if needed.
       jump_targets_(zone()->NewArray<BasicBlockRef>(bytecode().length() +
                                                     (is_inline() ? 1 : 0))),
@@ -234,6 +238,7 @@
     case Operation::kSubtract:
     case Operation::kMultiply:
     case Operation::kDivide:
+    case Operation::kModulus:
     case Operation::kBitwiseAnd:
     case Operation::kBitwiseOr:
     case Operation::kBitwiseXor:
@@ -276,6 +281,7 @@
   V(Subtract, Int32SubtractWithOverflow, 0) \
   V(Multiply, Int32MultiplyWithOverflow, 1) \
   V(Divide, Int32DivideWithOverflow, 1)     \
+  V(Modulus, Int32ModulusWithOverflow, {})  \
   V(BitwiseAnd, Int32BitwiseAnd, ~0)        \
   V(BitwiseOr, Int32BitwiseOr, 0)           \
   V(BitwiseXor, Int32BitwiseXor, 0)         \
@@ -373,11 +379,51 @@
 }
 
 template <Operation kOperation>
+ValueNode* MaglevGraphBuilder::TryFoldInt32BinaryOperation(ValueNode* left,
+                                                           ValueNode* right) {
+  switch (kOperation) {
+    case Operation::kModulus:
+      // x % x = 0
+      if (right == left) return GetInt32Constant(0);
+      break;
+    default:
+      // TODO(victorgomes): Implement more folds.
+      break;
+  }
+  return nullptr;
+}
+
+template <Operation kOperation>
+ValueNode* MaglevGraphBuilder::TryFoldInt32BinaryOperation(ValueNode* left,
+                                                           int right) {
+  switch (kOperation) {
+    case Operation::kModulus:
+      // x % 1 = 0
+      // x % -1 = 0
+      if (right == 1 || right == -1) return GetInt32Constant(0);
+      // TODO(victorgomes): We can emit faster mod operation if {right} is power
+      // of 2, unfortunately we need to know if {left} is negative or not.
+      // Maybe emit a Int32ModulusRightIsPowerOf2?
+      break;
+    default:
+      // TODO(victorgomes): Implement more folds.
+      break;
+  }
+  return nullptr;
+}
+
+template <Operation kOperation>
 void MaglevGraphBuilder::BuildInt32BinaryOperationNode() {
   // TODO(v8:7700): Do constant folding.
   ValueNode* left = LoadRegisterInt32(0);
   ValueNode* right = GetAccumulatorInt32();
 
+  if (ValueNode* result =
+          TryFoldInt32BinaryOperation<kOperation>(left, right)) {
+    SetAccumulator(result);
+    return;
+  }
+
   SetAccumulator(AddNewInt32BinaryOperationNode<kOperation>({left, right}));
 }
 
@@ -391,6 +437,11 @@
     // value, so we can just return.
     return;
   }
+  if (ValueNode* result =
+          TryFoldInt32BinaryOperation<kOperation>(left, constant)) {
+    SetAccumulator(result);
+    return;
+  }
   ValueNode* right = GetInt32Constant(constant);
   SetAccumulator(AddNewInt32BinaryOperationNode<kOperation>({left, right}));
 }
@@ -594,28 +645,13 @@
              kOperation == Operation::kStrictEqual);
       ValueNode *left, *right;
       if (IsRegisterEqualToAccumulator(0)) {
-        interpreter::Register reg = iterator_.GetRegisterOperand(0);
-        ValueNode* value = GetTaggedValue(reg);
-        if (!value->Is<CheckedInternalizedString>()) {
-          value = AddNewNode<CheckedInternalizedString>({value});
-          current_interpreter_frame_.set(reg, value);
-          current_interpreter_frame_.set(
-              interpreter::Register::virtual_accumulator(), value);
-        }
-        left = right = value;
+        left = right = GetInternalizedString(iterator_.GetRegisterOperand(0));
+        current_interpreter_frame_.set(
+            interpreter::Register::virtual_accumulator(), left);
       } else {
-        interpreter::Register reg = iterator_.GetRegisterOperand(0);
-        left = GetTaggedValue(reg);
-        if (!left->Is<CheckedInternalizedString>()) {
-          left = AddNewNode<CheckedInternalizedString>({left});
-          current_interpreter_frame_.set(reg, left);
-        }
-        right = GetAccumulatorTagged();
-        if (!right->Is<CheckedInternalizedString>()) {
-          right = AddNewNode<CheckedInternalizedString>({right});
-          current_interpreter_frame_.set(
-              interpreter::Register::virtual_accumulator(), right);
-        }
+        left = GetInternalizedString(iterator_.GetRegisterOperand(0));
+        right =
+            GetInternalizedString(interpreter::Register::virtual_accumulator());
       }
       if (TryBuildCompareOperation<BranchIfReferenceCompare>(kOperation, left,
                                                              right)) {
@@ -725,6 +761,24 @@
   return true;
 }
 
+ValueNode* MaglevGraphBuilder::LoadAndCacheContextSlot(
+    ValueNode* context, int offset, ContextSlotMutability slot_mutability) {
+  ValueNode*& cached_value =
+      slot_mutability == ContextSlotMutability::kMutable
+          ? known_node_aspects().loaded_context_slots[{context, offset}]
+          : known_node_aspects().loaded_context_constants[{context, offset}];
+  if (cached_value) {
+    if (v8_flags.trace_maglev_graph_building) {
+      std::cout << "  * Reusing cached context slot "
+                << PrintNodeLabel(graph_labeller(), context) << "[" << offset
+                << "]: " << PrintNode(graph_labeller(), cached_value)
+                << std::endl;
+    }
+    return cached_value;
+  }
+  return cached_value = AddNewNode<LoadTaggedField>({context}, offset);
+}
+
 void MaglevGraphBuilder::BuildLoadContextSlot(
     ValueNode* context, size_t depth, int slot_index,
     ContextSlotMutability slot_mutability) {
@@ -737,12 +791,17 @@
   }
 
   for (size_t i = 0; i < depth; ++i) {
-    context = AddNewNode<LoadTaggedField>(
-        {context}, Context::OffsetOfElementAt(Context::PREVIOUS_INDEX));
+    context = LoadAndCacheContextSlot(
+        context, Context::OffsetOfElementAt(Context::PREVIOUS_INDEX),
+        kImmutable);
   }
 
-  SetAccumulator(AddNewNode<LoadTaggedField>(
-      {context}, Context::OffsetOfElementAt(slot_index)));
+  // Always load the slot here as if it were mutable. Immutable slots have a
+  // narrow range of mutability if the context escapes before the slot is
+  // initialized, so we can't safely assume that the load can be cached in case
+  // it's a load before initialization (e.g. var a = a + 42).
+  current_interpreter_frame_.set_accumulator(LoadAndCacheContextSlot(
+      context, Context::OffsetOfElementAt(slot_index), kMutable));
 }
 
 void MaglevGraphBuilder::VisitLdaContextSlot() {
@@ -785,8 +844,9 @@
   }
 
   for (size_t i = 0; i < depth; ++i) {
-    context = AddNewNode<LoadTaggedField>(
-        {context}, Context::OffsetOfElementAt(Context::PREVIOUS_INDEX));
+    context = LoadAndCacheContextSlot(
+        context, Context::OffsetOfElementAt(Context::PREVIOUS_INDEX),
+        kImmutable);
   }
 
   AddNewNode<StoreTaggedFieldWithWriteBarrier>(
@@ -837,6 +897,10 @@
     SetAccumulator(GetRootConstant(RootIndex::kTrueValue));
     return;
   }
+  if (TryBuildCompareOperation<BranchIfReferenceCompare>(
+          Operation::kStrictEqual, lhs, rhs)) {
+    return;
+  }
   SetAccumulator(AddNewNode<TaggedEqual>({lhs, rhs}));
 }
 
@@ -888,6 +952,32 @@
   SetAccumulator(AddNewNode<TestTypeOf>({value}, literal));
 }
 
+bool MaglevGraphBuilder::TryBuildScriptContextConstantAccess(
+    const compiler::GlobalAccessFeedback& global_access_feedback) {
+  if (!global_access_feedback.immutable()) return false;
+
+  base::Optional<compiler::ObjectRef> maybe_slot_value =
+      global_access_feedback.script_context().get(
+          global_access_feedback.slot_index());
+  if (!maybe_slot_value) return false;
+
+  SetAccumulator(GetConstant(maybe_slot_value.value()));
+  return true;
+}
+
+bool MaglevGraphBuilder::TryBuildScriptContextAccess(
+    const compiler::GlobalAccessFeedback& global_access_feedback) {
+  if (!global_access_feedback.IsScriptContextSlot()) return false;
+  if (TryBuildScriptContextConstantAccess(global_access_feedback)) return true;
+
+  auto script_context = GetConstant(global_access_feedback.script_context());
+  current_interpreter_frame_.set_accumulator(LoadAndCacheContextSlot(
+      script_context,
+      Context::OffsetOfElementAt(global_access_feedback.slot_index()),
+      global_access_feedback.immutable() ? kImmutable : kMutable));
+  return true;
+}
+
 bool MaglevGraphBuilder::TryBuildPropertyCellAccess(
     const compiler::GlobalAccessFeedback& global_access_feedback) {
   // TODO(leszeks): A bunch of this is copied from
@@ -1066,112 +1156,261 @@
         return NodeType::kSymbol;
       } else if (ref.IsHeapNumber()) {
         return NodeType::kHeapNumber;
+      } else if (ref.IsJSReceiver()) {
+        return NodeType::kJSReceiverWithKnownMap;
       }
       return NodeType::kHeapObjectWithKnownMap;
     }
+    case Opcode::kToNumberOrNumeric:
+      if (node->Cast<ToNumberOrNumeric>()->mode() ==
+          Object::Conversion::kToNumber) {
+        return NodeType::kNumber;
+      }
+      // TODO(verwaest): Check what we need here.
+      return NodeType::kUnknown;
+    case Opcode::kToString:
+      return NodeType::kString;
+    case Opcode::kToObject:
+      return NodeType::kJSReceiver;
+    case Opcode::kToName:
+      return NodeType::kName;
     default:
       return NodeType::kUnknown;
   }
 }
 }  // namespace
 
-void MaglevGraphBuilder::BuildCheckSmi(ValueNode* object) {
-  NodeInfo* known_info = known_node_aspects().GetOrCreateInfoFor(object);
-  if (known_info->is_smi()) return;
-  known_info->type = StaticTypeForNode(object);
-  if (known_info->is_smi()) return;
+NodeInfo* MaglevGraphBuilder::CreateInfoIfNot(ValueNode* node, NodeType type) {
+  NodeType static_type = StaticTypeForNode(node);
+  if (NodeTypeIs(static_type, type)) return nullptr;
+  NodeInfo* known_info = known_node_aspects().GetOrCreateInfoFor(node);
+  if (NodeTypeIs(known_info->type, type)) return nullptr;
+  known_info->type = CombineType(known_info->type, static_type);
+  return known_info;
+}
+
+bool MaglevGraphBuilder::CheckType(ValueNode* node, NodeType type) {
+  if (NodeTypeIs(StaticTypeForNode(node), type)) return true;
+  auto it = known_node_aspects().FindInfo(node);
+  if (!known_node_aspects().IsValid(it)) return false;
+  return NodeTypeIs(it->second.type, type);
+}
+
+bool MaglevGraphBuilder::EnsureType(ValueNode* node, NodeType type,
+                                    NodeType* old) {
+  NodeInfo* known_info = CreateInfoIfNot(node, type);
+  if (known_info == nullptr) return true;
+  if (old != nullptr) *old = known_info->type;
+  known_info->type = CombineType(known_info->type, type);
+  return false;
+}
 
-  // TODO(leszeks): Figure out a way to also handle CheckedSmiUntag.
+ValueNode* MaglevGraphBuilder::BuildSmiUntag(ValueNode* node) {
+  if (EnsureType(node, NodeType::kSmi)) {
+    return AddNewNode<UnsafeSmiUntag>({node});
+  } else {
+    return AddNewNode<CheckedSmiUntag>({node});
+  }
+}
+
+void MaglevGraphBuilder::BuildCheckSmi(ValueNode* object) {
+  if (EnsureType(object, NodeType::kSmi)) return;
   AddNewNode<CheckSmi>({object});
-  known_info->type = NodeType::kSmi;
 }
 
 void MaglevGraphBuilder::BuildCheckHeapObject(ValueNode* object) {
-  NodeInfo* known_info = known_node_aspects().GetOrCreateInfoFor(object);
-  if (known_info->is_any_heap_object()) return;
-  known_info->type = StaticTypeForNode(object);
-  if (known_info->is_any_heap_object()) return;
-
+  if (EnsureType(object, NodeType::kAnyHeapObject)) return;
   AddNewNode<CheckHeapObject>({object});
-  known_info->type = NodeType::kAnyHeapObject;
 }
 
 namespace {
-CheckType GetCheckType(NodeInfo* known_info) {
-  if (known_info->is_any_heap_object()) {
-    return CheckType::kOmitHeapObjectCheck;
-  }
-  return CheckType::kCheckHeapObject;
+CheckType GetCheckType(NodeType type) {
+  return NodeTypeIs(type, NodeType::kAnyHeapObject)
+             ? CheckType::kOmitHeapObjectCheck
+             : CheckType::kCheckHeapObject;
 }
 }  // namespace
 
 void MaglevGraphBuilder::BuildCheckString(ValueNode* object) {
-  NodeInfo* known_info = known_node_aspects().GetOrCreateInfoFor(object);
-  if (known_info->is_string()) return;
-  known_info->type = StaticTypeForNode(object);
-  if (known_info->is_string()) return;
+  NodeType known_type;
+  if (EnsureType(object, NodeType::kString, &known_type)) return;
+  AddNewNode<CheckString>({object}, GetCheckType(known_type));
+}
 
-  AddNewNode<CheckString>({object}, GetCheckType(known_info));
-  known_info->type = NodeType::kString;
+void MaglevGraphBuilder::BuildCheckNumber(ValueNode* object) {
+  if (EnsureType(object, NodeType::kNumber)) return;
+  AddNewNode<CheckNumber>({object}, Object::Conversion::kToNumber);
 }
 
 void MaglevGraphBuilder::BuildCheckSymbol(ValueNode* object) {
-  NodeInfo* known_info = known_node_aspects().GetOrCreateInfoFor(object);
-  if (known_info->is_symbol()) return;
-  known_info->type = StaticTypeForNode(object);
-  if (known_info->is_symbol()) return;
-
-  AddNewNode<CheckSymbol>({object}, GetCheckType(known_info));
-  known_info->type = NodeType::kSymbol;
-}
-
-void MaglevGraphBuilder::BuildMapCheck(ValueNode* object,
-                                       const compiler::MapRef& map) {
-  ZoneMap<ValueNode*, compiler::MapRef>& map_of_maps =
-      map.is_stable() ? known_node_aspects().stable_maps
-                      : known_node_aspects().unstable_maps;
-  auto it = map_of_maps.find(object);
-  if (it != map_of_maps.end()) {
-    if (it->second.equals(map)) {
-      // Map is already checked.
-      return;
+  NodeType known_type;
+  if (EnsureType(object, NodeType::kSymbol, &known_type)) return;
+  AddNewNode<CheckSymbol>({object}, GetCheckType(known_type));
+}
+
+namespace {
+
+class KnownMapsMerger {
+ public:
+  explicit KnownMapsMerger(compiler::JSHeapBroker* broker, ValueNode* object,
+                           KnownNodeAspects& known_node_aspects,
+                           ZoneVector<compiler::MapRef> const& maps)
+      : broker_(broker),
+        maps_(maps),
+        known_maps_are_subset_of_maps_(true),
+        emit_check_with_migration_(false) {
+    // A non-value value here means the universal set, i.e., we don't know
+    // anything about the possible maps of the object.
+    base::Optional<ZoneHandleSet<Map>> known_stable_map_set =
+        GetKnownMapSet(object, known_node_aspects.stable_maps);
+    base::Optional<ZoneHandleSet<Map>> known_unstable_map_set =
+        GetKnownMapSet(object, known_node_aspects.unstable_maps);
+
+    IntersectKnownMaps(known_stable_map_set, true);
+    IntersectKnownMaps(known_unstable_map_set, false);
+
+    // Update known maps.
+    known_node_aspects.stable_maps[object] = stable_map_set_;
+    known_node_aspects.unstable_maps[object] = unstable_map_set_;
+  }
+
+  bool known_maps_are_subset_of_maps() const {
+    return known_maps_are_subset_of_maps_;
+  }
+  bool emit_check_with_migration() const { return emit_check_with_migration_; }
+
+  ZoneHandleSet<Map> intersect_set() const {
+    ZoneHandleSet<Map> map_set;
+    map_set.Union(stable_map_set_, zone());
+    map_set.Union(unstable_map_set_, zone());
+    return map_set;
+  }
+
+  NodeType node_type() const { return node_type_; }
+
+ private:
+  compiler::JSHeapBroker* broker_;
+  ZoneVector<compiler::MapRef> const& maps_;
+  bool known_maps_are_subset_of_maps_;
+  bool emit_check_with_migration_;
+  ZoneHandleSet<Map> stable_map_set_;
+  ZoneHandleSet<Map> unstable_map_set_;
+  NodeType node_type_ = NodeType::kJSReceiverWithKnownMap;
+
+  Zone* zone() const { return broker_->zone(); }
+
+  base::Optional<ZoneHandleSet<Map>> GetKnownMapSet(
+      ValueNode* object,
+      const ZoneMap<ValueNode*, ZoneHandleSet<Map>>& map_of_map_set) {
+    auto it = map_of_map_set.find(object);
+    if (it == map_of_map_set.end()) {
+      return {};
     }
-    // TODO(leszeks): Insert an unconditional deopt if the known map doesn't
-    // match the required map.
+    return it->second;
   }
-  NodeInfo* known_info = known_node_aspects().GetOrCreateInfoFor(object);
-  if (known_info->type == NodeType::kUnknown) {
-    known_info->type = StaticTypeForNode(object);
-    if (known_info->type == NodeType::kHeapObjectWithKnownMap) {
-      // The only case where the type becomes a heap-object with a known map is
-      // when the object is a constant.
-      DCHECK(object->Is<Constant>());
-      // For constants with stable maps that match the desired map, we don't
-      // need to emit a map check, and can use the dependency -- we can't do
-      // this for unstable maps because the constant could migrate during
-      // compilation.
-      // TODO(leszeks): Insert an unconditional deopt if the constant map
-      // doesn't match the required map.
-      compiler::MapRef constant_map = object->Cast<Constant>()->object().map();
-      if (constant_map.equals(map) && map.is_stable()) {
-        DCHECK_EQ(&map_of_maps, &known_node_aspects().stable_maps);
-        map_of_maps.emplace(object, map);
-        broker()->dependencies()->DependOnStableMap(map);
+
+  base::Optional<compiler::MapRef> GetMapRefFromMaps(Handle<Map> handle) {
+    auto it =
+        std::find_if(maps_.begin(), maps_.end(), [&](compiler::MapRef map_ref) {
+          return map_ref.object().is_identical_to(handle);
+        });
+    if (it == maps_.end()) return {};
+    return *it;
+  }
+
+  void InsertMap(compiler::MapRef map, bool add_dependency) {
+    if (map.is_migration_target()) {
+      emit_check_with_migration_ = true;
+    }
+    if (!map.IsJSReceiverMap()) node_type_ = NodeType::kHeapObjectWithKnownMap;
+    if (map.is_stable()) {
+      // TODO(victorgomes): Add a DCHECK_SLOW that checks if the map already
+      // exists in the CompilationDependencySet for the else branch.
+      if (add_dependency) {
+        broker_->dependencies()->DependOnStableMap(map);
+      }
+      stable_map_set_.insert(map.object(), zone());
+    } else {
+      unstable_map_set_.insert(map.object(), zone());
+    }
+  }
+
+  void IntersectKnownMaps(base::Optional<ZoneHandleSet<Map>>& known_maps,
+                          bool is_set_with_stable_maps) {
+    if (known_maps.has_value()) {
+      // TODO(v8:7700): Make intersection non-quadratic.
+      for (Handle<Map> handle : *known_maps) {
+        auto map = GetMapRefFromMaps(handle);
+        if (map.has_value()) {
+          InsertMap(*map, false);
+        } else {
+          known_maps_are_subset_of_maps_ = false;
+        }
+      }
+    } else {
+      // Intersect with the universal set.
+      known_maps_are_subset_of_maps_ = false;
+      for (compiler::MapRef map : maps_) {
+        if (map.is_stable() == is_set_with_stable_maps) {
+          InsertMap(map, true);
+        }
+      }
+    }
+  }
+};
+
+}  // namespace
+
+void MaglevGraphBuilder::BuildCheckMaps(
+    ValueNode* object, ZoneVector<compiler::MapRef> const& maps) {
+  // TODO(verwaest): Support other objects with possible known stable maps as
+  // well.
+  if (object->Is<Constant>()) {
+    // For constants with stable maps that match one of the desired maps, we
+    // don't need to emit a map check, and can use the dependency -- we
+    // can't do this for unstable maps because the constant could migrate
+    // during compilation.
+    compiler::MapRef constant_map = object->Cast<Constant>()->object().map();
+    if (std::find(maps.begin(), maps.end(), constant_map) != maps.end()) {
+      if (constant_map.is_stable()) {
+        broker()->dependencies()->DependOnStableMap(constant_map);
         return;
       }
+      // TODO(verwaest): Reduce maps to the constant map.
+    } else {
+      // TODO(leszeks): Insert an unconditional deopt if the constant map
+      // doesn't match the required map.
     }
   }
 
-  if (map.is_migration_target()) {
-    AddNewNode<CheckMapsWithMigration>({object}, map, GetCheckType(known_info));
-  } else {
-    AddNewNode<CheckMaps>({object}, map, GetCheckType(known_info));
+  NodeInfo* known_info = known_node_aspects().GetOrCreateInfoFor(object);
+  known_info->type = CombineType(known_info->type, StaticTypeForNode(object));
+
+  // Calculates if known maps are a subset of maps, their map intersection and
+  // whether we should emit check with migration.
+  KnownMapsMerger merger(broker(), object, known_node_aspects(), maps);
+
+  // If the known maps are the subset of the maps to check, we are done.
+  if (merger.known_maps_are_subset_of_maps()) {
+    DCHECK(NodeTypeIs(known_info->type, NodeType::kHeapObjectWithKnownMap));
+    return;
   }
-  map_of_maps.emplace(object, map);
-  if (map.is_stable()) {
-    broker()->dependencies()->DependOnStableMap(map);
+
+  // TODO(v8:7700): Insert an unconditional deopt here if intersect map sets are
+  // empty.
+
+  // TODO(v8:7700): Check if the {maps} - {known_maps} size is smaller than
+  // {maps} \intersect {known_maps}, we can emit CheckNotMaps instead.
+
+  // Emit checks.
+  if (merger.emit_check_with_migration()) {
+    AddNewNode<CheckMapsWithMigration>({object}, merger.intersect_set(),
+                                       GetCheckType(known_info->type));
+  } else {
+    AddNewNode<CheckMaps>({object}, merger.intersect_set(),
+                          GetCheckType(known_info->type));
   }
-  known_info->type = NodeType::kHeapObjectWithKnownMap;
+  known_info->type = merger.node_type();
 }
 
 bool MaglevGraphBuilder::TryFoldLoadDictPrototypeConstant(
@@ -1210,24 +1449,29 @@
 }
 
 bool MaglevGraphBuilder::TryFoldLoadConstantDataField(
-    compiler::PropertyAccessInfo access_info) {
+    compiler::PropertyAccessInfo access_info, ValueNode* lookup_start_object) {
+  if (!access_info.IsFastDataConstant()) return false;
+  base::Optional<compiler::JSObjectRef> source;
   if (access_info.holder().has_value()) {
-    base::Optional<compiler::ObjectRef> constant =
-        access_info.holder()->GetOwnFastDataProperty(
-            access_info.field_representation(), access_info.field_index(),
-            broker()->dependencies());
-    if (constant.has_value()) {
-      SetAccumulator(GetConstant(constant.value()));
-      return true;
-    }
+    source = access_info.holder();
+  } else if (Constant* n = lookup_start_object->TryCast<Constant>()) {
+    if (!n->ref().IsJSObject()) return false;
+    source = n->ref().AsJSObject();
+  } else {
+    return false;
   }
-  // TODO(victorgomes): Check if lookup_start_object is a constant object and
-  // unfold the load.
-  return false;
+  base::Optional<compiler::ObjectRef> constant =
+      source.value().GetOwnFastDataProperty(access_info.field_representation(),
+                                            access_info.field_index(),
+                                            broker()->dependencies());
+  if (!constant.has_value()) return false;
+  SetAccumulator(GetConstant(constant.value()));
+  return true;
 }
 
 bool MaglevGraphBuilder::TryBuildPropertyGetterCall(
-    compiler::PropertyAccessInfo access_info, ValueNode* receiver) {
+    compiler::PropertyAccessInfo access_info, ValueNode* receiver,
+    ValueNode* lookup_start_object) {
   compiler::ObjectRef constant = access_info.constant().value();
 
   if (access_info.IsDictionaryProtoAccessorConstant()) {
@@ -1240,8 +1484,13 @@
 
   // Introduce the call to the getter function.
   if (constant.IsJSFunction()) {
-    Call* call = CreateNewNode<Call>(Call::kFixedInputCount + 1,
-                                     ConvertReceiverMode::kNotNullOrUndefined,
+    ConvertReceiverMode receiver_mode =
+        receiver == lookup_start_object
+            ? ConvertReceiverMode::kNotNullOrUndefined
+            : ConvertReceiverMode::kAny;
+    Call* call = CreateNewNode<Call>(Call::kFixedInputCount + 1, receiver_mode,
+                                     Call::TargetType::kJSFunction,
+                                     compiler::FeedbackSource(),
                                      GetConstant(constant), GetContext());
     call->set_arg(0, receiver);
     SetAccumulator(AddNode(call));
@@ -1257,9 +1506,10 @@
     ValueNode* value) {
   compiler::ObjectRef constant = access_info.constant().value();
   if (constant.IsJSFunction()) {
-    Call* call = CreateNewNode<Call>(Call::kFixedInputCount + 2,
-                                     ConvertReceiverMode::kNotNullOrUndefined,
-                                     GetConstant(constant), GetContext());
+    Call* call = CreateNewNode<Call>(
+        Call::kFixedInputCount + 2, ConvertReceiverMode::kNotNullOrUndefined,
+        Call::TargetType::kJSFunction, compiler::FeedbackSource(),
+        GetConstant(constant), GetContext());
     call->set_arg(0, receiver);
     call->set_arg(1, value);
     SetAccumulator(AddNode(call));
@@ -1272,7 +1522,7 @@
 
 void MaglevGraphBuilder::BuildLoadField(
     compiler::PropertyAccessInfo access_info, ValueNode* lookup_start_object) {
-  if (TryFoldLoadConstantDataField(access_info)) return;
+  if (TryFoldLoadConstantDataField(access_info, lookup_start_object)) return;
 
   // Resolve property holder.
   ValueNode* load_source;
@@ -1294,8 +1544,29 @@
     SetAccumulator(
         AddNewNode<LoadDoubleField>({load_source}, field_index.offset()));
   } else {
-    SetAccumulator(
-        AddNewNode<LoadTaggedField>({load_source}, field_index.offset()));
+    ValueNode* value =
+        AddNewNode<LoadTaggedField>({load_source}, field_index.offset());
+    SetAccumulator(value);
+    // Insert stable field information if present.
+    if (access_info.field_representation().IsSmi()) {
+      NodeInfo* known_info = known_node_aspects().GetOrCreateInfoFor(value);
+      known_info->type = NodeType::kSmi;
+    } else if (access_info.field_representation().IsHeapObject()) {
+      NodeInfo* known_info = known_node_aspects().GetOrCreateInfoFor(value);
+      if (access_info.field_map().has_value() &&
+          access_info.field_map().value().is_stable()) {
+        DCHECK(access_info.field_map().value().IsJSReceiverMap());
+        known_info->type = NodeType::kJSReceiverWithKnownMap;
+        auto map = access_info.field_map().value();
+        ZoneHandleSet<Map> stable_maps(map.object());
+        ZoneHandleSet<Map> unstable_maps;
+        known_node_aspects().stable_maps.emplace(value, stable_maps);
+        known_node_aspects().unstable_maps.emplace(value, unstable_maps);
+        broker()->dependencies()->DependOnStableMap(map);
+      } else {
+        known_info->type = NodeType::kAnyHeapObject;
+      }
+    }
   }
 }
 
@@ -1304,11 +1575,14 @@
   FieldIndex field_index = access_info.field_index();
   Representation field_representation = access_info.field_representation();
 
-  // TODO(victorgomes): Support double stores.
-  if (field_representation.IsDouble()) return false;
-
-  // TODO(victorgomes): Support transition maps.
-  if (access_info.HasTransitionMap()) return false;
+  if (access_info.HasTransitionMap()) {
+    compiler::MapRef transition = access_info.transition_map().value();
+    compiler::MapRef original_map = transition.GetBackPointer().AsMap();
+    // TODO(verwaest): Support growing backing stores.
+    if (original_map.UnusedPropertyFields() == 0) return false;
+  } else if (access_info.IsFastDataConstant()) {
+    return false;
+  }
 
   ValueNode* store_target;
   if (field_index.is_inobject()) {
@@ -1319,33 +1593,70 @@
         {receiver}, JSReceiver::kPropertiesOrHashOffset);
   }
 
-  if (field_representation.IsSmi()) {
-    ValueNode* value = GetAccumulatorTagged();
-    BuildCheckSmi(value);
-    AddNewNode<StoreTaggedFieldNoWriteBarrier>({store_target, value},
-                                               field_index.offset());
-  } else if (field_representation.IsDouble()) {
-    // TODO(victorgomes): Implement store double.
-    UNREACHABLE();
+  ValueNode* value;
+  if (field_representation.IsDouble()) {
+    value = GetAccumulatorFloat64();
+    if (access_info.HasTransitionMap()) {
+      // Allocate the mutable double box owned by the field.
+      value = AddNewNode<Float64Box>({value});
+    }
   } else {
-    ValueNode* value = GetAccumulatorTagged();
-    if (field_representation.IsHeapObject()) {
+    value = GetAccumulatorTagged();
+    if (field_representation.IsSmi()) {
+      BuildCheckSmi(value);
+    } else if (field_representation.IsHeapObject()) {
       // Emit a map check for the field type, if needed, otherwise just a
       // HeapObject check.
       if (access_info.field_map().has_value()) {
-        BuildMapCheck(value, access_info.field_map().value());
+        ZoneVector<compiler::MapRef> maps({access_info.field_map().value()},
+                                          zone());
+        BuildCheckMaps(value, maps);
       } else {
         BuildCheckHeapObject(value);
       }
     }
+  }
+
+  if (field_representation.IsSmi()) {
+    AddNewNode<StoreTaggedFieldNoWriteBarrier>({store_target, value},
+                                               field_index.offset());
+  } else if (value->use_double_register()) {
+    DCHECK(field_representation.IsDouble());
+    DCHECK(!access_info.HasTransitionMap());
+    AddNewNode<StoreDoubleField>({store_target, value}, field_index.offset());
+  } else {
+    DCHECK(field_representation.IsHeapObject() ||
+           field_representation.IsTagged() ||
+           (field_representation.IsDouble() && access_info.HasTransitionMap()));
     AddNewNode<StoreTaggedFieldWithWriteBarrier>({store_target, value},
                                                  field_index.offset());
   }
+
+  if (access_info.HasTransitionMap()) {
+    compiler::MapRef transition = access_info.transition_map().value();
+    AddNewNode<StoreMap>({receiver}, transition);
+    NodeInfo* node_info = known_node_aspects().GetOrCreateInfoFor(receiver);
+    DCHECK(transition.IsJSReceiverMap());
+    node_info->type = NodeType::kJSReceiverWithKnownMap;
+    if (transition.is_stable()) {
+      ZoneHandleSet<Map> stable_maps(transition.object());
+      ZoneHandleSet<Map> unstable_maps;
+      known_node_aspects().stable_maps.emplace(receiver, stable_maps);
+      known_node_aspects().unstable_maps.emplace(receiver, unstable_maps);
+      broker()->dependencies()->DependOnStableMap(transition);
+    } else {
+      ZoneHandleSet<Map> stable_maps;
+      ZoneHandleSet<Map> unstable_maps(transition.object());
+      known_node_aspects().stable_maps.emplace(receiver, stable_maps);
+      known_node_aspects().unstable_maps.emplace(receiver, unstable_maps);
+    }
+  }
+
   return true;
 }
 
 bool MaglevGraphBuilder::TryBuildPropertyLoad(
-    ValueNode* receiver, ValueNode* lookup_start_object,
+    ValueNode* receiver, ValueNode* lookup_start_object, compiler::NameRef name,
     compiler::PropertyAccessInfo const& access_info) {
   if (access_info.holder().has_value() && !access_info.HasDictionaryHolder()) {
     broker()->dependencies()->DependOnStablePrototypeChains(
@@ -1362,12 +1673,16 @@
     case compiler::PropertyAccessInfo::kDataField:
     case compiler::PropertyAccessInfo::kFastDataConstant:
       BuildLoadField(access_info, lookup_start_object);
+      RecordKnownProperty(lookup_start_object, name,
+                          current_interpreter_frame_.accumulator(),
+                          access_info.IsFastDataConstant());
       return true;
     case compiler::PropertyAccessInfo::kDictionaryProtoDataConstant:
       return TryFoldLoadDictPrototypeConstant(access_info);
     case compiler::PropertyAccessInfo::kFastAccessorConstant:
     case compiler::PropertyAccessInfo::kDictionaryProtoAccessorConstant:
-      return TryBuildPropertyGetterCall(access_info, receiver);
+      return TryBuildPropertyGetterCall(access_info, receiver,
+                                        lookup_start_object);
     case compiler::PropertyAccessInfo::kModuleExport: {
       ValueNode* cell = GetConstant(access_info.constant().value().AsCell());
       SetAccumulator(AddNewNode<LoadTaggedField>({cell}, Cell::kValueOffset));
@@ -1376,12 +1691,15 @@
     case compiler::PropertyAccessInfo::kStringLength:
       DCHECK_EQ(receiver, lookup_start_object);
       SetAccumulator(AddNewNode<StringLength>({receiver}));
+      RecordKnownProperty(lookup_start_object, name,
+                          current_interpreter_frame_.accumulator(), true);
       return true;
   }
 }
 
 bool MaglevGraphBuilder::TryBuildPropertyStore(
-    ValueNode* receiver, compiler::PropertyAccessInfo const& access_info) {
+    ValueNode* receiver, compiler::NameRef name,
+    compiler::PropertyAccessInfo const& access_info) {
   if (access_info.holder().has_value()) {
     broker()->dependencies()->DependOnStablePrototypeChains(
         access_info.lookup_start_object_maps(), kStartAtPrototype,
@@ -1393,28 +1711,52 @@
                                       GetAccumulatorTagged());
   } else {
     DCHECK(access_info.IsDataField() || access_info.IsFastDataConstant());
-    return TryBuildStoreField(access_info, receiver);
+    if (TryBuildStoreField(access_info, receiver)) {
+      RecordKnownProperty(receiver, name,
+                          current_interpreter_frame_.accumulator(),
+                          access_info.IsFastDataConstant());
+      return true;
+    }
+    return false;
   }
 }
 
 bool MaglevGraphBuilder::TryBuildPropertyAccess(
-    ValueNode* receiver, ValueNode* lookup_start_object,
+    ValueNode* receiver, ValueNode* lookup_start_object, compiler::NameRef name,
     compiler::PropertyAccessInfo const& access_info,
     compiler::AccessMode access_mode) {
   switch (access_mode) {
     case compiler::AccessMode::kLoad:
-      return TryBuildPropertyLoad(receiver, lookup_start_object, access_info);
+      return TryBuildPropertyLoad(receiver, lookup_start_object, name,
+                                  access_info);
     case compiler::AccessMode::kStore:
     case compiler::AccessMode::kStoreInLiteral:
     case compiler::AccessMode::kDefine:
       DCHECK_EQ(receiver, lookup_start_object);
-      return TryBuildPropertyStore(receiver, access_info);
+      return TryBuildPropertyStore(receiver, name, access_info);
     case compiler::AccessMode::kHas:
       // TODO(victorgomes): BuildPropertyTest.
       return false;
   }
 }
 
+namespace {
+bool HasOnlyStringMaps(ZoneVector<compiler::MapRef> const& maps) {
+  for (compiler::MapRef map : maps) {
+    if (!map.IsStringMap()) return false;
+  }
+  return true;
+}
+
+bool HasOnlyNumberMaps(ZoneVector<compiler::MapRef> const& maps) {
+  for (compiler::MapRef map : maps) {
+    if (map.instance_type() != HEAP_NUMBER_TYPE) return false;
+  }
+  return true;
+}
+
+}  // namespace
+
 bool MaglevGraphBuilder::TryBuildNamedAccess(
     ValueNode* receiver, ValueNode* lookup_start_object,
     compiler::NamedAccessFeedback const& feedback,
@@ -1422,12 +1764,21 @@
   ZoneVector<compiler::PropertyAccessInfo> access_infos(zone());
   {
     ZoneVector<compiler::PropertyAccessInfo> access_infos_for_feedback(zone());
-    for (const compiler::MapRef& map : feedback.maps()) {
-      if (map.is_deprecated()) continue;
+    if (Constant* n = lookup_start_object->TryCast<Constant>()) {
+      compiler::MapRef constant_map = n->object().map();
       compiler::PropertyAccessInfo access_info =
-          broker()->GetPropertyAccessInfo(map, feedback.name(), access_mode,
+          broker()->GetPropertyAccessInfo(constant_map, feedback.name(),
+                                          access_mode,
                                           broker()->dependencies());
       access_infos_for_feedback.push_back(access_info);
+    } else {
+      for (const compiler::MapRef& map : feedback.maps()) {
+        if (map.is_deprecated()) continue;
+        compiler::PropertyAccessInfo access_info =
+            broker()->GetPropertyAccessInfo(map, feedback.name(), access_mode,
+                                            broker()->dependencies());
+        access_infos_for_feedback.push_back(access_info);
+      }
     }
 
     compiler::AccessInfoFactory access_info_factory(
@@ -1441,36 +1792,99 @@
   // Check for monomorphic case.
   if (access_infos.size() == 1) {
     compiler::PropertyAccessInfo access_info = access_infos.front();
-    const compiler::MapRef& map =
-        access_info.lookup_start_object_maps().front();
-    if (map.IsStringMap()) {
+    const ZoneVector<compiler::MapRef>& maps =
+        access_info.lookup_start_object_maps();
+    if (HasOnlyStringMaps(maps)) {
       // Check for string maps before checking if we need to do an access
       // check. Primitive strings always get the prototype from the native
       // context they're operated on, so they don't need the access check.
       BuildCheckString(lookup_start_object);
+    } else if (HasOnlyNumberMaps(maps)) {
+      BuildCheckNumber(lookup_start_object);
     } else {
-      BuildMapCheck(lookup_start_object, map);
+      BuildCheckMaps(lookup_start_object, maps);
     }
 
     // Generate the actual property access.
-    return TryBuildPropertyAccess(receiver, lookup_start_object, access_info,
-                                  access_mode);
+    return TryBuildPropertyAccess(receiver, lookup_start_object,
+                                  feedback.name(), access_info, access_mode);
   } else {
     // TODO(victorgomes): polymorphic case.
     return false;
   }
 }
 
+ValueNode* MaglevGraphBuilder::GetInt32ElementIndex(ValueNode* object) {
+  switch (object->properties().value_representation()) {
+    case ValueRepresentation::kTagged:
+      if (SmiConstant* constant = object->TryCast<SmiConstant>()) {
+        return GetInt32Constant(constant->value().value());
+      } else if (CheckType(object, NodeType::kSmi)) {
+        NodeInfo* node_info = known_node_aspects().GetOrCreateInfoFor(object);
+        if (!node_info->int32_alternative) {
+          node_info->int32_alternative = AddNewNode<UnsafeSmiUntag>({object});
+        }
+        return node_info->int32_alternative;
+      } else {
+        // TODO(leszeks): Cache this knowledge/converted value somehow on
+        // the node info.
+        return AddNewNode<CheckedObjectToIndex>({object});
+      }
+    case ValueRepresentation::kInt32:
+      // Already good.
+      return object;
+    case ValueRepresentation::kFloat64:
+      // TODO(leszeks): Pass in the index register (probably the
+      // accumulator), so that we can save this truncation on there as a
+      // conversion node.
+      return AddNewNode<CheckedTruncateFloat64ToInt32>({object});
+  }
+}
+
+bool MaglevGraphBuilder::TryBuildElementAccessOnString(
+    ValueNode* object, ValueNode* index_object,
+    compiler::KeyedAccessMode const& keyed_mode) {
+  // Strings are immutable and `in` cannot be used on strings
+  if (keyed_mode.access_mode() != compiler::AccessMode::kLoad) return false;
+
+  // TODO(victorgomes): Deal with LOAD_IGNORE_OUT_OF_BOUNDS.
+  if (keyed_mode.load_mode() == LOAD_IGNORE_OUT_OF_BOUNDS) return false;
+
+  DCHECK_EQ(keyed_mode.load_mode(), STANDARD_LOAD);
+
+  // Ensure that {object} is actually a String.
+  BuildCheckString(object);
+
+  ValueNode* length = AddNewNode<StringLength>({object});
+  ValueNode* index = GetInt32ElementIndex(index_object);
+  AddNewNode<CheckInt32Condition>({index, length}, AssertCondition::kLess,
+                                  DeoptimizeReason::kOutOfBounds);
+
+  SetAccumulator(AddNewNode<StringAt>({object, index}));
+  return true;
+}
+
 bool MaglevGraphBuilder::TryBuildElementAccess(
-    ValueNode* object, ValueNode* index,
+    ValueNode* object, ValueNode* index_object,
     compiler::ElementAccessFeedback const& feedback) {
   // TODO(victorgomes): Implement other access modes.
   if (feedback.keyed_mode().access_mode() != compiler::AccessMode::kLoad) {
     return false;
   }
 
+  // TODO(leszeks): Add non-deopting bounds check (has to support undefined
+  // values).
+  if (feedback.keyed_mode().load_mode() != STANDARD_LOAD) {
+    return false;
+  }
+
   // TODO(victorgomes): Add fast path for loading from HeapConstant.
-  // TODO(victorgomes): Add fast path for loading from String.
+
+  if (!feedback.transition_groups().empty() &&
+      feedback.HasOnlyStringMaps(broker())) {
+    return TryBuildElementAccessOnString(object, index_object,
+                                         feedback.keyed_mode());
+  }
 
   compiler::AccessInfoFactory access_info_factory(
       broker(), broker()->dependencies(), zone());
@@ -1494,42 +1908,13 @@
 
     const compiler::MapRef& map =
         access_info.lookup_start_object_maps().front();
-    BuildMapCheck(object, map);
-
-    switch (index->properties().value_representation()) {
-      case ValueRepresentation::kTagged: {
-        if (SmiConstant* constant = index->TryCast<SmiConstant>()) {
-          index = GetInt32Constant(constant->value().value());
-        } else {
-          NodeInfo* node_info = known_node_aspects().GetOrCreateInfoFor(index);
-          if (node_info->is_smi()) {
-            if (!node_info->int32_alternative) {
-              // TODO(leszeks): This could be unchecked.
-              node_info->int32_alternative =
-                  AddNewNode<CheckedSmiUntag>({index});
-            }
-            index = node_info->int32_alternative;
-          } else {
-            // TODO(leszeks): Cache this knowledge/converted value somehow on
-            // the node info.
-            index = AddNewNode<CheckedObjectToIndex>({index});
-          }
-        }
-        break;
-      }
-      case ValueRepresentation::kInt32: {
-        // Already good.
-        break;
-      }
-      case ValueRepresentation::kFloat64: {
-        // TODO(leszeks): Pass in the index register (probably the
-        // accumulator), so that we can save this truncation on there as a
-        // conversion node.
-        index = AddNewNode<CheckedTruncateFloat64ToInt32>({index});
-        break;
-      }
+    if (access_info.lookup_start_object_maps().size() != 1) {
+      // TODO(victorgomes): polymorphic case.
+      return false;
     }
+    BuildCheckMaps(object, access_info.lookup_start_object_maps());
 
+    ValueNode* index = GetInt32ElementIndex(index_object);
     if (map.IsJSArrayMap()) {
       AddNewNode<CheckJSArrayBounds>({object, index});
     } else {
@@ -1550,6 +1935,42 @@
   }
 }
 
+void MaglevGraphBuilder::RecordKnownProperty(ValueNode* lookup_start_object,
+                                             compiler::NameRef name,
+                                             ValueNode* value, bool is_const) {
+  auto& loaded_properties =
+      is_const ? known_node_aspects().loaded_constant_properties
+               : known_node_aspects().loaded_properties;
+  loaded_properties.emplace(std::make_pair(lookup_start_object, name), value);
+}
+
+bool MaglevGraphBuilder::TryReuseKnownPropertyLoad(
+    ValueNode* lookup_start_object, compiler::NameRef name) {
+  if (auto it = known_node_aspects().loaded_properties.find(
+          {lookup_start_object, name});
+      it != known_node_aspects().loaded_properties.end()) {
+    current_interpreter_frame_.set_accumulator(it->second);
+    if (v8_flags.trace_maglev_graph_building) {
+      std::cout << "  * Reusing non-constant loaded property "
+                << PrintNodeLabel(graph_labeller(), it->second) << ": "
+                << PrintNode(graph_labeller(), it->second) << std::endl;
+    }
+    return true;
+  }
+  if (auto it = known_node_aspects().loaded_constant_properties.find(
+          {lookup_start_object, name});
+      it != known_node_aspects().loaded_constant_properties.end()) {
+    current_interpreter_frame_.set_accumulator(it->second);
+    if (v8_flags.trace_maglev_graph_building) {
+      std::cout << "  * Reusing constant loaded property "
+                << PrintNodeLabel(graph_labeller(), it->second) << ": "
+                << PrintNode(graph_labeller(), it->second) << std::endl;
+    }
+    return true;
+  }
+  return false;
+}
+
 void MaglevGraphBuilder::VisitGetNamedProperty() {
   // GetNamedProperty <object> <name_index> <slot>
   ValueNode* object = LoadRegisterTagged(0);
@@ -1568,6 +1989,7 @@
       return;
 
     case compiler::ProcessedFeedback::kNamedAccess:
+      if (TryReuseKnownPropertyLoad(object, name)) return;
       if (TryBuildNamedAccess(object, object,
                               processed_feedback.AsNamedAccess(),
                               compiler::AccessMode::kLoad)) {
@@ -1609,6 +2031,7 @@
       return;
 
     case compiler::ProcessedFeedback::kNamedAccess:
+      if (TryReuseKnownPropertyLoad(lookup_start_object, name)) return;
       if (TryBuildNamedAccess(receiver, lookup_start_object,
                               processed_feedback.AsNamedAccess(),
                               compiler::AccessMode::kLoad)) {
@@ -1655,6 +2078,19 @@
       break;
     }
 
+    case compiler::ProcessedFeedback::kNamedAccess: {
+      ValueNode* key = GetAccumulatorTagged();
+      compiler::NameRef name = processed_feedback.AsNamedAccess().name();
+      if (!BuildCheckValue(key, name)) return;
+      if (TryReuseKnownPropertyLoad(object, name)) return;
+      if (TryBuildNamedAccess(object, object,
+                              processed_feedback.AsNamedAccess(),
+                              compiler::AccessMode::kLoad)) {
+        return;
+      }
+      break;
+    }
+
     default:
       break;
   }
@@ -1684,11 +2120,13 @@
   }
 
   for (size_t i = 0; i < depth; i++) {
-    context = AddNewNode<LoadTaggedField>(
-        {context}, Context::OffsetOfElementAt(Context::PREVIOUS_INDEX));
-  }
-  ValueNode* module = AddNewNode<LoadTaggedField>(
-      {context}, Context::OffsetOfElementAt(Context::EXTENSION_INDEX));
+    context = LoadAndCacheContextSlot(
+        context, Context::OffsetOfElementAt(Context::PREVIOUS_INDEX),
+        kImmutable);
+  }
+  ValueNode* module = LoadAndCacheContextSlot(
+      context, Context::OffsetOfElementAt(Context::EXTENSION_INDEX),
+      kImmutable);
   ValueNode* exports_or_imports;
   if (cell_index > 0) {
     exports_or_imports = AddNewNode<LoadTaggedField>(
@@ -1729,11 +2167,13 @@
   }
 
   for (size_t i = 0; i < depth; i++) {
-    context = AddNewNode<LoadTaggedField>(
-        {context}, Context::OffsetOfElementAt(Context::PREVIOUS_INDEX));
-  }
-  ValueNode* module = AddNewNode<LoadTaggedField>(
-      {context}, Context::OffsetOfElementAt(Context::EXTENSION_INDEX));
+    context = LoadAndCacheContextSlot(
+        context, Context::OffsetOfElementAt(Context::PREVIOUS_INDEX),
+        kImmutable);
+  }
+  ValueNode* module = LoadAndCacheContextSlot(
+      context, Context::OffsetOfElementAt(Context::EXTENSION_INDEX),
+      kImmutable);
   ValueNode* exports = AddNewNode<LoadTaggedField>(
       {module}, SourceTextModule::kRegularExportsOffset);
   // The actual array index is (cell_index - 1).
@@ -1758,10 +2198,9 @@
   const compiler::GlobalAccessFeedback& global_access_feedback =
       access_feedback.AsGlobalAccess();
 
+  if (TryBuildScriptContextAccess(global_access_feedback)) return;
   if (TryBuildPropertyCellAccess(global_access_feedback)) return;
 
-  // TODO(leszeks): Handle the IsScriptContextSlot case.
-
   ValueNode* context = GetContext();
   SetAccumulator(
       AddNewNode<LoadGlobal>({context}, name, feedback_source, typeof_mode));
@@ -1999,11 +2438,11 @@
 void MaglevGraphBuilder::VisitToBooleanLogicalNot() {
   ValueNode* value = GetAccumulatorTagged();
   switch (value->opcode()) {
-#define CASE(Name)                                                            \
-  case Opcode::k##Name: {                                                     \
-    SetAccumulator(                                                           \
-        GetBooleanConstant(value->Cast<Name>()->ToBoolean(local_isolate()))); \
-    break;                                                                    \
+#define CASE(Name)                                                             \
+  case Opcode::k##Name: {                                                      \
+    SetAccumulator(                                                            \
+        GetBooleanConstant(!value->Cast<Name>()->ToBoolean(local_isolate()))); \
+    break;                                                                     \
   }
     CONSTANT_VALUE_NODE_LIST(CASE)
 #undef CASE
@@ -2017,11 +2456,11 @@
   // Invariant: accumulator must already be a boolean value.
   ValueNode* value = GetAccumulatorTagged();
   switch (value->opcode()) {
-#define CASE(Name)                                                            \
-  case Opcode::k##Name: {                                                     \
-    SetAccumulator(                                                           \
-        GetBooleanConstant(value->Cast<Name>()->ToBoolean(local_isolate()))); \
-    break;                                                                    \
+#define CASE(Name)                                                             \
+  case Opcode::k##Name: {                                                      \
+    SetAccumulator(                                                            \
+        GetBooleanConstant(!value->Cast<Name>()->ToBoolean(local_isolate()))); \
+    break;                                                                     \
   }
     CONSTANT_VALUE_NODE_LIST(CASE)
 #undef CASE
@@ -2162,49 +2601,139 @@
       ->SetToBlockAndReturnNext(current_block_);
 }
 
-// TODO(v8:7700): Read feedback and implement inlining
-void MaglevGraphBuilder::BuildCallFromRegisterList(
-    ConvertReceiverMode receiver_mode) {
-  ValueNode* function = LoadRegisterTagged(0);
+bool MaglevGraphBuilder::TryReduceStringFromCharCode(
+    compiler::JSFunctionRef target, const CallArguments& args) {
+  if (args.count() != 1) return false;
+  SetAccumulator(AddNewNode<BuiltinStringFromCharCode>({GetInt32(args[0])}));
+  return true;
+}
 
-  interpreter::RegisterList args = iterator_.GetRegisterListOperand(1);
-  ValueNode* context = GetContext();
+bool MaglevGraphBuilder::TryReduceStringPrototypeCharCodeAt(
+    compiler::JSFunctionRef target, const CallArguments& args) {
+  ValueNode* receiver = GetTaggedReceiver(args);
+  ValueNode* index;
+  if (args.count() == 0) {
+    // Index is the undefined object. ToIntegerOrInfinity(undefined) = 0.
+    index = GetInt32Constant(0);
+  } else {
+    index = GetInt32ElementIndex(args[0]);
+  }
+  // Any other argument is ignored.
+  // Ensure that {receiver} is actually a String.
+  BuildCheckString(receiver);
+  // And index is below length.
+  ValueNode* length = AddNewNode<StringLength>({receiver});
+  AddNewNode<CheckInt32Condition>({index, length}, AssertCondition::kLess,
+                                  DeoptimizeReason::kOutOfBounds);
+  SetAccumulator(
+      AddNewNode<BuiltinStringPrototypeCharCodeAt>({receiver, index}));
+  return true;
+}
 
-  size_t input_count = args.register_count() + Call::kFixedInputCount;
-  if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
-    input_count++;
+bool MaglevGraphBuilder::TryReduceFunctionPrototypeCall(
+    compiler::JSFunctionRef target, const CallArguments& args) {
+  // Use Function.prototype.call context, to ensure any exception is thrown in
+  // the correct context.
+  ValueNode* context = GetConstant(target.context());
+  ValueNode* receiver = GetTaggedReceiver(args);
+  compiler::FeedbackSource feedback_source;
+  BuildGenericCall(receiver, context, Call::TargetType::kAny,
+                   args.PopReceiver(ConvertReceiverMode::kAny),
+                   feedback_source);
+  return true;
+}
+
+bool MaglevGraphBuilder::TryReduceBuiltin(compiler::JSFunctionRef target,
+                                          const CallArguments& args) {
+  if (!target.shared().HasBuiltinId()) return false;
+  switch (target.shared().builtin_id()) {
+#define CASE(Name)       \
+  case Builtin::k##Name: \
+    return TryReduce##Name(target, args);
+    MAGLEV_REDUCED_BUILTIN(CASE)
+#undef CASE
+    default:
+      // TODO(v8:7700): Inline more builtins.
+      return false;
   }
+}
 
-  Call* call =
-      CreateNewNode<Call>(input_count, receiver_mode, function, context);
-  int arg_index = 0;
-  if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
-    call->set_arg(arg_index++, GetRootConstant(RootIndex::kUndefinedValue));
+ValueNode* MaglevGraphBuilder::GetConvertReceiver(
+    compiler::JSFunctionRef function, const CallArguments& args) {
+  compiler::SharedFunctionInfoRef shared = function.shared();
+  if (shared.native() || shared.language_mode() == LanguageMode::kStrict) {
+    if (args.receiver_mode() == ConvertReceiverMode::kNullOrUndefined) {
+      return GetRootConstant(RootIndex::kUndefinedValue);
+    } else {
+      return GetTaggedValue(*args.receiver());
+    }
   }
-  for (int i = 0; i < args.register_count(); ++i) {
-    call->set_arg(arg_index++, GetTaggedValue(args[i]));
+  if (args.receiver_mode() == ConvertReceiverMode::kNullOrUndefined) {
+    return GetConstant(function.native_context().global_proxy_object());
+  }
+  ValueNode* receiver = GetTaggedValue(*args.receiver());
+  if (CheckType(receiver, NodeType::kJSReceiver)) return receiver;
+  if (Constant* constant = receiver->TryCast<Constant>()) {
+    const Handle<HeapObject> object = constant->object().object();
+    if (object->IsUndefined() || object->IsNull()) {
+      return GetConstant(function.native_context().global_proxy_object());
+    } else if (object->IsJSReceiver()) {
+      return constant;
+    }
   }
+  return AddNewNode<ConvertReceiver>({receiver}, function,
+                                     args.receiver_mode());
+}
 
+bool MaglevGraphBuilder::TryBuildCallKnownJSFunction(
+    compiler::JSFunctionRef function, const CallArguments& args) {
+  // Don't inline CallFunction stub across native contexts.
+  if (function.native_context() != broker()->target_native_context()) {
+    return false;
+  }
+  ValueNode* receiver = GetConvertReceiver(function, args);
+  size_t input_count = args.count() + CallKnownJSFunction::kFixedInputCount;
+  CallKnownJSFunction* call =
+      CreateNewNode<CallKnownJSFunction>(input_count, function, receiver);
+  for (int i = 0; i < args.count(); i++) {
+    call->set_arg(i, GetTaggedValue(args[i]));
+  }
   SetAccumulator(AddNode(call));
+  return true;
 }
 
-void MaglevGraphBuilder::BuildCallFromRegisters(
-    int argc_count, ConvertReceiverMode receiver_mode) {
-  // Indices and counts of operands on the bytecode.
-  const int kFirstArgumentOperandIndex = 1;
-  const int kReceiverOperandCount =
-      (receiver_mode == ConvertReceiverMode::kNullOrUndefined) ? 0 : 1;
-  const int kReceiverAndArgOperandCount = kReceiverOperandCount + argc_count;
-  const int kSlotOperandIndex =
-      kFirstArgumentOperandIndex + kReceiverAndArgOperandCount;
+void MaglevGraphBuilder::BuildGenericCall(
+    ValueNode* target, ValueNode* context, Call::TargetType target_type,
+    const CallArguments& args, compiler::FeedbackSource& feedback_source) {
+  size_t input_count = args.count_with_receiver() + Call::kFixedInputCount;
+  Call* call =
+      CreateNewNode<Call>(input_count, args.receiver_mode(), target_type,
+                          feedback_source, target, context);
+  int arg_index = 0;
+  call->set_arg(arg_index++, GetTaggedReceiver(args));
+  for (int i = 0; i < args.count(); ++i) {
+    call->set_arg(arg_index++, GetTaggedValue(args[i]));
+  }
+  SetAccumulator(AddNode(call));
+}
 
-  DCHECK_LE(argc_count, 2);
-  ValueNode* function = LoadRegisterTagged(0);
-  ValueNode* context = GetContext();
-  FeedbackSlot slot = GetSlotOperand(kSlotOperandIndex);
+bool MaglevGraphBuilder::BuildCheckValue(ValueNode* node,
+                                         const compiler::HeapObjectRef& ref) {
+  if (node->Is<Constant>()) {
+    if (node->Cast<Constant>()->object().equals(ref)) return true;
+    EmitUnconditionalDeopt(DeoptimizeReason::kUnknown);
+    return false;
+  }
+  AddNewNode<CheckValue>({node}, ref);
+  return true;
+}
 
+void MaglevGraphBuilder::BuildCall(ValueNode* target_node,
+                                   const CallArguments& args,
+                                   compiler::FeedbackSource& feedback_source) {
+  Call::TargetType target_type = Call::TargetType::kAny;
   const compiler::ProcessedFeedback& processed_feedback =
-      broker()->GetFeedbackForCall(compiler::FeedbackSource(feedback(), slot));
+      broker()->GetFeedbackForCall(feedback_source);
   switch (processed_feedback.kind()) {
     case compiler::ProcessedFeedback::kInsufficient:
       EmitUnconditionalDeopt(
@@ -2212,8 +2741,6 @@
       return;
 
     case compiler::ProcessedFeedback::kCall: {
-      if (!v8_flags.maglev_inlining) break;
-
       const compiler::CallFeedback& call_feedback = processed_feedback.AsCall();
       CallFeedbackContent content = call_feedback.call_feedback_content();
       if (content != CallFeedbackContent::kTarget) break;
@@ -2224,36 +2751,90 @@
 
       compiler::HeapObjectRef target = maybe_target.value();
       if (!target.IsJSFunction()) break;
-
       compiler::JSFunctionRef function = target.AsJSFunction();
-      base::Optional<compiler::FeedbackVectorRef> maybe_feedback_vector =
-          function.feedback_vector(broker()->dependencies());
-      if (!maybe_feedback_vector.has_value()) break;
 
-      return InlineCallFromRegisters(argc_count, receiver_mode, function);
+      // Do not reduce calls to functions with break points.
+      if (function.shared().HasBreakInfo()) break;
+
+      // Reset the feedback source
+      feedback_source = compiler::FeedbackSource();
+      target_type = Call::TargetType::kJSFunction;
+      if (!BuildCheckValue(target_node, target)) return;
+
+      if (function.object()->IsJSClassConstructor()) {
+        // If we have a class constructor, we should raise an exception.
+        SetAccumulator(BuildCallRuntime(
+            Runtime::kThrowConstructorNonCallableError, {target_node}));
+        return;
+      }
+
+      DCHECK(function.object()->IsCallable());
+      if (TryReduceBuiltin(function, args)) {
+        return;
+      }
+      if (TryBuildCallKnownJSFunction(function, args)) {
+        return;
+      }
+      break;
     }
 
     default:
       break;
   }
+
   // On fallthrough, create a generic call.
+  ValueNode* context = GetContext();
+  BuildGenericCall(target_node, context, target_type, args, feedback_source);
+}
 
-  int argc_count_with_recv = argc_count + 1;
-  size_t input_count = argc_count_with_recv + Call::kFixedInputCount;
+void MaglevGraphBuilder::BuildCallFromRegisterList(
+    ConvertReceiverMode receiver_mode) {
+  ValueNode* target = LoadRegisterTagged(0);
+  interpreter::RegisterList reg_list = iterator_.GetRegisterListOperand(1);
+  FeedbackSlot slot = GetSlotOperand(3);
+  compiler::FeedbackSource feedback_source(feedback(), slot);
+  CallArguments args(receiver_mode, reg_list);
+  BuildCall(target, args, feedback_source);
+}
 
-  int arg_index = 0;
-  int reg_count = argc_count_with_recv;
-  Call* call =
-      CreateNewNode<Call>(input_count, receiver_mode, function, context);
-  if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
-    reg_count = argc_count;
-    call->set_arg(arg_index++, GetRootConstant(RootIndex::kUndefinedValue));
-  }
-  for (int i = 0; i < reg_count; i++) {
-    call->set_arg(arg_index++, LoadRegisterTagged(i + 1));
+void MaglevGraphBuilder::BuildCallFromRegisters(
+    int arg_count, ConvertReceiverMode receiver_mode) {
+  ValueNode* target = LoadRegisterTagged(0);
+  const int receiver_count =
+      (receiver_mode == ConvertReceiverMode::kNullOrUndefined) ? 0 : 1;
+  const int reg_count = arg_count + receiver_count;
+  int slot_operand_index = arg_count + receiver_count + 1;
+  FeedbackSlot slot = GetSlotOperand(slot_operand_index);
+  compiler::FeedbackSource feedback_source(feedback(), slot);
+  switch (reg_count) {
+    case 0: {
+      CallArguments args(receiver_mode, reg_count);
+      BuildCall(target, args, feedback_source);
+      break;
+    }
+    case 1: {
+      CallArguments args(receiver_mode, reg_count,
+                         iterator_.GetRegisterOperand(1));
+      BuildCall(target, args, feedback_source);
+      break;
+    }
+    case 2: {
+      CallArguments args(receiver_mode, reg_count,
+                         iterator_.GetRegisterOperand(1),
+                         iterator_.GetRegisterOperand(2));
+      BuildCall(target, args, feedback_source);
+      break;
+    }
+    case 3: {
+      CallArguments args(
+          receiver_mode, reg_count, iterator_.GetRegisterOperand(1),
+          iterator_.GetRegisterOperand(2), iterator_.GetRegisterOperand(3));
+      BuildCall(target, args, feedback_source);
+      break;
+    }
+    default:
+      UNREACHABLE();
   }
-
-  SetAccumulator(AddNode(call));
 }
 
 void MaglevGraphBuilder::VisitCallAnyReceiver() {
@@ -2288,10 +2869,12 @@
   ValueNode* function = LoadRegisterTagged(0);
   interpreter::RegisterList args = iterator_.GetRegisterListOperand(1);
   ValueNode* context = GetContext();
+  FeedbackSlot slot = GetSlotOperand(3);
+  compiler::FeedbackSource feedback_source(feedback(), slot);
 
   size_t input_count = args.register_count() + CallWithSpread::kFixedInputCount;
-  CallWithSpread* call =
-      CreateNewNode<CallWithSpread>(input_count, function, context);
+  CallWithSpread* call = CreateNewNode<CallWithSpread>(
+      input_count, feedback_source, function, context);
   for (int i = 0; i < args.register_count(); ++i) {
     call->set_arg(i, GetTaggedValue(args[i]));
   }
@@ -2319,16 +2902,18 @@
   compiler::NativeContextRef native_context = broker()->target_native_context();
   ValueNode* context = GetConstant(native_context);
   uint32_t slot = iterator_.GetNativeContextIndexOperand(0);
-  ValueNode* callee = AddNewNode<LoadTaggedField>(
-      {context}, NativeContext::OffsetOfElementAt(slot));
+  ValueNode* callee = LoadAndCacheContextSlot(
+      context, NativeContext::OffsetOfElementAt(slot), kMutable);
   // Call the function.
   interpreter::RegisterList args = iterator_.GetRegisterListOperand(1);
   int kTheReceiver = 1;
   size_t input_count =
       args.register_count() + Call::kFixedInputCount + kTheReceiver;
 
-  Call* call = CreateNewNode<Call>(
-      input_count, ConvertReceiverMode::kNullOrUndefined, callee, GetContext());
+  Call* call =
+      CreateNewNode<Call>(input_count, ConvertReceiverMode::kNullOrUndefined,
+                          Call::TargetType::kJSFunction,
+                          compiler::FeedbackSource(), callee, GetContext());
   int arg_index = 0;
   call->set_arg(arg_index++, GetRootConstant(RootIndex::kUndefinedValue));
   for (int i = 0; i < args.register_count(); ++i) {
@@ -2538,12 +3123,14 @@
   ValueNode* constructor = LoadRegisterTagged(0);
   interpreter::RegisterList args = iterator_.GetRegisterListOperand(1);
   ValueNode* context = GetContext();
+  FeedbackSlot slot = GetSlotOperand(3);
+  compiler::FeedbackSource feedback_source(feedback(), slot);
 
   int kReceiver = 1;
   size_t input_count =
       args.register_count() + kReceiver + ConstructWithSpread::kFixedInputCount;
   ConstructWithSpread* construct = CreateNewNode<ConstructWithSpread>(
-      input_count, constructor, new_target, context);
+      input_count, feedback_source, constructor, new_target, context);
   int arg_index = 0;
   // Add undefined receiver.
   construct->set_arg(arg_index++, GetRootConstant(RootIndex::kUndefinedValue));
@@ -2572,6 +3159,274 @@
   VisitCompareOperation<Operation::kGreaterThanOrEqual>();
 }
 
+MaglevGraphBuilder::InferHasInPrototypeChainResult
+MaglevGraphBuilder::InferHasInPrototypeChain(
+    ValueNode* receiver, compiler::HeapObjectRef prototype) {
+  auto stable_it = known_node_aspects().stable_maps.find(receiver);
+  auto unstable_it = known_node_aspects().unstable_maps.find(receiver);
+  auto stable_end = known_node_aspects().stable_maps.end();
+  auto unstable_end = known_node_aspects().unstable_maps.end();
+  // If either of the map sets is not found, then we don't know anything about
+  // the map of the receiver, so bail.
+  if (stable_it == stable_end || unstable_it == unstable_end) {
+    return kMayBeInPrototypeChain;
+  }
+
+  ZoneVector<compiler::MapRef> receiver_map_refs(zone());
+
+  // Try to determine either that all of the {receiver_maps} have the given
+  // {prototype} in their chain, or that none do. If we can't tell, return
+  // kMayBeInPrototypeChain.
+  bool all = true;
+  bool none = true;
+  for (const ZoneHandleSet<Map>& map_set :
+       {stable_it->second, unstable_it->second}) {
+    for (Handle<Map> map_handle : map_set) {
+      compiler::MapRef map = MakeRefAssumeMemoryFence(broker(), map_handle);
+      receiver_map_refs.push_back(map);
+      while (true) {
+        if (IsSpecialReceiverInstanceType(map.instance_type())) {
+          return kMayBeInPrototypeChain;
+        }
+        if (!map.IsJSObjectMap()) {
+          all = false;
+          break;
+        }
+        compiler::HeapObjectRef map_prototype = map.prototype();
+        if (map_prototype.equals(prototype)) {
+          none = false;
+          break;
+        }
+        map = map_prototype.map();
+        // TODO(v8:11457) Support dictionary mode protoypes here.
+        if (!map.is_stable() || map.is_dictionary_map()) {
+          return kMayBeInPrototypeChain;
+        }
+        if (map.oddball_type() == compiler::OddballType::kNull) {
+          all = false;
+          break;
+        }
+      }
+    }
+  }
+  DCHECK_IMPLIES(all, !none);
+  if (!all && !none) return kMayBeInPrototypeChain;
+
+  {
+    base::Optional<compiler::JSObjectRef> last_prototype;
+    if (all) {
+      // We don't need to protect the full chain if we found the prototype, we
+      // can stop at {prototype}.  In fact we could stop at the one before
+      // {prototype} but since we're dealing with multiple receiver maps this
+      // might be a different object each time, so it's much simpler to include
+      // {prototype}. That does, however, mean that we must check {prototype}'s
+      // map stability.
+      if (!prototype.map().is_stable()) return kMayBeInPrototypeChain;
+      last_prototype = prototype.AsJSObject();
+    }
+    broker()->dependencies()->DependOnStablePrototypeChains(
+        receiver_map_refs, kStartAtPrototype, last_prototype);
+  }
+
+  DCHECK_EQ(all, !none);
+  return all ? kIsInPrototypeChain : kIsNotInPrototypeChain;
+}
+
+bool MaglevGraphBuilder::TryBuildFastHasInPrototypeChain(
+    ValueNode* object, compiler::ObjectRef prototype) {
+  if (!prototype.IsHeapObject()) return false;
+  auto in_prototype_chain =
+      InferHasInPrototypeChain(object, prototype.AsHeapObject());
+  if (in_prototype_chain == kMayBeInPrototypeChain) return false;
+
+  SetAccumulator(GetBooleanConstant(in_prototype_chain == kIsInPrototypeChain));
+  return true;
+}
+
+void MaglevGraphBuilder::BuildHasInPrototypeChain(
+    ValueNode* object, compiler::ObjectRef prototype) {
+  if (TryBuildFastHasInPrototypeChain(object, prototype)) return;
+
+  SetAccumulator(BuildCallRuntime(Runtime::kHasInPrototypeChain,
+                                  {object, GetConstant(prototype)}));
+}
+
+bool MaglevGraphBuilder::TryBuildFastOrdinaryHasInstance(
+    ValueNode* object, compiler::JSObjectRef callable,
+    ValueNode* callable_node_if_not_constant) {
+  const bool is_constant = callable_node_if_not_constant == nullptr;
+  if (!is_constant) return false;
+
+  if (callable.IsJSBoundFunction()) {
+    // OrdinaryHasInstance on bound functions turns into a recursive
+    // invocation of the instanceof operator again.
+    compiler::JSBoundFunctionRef function = callable.AsJSBoundFunction();
+    compiler::JSReceiverRef bound_target_function =
+        function.bound_target_function();
+
+    if (!bound_target_function.IsJSObject() ||
+        !TryBuildFastInstanceOf(object, bound_target_function.AsJSObject(),
+                                nullptr)) {
+      // If we can't build a fast instance-of, build a slow one with the
+      // partial optimisation of using the bound target function constant.
+      SetAccumulator(BuildCallBuiltin<Builtin::kInstanceOf>(
+          {object, GetConstant(bound_target_function)}));
+    }
+    return true;
+  }
+
+  if (callable.IsJSFunction()) {
+    // Optimize if we currently know the "prototype" property.
+    compiler::JSFunctionRef function = callable.AsJSFunction();
+
+    // TODO(v8:7700): Remove the has_prototype_slot condition once the broker
+    // is always enabled.
+    if (!function.map().has_prototype_slot() ||
+        !function.has_instance_prototype(broker()->dependencies()) ||
+        function.PrototypeRequiresRuntimeLookup(broker()->dependencies())) {
+      return false;
+    }
+
+    compiler::ObjectRef prototype =
+        broker()->dependencies()->DependOnPrototypeProperty(function);
+    BuildHasInPrototypeChain(object, prototype);
+    return true;
+  }
+
+  return false;
+}
+
+void MaglevGraphBuilder::BuildOrdinaryHasInstance(
+    ValueNode* object, compiler::JSObjectRef callable,
+    ValueNode* callable_node_if_not_constant) {
+  if (TryBuildFastOrdinaryHasInstance(object, callable,
+                                      callable_node_if_not_constant))
+    return;
+
+  SetAccumulator(BuildCallBuiltin<Builtin::kOrdinaryHasInstance>(
+      {object, callable_node_if_not_constant ? callable_node_if_not_constant
+                                             : GetConstant(callable)}));
+}
+
+bool MaglevGraphBuilder::TryBuildFastInstanceOf(
+    ValueNode* object, compiler::JSObjectRef callable,
+    ValueNode* callable_node_if_not_constant) {
+  compiler::MapRef receiver_map = callable.map();
+  compiler::NameRef name =
+      MakeRef(broker(), local_isolate()->factory()->has_instance_symbol());
+  compiler::PropertyAccessInfo access_info = broker()->GetPropertyAccessInfo(
+      receiver_map, name, compiler::AccessMode::kLoad,
+      broker()->dependencies());
+
+  // TODO(v8:11457) Support dictionary mode holders here.
+  if (access_info.IsInvalid() || access_info.HasDictionaryHolder()) {
+    return false;
+  }
+  access_info.RecordDependencies(broker()->dependencies());
+
+  if (access_info.IsNotFound()) {
+    // If there's no @@hasInstance handler, the OrdinaryHasInstance operation
+    // takes over, but that requires the constructor to be callable.
+    if (!receiver_map.is_callable()) {
+      return false;
+    }
+
+    broker()->dependencies()->DependOnStablePrototypeChains(
+        access_info.lookup_start_object_maps(), kStartAtPrototype);
+
+    // Monomorphic property access.
+    if (callable_node_if_not_constant) {
+      BuildCheckMaps(callable_node_if_not_constant,
+                     access_info.lookup_start_object_maps());
+    }
+
+    BuildOrdinaryHasInstance(object, callable, callable_node_if_not_constant);
+    return true;
+  }
+
+  if (access_info.IsFastDataConstant()) {
+    base::Optional<compiler::JSObjectRef> holder = access_info.holder();
+    bool found_on_proto = holder.has_value();
+    compiler::JSObjectRef holder_ref =
+        found_on_proto ? holder.value() : callable;
+    base::Optional<compiler::ObjectRef> has_instance_field =
+        holder_ref.GetOwnFastDataProperty(access_info.field_representation(),
+                                          access_info.field_index(),
+                                          broker()->dependencies());
+    if (!has_instance_field.has_value() ||
+        !has_instance_field->IsHeapObject() ||
+        !has_instance_field->AsHeapObject().map().is_callable()) {
+      return false;
+    }
+
+    if (found_on_proto) {
+      broker()->dependencies()->DependOnStablePrototypeChains(
+          access_info.lookup_start_object_maps(), kStartAtPrototype,
+          holder.value());
+    }
+
+    ValueNode* callable_node;
+    if (callable_node_if_not_constant) {
+      // Check that {callable_node_if_not_constant} is actually {callable}.
+      AddNewNode<CheckValue>({callable_node_if_not_constant}, callable);
+      callable_node = callable_node_if_not_constant;
+    } else {
+      callable_node = GetConstant(callable);
+    }
+
+    // Call @@hasInstance
+    Call* call = AddNewNode<Call>(
+        Call::kFixedInputCount + 2, ConvertReceiverMode::kNotNullOrUndefined,
+        Call::TargetType::kJSFunction, compiler::FeedbackSource(),
+        GetConstant(*has_instance_field), GetContext());
+    call->set_arg(0, callable_node);
+    call->set_arg(1, object);
+
+    // Make sure that a lazy deopt after the @@hasInstance call also performs
+    // ToBoolean before returning to the interpreter.
+    // TODO(leszeks): Wrap this in a helper.
+    new (call->lazy_deopt_info()) LazyDeoptInfo(
+        zone(),
+        BuiltinContinuationDeoptFrame(
+            Builtin::kToBooleanLazyDeoptContinuation, {}, GetContext(),
+            zone()->New<InterpretedDeoptFrame>(
+                call->lazy_deopt_info()->top_frame().as_interpreted())));
+
+    SetAccumulator(AddNewNode<ToBoolean>({call}));
+    return true;
+  }
+
+  return false;
+}
+
+bool MaglevGraphBuilder::TryBuildFastInstanceOfWithFeedback(
+    ValueNode* object, ValueNode* callable,
+    compiler::FeedbackSource feedback_source) {
+  compiler::ProcessedFeedback const& feedback =
+      broker()->GetFeedbackForInstanceOf(feedback_source);
+
+  // TurboFan emits generic code when there's no feedback, rather than
+  // deopting.
+  if (feedback.IsInsufficient()) return false;
+
+  // Check if the right hand side is a known receiver, or
+  // we have feedback from the InstanceOfIC.
+  if (callable->Is<Constant>() &&
+      callable->Cast<Constant>()->object().IsJSObject()) {
+    compiler::JSObjectRef callable_ref =
+        callable->Cast<Constant>()->object().AsJSObject();
+    return TryBuildFastInstanceOf(object, callable_ref, nullptr);
+  }
+  if (feedback_source.IsValid()) {
+    base::Optional<compiler::JSObjectRef> callable_from_feedback =
+        feedback.AsInstanceOf().value();
+    if (callable_from_feedback) {
+      return TryBuildFastInstanceOf(object, *callable_from_feedback, callable);
+    }
+  }
+  return false;
+}
+
 void MaglevGraphBuilder::VisitTestInstanceOf() {
   // TestInstanceOf <src> <feedback_slot>
   ValueNode* object = LoadRegisterTagged(0);
@@ -2579,8 +3434,10 @@
   FeedbackSlot slot = GetSlotOperand(1);
   compiler::FeedbackSource feedback_source{feedback(), slot};
 
-  // TODO(victorgomes): Check feedback slot and a do static lookup for
-  // @@hasInstance.
+  if (TryBuildFastInstanceOfWithFeedback(object, callable, feedback_source)) {
+    return;
+  }
+
   ValueNode* context = GetContext();
   SetAccumulator(
       AddNewNode<TestInstanceOf>({context, object, callable}, feedback_source));
@@ -2604,7 +3461,12 @@
   // ToObject <dst>
   ValueNode* value = GetAccumulatorTagged();
   interpreter::Register destination = iterator_.GetRegisterOperand(0);
-  StoreRegister(destination, AddNewNode<ToName>({GetContext(), value}));
+  if (CheckType(value, NodeType::kName)) {
+    MoveNodeBetweenRegisters(interpreter::Register::virtual_accumulator(),
+                             destination);
+  } else {
+    StoreRegister(destination, AddNewNode<ToName>({GetContext(), value}));
+  }
 }
 
 void MaglevGraphBuilder::BuildToNumberOrToNumeric(Object::Conversion mode) {
@@ -2620,9 +3482,14 @@
     case BinaryOperationHint::kNumber:
     case BinaryOperationHint::kBigInt:
     case BinaryOperationHint::kBigInt64:
+      if (mode == Object::Conversion::kToNumber &&
+          EnsureType(value, NodeType::kNumber)) {
+        return;
+      }
       AddNewNode<CheckNumber>({value}, mode);
       break;
     default:
+      if (CheckType(value, NodeType::kNumber)) return;
       SetAccumulator(
           AddNewNode<ToNumberOrNumeric>({GetContext(), value}, mode));
       break;
@@ -2640,13 +3507,19 @@
   // ToObject <dst>
   ValueNode* value = GetAccumulatorTagged();
   interpreter::Register destination = iterator_.GetRegisterOperand(0);
-  StoreRegister(destination, AddNewNode<ToObject>({GetContext(), value}));
+  if (CheckType(value, NodeType::kJSReceiver)) {
+    MoveNodeBetweenRegisters(interpreter::Register::virtual_accumulator(),
+                             destination);
+  } else {
+    StoreRegister(destination, AddNewNode<ToObject>({GetContext(), value}));
+  }
 }
 
 void MaglevGraphBuilder::VisitToString() {
   // ToString
   ValueNode* value = GetAccumulatorTagged();
-  // TODO(victorgomes): Add fast path for constant nodes.
+  if (CheckType(value, NodeType::kString)) return;
+  // TODO(victorgomes): Add fast path for constant primitives.
   SetAccumulator(AddNewNode<ToString>({GetContext(), value}));
 }
 
@@ -2854,23 +3727,25 @@
   const FeedbackSlot feedback_slot = iterator_.GetSlotOperand(2);
   int target = iterator_.GetJumpTargetOffset();
 
-  if (relative_jump_bytecode_offset > 0) {
-    AddNewNode<ReduceInterruptBudget>({}, relative_jump_bytecode_offset);
+  if (!is_toptier()) {
+    if (relative_jump_bytecode_offset > 0) {
+      AddNewNode<ReduceInterruptBudget>({}, relative_jump_bytecode_offset);
+    }
+    AddNewNode<JumpLoopPrologue>({}, loop_offset, feedback_slot,
+                                 BytecodeOffset(iterator_.current_offset()),
+                                 compilation_unit_);
   }
-  AddNewNode<JumpLoopPrologue>({}, loop_offset, feedback_slot,
-                               BytecodeOffset(iterator_.current_offset()),
-                               compilation_unit_);
   BasicBlock* block =
       FinishBlock<JumpLoop>({}, jump_targets_[target].block_ptr());
 
-  merge_states_[target]->MergeLoop(*compilation_unit_,
+  merge_states_[target]->MergeLoop(*compilation_unit_, graph_->smi(),
                                    current_interpreter_frame_, block, target);
   block->set_predecessor_id(merge_states_[target]->predecessor_count() - 1);
 }
 void MaglevGraphBuilder::VisitJump() {
   const uint32_t relative_jump_bytecode_offset =
       iterator_.GetRelativeJumpTargetOffset();
-  if (relative_jump_bytecode_offset > 0) {
+  if (!is_toptier() && relative_jump_bytecode_offset > 0) {
     AddNewNode<IncreaseInterruptBudget>({}, relative_jump_bytecode_offset);
   }
   BasicBlock* block =
@@ -2913,8 +3788,9 @@
         NumPredecessors(target), predecessor, liveness);
   } else {
     // If there already is a frame state, merge.
-    merge_states_[target]->Merge(*compilation_unit_, current_interpreter_frame_,
-                                 predecessor, target);
+    merge_states_[target]->Merge(*compilation_unit_, graph_->smi(),
+                                 current_interpreter_frame_, predecessor,
+                                 target);
   }
 }
 
@@ -2964,8 +3840,9 @@
     // Again, all returns should have the same liveness, so double check this.
     DCHECK(GetInLiveness()->Equals(
         *merge_states_[target]->frame_state().liveness()));
-    merge_states_[target]->Merge(*compilation_unit_, current_interpreter_frame_,
-                                 predecessor, target);
+    merge_states_[target]->Merge(*compilation_unit_, graph_->smi(),
+                                 current_interpreter_frame_, predecessor,
+                                 target);
   }
 }
 
@@ -2974,12 +3851,26 @@
                                                    RootIndex root_index) {
   int fallthrough_offset = next_offset();
   int jump_offset = iterator_.GetJumpTargetOffset();
+  if (RootConstant* c = node->TryCast<RootConstant>()) {
+    bool constant_is_match = c->index() == root_index;
+    bool is_jump_taken = constant_is_match == (jump_type == kJumpIfTrue);
+    if (is_jump_taken) {
+      BasicBlock* block = FinishBlock<Jump>({}, &jump_targets_[jump_offset]);
+      MergeDeadIntoFrameState(fallthrough_offset);
+      MergeIntoFrameState(block, jump_offset);
+    } else {
+      MergeDeadIntoFrameState(jump_offset);
+    }
+    return;
+  }
+
   BasicBlockRef* true_target = jump_type == kJumpIfTrue
                                    ? &jump_targets_[jump_offset]
                                    : &jump_targets_[fallthrough_offset];
   BasicBlockRef* false_target = jump_type == kJumpIfFalse
                                     ? &jump_targets_[jump_offset]
                                     : &jump_targets_[fallthrough_offset];
+
   BasicBlock* block = FinishBlock<BranchIfRootConstant>(
       {node}, true_target, false_target, root_index);
   if (jump_type == kJumpIfTrue) {
@@ -3128,6 +4019,7 @@
   // ForInContinue <index> <cache_length>
   ValueNode* index = LoadRegisterTagged(0);
   ValueNode* cache_length = LoadRegisterTagged(1);
+  // TODO(verwaest): Fold with the next instruction.
   SetAccumulator(AddNewNode<TaggedNotEqual>({index, cache_length}));
 }
 
@@ -3174,7 +4066,7 @@
 void MaglevGraphBuilder::VisitReturn() {
   // See also: InterpreterAssembler::UpdateInterruptBudgetOnReturn.
   const uint32_t relative_jump_bytecode_offset = iterator_.current_offset();
-  if (relative_jump_bytecode_offset > 0) {
+  if (!is_toptier() && relative_jump_bytecode_offset > 0) {
     AddNewNode<ReduceInterruptBudget>({}, relative_jump_bytecode_offset);
   }
 
@@ -3288,7 +4180,7 @@
     BasicBlockRef* ref = &targets[offset.case_value - case_value_base];
     new (ref) BasicBlockRef(&jump_targets_[offset.target_offset]);
   }
-  ValueNode* case_value = AddNewNode<CheckedSmiUntag>({state});
+  ValueNode* case_value = AddNewNode<UnsafeSmiUntag>({state});
   BasicBlock* generator_prologue_block = FinishBlock<Switch>(
       {case_value}, case_value_base, targets, offsets.size());
   for (interpreter::JumpTableTargetOffset offset : offsets) {
@@ -3322,7 +4214,7 @@
   AddNode(node);
 
   const uint32_t relative_jump_bytecode_offset = iterator_.current_offset();
-  if (relative_jump_bytecode_offset > 0) {
+  if (!is_toptier() && relative_jump_bytecode_offset > 0) {
     AddNewNode<ReduceInterruptBudget>({}, relative_jump_bytecode_offset);
   }
   FinishBlock<Return>({GetAccumulatorTagged()});
@@ -3340,7 +4232,7 @@
     // register file length.
     ValueNode* array_length_smi =
         AddNewNode<LoadTaggedField>({array}, FixedArrayBase::kLengthOffset);
-    ValueNode* array_length = AddNewNode<CheckedSmiUntag>({array_length_smi});
+    ValueNode* array_length = AddNewNode<UnsafeSmiUntag>({array_length_smi});
     ValueNode* register_size = GetInt32Constant(
         parameter_count_without_receiver() + registers.register_count());
     AddNewNode<AssertInt32>(
diff -r -u --color up/v8/src/maglev/maglev-graph-builder.h nw/v8/src/maglev/maglev-graph-builder.h
--- up/v8/src/maglev/maglev-graph-builder.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/maglev/maglev-graph-builder.h	2023-01-19 16:46:36.298109548 -0500
@@ -12,6 +12,8 @@
 
 #include "src/base/logging.h"
 #include "src/base/optional.h"
+#include "src/codegen/source-position-table.h"
+#include "src/common/globals.h"
 #include "src/compiler/bytecode-analysis.h"
 #include "src/compiler/bytecode-liveness-map.h"
 #include "src/compiler/heap-refs.h"
@@ -68,6 +70,7 @@
 
   void BuildBody() {
     for (iterator_.Reset(); !iterator_.done(); iterator_.Advance()) {
+      local_isolate_->heap()->Safepoint();
       VisitSingleBytecode();
     }
   }
@@ -75,14 +78,31 @@
   Graph* graph() const { return graph_; }
 
  private:
-  BasicBlock* CreateEmptyBlock(int offset) {
+  bool CheckType(ValueNode* node, NodeType type);
+  NodeInfo* CreateInfoIfNot(ValueNode* node, NodeType type);
+  bool EnsureType(ValueNode* node, NodeType type, NodeType* old = nullptr);
+  bool is_toptier() {
+    return v8_flags.lower_tier_as_toptier && !v8_flags.turbofan;
+  }
+  BasicBlock* CreateEdgeSplitBlock(int offset,
+                                   int interrupt_budget_correction) {
     if (v8_flags.trace_maglev_graph_building) {
       std::cout << "== New empty block ==" << std::endl;
     }
     DCHECK_NULL(current_block_);
     current_block_ = zone()->New<BasicBlock>(nullptr);
+    // Add an interrupt budget correction if necessary. This makes the edge
+    // split block no longer empty, which is unexpected, but we're not changing
+    // interpreter frame state, so that's ok.
+    if (!is_toptier() && interrupt_budget_correction != 0) {
+      DCHECK_GT(interrupt_budget_correction, 0);
+      AddNewNode<IncreaseInterruptBudget>({}, interrupt_budget_correction);
+    }
     BasicBlock* result = FinishBlock<Jump>({}, &jump_targets_[offset]);
-    result->set_empty_block();
+    result->set_edge_split_block();
+#ifdef DEBUG
+    new_nodes_.clear();
+#endif
     return result;
   }
 
@@ -95,7 +115,7 @@
 
     // Merges aren't simple fallthroughs, so we should reset the checkpoint
     // validity.
-    latest_checkpointed_state_.reset();
+    latest_checkpointed_frame_.reset();
 
     // Register exception phis.
     if (has_graph_labeller()) {
@@ -117,7 +137,7 @@
 
     // Merges aren't simple fallthroughs, so we should reset the checkpoint
     // validity.
-    latest_checkpointed_state_.reset();
+    latest_checkpointed_frame_.reset();
 
     if (merge_state.predecessor_count() == 1) return;
 
@@ -136,7 +156,8 @@
       ControlNode* control = predecessor->control_node();
       if (control->Is<ConditionalControlNode>()) {
         // CreateEmptyBlock automatically registers itself with the offset.
-        predecessor = CreateEmptyBlock(offset);
+        predecessor = CreateEdgeSplitBlock(
+            offset, old_jump_targets->interrupt_budget_correction());
         // Set the old predecessor's (the conditional block) reference to
         // point to the new empty predecessor block.
         old_jump_targets =
@@ -210,13 +231,31 @@
       // Any other bytecode that doesn't return or throw will merge into the
       // fallthrough.
       MergeDeadIntoFrameState(iterator_.next_offset());
+    } else if (interpreter::Bytecodes::Returns(bytecode) && is_inline()) {
+      MergeDeadIntoFrameState(inline_exit_offset());
     }
 
     // TODO(leszeks): We could now continue iterating the bytecode
   }
 
+  void UpdateSourceAndBytecodePosition(int offset) {
+    if (source_position_iterator_.done()) return;
+    if (source_position_iterator_.code_offset() == offset) {
+      // TODO(leszeks): Add inlining support.
+      const int kInliningId = SourcePosition::kNotInlined;
+      current_source_position_ = SourcePosition(
+          source_position_iterator_.source_position().ScriptOffset(),
+          kInliningId);
+      source_position_iterator_.Advance();
+    } else {
+      DCHECK_GT(source_position_iterator_.code_offset(), offset);
+    }
+  }
+
   void VisitSingleBytecode() {
     int offset = iterator_.current_offset();
+    UpdateSourceAndBytecodePosition(offset);
+
     MergePointInterpreterFrameState* merge_state = merge_states_[offset];
     if (V8_UNLIKELY(merge_state != nullptr)) {
       if (current_block_ != nullptr) {
@@ -224,8 +263,8 @@
         // bytecodes in this basic block were only register juggling.
         // DCHECK(!current_block_->nodes().is_empty());
         BasicBlock* predecessor = FinishBlock<Jump>({}, &jump_targets_[offset]);
-        merge_state->Merge(*compilation_unit_, current_interpreter_frame_,
-                           predecessor, offset);
+        merge_state->Merge(*compilation_unit_, graph_->smi(),
+                           current_interpreter_frame_, predecessor, offset);
       }
       if (v8_flags.trace_maglev_graph_building) {
         auto detail = merge_state->is_exception_handler() ? "exception handler"
@@ -354,12 +393,10 @@
   template <typename NodeT, typename... Args>
   NodeT* CreateNewNodeHelper(Args&&... args) {
     if constexpr (NodeT::kProperties.can_eager_deopt()) {
-      return NodeBase::New<NodeT>(zone(), *compilation_unit_,
-                                  GetLatestCheckpointedState(),
+      return NodeBase::New<NodeT>(zone(), GetLatestCheckpointedFrame(),
                                   std::forward<Args>(args)...);
     } else if constexpr (NodeT::kProperties.can_lazy_deopt()) {
-      return NodeBase::New<NodeT>(zone(), *compilation_unit_,
-                                  GetCheckpointedStateForLazyDeopt(),
+      return NodeBase::New<NodeT>(zone(), GetDeoptFrameForLazyDeopt(),
                                   std::forward<Args>(args)...);
     } else {
       return NodeBase::New<NodeT>(zone(), std::forward<Args>(args)...);
@@ -389,6 +426,8 @@
   bool TrySpecializeLoadContextSlotToFunctionContext(
       ValueNode** context, size_t* depth, int slot_index,
       ContextSlotMutability slot_mutability);
+  ValueNode* LoadAndCacheContextSlot(ValueNode* context, int offset,
+                                     ContextSlotMutability slot_mutability);
   void BuildLoadContextSlot(ValueNode* context, size_t depth, int slot_index,
                             ContextSlotMutability slot_mutability);
 
@@ -539,6 +578,8 @@
   }
 
   Int32Constant* GetInt32Constant(int constant) {
+    // The constant must fit in a Smi, since it could be later tagged in a Phi.
+    DCHECK(Smi::IsValid(constant));
     auto it = graph_->int32().find(constant);
     if (it == graph_->int32().end()) {
       Int32Constant* node = CreateNewNode<Int32Constant>(0, constant);
@@ -627,6 +668,30 @@
     UNREACHABLE();
   }
 
+  void SetKnownType(ValueNode* node, NodeType type) {
+    NodeInfo* known_info = known_node_aspects().GetOrCreateInfoFor(node);
+    known_info->type = type;
+  }
+
+  ValueNode* GetInternalizedString(interpreter::Register reg) {
+    ValueNode* node = GetTaggedValue(reg);
+    if (known_node_aspects()
+            .GetOrCreateInfoFor(node)
+            ->is_internalized_string()) {
+      return node;
+    }
+    if (Constant* constant = node->TryCast<Constant>()) {
+      if (constant->object().IsInternalizedString()) {
+        SetKnownType(constant, NodeType::kInternalizedString);
+        return constant;
+      }
+    }
+    node = AddNewNode<CheckedInternalizedString>({node});
+    SetKnownType(node, NodeType::kInternalizedString);
+    current_interpreter_frame_.set(reg, node);
+    return node;
+  }
+
   ValueNode* GetInt32(interpreter::Register reg) {
     ValueNode* value = current_interpreter_frame_.get(reg);
     switch (value->properties().value_representation()) {
@@ -636,7 +701,7 @@
         }
         NodeInfo* node_info = known_node_aspects().GetOrCreateInfoFor(value);
         if (node_info->int32_alternative == nullptr) {
-          node_info->int32_alternative = AddNewNode<CheckedSmiUntag>({value});
+          node_info->int32_alternative = BuildSmiUntag(value);
         }
         return node_info->int32_alternative;
       }
@@ -654,8 +719,7 @@
     UNREACHABLE();
   }
 
-  ValueNode* GetFloat64(interpreter::Register reg) {
-    ValueNode* value = current_interpreter_frame_.get(reg);
+  ValueNode* GetFloat64(ValueNode* value) {
     switch (value->properties().value_representation()) {
       case ValueRepresentation::kTagged: {
         NodeInfo* node_info = known_node_aspects().GetOrCreateInfoFor(value);
@@ -679,6 +743,10 @@
     UNREACHABLE();
   }
 
+  ValueNode* GetFloat64(interpreter::Register reg) {
+    return GetFloat64(current_interpreter_frame_.get(reg));
+  }
+
   ValueNode* GetAccumulatorTagged() {
     return GetTaggedValue(interpreter::Register::virtual_accumulator());
   }
@@ -771,28 +839,29 @@
     current_interpreter_frame_.set(target1, second_value);
   }
 
-  CheckpointedInterpreterState GetLatestCheckpointedState() {
-    if (!latest_checkpointed_state_) {
-      latest_checkpointed_state_.emplace(
-          BytecodeOffset(iterator_.current_offset()),
+  InterpretedDeoptFrame GetLatestCheckpointedFrame() {
+    if (!latest_checkpointed_frame_) {
+      latest_checkpointed_frame_.emplace(
+          *compilation_unit_,
           zone()->New<CompactInterpreterFrameState>(
               *compilation_unit_, GetInLiveness(), current_interpreter_frame_),
-          parent_ == nullptr
-              ? nullptr
-              // TODO(leszeks): Don't always allocate for the parent state,
-              // maybe cache it on the graph builder?
-              : zone()->New<CheckpointedInterpreterState>(
-                    parent_->GetLatestCheckpointedState()));
-    }
-    return *latest_checkpointed_state_;
+          BytecodeOffset(iterator_.current_offset()), current_source_position_,
+          // TODO(leszeks): Don't always allocate for the parent state,
+          // maybe cache it on the graph builder?
+          parent_
+              ? zone()->New<DeoptFrame>(parent_->GetLatestCheckpointedFrame())
+              : nullptr);
+    }
+    return *latest_checkpointed_frame_;
   }
 
-  CheckpointedInterpreterState GetCheckpointedStateForLazyDeopt() {
-    return CheckpointedInterpreterState(
-        BytecodeOffset(iterator_.current_offset()),
+  InterpretedDeoptFrame GetDeoptFrameForLazyDeopt() {
+    return InterpretedDeoptFrame(
+        *compilation_unit_,
         zone()->New<CompactInterpreterFrameState>(
             *compilation_unit_, GetOutLiveness(), current_interpreter_frame_),
-        // TODO(leszeks): Support lazy deopts in inlined functions.
+        BytecodeOffset(iterator_.current_offset()), current_source_position_,
+        // TODO(leszeks): Support inlining for lazy deopts.
         nullptr);
   }
 
@@ -803,23 +872,35 @@
     DCHECK_EQ(NodeT::kProperties.can_lazy_deopt(),
               value->properties().can_lazy_deopt());
     if constexpr (NodeT::kProperties.can_lazy_deopt()) {
-      DCHECK(result_location.is_valid());
-      DCHECK(!value->lazy_deopt_info()->result_location.is_valid());
-      value->lazy_deopt_info()->result_location = result_location;
-      value->lazy_deopt_info()->result_size = result_size;
+      value->lazy_deopt_info()->SetResultLocation(result_location, result_size);
     }
   }
 
   void MarkPossibleSideEffect() {
     // If there was a potential side effect, invalidate the previous checkpoint.
-    latest_checkpointed_state_.reset();
+    latest_checkpointed_frame_.reset();
 
     // A side effect could change existing objects' maps. For stable maps we
     // know this hasn't happened (because we added a dependency on the maps
     // staying stable and therefore not possible to transition away from), but
     // we can no longer assume that objects with unstable maps still have the
-    // same map.
-    known_node_aspects().unstable_maps.clear();
+    // same map. Unstable maps can also transition to stable ones, so the
+    // set of stable maps becomes invalid for a not that had a unstable map.
+    auto it = known_node_aspects().unstable_maps.begin();
+    while (it != known_node_aspects().unstable_maps.end()) {
+      if (it->second.size() == 0) {
+        it++;
+      } else {
+        known_node_aspects().stable_maps.erase(it->first);
+        it = known_node_aspects().unstable_maps.erase(it);
+      }
+    }
+    // Similarly, side-effects can change object contents, so we have to clear
+    // our known loaded properties -- however, constant properties are known
+    // to not change (and we added a dependency on this), so we don't have to
+    // clear those.
+    known_node_aspects().loaded_properties.clear();
+    known_node_aspects().loaded_context_slots.clear();
   }
 
   int next_offset() const {
@@ -882,7 +963,7 @@
       jump_target_refs_head =
           jump_target_refs_head->SetToBlockAndReturnNext(block);
     }
-    if (interrupt_budget_correction != 0) {
+    if (!is_toptier() && interrupt_budget_correction != 0) {
       DCHECK_GT(interrupt_budget_correction, 0);
       AddNewNode<IncreaseInterruptBudget>({}, interrupt_budget_correction);
     }
@@ -910,47 +991,233 @@
                                ConvertReceiverMode receiver_mode,
                                compiler::JSFunctionRef function);
 
+  class CallArguments {
+   public:
+    enum Mode {
+      kFromRegisters,
+      kFromRegisterList,
+    };
+
+    CallArguments(ConvertReceiverMode receiver_mode, int reg_count,
+                  interpreter::Register r0 = interpreter::Register(),
+                  interpreter::Register r1 = interpreter::Register(),
+                  interpreter::Register r2 = interpreter::Register())
+        : receiver_mode_(receiver_mode),
+          call_mode_(kFromRegisters),
+          reg_count_(reg_count) {
+      DCHECK_GE(reg_count, 0);
+      DCHECK_LT(reg_count, 4);
+      DCHECK_IMPLIES(receiver_mode_ != ConvertReceiverMode::kNullOrUndefined,
+                     reg_count > 0);
+      DCHECK_IMPLIES(reg_count > 0, r0.is_valid());
+      regs_[0] = r0;
+      DCHECK_IMPLIES(reg_count > 1, r1.is_valid());
+      regs_[1] = r1;
+      DCHECK_IMPLIES(reg_count > 2, r2.is_valid());
+      regs_[2] = r2;
+    }
+
+    CallArguments(ConvertReceiverMode receiver_mode,
+                  interpreter::RegisterList reglist)
+        : receiver_mode_(receiver_mode),
+          call_mode_(kFromRegisterList),
+          reglist_(reglist) {}
+
+    base::Optional<interpreter::Register> receiver() const {
+      if (receiver_mode_ == ConvertReceiverMode::kNullOrUndefined) {
+        return {};
+      }
+      if (call_mode_ == kFromRegisters) {
+        DCHECK_GT(reg_count_, 0);
+        return regs_[0];
+      }
+      return reglist_[0];
+    }
+
+    int count() const {
+      int register_count =
+          call_mode_ == kFromRegisters ? reg_count_ : reglist_.register_count();
+      if (receiver_mode_ == ConvertReceiverMode::kNullOrUndefined) {
+        return register_count;
+      }
+      return register_count - 1;
+    }
+
+    int count_with_receiver() const { return count() + 1; }
+
+    const interpreter::Register operator[](size_t i) const {
+      if (receiver_mode_ != ConvertReceiverMode::kNullOrUndefined) {
+        i++;
+      }
+      if (call_mode_ == kFromRegisters) {
+        DCHECK_LT(i, reg_count_);
+        DCHECK_GE(i, 0);
+        return regs_[i];
+      }
+      return reglist_[i];
+    }
+
+    ConvertReceiverMode receiver_mode() const { return receiver_mode_; }
+
+    CallArguments PopReceiver(ConvertReceiverMode new_receiver_mode) const {
+      DCHECK_NE(receiver_mode_, ConvertReceiverMode::kNullOrUndefined);
+      DCHECK_NE(new_receiver_mode, ConvertReceiverMode::kNullOrUndefined);
+      // If there is no non-receiver argument to become the new receiver,
+      // consider the new receiver to be known undefined.
+      if (count() == 0) {
+        new_receiver_mode = ConvertReceiverMode::kNullOrUndefined;
+      }
+      if (call_mode_ == kFromRegisters) {
+        return CallArguments(new_receiver_mode, reg_count_ - 1, regs_[1],
+                             regs_[2]);
+      }
+      return CallArguments(new_receiver_mode, reglist_.PopLeft());
+    }
+
+   private:
+    const ConvertReceiverMode receiver_mode_;
+    const Mode call_mode_;
+    union {
+      struct {
+        interpreter::Register regs_[3];
+        int reg_count_;
+      };
+      interpreter::RegisterList reglist_;
+    };
+  };
+
+  ValueNode* GetTaggedReceiver(const CallArguments& args) {
+    auto maybe_receiver = args.receiver();
+    if (maybe_receiver.has_value()) {
+      return GetTaggedValue(*maybe_receiver);
+    }
+    DCHECK_EQ(args.receiver_mode(), ConvertReceiverMode::kNullOrUndefined);
+    return GetRootConstant(RootIndex::kUndefinedValue);
+  }
+  ValueNode* GetConvertReceiver(compiler::JSFunctionRef function,
+                                const CallArguments& args);
+
+#define MAGLEV_REDUCED_BUILTIN(V) \
+  V(FunctionPrototypeCall)        \
+  V(StringFromCharCode)           \
+  V(StringPrototypeCharCodeAt)
+
+#define DEFINE_BUILTIN_REDUCER(Name)                           \
+  bool TryReduce##Name(compiler::JSFunctionRef builtin_target, \
+                       const CallArguments& args);
+  MAGLEV_REDUCED_BUILTIN(DEFINE_BUILTIN_REDUCER)
+#undef DEFINE_BUILTIN_REDUCER
+
+  bool TryReduceBuiltin(compiler::JSFunctionRef builtin_target,
+                        const CallArguments& args);
+
+  bool TryBuildCallKnownJSFunction(compiler::JSFunctionRef function,
+                                   const CallArguments& args);
+
+  void BuildGenericCall(ValueNode* target, ValueNode* context,
+                        Call::TargetType target_type, const CallArguments& args,
+                        compiler::FeedbackSource& feedback_source);
+  void BuildCall(ValueNode* target_node, const CallArguments& args,
+                 compiler::FeedbackSource& feedback_source);
   void BuildCallFromRegisterList(ConvertReceiverMode receiver_mode);
   void BuildCallFromRegisters(int argc_count,
                               ConvertReceiverMode receiver_mode);
 
+  bool TryBuildScriptContextConstantAccess(
+      const compiler::GlobalAccessFeedback& global_access_feedback);
+  bool TryBuildScriptContextAccess(
+      const compiler::GlobalAccessFeedback& global_access_feedback);
   bool TryBuildPropertyCellAccess(
       const compiler::GlobalAccessFeedback& global_access_feedback);
 
+  ValueNode* BuildSmiUntag(ValueNode* node);
+
   void BuildCheckSmi(ValueNode* object);
+  void BuildCheckNumber(ValueNode* object);
   void BuildCheckHeapObject(ValueNode* object);
   void BuildCheckString(ValueNode* object);
   void BuildCheckSymbol(ValueNode* object);
-  void BuildMapCheck(ValueNode* object, const compiler::MapRef& map);
+  void BuildCheckMaps(ValueNode* object,
+                      ZoneVector<compiler::MapRef> const& maps);
+  // Emits an unconditional deopt and returns false if the node is a constant
+  // that doesn't match the ref.
+  bool BuildCheckValue(ValueNode* node, const compiler::HeapObjectRef& ref);
+
+  ValueNode* GetInt32ElementIndex(interpreter::Register reg) {
+    ValueNode* index_object = current_interpreter_frame_.get(reg);
+    return GetInt32ElementIndex(index_object);
+  }
+  ValueNode* GetInt32ElementIndex(ValueNode* index_object);
 
   bool TryFoldLoadDictPrototypeConstant(
       compiler::PropertyAccessInfo access_info);
-  bool TryFoldLoadConstantDataField(compiler::PropertyAccessInfo access_info);
+  bool TryFoldLoadConstantDataField(compiler::PropertyAccessInfo access_info,
+                                    ValueNode* lookup_start_object);
 
   void BuildLoadField(compiler::PropertyAccessInfo access_info,
                       ValueNode* lookup_start_object);
   bool TryBuildStoreField(compiler::PropertyAccessInfo access_info,
                           ValueNode* receiver);
   bool TryBuildPropertyGetterCall(compiler::PropertyAccessInfo access_info,
-                                  ValueNode* receiver);
+                                  ValueNode* receiver,
+                                  ValueNode* lookup_start_object);
   bool TryBuildPropertySetterCall(compiler::PropertyAccessInfo access_info,
                                   ValueNode* receiver, ValueNode* value);
 
   bool TryBuildPropertyLoad(ValueNode* receiver, ValueNode* lookup_start_object,
+                            compiler::NameRef name,
                             compiler::PropertyAccessInfo const& access_info);
-  bool TryBuildPropertyStore(ValueNode* receiver,
+  bool TryBuildPropertyStore(ValueNode* receiver, compiler::NameRef name,
                              compiler::PropertyAccessInfo const& access_info);
   bool TryBuildPropertyAccess(ValueNode* receiver,
                               ValueNode* lookup_start_object,
+                              compiler::NameRef name,
                               compiler::PropertyAccessInfo const& access_info,
                               compiler::AccessMode access_mode);
 
+  bool TryBuildElementAccessOnString(
+      ValueNode* object, ValueNode* index,
+      compiler::KeyedAccessMode const& keyed_mode);
+
   bool TryBuildNamedAccess(ValueNode* receiver, ValueNode* lookup_start_object,
                            compiler::NamedAccessFeedback const& feedback,
                            compiler::AccessMode access_mode);
   bool TryBuildElementAccess(ValueNode* object, ValueNode* index,
                              compiler::ElementAccessFeedback const& feedback);
 
+  // Load elimination -- when loading or storing a simple property without
+  // side effects, record its value, and allow that value to be re-used on
+  // subsequent loads.
+  void RecordKnownProperty(ValueNode* lookup_start_object,
+                           compiler::NameRef name, ValueNode* value,
+                           bool is_const);
+  bool TryReuseKnownPropertyLoad(ValueNode* lookup_start_object,
+                                 compiler::NameRef name);
+
+  enum InferHasInPrototypeChainResult {
+    kMayBeInPrototypeChain,
+    kIsInPrototypeChain,
+    kIsNotInPrototypeChain
+  };
+  InferHasInPrototypeChainResult InferHasInPrototypeChain(
+      ValueNode* receiver, compiler::HeapObjectRef prototype);
+  bool TryBuildFastHasInPrototypeChain(ValueNode* object,
+                                       compiler::ObjectRef prototype);
+  void BuildHasInPrototypeChain(ValueNode* object,
+                                compiler::ObjectRef prototype);
+  bool TryBuildFastOrdinaryHasInstance(ValueNode* object,
+                                       compiler::JSObjectRef callable,
+                                       ValueNode* callable_node);
+  void BuildOrdinaryHasInstance(ValueNode* object,
+                                compiler::JSObjectRef callable,
+                                ValueNode* callable_node);
+  bool TryBuildFastInstanceOf(ValueNode* object,
+                              compiler::JSObjectRef callable_ref,
+                              ValueNode* callable_node);
+  bool TryBuildFastInstanceOfWithFeedback(
+      ValueNode* object, ValueNode* callable,
+      compiler::FeedbackSource feedback_source);
+
   template <Operation kOperation>
   void BuildGenericUnaryOperationNode();
   template <Operation kOperation>
@@ -966,6 +1233,11 @@
       std::initializer_list<ValueNode*> inputs);
 
   template <Operation kOperation>
+  ValueNode* TryFoldInt32BinaryOperation(ValueNode* left, ValueNode* right);
+  template <Operation kOperation>
+  ValueNode* TryFoldInt32BinaryOperation(ValueNode* left, int right);
+
+  template <Operation kOperation>
   void BuildInt32BinaryOperationNode();
   template <Operation kOperation>
   void BuildInt32BinarySmiOperationNode();
@@ -1056,7 +1328,7 @@
     return compilation_unit_->bytecode();
   }
   const compiler::BytecodeAnalysis& bytecode_analysis() const {
-    return compilation_unit_->bytecode_analysis();
+    return bytecode_analysis_;
   }
   LocalIsolate* local_isolate() const { return local_isolate_; }
   Zone* zone() const { return compilation_unit_->zone(); }
@@ -1087,12 +1359,15 @@
   MaglevCompilationUnit* const compilation_unit_;
   MaglevGraphBuilder* const parent_;
   Graph* const graph_;
+  compiler::BytecodeAnalysis bytecode_analysis_;
   interpreter::BytecodeArrayIterator iterator_;
+  SourcePositionTableIterator source_position_iterator_;
   uint32_t* predecessors_;
 
   // Current block information.
   BasicBlock* current_block_ = nullptr;
-  base::Optional<CheckpointedInterpreterState> latest_checkpointed_state_;
+  base::Optional<InterpretedDeoptFrame> latest_checkpointed_frame_;
+  SourcePosition current_source_position_;
 
   BasicBlockRef* jump_targets_;
   MergePointInterpreterFrameState** merge_states_;
diff -r -u --color up/v8/src/maglev/maglev-graph-printer.cc nw/v8/src/maglev/maglev-graph-printer.cc
--- up/v8/src/maglev/maglev-graph-printer.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/maglev/maglev-graph-printer.cc	2023-01-19 16:46:36.298109548 -0500
@@ -365,74 +365,132 @@
 
 namespace {
 
-template <typename NodeT>
-void PrintEagerDeopt(std::ostream& os, std::vector<BasicBlock*> targets,
-                     NodeT* node, MaglevGraphLabeller* graph_labeller,
-                     int max_node_id) {
+void PrintSingleDeoptFrame(
+    std::ostream& os, MaglevGraphLabeller* graph_labeller,
+    const DeoptFrame& frame, InputLocation*& current_input_location,
+    LazyDeoptInfo* lazy_deopt_info_if_top_frame = nullptr) {
+  switch (frame.type()) {
+    case DeoptFrame::FrameType::kInterpretedFrame: {
+      os << "@" << frame.as_interpreted().bytecode_position() << " : {";
+      bool first = true;
+      frame.as_interpreted().frame_state()->ForEachValue(
+          frame.as_interpreted().unit(),
+          [&](ValueNode* node, interpreter::Register reg) {
+            if (first) {
+              first = false;
+            } else {
+              os << ", ";
+            }
+            os << reg.ToString() << ":";
+            if (lazy_deopt_info_if_top_frame &&
+                lazy_deopt_info_if_top_frame->IsResultRegister(reg)) {
+              os << "<result>";
+            } else {
+              os << PrintNodeLabel(graph_labeller, node) << ":"
+                 << current_input_location->operand();
+              current_input_location++;
+            }
+          });
+      os << "}";
+      break;
+    }
+    case DeoptFrame::FrameType::kBuiltinContinuationFrame: {
+      os << "@" << Builtins::name(frame.as_builtin_continuation().builtin_id())
+         << " : {";
+      int arg_index = 0;
+      for (ValueNode* node : frame.as_builtin_continuation().parameters()) {
+        os << "a" << arg_index << ":" << PrintNodeLabel(graph_labeller, node)
+           << ":" << current_input_location->operand();
+        arg_index++;
+        current_input_location++;
+        os << ", ";
+      }
+      os << "<context>:"
+         << PrintNodeLabel(graph_labeller,
+                           frame.as_builtin_continuation().context())
+         << ":" << current_input_location->operand();
+      current_input_location++;
+      os << "}";
+      break;
+    }
+  }
+}
+
+void RecursivePrintEagerDeopt(std::ostream& os,
+                              std::vector<BasicBlock*> targets,
+                              const DeoptFrame& frame,
+                              MaglevGraphLabeller* graph_labeller,
+                              int max_node_id,
+                              InputLocation*& current_input_location) {
+  if (frame.parent()) {
+    RecursivePrintEagerDeopt(os, targets, *frame.parent(), graph_labeller,
+                             max_node_id, current_input_location);
+  }
+
   PrintVerticalArrows(os, targets);
   PrintPadding(os, graph_labeller, max_node_id, 0);
+  if (!frame.parent()) {
+    os << "  ↱ eager ";
+  } else {
+    os << "  │       ";
+  }
+  PrintSingleDeoptFrame(os, graph_labeller, frame, current_input_location);
+  os << "\n";
+}
 
+void PrintEagerDeopt(std::ostream& os, std::vector<BasicBlock*> targets,
+                     NodeBase* node, MaglevGraphLabeller* graph_labeller,
+                     int max_node_id) {
   EagerDeoptInfo* deopt_info = node->eager_deopt_info();
-  os << "  ↱ eager @" << deopt_info->state.bytecode_position << " : {";
-  bool first = true;
-  int index = 0;
-  deopt_info->state.register_frame->ForEachValue(
-      deopt_info->unit, [&](ValueNode* node, interpreter::Register reg) {
-        if (first) {
-          first = false;
-        } else {
-          os << ", ";
-        }
-        os << reg.ToString() << ":" << PrintNodeLabel(graph_labeller, node)
-           << ":" << deopt_info->input_locations[index].operand();
-        index++;
-      });
-  os << "}\n";
+  InputLocation* current_input_location = deopt_info->input_locations();
+  RecursivePrintEagerDeopt(os, targets, deopt_info->top_frame(), graph_labeller,
+                           max_node_id, current_input_location);
 }
+
 void MaybePrintEagerDeopt(std::ostream& os, std::vector<BasicBlock*> targets,
                           NodeBase* node, MaglevGraphLabeller* graph_labeller,
                           int max_node_id) {
-  switch (node->opcode()) {
-#define CASE(Name)                                                           \
-  case Opcode::k##Name:                                                      \
-    if constexpr (Name::kProperties.can_eager_deopt()) {                     \
-      PrintEagerDeopt<Name>(os, targets, node->Cast<Name>(), graph_labeller, \
-                            max_node_id);                                    \
-    }                                                                        \
-    break;
-    NODE_BASE_LIST(CASE)
-#undef CASE
+  if (node->properties().can_eager_deopt()) {
+    PrintEagerDeopt(os, targets, node, graph_labeller, max_node_id);
+  }
+}
+
+void RecursivePrintLazyDeopt(std::ostream& os, std::vector<BasicBlock*> targets,
+                             const DeoptFrame& frame,
+                             MaglevGraphLabeller* graph_labeller,
+                             int max_node_id,
+                             InputLocation*& current_input_location) {
+  if (frame.parent()) {
+    RecursivePrintLazyDeopt(os, targets, *frame.parent(), graph_labeller,
+                            max_node_id, current_input_location);
   }
+
+  PrintVerticalArrows(os, targets);
+  PrintPadding(os, graph_labeller, max_node_id, 0);
+  os << "  │      ";
+  PrintSingleDeoptFrame(os, graph_labeller, frame, current_input_location);
+  os << "\n";
 }
 
 template <typename NodeT>
 void PrintLazyDeopt(std::ostream& os, std::vector<BasicBlock*> targets,
                     NodeT* node, MaglevGraphLabeller* graph_labeller,
                     int max_node_id) {
+  LazyDeoptInfo* deopt_info = node->lazy_deopt_info();
+  InputLocation* current_input_location = deopt_info->input_locations();
+  const DeoptFrame& top_frame = deopt_info->top_frame();
+  if (top_frame.parent()) {
+    RecursivePrintLazyDeopt(os, targets, *top_frame.parent(), graph_labeller,
+                            max_node_id, current_input_location);
+  }
+
   PrintVerticalArrows(os, targets);
   PrintPadding(os, graph_labeller, max_node_id, 0);
 
-  LazyDeoptInfo* deopt_info = node->lazy_deopt_info();
-  os << "  ↳ lazy @" << deopt_info->state.bytecode_position << " : {";
-  bool first = true;
-  int index = 0;
-  deopt_info->state.register_frame->ForEachValue(
-      deopt_info->unit, [&](ValueNode* node, interpreter::Register reg) {
-        if (first) {
-          first = false;
-        } else {
-          os << ", ";
-        }
-        os << reg.ToString() << ":";
-        if (deopt_info->IsResultRegister(reg)) {
-          os << "<result>";
-        } else {
-          os << PrintNodeLabel(graph_labeller, node) << ":"
-             << deopt_info->input_locations[index].operand();
-          index++;
-        }
-      });
-  os << "}\n";
+  os << "  ↳ lazy ";
+  PrintSingleDeoptFrame(os, graph_labeller, top_frame, current_input_location,
+                        deopt_info);
+  os << "\n";
 }
 
 template <typename NodeT>
@@ -458,13 +516,20 @@
   auto* liveness = block->state()->frame_state().liveness();
   LazyDeoptInfo* deopt_info = node->lazy_deopt_info();
 
+  const InterpretedDeoptFrame& lazy_frame =
+      deopt_info->top_frame().type() ==
+              DeoptFrame::FrameType::kBuiltinContinuationFrame
+          ? deopt_info->top_frame().parent()->as_interpreted()
+          : deopt_info->top_frame().as_interpreted();
+
   PrintVerticalArrows(os, targets);
   PrintPadding(os, graph_labeller, max_node_id, 0);
 
   os << "  ↳ throw @" << handler_offset << " : {";
   bool first = true;
-  deopt_info->state.register_frame->ForEachValue(
-      deopt_info->unit, [&](ValueNode* node, interpreter::Register reg) {
+  lazy_frame.as_interpreted().frame_state()->ForEachValue(
+      lazy_frame.as_interpreted().unit(),
+      [&](ValueNode* node, interpreter::Register reg) {
         if (!reg.is_parameter() && !liveness->RegisterIsLive(reg.index())) {
           // Skip, since not live at the handler offset.
           return;
diff -r -u --color up/v8/src/maglev/maglev-graph-verifier.h nw/v8/src/maglev/maglev-graph-verifier.h
--- up/v8/src/maglev/maglev-graph-verifier.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/maglev/maglev-graph-verifier.h	2023-01-19 16:46:36.298109548 -0500
@@ -28,21 +28,6 @@
   return os;
 }
 
-namespace {
-ValueRepresentation ToValueRepresentation(MachineType type) {
-  switch (type.representation()) {
-    case MachineRepresentation::kTagged:
-    case MachineRepresentation::kTaggedSigned:
-    case MachineRepresentation::kTaggedPointer:
-      return ValueRepresentation::kTagged;
-    case MachineRepresentation::kFloat64:
-      return ValueRepresentation::kFloat64;
-    default:
-      return ValueRepresentation::kInt32;
-  }
-}
-}  // namespace
-
 class Graph;
 
 // TODO(victorgomes): Currently it only verifies the inputs for all ValueNodes
@@ -59,6 +44,19 @@
   void PostProcessGraph(Graph* graph) {}
   void PreProcessBasicBlock(BasicBlock* block) {}
 
+  static ValueRepresentation ToValueRepresentation(MachineType type) {
+    switch (type.representation()) {
+      case MachineRepresentation::kTagged:
+      case MachineRepresentation::kTaggedSigned:
+      case MachineRepresentation::kTaggedPointer:
+        return ValueRepresentation::kTagged;
+      case MachineRepresentation::kFloat64:
+        return ValueRepresentation::kFloat64;
+      default:
+        return ValueRepresentation::kInt32;
+    }
+  }
+
   void CheckValueInputIs(NodeBase* node, int i, ValueRepresentation expected) {
     ValueNode* input = node->input(i).node();
     ValueRepresentation got = input->properties().value_representation();
@@ -107,6 +105,7 @@
         DCHECK_EQ(node->input_count(), 0);
         break;
       case Opcode::kCheckedSmiUntag:
+      case Opcode::kUnsafeSmiUntag:
       case Opcode::kGenericBitwiseNot:
       case Opcode::kGenericDecrement:
       case Opcode::kGenericIncrement:
@@ -117,6 +116,7 @@
       // TODO(victorgomes): Can we check that the input is actually a receiver?
       case Opcode::kCheckHeapObject:
       case Opcode::kCheckMaps:
+      case Opcode::kCheckValue:
       case Opcode::kCheckMapsWithMigration:
       case Opcode::kCheckSmi:
       case Opcode::kCheckNumber:
@@ -124,6 +124,7 @@
       case Opcode::kCheckSymbol:
       case Opcode::kCheckedInternalizedString:
       case Opcode::kCheckedObjectToIndex:
+      case Opcode::kConvertReceiver:
       // TODO(victorgomes): Can we check that the input is Boolean?
       case Opcode::kBranchIfToBooleanTrue:
       case Opcode::kBranchIfRootConstant:
@@ -137,7 +138,9 @@
       case Opcode::kGetTemplateObject:
       case Opcode::kLogicalNot:
       case Opcode::kSetPendingMessage:
+      case Opcode::kStoreMap:
       case Opcode::kStringLength:
+      case Opcode::kToBoolean:
       case Opcode::kToBooleanLogicalNot:
       case Opcode::kTestUndetectable:
       case Opcode::kTestTypeOf:
@@ -152,6 +155,7 @@
       case Opcode::kCheckedSmiTag:
       case Opcode::kUnsafeSmiTag:
       case Opcode::kChangeInt32ToFloat64:
+      case Opcode::kBuiltinStringFromCharCode:
         DCHECK_EQ(node->input_count(), 1);
         CheckValueInputIs(node, 0, ValueRepresentation::kInt32);
         break;
@@ -225,7 +229,7 @@
       case Opcode::kInt32MultiplyWithOverflow:
       case Opcode::kInt32DivideWithOverflow:
       // case Opcode::kInt32ExponentiateWithOverflow:
-      // case Opcode::kInt32ModulusWithOverflow:
+      case Opcode::kInt32ModulusWithOverflow:
       case Opcode::kInt32BitwiseAnd:
       case Opcode::kInt32BitwiseOr:
       case Opcode::kInt32BitwiseXor:
@@ -239,6 +243,7 @@
       case Opcode::kInt32GreaterThan:
       case Opcode::kInt32GreaterThanOrEqual:
       case Opcode::kBranchIfInt32Compare:
+      case Opcode::kCheckInt32Condition:
         DCHECK_EQ(node->input_count(), 2);
         CheckValueInputIs(node, 0, ValueRepresentation::kInt32);
         CheckValueInputIs(node, 1, ValueRepresentation::kInt32);
@@ -263,7 +268,13 @@
         CheckValueInputIs(node, 0, ValueRepresentation::kFloat64);
         CheckValueInputIs(node, 1, ValueRepresentation::kFloat64);
         break;
+      case Opcode::kStoreDoubleField:
+        DCHECK_EQ(node->input_count(), 2);
+        CheckValueInputIs(node, 0, ValueRepresentation::kTagged);
+        CheckValueInputIs(node, 1, ValueRepresentation::kFloat64);
+        break;
       case Opcode::kCall:
+      case Opcode::kCallKnownJSFunction:
       case Opcode::kCallRuntime:
       case Opcode::kCallWithSpread:
       case Opcode::kConstruct:
@@ -280,6 +291,8 @@
       case Opcode::kCheckJSObjectElementsBounds:
       case Opcode::kLoadTaggedElement:
       case Opcode::kLoadDoubleElement:
+      case Opcode::kStringAt:
+      case Opcode::kBuiltinStringPrototypeCharCodeAt:
         DCHECK_EQ(node->input_count(), 2);
         CheckValueInputIs(node, 0, ValueRepresentation::kTagged);
         CheckValueInputIs(node, 1, ValueRepresentation::kInt32);
diff -r -u --color up/v8/src/maglev/maglev-interpreter-frame-state.cc nw/v8/src/maglev/maglev-interpreter-frame-state.cc
--- up/v8/src/maglev/maglev-interpreter-frame-state.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/maglev/maglev-interpreter-frame-state.cc	2023-01-19 16:46:36.298109548 -0500
@@ -33,21 +33,7 @@
   }
   frame_state.ForEachParameter(
       unit, [&](ValueNode*& entry, interpreter::Register reg) {
-        if (!is_inline && reg.is_receiver()) {
-          // The receiver is a special case for a fairly silly reason:
-          // OptimizedFrame::Summarize requires the receiver (and the function)
-          // to be in a stack slot, since it's value must be available even
-          // though we're not deoptimizing (and thus register states are not
-          // available). Exception phis could be allocated in a register.
-          // Since the receiver is immutable, simply reuse its InitialValue
-          // node.
-          // For inlined functions / nested graph generation, this a) doesn't
-          // work (there's no receiver stack slot); and b) isn't necessary
-          // (Summarize only looks at noninlined functions).
-          entry = graph->parameters()[0];
-        } else {
-          entry = state->NewExceptionPhi(zone, reg, handler_offset);
-        }
+        entry = state->NewExceptionPhi(zone, reg, handler_offset);
       });
   frame_state.context(unit) =
       state->NewExceptionPhi(zone, context_register, handler_offset);
diff -r -u --color up/v8/src/maglev/maglev-interpreter-frame-state.h nw/v8/src/maglev/maglev-interpreter-frame-state.h
--- up/v8/src/maglev/maglev-interpreter-frame-state.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/maglev/maglev-interpreter-frame-state.h	2023-01-19 16:46:36.298109548 -0500
@@ -15,6 +15,7 @@
 #include "src/maglev/maglev-ir.h"
 #include "src/maglev/maglev-regalloc-data.h"
 #include "src/maglev/maglev-register-frame-array.h"
+#include "src/zone/zone-handle-set.h"
 #include "src/zone/zone.h"
 
 namespace v8 {
@@ -28,14 +29,15 @@
 // left map is mutated to become the result of the intersection. Values that
 // are in both maps are passed to the merging function to be merged with each
 // other -- again, the LHS here is expected to be mutated.
-template <typename Value, typename MergeFunc>
-void DestructivelyIntersect(ZoneMap<ValueNode*, Value>& lhs_map,
-                            const ZoneMap<ValueNode*, Value>& rhs_map,
-                            MergeFunc&& func) {
+template <typename Key, typename Value,
+          typename MergeFunc = std::equal_to<Value>>
+void DestructivelyIntersect(ZoneMap<Key, Value>& lhs_map,
+                            const ZoneMap<Key, Value>& rhs_map,
+                            MergeFunc&& func = MergeFunc()) {
   // Walk the two maps in lock step. This relies on the fact that ZoneMaps are
   // sorted.
-  typename ZoneMap<ValueNode*, Value>::iterator lhs_it = lhs_map.begin();
-  typename ZoneMap<ValueNode*, Value>::const_iterator rhs_it = rhs_map.begin();
+  typename ZoneMap<Key, Value>::iterator lhs_it = lhs_map.begin();
+  typename ZoneMap<Key, Value>::const_iterator rhs_it = rhs_map.begin();
   while (lhs_it != lhs_map.end() && rhs_it != rhs_map.end()) {
     if (lhs_it->first < rhs_it->first) {
       // Remove from LHS elements that are not in RHS.
@@ -64,30 +66,45 @@
 
 // The intersection (using `&`) of any two NodeTypes must be a valid NodeType
 // (possibly "kUnknown").
+// All heap object types include the heap object bit, so that they can be
+// checked for AnyHeapObject with a single bit check.
 // TODO(leszeks): Figure out how to represent Number/Numeric with this encoding.
+#define NODE_TYPE_LIST(V)                              \
+  V(Unknown, 0)                                        \
+  V(Number, (1 << 0))                                  \
+  V(Smi, (1 << 1) | kNumber)                           \
+  V(AnyHeapObject, (1 << 2))                           \
+  V(Name, (1 << 3) | kAnyHeapObject)                   \
+  V(String, (1 << 4) | kName)                          \
+  V(InternalizedString, (1 << 5) | kString)            \
+  V(Symbol, (1 << 6) | kName)                          \
+  V(JSReceiver, (1 << 7) | kAnyHeapObject)             \
+  V(HeapObjectWithKnownMap, (1 << 8) | kAnyHeapObject) \
+  V(HeapNumber, kHeapObjectWithKnownMap | kNumber)     \
+  V(JSReceiverWithKnownMap, kJSReceiver | kHeapObjectWithKnownMap)
+
 enum class NodeType {
-  kUnknown = 0,
-  kSmi = (1 << 0),
-  kAnyHeapObject = (1 << 1),
-  // All heap object types include the heap object bit, so that they can be
-  // checked for AnyHeapObject with a single bit check.
-  kString = (1 << 2) | kAnyHeapObject,
-  kSymbol = (1 << 3) | kAnyHeapObject,
-  kHeapNumber = (1 << 4) | kAnyHeapObject,
-  kHeapObjectWithKnownMap = (1 << 5) | kAnyHeapObject,
+#define DEFINE_NODE_TYPE(Name, Value) k##Name = Value,
+  NODE_TYPE_LIST(DEFINE_NODE_TYPE)
+#undef DEFINE_NODE_TYPE
 };
 
-inline bool NodeTypeIsSmi(NodeType type) { return type == NodeType::kSmi; }
-inline bool NodeTypeIsAnyHeapObject(NodeType type) {
-  return static_cast<int>(type) & static_cast<int>(NodeType::kAnyHeapObject);
-}
-inline bool NodeTypeIsString(NodeType type) {
-  return type == NodeType::kString;
+inline NodeType CombineType(NodeType left, NodeType right) {
+  return static_cast<NodeType>(static_cast<int>(left) |
+                               static_cast<int>(right));
 }
-inline bool NodeTypeIsSymbol(NodeType type) {
-  return type == NodeType::kSymbol;
+inline bool NodeTypeIs(NodeType type, NodeType to_check) {
+  int right = static_cast<int>(to_check);
+  return (static_cast<int>(type) & right) == right;
 }
 
+#define DEFINE_NODE_TYPE_CHECK(Type, _)         \
+  inline bool NodeTypeIs##Type(NodeType type) { \
+    return NodeTypeIs(type, NodeType::k##Type); \
+  }
+NODE_TYPE_LIST(DEFINE_NODE_TYPE_CHECK)
+#undef DEFINE_NODE_TYPE_CHECK
+
 struct NodeInfo {
   NodeType type = NodeType::kUnknown;
 
@@ -107,6 +124,9 @@
   bool is_smi() const { return NodeTypeIsSmi(type); }
   bool is_any_heap_object() const { return NodeTypeIsAnyHeapObject(type); }
   bool is_string() const { return NodeTypeIsString(type); }
+  bool is_internalized_string() const {
+    return NodeTypeIsInternalizedString(type);
+  }
   bool is_symbol() const { return NodeTypeIsSymbol(type); }
 
   // Mutate this node info by merging in another node info, with the result
@@ -128,48 +148,76 @@
 
 struct KnownNodeAspects {
   explicit KnownNodeAspects(Zone* zone)
-      : node_infos(zone), stable_maps(zone), unstable_maps(zone) {}
+      : node_infos(zone),
+        stable_maps(zone),
+        unstable_maps(zone),
+        loaded_constant_properties(zone),
+        loaded_properties(zone),
+        loaded_context_constants(zone),
+        loaded_context_slots(zone) {}
 
-  KnownNodeAspects(const KnownNodeAspects& other) = delete;
+  // Copy constructor is defaulted but private so that we explicitly call the
+  // Clone method.
   KnownNodeAspects& operator=(const KnownNodeAspects& other) = delete;
   KnownNodeAspects(KnownNodeAspects&& other) = delete;
   KnownNodeAspects& operator=(KnownNodeAspects&& other) = delete;
 
   KnownNodeAspects* Clone(Zone* zone) const {
-    KnownNodeAspects* clone = zone->New<KnownNodeAspects>(zone);
-    clone->node_infos = node_infos;
-    clone->stable_maps = stable_maps;
-    clone->unstable_maps = unstable_maps;
-    return clone;
+    return zone->New<KnownNodeAspects>(*this);
   }
 
   // Loop headers can safely clone the node types, since those won't be
   // invalidated in the loop body, and similarly stable maps will have
   // dependencies installed. Unstable maps however might be invalidated by
   // calls, and we don't know about these until it's too late.
-  KnownNodeAspects* CloneWithoutUnstableMaps(Zone* zone) const {
+  KnownNodeAspects* CloneForLoopHeader(Zone* zone) const {
     KnownNodeAspects* clone = zone->New<KnownNodeAspects>(zone);
     clone->node_infos = node_infos;
     clone->stable_maps = stable_maps;
+    clone->loaded_constant_properties = loaded_constant_properties;
+    clone->loaded_context_constants = loaded_context_constants;
     return clone;
   }
 
+  ZoneMap<ValueNode*, NodeInfo>::iterator FindInfo(ValueNode* node) {
+    return node_infos.find(node);
+  }
+  bool IsValid(ZoneMap<ValueNode*, NodeInfo>::iterator& it) {
+    return it != node_infos.end();
+  }
+
   NodeInfo* GetOrCreateInfoFor(ValueNode* node) { return &node_infos[node]; }
 
-  void Merge(const KnownNodeAspects& other) {
+  void Merge(const KnownNodeAspects& other, Zone* zone) {
     DestructivelyIntersect(node_infos, other.node_infos,
                            [](NodeInfo& lhs, const NodeInfo& rhs) {
                              lhs.MergeWith(rhs);
                              return !lhs.is_empty();
                            });
-    DestructivelyIntersect(stable_maps, other.stable_maps,
-                           [](compiler::MapRef lhs, compiler::MapRef rhs) {
-                             return lhs.equals(rhs);
-                           });
-    DestructivelyIntersect(unstable_maps, other.unstable_maps,
-                           [](compiler::MapRef lhs, compiler::MapRef rhs) {
-                             return lhs.equals(rhs);
-                           });
+    DestructivelyIntersect(
+        stable_maps, other.stable_maps,
+        [zone](ZoneHandleSet<Map>& lhs, const ZoneHandleSet<Map>& rhs) {
+          for (Handle<Map> map : rhs) {
+            lhs.insert(map, zone);
+          }
+          // We should always add the value even if the set is empty.
+          return true;
+        });
+    DestructivelyIntersect(
+        unstable_maps, other.unstable_maps,
+        [zone](ZoneHandleSet<Map>& lhs, const ZoneHandleSet<Map>& rhs) {
+          for (Handle<Map> map : rhs) {
+            lhs.insert(map, zone);
+          }
+          // We should always add the value even if the set is empty.
+          return true;
+        });
+    DestructivelyIntersect(loaded_constant_properties,
+                           other.loaded_constant_properties);
+    DestructivelyIntersect(loaded_properties, other.loaded_properties);
+    DestructivelyIntersect(loaded_context_constants,
+                           other.loaded_context_constants);
+    DestructivelyIntersect(loaded_context_slots, other.loaded_context_slots);
   }
 
   // TODO(leszeks): Store these more efficiently than with std::map -- in
@@ -178,10 +226,29 @@
 
   // Permanently valid if checked in a dominator.
   ZoneMap<ValueNode*, NodeInfo> node_infos;
+  // TODO(v8:7700): Investigate a better data structure to use than
+  // ZoneHandleSet.
   // Valid across side-effecting calls, as long as we install a dependency.
-  ZoneMap<ValueNode*, compiler::MapRef> stable_maps;
+  ZoneMap<ValueNode*, ZoneHandleSet<Map>> stable_maps;
   // Flushed after side-effecting calls.
-  ZoneMap<ValueNode*, compiler::MapRef> unstable_maps;
+  ZoneMap<ValueNode*, ZoneHandleSet<Map>> unstable_maps;
+
+  // Valid across side-effecting calls, as long as we install a dependency.
+  ZoneMap<std::pair<ValueNode*, compiler::NameRef>, ValueNode*>
+      loaded_constant_properties;
+  // Flushed after side-effecting calls.
+  ZoneMap<std::pair<ValueNode*, compiler::NameRef>, ValueNode*>
+      loaded_properties;
+
+  // Unconditionally valid across side-effecting calls.
+  ZoneMap<std::tuple<ValueNode*, int>, ValueNode*> loaded_context_constants;
+  // Flushed after side-effecting calls.
+  ZoneMap<std::tuple<ValueNode*, int>, ValueNode*> loaded_context_slots;
+
+ private:
+  friend KnownNodeAspects* Zone::New<KnownNodeAspects, const KnownNodeAspects&>(
+      const KnownNodeAspects&);
+  KnownNodeAspects(const KnownNodeAspects& other) V8_NOEXCEPT = default;
 };
 
 class InterpreterFrameState {
@@ -422,24 +489,6 @@
     kLoopHeader,
     kExceptionHandlerStart,
   };
-  void CheckIsLoopPhiIfNeeded(const MaglevCompilationUnit& compilation_unit,
-                              int merge_offset, interpreter::Register reg,
-                              ValueNode* value) {
-#ifdef DEBUG
-    const auto& analysis = compilation_unit.bytecode_analysis();
-    if (!analysis.IsLoopHeader(merge_offset)) return;
-    auto& assignments = analysis.GetLoopInfoFor(merge_offset).assignments();
-    if (reg.is_parameter()) {
-      if (reg.is_current_context()) return;
-      if (!assignments.ContainsParameter(reg.ToParameterIndex())) return;
-    } else {
-      DCHECK(
-          analysis.GetInLivenessFor(merge_offset)->RegisterIsLive(reg.index()));
-      if (!assignments.ContainsLocal(reg.index())) return;
-    }
-    DCHECK(value->Is<Phi>());
-#endif
-  }
 
   static MergePointInterpreterFrameState* New(
       const MaglevCompilationUnit& info, const InterpreterFrameState& state,
@@ -510,6 +559,7 @@
   // Merges an unmerged framestate with a possibly merged framestate into |this|
   // framestate.
   void Merge(MaglevCompilationUnit& compilation_unit,
+             ZoneMap<int, SmiConstant*>& smi_constants,
              InterpreterFrameState& unmerged, BasicBlock* predecessor,
              int merge_offset) {
     DCHECK_GT(predecessor_count_, 1);
@@ -521,8 +571,6 @@
     }
     frame_state_.ForEachValue(compilation_unit, [&](ValueNode*& value,
                                                     interpreter::Register reg) {
-      CheckIsLoopPhiIfNeeded(compilation_unit, merge_offset, reg, value);
-
       if (v8_flags.trace_maglev_graph_building) {
         std::cout << "  " << reg.ToString() << ": "
                   << PrintNodeLabel(compilation_unit.graph_labeller(), value)
@@ -530,8 +578,9 @@
                   << PrintNodeLabel(compilation_unit.graph_labeller(),
                                     unmerged.get(reg));
       }
-      value = MergeValue(compilation_unit, reg, unmerged.known_node_aspects(),
-                         value, unmerged.get(reg), merge_offset);
+      value = MergeValue(compilation_unit, smi_constants, reg,
+                         unmerged.known_node_aspects(), value,
+                         unmerged.get(reg), merge_offset);
       if (v8_flags.trace_maglev_graph_building) {
         std::cout << " => "
                   << PrintNodeLabel(compilation_unit.graph_labeller(), value)
@@ -543,11 +592,11 @@
     if (known_node_aspects_ == nullptr) {
       DCHECK(is_unmerged_loop());
       DCHECK_EQ(predecessors_so_far_, 0);
-      known_node_aspects_ =
-          unmerged.known_node_aspects().CloneWithoutUnstableMaps(
-              compilation_unit.zone());
+      known_node_aspects_ = unmerged.known_node_aspects().CloneForLoopHeader(
+          compilation_unit.zone());
     } else {
-      known_node_aspects_->Merge(unmerged.known_node_aspects());
+      known_node_aspects_->Merge(unmerged.known_node_aspects(),
+                                 compilation_unit.zone());
     }
 
     predecessors_so_far_++;
@@ -557,6 +606,7 @@
   // Merges an unmerged framestate with a possibly merged framestate into |this|
   // framestate.
   void MergeLoop(MaglevCompilationUnit& compilation_unit,
+                 ZoneMap<int, SmiConstant*>& smi_constants,
                  InterpreterFrameState& loop_end_state,
                  BasicBlock* loop_end_block, int merge_offset) {
     // This should be the last predecessor we try to merge.
@@ -569,8 +619,6 @@
     }
     frame_state_.ForEachValue(compilation_unit, [&](ValueNode* value,
                                                     interpreter::Register reg) {
-      CheckIsLoopPhiIfNeeded(compilation_unit, merge_offset, reg, value);
-
       if (v8_flags.trace_maglev_graph_building) {
         std::cout << "  " << reg.ToString() << ": "
                   << PrintNodeLabel(compilation_unit.graph_labeller(), value)
@@ -578,8 +626,9 @@
                   << PrintNodeLabel(compilation_unit.graph_labeller(),
                                     loop_end_state.get(reg));
       }
-      MergeLoopValue(compilation_unit, reg, loop_end_state.known_node_aspects(),
-                     value, loop_end_state.get(reg), merge_offset);
+      MergeLoopValue(compilation_unit, smi_constants, reg,
+                     loop_end_state.known_node_aspects(), value,
+                     loop_end_state.get(reg), merge_offset);
       if (v8_flags.trace_maglev_graph_building) {
         std::cout << " => "
                   << PrintNodeLabel(compilation_unit.graph_labeller(), value)
@@ -602,7 +651,6 @@
 
     frame_state_.ForEachValue(
         compilation_unit, [&](ValueNode* value, interpreter::Register reg) {
-          CheckIsLoopPhiIfNeeded(compilation_unit, merge_offset, reg, value);
           ReducePhiPredecessorCount(reg, value, merge_offset);
         });
   }
@@ -683,29 +731,51 @@
         basic_block_type_(type),
         frame_state_(info, liveness) {}
 
+  // TODO(victorgomes): Consider refactor this function to share code with
+  // MaglevGraphBuilder::GetSmiConstant.
+  SmiConstant* GetSmiConstant(MaglevCompilationUnit& compilation_unit,
+                              ZoneMap<int, SmiConstant*>& smi_constants,
+                              int constant) {
+    DCHECK(Smi::IsValid(constant));
+    auto it = smi_constants.find(constant);
+    if (it == smi_constants.end()) {
+      SmiConstant* node = Node::New<SmiConstant>(compilation_unit.zone(), 0,
+                                                 Smi::FromInt(constant));
+      compilation_unit.RegisterNodeInGraphLabeller(node);
+      smi_constants.emplace(constant, node);
+      return node;
+    }
+    return it->second;
+  }
+
   ValueNode* FromInt32ToTagged(MaglevCompilationUnit& compilation_unit,
+                               ZoneMap<int, SmiConstant*>& smi_constants,
                                KnownNodeAspects& known_node_aspects,
                                ValueNode* value) {
     DCHECK_EQ(value->properties().value_representation(),
               ValueRepresentation::kInt32);
     DCHECK(!value->properties().is_conversion());
 #define IS_INT32_OP_NODE(Name) || value->Is<Name>()
-    DCHECK(value->Is<Int32Constant>() ||
-           value->Is<StringLength>()
+    DCHECK(value->Is<Int32Constant>() || value->Is<StringLength>() ||
+           value->Is<BuiltinStringPrototypeCharCodeAt>()
                INT32_OPERATIONS_NODE_LIST(IS_INT32_OP_NODE));
 #undef IS_INT32_OP_NODE
     NodeInfo* node_info = known_node_aspects.GetOrCreateInfoFor(value);
     if (!node_info->tagged_alternative) {
       // Create a tagged version.
       ValueNode* tagged;
-      if (value->Is<StringLength>()) {
+      if (value->Is<Int32Constant>()) {
+        int32_t constant = value->Cast<Int32Constant>()->value();
+        return GetSmiConstant(compilation_unit, smi_constants, constant);
+      } else if (value->Is<StringLength>() ||
+                 value->Is<BuiltinStringPrototypeCharCodeAt>()) {
         static_assert(String::kMaxLength <= kSmiMaxValue,
                       "String length must fit into a Smi");
         tagged = Node::New<UnsafeSmiTag>(compilation_unit.zone(), {value});
       } else {
         tagged = Node::New<CheckedSmiTag, std::initializer_list<ValueNode*>>(
-            compilation_unit.zone(), compilation_unit,
-            value->eager_deopt_info()->state, {value});
+            compilation_unit.zone(),
+            DeoptFrame(value->eager_deopt_info()->top_frame()), {value});
       }
 
       Node::List::AddAfter(value, tagged);
@@ -739,19 +809,22 @@
   // TODO(victorgomes): Consider refactor this function to share code with
   // MaglevGraphBuilder::GetTagged.
   ValueNode* EnsureTagged(MaglevCompilationUnit& compilation_unit,
+                          ZoneMap<int, SmiConstant*>& smi_constants,
                           KnownNodeAspects& known_node_aspects,
                           ValueNode* value) {
     switch (value->properties().value_representation()) {
       case ValueRepresentation::kTagged:
         return value;
       case ValueRepresentation::kInt32:
-        return FromInt32ToTagged(compilation_unit, known_node_aspects, value);
+        return FromInt32ToTagged(compilation_unit, smi_constants,
+                                 known_node_aspects, value);
       case ValueRepresentation::kFloat64:
         return FromFloat64ToTagged(compilation_unit, known_node_aspects, value);
     }
   }
 
   ValueNode* MergeValue(MaglevCompilationUnit& compilation_unit,
+                        ZoneMap<int, SmiConstant*>& smi_constants,
                         interpreter::Register owner,
                         KnownNodeAspects& unmerged_aspects, ValueNode* merged,
                         ValueNode* unmerged, int merge_offset) {
@@ -768,7 +841,8 @@
       // It's possible that merged == unmerged at this point since loop-phis are
       // not dropped if they are only assigned to themselves in the loop.
       DCHECK_EQ(result->owner(), owner);
-      unmerged = EnsureTagged(compilation_unit, unmerged_aspects, unmerged);
+      unmerged = EnsureTagged(compilation_unit, smi_constants, unmerged_aspects,
+                              unmerged);
       result->set_input(predecessors_so_far_, unmerged);
       return result;
     }
@@ -777,8 +851,10 @@
 
     // We guarantee that the values are tagged.
     // TODO(victorgomes): Support Phi nodes of untagged values.
-    merged = EnsureTagged(compilation_unit, *known_node_aspects_, merged);
-    unmerged = EnsureTagged(compilation_unit, unmerged_aspects, unmerged);
+    merged = EnsureTagged(compilation_unit, smi_constants, *known_node_aspects_,
+                          merged);
+    unmerged = EnsureTagged(compilation_unit, smi_constants, unmerged_aspects,
+                            unmerged);
 
     // Tagged versions could point to the same value, avoid Phi nodes in this
     // case.
@@ -827,6 +903,7 @@
   }
 
   void MergeLoopValue(MaglevCompilationUnit& compilation_unit,
+                      ZoneMap<int, SmiConstant*>& smi_constants,
                       interpreter::Register owner,
                       KnownNodeAspects& unmerged_aspects, ValueNode* merged,
                       ValueNode* unmerged, int merge_offset) {
@@ -842,7 +919,8 @@
       return;
     }
     DCHECK_EQ(result->owner(), owner);
-    unmerged = EnsureTagged(compilation_unit, unmerged_aspects, unmerged);
+    unmerged = EnsureTagged(compilation_unit, smi_constants, unmerged_aspects,
+                            unmerged);
     result->set_input(predecessor_count_ - 1, unmerged);
   }
 
diff -r -u --color up/v8/src/maglev/maglev-ir-inl.h nw/v8/src/maglev/maglev-ir-inl.h
--- up/v8/src/maglev/maglev-ir-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/maglev/maglev-ir-inl.h	2023-01-19 16:46:36.298109548 -0500
@@ -15,43 +15,64 @@
 namespace detail {
 
 template <typename Function>
-void DeepForEachInputImpl(const MaglevCompilationUnit& unit,
-                          const CheckpointedInterpreterState* state,
+void DeepForEachInputImpl(const DeoptFrame& frame,
                           InputLocation* input_locations, int& index,
                           Function&& f) {
-  if (state->parent) {
-    DeepForEachInputImpl(*unit.caller(), state->parent, input_locations, index,
-                         f);
+  if (frame.parent()) {
+    DeepForEachInputImpl(*frame.parent(), input_locations, index, f);
+  }
+  switch (frame.type()) {
+    case DeoptFrame::FrameType::kInterpretedFrame:
+      frame.as_interpreted().frame_state()->ForEachValue(
+          frame.as_interpreted().unit(),
+          [&](ValueNode* node, interpreter::Register reg) {
+            f(node, &input_locations[index++]);
+          });
+      break;
+    case DeoptFrame::FrameType::kBuiltinContinuationFrame:
+      for (ValueNode* node : frame.as_builtin_continuation().parameters()) {
+        f(node, &input_locations[index++]);
+      }
+      f(frame.as_builtin_continuation().context(), &input_locations[index++]);
+      UNREACHABLE();
   }
-  state->register_frame->ForEachValue(
-      unit, [&](ValueNode* node, interpreter::Register reg) {
-        f(node, reg, &input_locations[index++]);
-      });
 }
 
 template <typename Function>
 void DeepForEachInput(const EagerDeoptInfo* deopt_info, Function&& f) {
   int index = 0;
-  DeepForEachInputImpl(deopt_info->unit, &deopt_info->state,
-                       deopt_info->input_locations, index, f);
+  DeepForEachInputImpl(deopt_info->top_frame(), deopt_info->input_locations(),
+                       index, f);
 }
 
 template <typename Function>
 void DeepForEachInput(const LazyDeoptInfo* deopt_info, Function&& f) {
   int index = 0;
-  if (deopt_info->state.parent) {
-    DeepForEachInputImpl(*deopt_info->unit.caller(), deopt_info->state.parent,
-                         deopt_info->input_locations, index, f);
+  InputLocation* input_locations = deopt_info->input_locations();
+  const DeoptFrame& top_frame = deopt_info->top_frame();
+  if (top_frame.parent()) {
+    DeepForEachInputImpl(*top_frame.parent(), input_locations, index, f);
   }
   // Handle the top-of-frame info separately, since we have to skip the result
   // location.
-  deopt_info->state.register_frame->ForEachValue(
-      deopt_info->unit, [&](ValueNode* node, interpreter::Register reg) {
-        // Skip over the result location since it is irrelevant for lazy deopts
-        // (unoptimized code will recreate the result).
-        if (deopt_info->IsResultRegister(reg)) return;
-        f(node, reg, &deopt_info->input_locations[index++]);
-      });
+  switch (top_frame.type()) {
+    case DeoptFrame::FrameType::kInterpretedFrame:
+      top_frame.as_interpreted().frame_state()->ForEachValue(
+          top_frame.as_interpreted().unit(),
+          [&](ValueNode* node, interpreter::Register reg) {
+            // Skip over the result location since it is irrelevant for lazy
+            // deopts (unoptimized code will recreate the result).
+            if (deopt_info->IsResultRegister(reg)) return;
+            f(node, &input_locations[index++]);
+          });
+      break;
+    case DeoptFrame::FrameType::kBuiltinContinuationFrame:
+      for (ValueNode* node : top_frame.as_builtin_continuation().parameters()) {
+        f(node, &input_locations[index++]);
+      };
+      f(top_frame.as_builtin_continuation().context(),
+        &input_locations[index++]);
+  }
 }
 
 }  // namespace detail
diff -r -u --color up/v8/src/maglev/maglev-ir.cc nw/v8/src/maglev/maglev-ir.cc
--- up/v8/src/maglev/maglev-ir.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/maglev/maglev-ir.cc	2023-01-19 16:46:36.298109548 -0500
@@ -9,11 +9,11 @@
 #include "src/baseline/baseline-assembler-inl.h"
 #include "src/builtins/builtins-constructor.h"
 #include "src/codegen/interface-descriptors-inl.h"
-#include "src/codegen/interface-descriptors.h"
 #include "src/codegen/maglev-safepoint-table.h"
 #include "src/codegen/register.h"
 #include "src/codegen/reglist.h"
 #include "src/codegen/x64/assembler-x64.h"
+#include "src/codegen/x64/register-x64.h"
 #include "src/common/globals.h"
 #include "src/compiler/backend/instruction.h"
 #include "src/deoptimizer/deoptimize-reason.h"
@@ -77,13 +77,18 @@
 
 void UseRegister(Input& input) {
   input.SetUnallocated(compiler::UnallocatedOperand::MUST_HAVE_REGISTER,
+                       compiler::UnallocatedOperand::USED_AT_END,
+                       GetVirtualRegister(input.node()));
+}
+void UseAndClobberRegister(Input& input) {
+  input.SetUnallocated(compiler::UnallocatedOperand::MUST_HAVE_REGISTER,
                        compiler::UnallocatedOperand::USED_AT_START,
                        GetVirtualRegister(input.node()));
 }
 void UseAny(Input& input) {
   input.SetUnallocated(
       compiler::UnallocatedOperand::REGISTER_OR_SLOT_OR_CONSTANT,
-      compiler::UnallocatedOperand::USED_AT_START,
+      compiler::UnallocatedOperand::USED_AT_END,
       GetVirtualRegister(input.node()));
 }
 void UseFixed(Input& input, Register reg) {
@@ -95,57 +100,27 @@
                        reg.code(), GetVirtualRegister(input.node()));
 }
 
-// ---
-// Code gen helpers.
-// ---
-
-class SaveRegisterStateForCall {
- public:
-  SaveRegisterStateForCall(MaglevAssembler* masm, RegisterSnapshot snapshot)
-      : masm(masm), snapshot_(snapshot) {
-    __ PushAll(snapshot_.live_registers);
-    __ PushAll(snapshot_.live_double_registers, kDoubleSize);
-  }
-
-  ~SaveRegisterStateForCall() {
-    __ PopAll(snapshot_.live_double_registers, kDoubleSize);
-    __ PopAll(snapshot_.live_registers);
-  }
-
-  MaglevSafepointTableBuilder::Safepoint DefineSafepoint() {
-    // TODO(leszeks): Avoid emitting safepoints when there are no registers to
-    // save.
-    auto safepoint = masm->safepoint_table_builder()->DefineSafepoint(masm);
-    int pushed_reg_index = 0;
-    for (Register reg : snapshot_.live_registers) {
-      if (snapshot_.live_tagged_registers.has(reg)) {
-        safepoint.DefineTaggedRegister(pushed_reg_index);
+void AddDeoptRegistersToSnapshot(RegisterSnapshot* snapshot,
+                                 const EagerDeoptInfo* deopt_info) {
+  detail::DeepForEachInput(deopt_info, [&](ValueNode* node,
+                                           InputLocation* input) {
+    if (!input->IsAnyRegister()) return;
+    if (input->IsDoubleRegister()) {
+      snapshot->live_double_registers.set(input->AssignedDoubleRegister());
+    } else {
+      snapshot->live_registers.set(input->AssignedGeneralRegister());
+      if (node->is_tagged()) {
+        snapshot->live_tagged_registers.set(input->AssignedGeneralRegister());
       }
-      pushed_reg_index++;
     }
-    int num_pushed_double_reg = snapshot_.live_double_registers.Count();
-    safepoint.SetNumPushedRegisters(pushed_reg_index + num_pushed_double_reg);
-    return safepoint;
-  }
-
-  MaglevSafepointTableBuilder::Safepoint DefineSafepointWithLazyDeopt(
-      LazyDeoptInfo* lazy_deopt_info) {
-    lazy_deopt_info->deopting_call_return_pc = masm->pc_offset_for_safepoint();
-    masm->code_gen_state()->PushLazyDeopt(lazy_deopt_info);
-    return DefineSafepoint();
-  }
-
- private:
-  MaglevAssembler* masm;
-  RegisterSnapshot snapshot_;
-};
+  });
+}
 
 #ifdef DEBUG
 RegList GetGeneralRegistersUsedAsInputs(const EagerDeoptInfo* deopt_info) {
   RegList regs;
   detail::DeepForEachInput(deopt_info,
-                           [&regs](ValueNode* value, interpreter::Register reg,
-                                   InputLocation* input) {
+                           [&regs](ValueNode* value, InputLocation* input) {
                              if (input->IsGeneralRegister()) {
                                regs.set(input->AssignedGeneralRegister());
                              }
@@ -159,134 +134,6 @@
 #define DCHECK_REGLIST_EMPTY(...) DCHECK_EQ((__VA_ARGS__), RegList{})
 
 // ---
-// Inlined computations.
-// ---
-
-void AllocateRaw(MaglevAssembler* masm, RegisterSnapshot& register_snapshot,
-                 Register object, int size_in_bytes,
-                 AllocationType alloc_type = AllocationType::kYoung,
-                 AllocationAlignment alignment = kTaggedAligned) {
-  // TODO(victorgomes): Call the runtime for large object allocation.
-  // TODO(victorgomes): Support double alignment.
-  DCHECK_EQ(alignment, kTaggedAligned);
-  size_in_bytes = ALIGN_TO_ALLOCATION_ALIGNMENT(size_in_bytes);
-  if (v8_flags.single_generation) {
-    alloc_type = AllocationType::kOld;
-  }
-  bool in_new_space = alloc_type == AllocationType::kYoung;
-  Isolate* isolate = masm->isolate();
-  ExternalReference top =
-      in_new_space
-          ? ExternalReference::new_space_allocation_top_address(isolate)
-          : ExternalReference::old_space_allocation_top_address(isolate);
-  ExternalReference limit =
-      in_new_space
-          ? ExternalReference::new_space_allocation_limit_address(isolate)
-          : ExternalReference::old_space_allocation_limit_address(isolate);
-
-  ZoneLabelRef done(masm);
-  Register new_top = kScratchRegister;
-  // Check if there is enough space.
-  __ Move(object, __ ExternalReferenceAsOperand(top));
-  __ leaq(new_top, Operand(object, size_in_bytes));
-  __ cmpq(new_top, __ ExternalReferenceAsOperand(limit));
-  // Otherwise call runtime.
-  __ JumpToDeferredIf(
-      greater_equal,
-      [](MaglevAssembler* masm, RegisterSnapshot register_snapshot,
-         Register object, Builtin builtin, int size_in_bytes,
-         ZoneLabelRef done) {
-        // Remove {object} from snapshot, since it is the returned allocated
-        // HeapObject.
-        register_snapshot.live_registers.clear(object);
-        register_snapshot.live_tagged_registers.clear(object);
-        {
-          SaveRegisterStateForCall save_register_state(masm, register_snapshot);
-          using D = AllocateDescriptor;
-          __ Move(D::GetRegisterParameter(D::kRequestedSize), size_in_bytes);
-          __ CallBuiltin(builtin);
-          save_register_state.DefineSafepoint();
-          __ Move(object, kReturnRegister0);
-        }
-        __ jmp(*done);
-      },
-      register_snapshot, object,
-      in_new_space ? Builtin::kAllocateRegularInYoungGeneration
-                   : Builtin::kAllocateRegularInOldGeneration,
-      size_in_bytes, done);
-  // Store new top and tag object.
-  __ movq(__ ExternalReferenceAsOperand(top), new_top);
-  __ addq(object, Immediate(kHeapObjectTag));
-  __ bind(*done);
-}
-
-void ToBoolean(MaglevAssembler* masm, Register value, ZoneLabelRef is_true,
-               ZoneLabelRef is_false, bool fallthrough_when_true) {
-  Register map = kScratchRegister;
-
-  // Check if {{value}} is Smi.
-  __ CheckSmi(value);
-  __ JumpToDeferredIf(
-      zero,
-      [](MaglevAssembler* masm, Register value, ZoneLabelRef is_true,
-         ZoneLabelRef is_false) {
-        // Check if {value} is not zero.
-        __ SmiCompare(value, Smi::FromInt(0));
-        __ j(equal, *is_false);
-        __ jmp(*is_true);
-      },
-      value, is_true, is_false);
-
-  // Check if {{value}} is false.
-  __ CompareRoot(value, RootIndex::kFalseValue);
-  __ j(equal, *is_false);
-
-  // Check if {{value}} is empty string.
-  __ CompareRoot(value, RootIndex::kempty_string);
-  __ j(equal, *is_false);
-
-  // Check if {{value}} is undetectable.
-  __ LoadMap(map, value);
-  __ testl(FieldOperand(map, Map::kBitFieldOffset),
-           Immediate(Map::Bits1::IsUndetectableBit::kMask));
-  __ j(not_zero, *is_false);
-
-  // Check if {{value}} is a HeapNumber.
-  __ CompareRoot(map, RootIndex::kHeapNumberMap);
-  __ JumpToDeferredIf(
-      equal,
-      [](MaglevAssembler* masm, Register value, ZoneLabelRef is_true,
-         ZoneLabelRef is_false) {
-        // Sets scratch register to 0.0.
-        __ Xorpd(kScratchDoubleReg, kScratchDoubleReg);
-        // Sets ZF if equal to 0.0, -0.0 or NaN.
-        __ Ucomisd(kScratchDoubleReg,
-                   FieldOperand(value, HeapNumber::kValueOffset));
-        __ j(zero, *is_false);
-        __ jmp(*is_true);
-      },
-      value, is_true, is_false);
-
-  // Check if {{value}} is a BigInt.
-  __ CompareRoot(map, RootIndex::kBigIntMap);
-  __ JumpToDeferredIf(
-      equal,
-      [](MaglevAssembler* masm, Register value, ZoneLabelRef is_true,
-         ZoneLabelRef is_false) {
-        __ testl(FieldOperand(value, BigInt::kBitfieldOffset),
-                 Immediate(BigInt::LengthBits::kMask));
-        __ j(zero, *is_false);
-        __ jmp(*is_true);
-      },
-      value, is_true, is_false);
-
-  // Otherwise true.
-  if (!fallthrough_when_true) {
-    __ jmp(*is_true);
-  }
-}
-
-// ---
 // Print
 // ---
 
@@ -340,9 +187,27 @@
   }
 }
 
+class MaybeUnparkForPrint {
+ public:
+  MaybeUnparkForPrint() {
+    LocalHeap* local_heap = LocalHeap::Current();
+    if (!local_heap) {
+      local_heap = Isolate::Current()->main_thread_local_heap();
+    }
+    DCHECK_NOT_NULL(local_heap);
+    if (local_heap->IsParked()) {
+      scope_.emplace(local_heap);
+    }
+  }
+
+ private:
+  base::Optional<UnparkedScope> scope_;
+};
+
 template <typename NodeT>
 void PrintImpl(std::ostream& os, MaglevGraphLabeller* graph_labeller,
                const NodeT* node, bool skip_targets) {
+  MaybeUnparkForPrint unpark;
   os << node->opcode();
   node->PrintParams(os, graph_labeller);
   PrintInputs(os, graph_labeller, node);
@@ -373,40 +238,42 @@
 }
 
 namespace {
-size_t GetInputLocationsArraySize(const MaglevCompilationUnit& compilation_unit,
-                                  const CheckpointedInterpreterState& state) {
-  size_t size = state.register_frame->size(compilation_unit);
-  const CheckpointedInterpreterState* parent = state.parent;
-  const MaglevCompilationUnit* parent_unit = compilation_unit.caller();
-  while (parent != nullptr) {
-    size += parent->register_frame->size(*parent_unit);
-    parent = parent->parent;
-    parent_unit = parent_unit->caller();
-  }
+size_t GetInputLocationsArraySize(const DeoptFrame& top_frame) {
+  size_t size = 0;
+  const DeoptFrame* frame = &top_frame;
+  do {
+    switch (frame->type()) {
+      case DeoptFrame::FrameType::kInterpretedFrame:
+        size += frame->as_interpreted().frame_state()->size(
+            frame->as_interpreted().unit());
+        break;
+      case DeoptFrame::FrameType::kBuiltinContinuationFrame:
+        size += frame->as_builtin_continuation().parameters().size() + 1;
+        break;
+    }
+    frame = frame->parent();
+  } while (frame != nullptr);
   return size;
 }
 }  // namespace
 
-DeoptInfo::DeoptInfo(Zone* zone, const MaglevCompilationUnit& compilation_unit,
-                     CheckpointedInterpreterState state)
-    : unit(compilation_unit),
-      state(state),
-      input_locations(zone->NewArray<InputLocation>(
-          GetInputLocationsArraySize(compilation_unit, state))) {
+DeoptInfo::DeoptInfo(Zone* zone, DeoptFrame top_frame)
+    : top_frame_(top_frame),
+      input_locations_(zone->NewArray<InputLocation>(
+          GetInputLocationsArraySize(top_frame))) {
   // Initialise InputLocations so that they correctly don't have a next use id.
-  for (size_t i = 0; i < GetInputLocationsArraySize(compilation_unit, state);
-       ++i) {
-    new (&input_locations[i]) InputLocation();
+  for (size_t i = 0; i < GetInputLocationsArraySize(top_frame); ++i) {
+    new (&input_locations_[i]) InputLocation();
   }
 }
 
 bool LazyDeoptInfo::IsResultRegister(interpreter::Register reg) const {
-  if (V8_LIKELY(result_size == 1)) {
-    return reg == result_location;
+  if (V8_LIKELY(result_size_ == 1)) {
+    return reg == result_location_;
   }
-  DCHECK_EQ(result_size, 2);
-  return reg == result_location ||
-         reg == interpreter::Register(result_location.index() + 1);
+  DCHECK_EQ(result_size_, 2);
+  return reg == result_location_ ||
+         reg == interpreter::Register(result_location_.index() + 1);
 }
 
 // ---
@@ -761,8 +628,8 @@
   DCHECK_EQ(ToRegister(cache_index()), D::GetRegisterParameter(D::kCacheIndex));
   __ Move(D::GetRegisterParameter(D::kSlot), Immediate(feedback().index()));
   // Feedback vector is pushed into the stack.
-  DCHECK_EQ(D::GetRegisterParameterCount(), D::kFeedbackVector);
-  DCHECK_EQ(D::GetStackParameterCount(), 1);
+  static_assert(D::GetStackParameterIndex(D::kFeedbackVector) == 0);
+  static_assert(D::GetStackParameterCount() == 1);
   __ Push(feedback().vector);
   __ CallBuiltin(Builtin::kForInNext);
   masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
@@ -996,7 +863,7 @@
                                             const ProcessingState& state) {
   Register object = ToRegister(result());
   RegisterSnapshot save_registers = register_snapshot();
-  AllocateRaw(masm, save_registers, object, map().instance_size());
+  __ Allocate(save_registers, object, map().instance_size());
   __ Move(kScratchRegister, map().object());
   __ StoreTaggedField(FieldOperand(object, HeapObject::kMapOffset),
                       kScratchRegister);
@@ -1226,18 +1093,58 @@
                              const ProcessingState& state) {
   Register object = ToRegister(receiver_input());
 
+  // TODO(victorgomes): This can happen, because we do not emit an unconditional
+  // deopt when we intersect the map sets.
+  if (maps().is_empty()) {
+    __ RegisterEagerDeopt(eager_deopt_info(), DeoptimizeReason::kWrongMap);
+    __ jmp(eager_deopt_info()->deopt_entry_label());
+    return;
+  }
+
   if (check_type_ == CheckType::kOmitHeapObjectCheck) {
     __ AssertNotSmi(object);
   } else {
     Condition is_smi = __ CheckSmi(object);
     __ EmitEagerDeoptIf(is_smi, DeoptimizeReason::kWrongMap, this);
   }
-  __ Cmp(FieldOperand(object, HeapObject::kMapOffset), map().object());
+
+  Label done;
+  size_t map_count = maps().size();
+  for (size_t i = 0; i < map_count - 1; ++i) {
+    Handle<Map> map = maps().at(i);
+    __ Cmp(FieldOperand(object, HeapObject::kMapOffset), map);
+    __ j(equal, &done, Label::kNear);
+  }
+  Handle<Map> last_map = maps().at(map_count - 1);
+  __ Cmp(FieldOperand(object, HeapObject::kMapOffset), last_map);
   __ EmitEagerDeoptIf(not_equal, DeoptimizeReason::kWrongMap, this);
+  __ bind(&done);
 }
 void CheckMaps::PrintParams(std::ostream& os,
                             MaglevGraphLabeller* graph_labeller) const {
-  os << "(" << *map().object() << ")";
+  os << "(";
+  size_t map_count = maps().size();
+  if (map_count > 0) {
+    for (size_t i = 0; i < map_count - 1; ++i) {
+      os << maps().at(i) << ", ";
+    }
+    os << maps().at(map_count - 1);
+  }
+  os << ")";
+}
+void CheckValue::AllocateVreg(MaglevVregAllocationState* vreg_state) {
+  UseRegister(target_input());
+}
+void CheckValue::GenerateCode(MaglevAssembler* masm,
+                              const ProcessingState& state) {
+  Register target = ToRegister(target_input());
+
+  __ Cmp(target, value().object());
+  __ EmitEagerDeoptIf(not_equal, DeoptimizeReason::kWrongValue, this);
+}
+void CheckValue::PrintParams(std::ostream& os,
+                             MaglevGraphLabeller* graph_labeller) const {
+  os << "(" << *value().object() << ")";
 }
 void CheckSmi::AllocateVreg(MaglevVregAllocationState* vreg_state) {
   UseRegister(receiver_input());
@@ -1331,74 +1238,123 @@
 }
 void CheckMapsWithMigration::GenerateCode(MaglevAssembler* masm,
                                           const ProcessingState& state) {
+  __ RegisterEagerDeopt(eager_deopt_info(), DeoptimizeReason::kWrongMap);
+
+  // TODO(victorgomes): This can happen, because we do not emit an unconditional
+  // deopt when we intersect the map sets.
+  if (maps().is_empty()) {
+    __ jmp(eager_deopt_info()->deopt_entry_label());
+    return;
+  }
+
   Register object = ToRegister(receiver_input());
 
   if (check_type_ == CheckType::kOmitHeapObjectCheck) {
     __ AssertNotSmi(object);
   } else {
     Condition is_smi = __ CheckSmi(object);
-    __ EmitEagerDeoptIf(is_smi, DeoptimizeReason::kWrongMap, this);
+    __ j(is_smi, eager_deopt_info()->deopt_entry_label());
   }
-  __ Cmp(FieldOperand(object, HeapObject::kMapOffset), map().object());
 
-  ZoneLabelRef migration_done(masm);
-  __ JumpToDeferredIf(
-      not_equal,
-      [](MaglevAssembler* masm, ZoneLabelRef migration_done, Register object,
-         CheckMapsWithMigration* node, EagerDeoptInfo* deopt_info) {
-        __ RegisterEagerDeopt(deopt_info, DeoptimizeReason::kWrongMap);
-
-        // Reload the map to avoid needing to save it on a temporary in the fast
-        // path.
-        __ LoadMap(kScratchRegister, object);
-        // If the map is not deprecated, deopt straight away.
-        __ movl(kScratchRegister,
-                FieldOperand(kScratchRegister, Map::kBitField3Offset));
-        __ testl(kScratchRegister,
-                 Immediate(Map::Bits3::IsDeprecatedBit::kMask));
-        __ j(zero, &deopt_info->deopt_entry_label);
-
-        // Otherwise, try migrating the object. If the migration
-        // returns Smi zero, then it failed and we should deopt.
-        Register return_val = Register::no_reg();
-        {
-          SaveRegisterStateForCall save_register_state(
-              masm, node->register_snapshot());
+  ZoneLabelRef done(masm);
+  size_t map_count = maps().size();
+  for (size_t i = 0; i < map_count; ++i) {
+    ZoneLabelRef continue_label(masm);
+    Handle<Map> map = maps().at(i);
+    __ Cmp(FieldOperand(object, HeapObject::kMapOffset), map);
+
+    bool last_map = (i == map_count - 1);
+    if (map->is_migration_target()) {
+      __ JumpToDeferredIf(
+          not_equal,
+          [](MaglevAssembler* masm, ZoneLabelRef continue_label,
+             ZoneLabelRef done, Register object, int map_index,
+             CheckMapsWithMigration* node) {
+            // Reload the map to avoid needing to save it on a temporary in the
+            // fast path.
+            __ LoadMap(kScratchRegister, object);
+            // If the map is not deprecated, we fail the map check, continue to
+            // the next one.
+            __ movl(kScratchRegister,
+                    FieldOperand(kScratchRegister, Map::kBitField3Offset));
+            __ testl(kScratchRegister,
+                     Immediate(Map::Bits3::IsDeprecatedBit::kMask));
+            __ j(zero, *continue_label);
+
+            // Otherwise, try migrating the object. If the migration
+            // returns Smi zero, then it failed the migration.
+            Register return_val = Register::no_reg();
+            {
+              RegisterSnapshot register_snapshot = node->register_snapshot();
+              // We can eager deopt after the snapshot, so make sure the nodes
+              // used by the deopt are included in it.
+              // TODO(leszeks): This is a bit of a footgun -- we likely want the
+              // snapshot to always include eager deopt input registers.
+              AddDeoptRegistersToSnapshot(&register_snapshot,
+                                          node->eager_deopt_info());
+              SaveRegisterStateForCall save_register_state(masm,
+                                                           register_snapshot);
+
+              __ Push(object);
+              __ Move(kContextRegister, masm->native_context().object());
+              __ CallRuntime(Runtime::kTryMigrateInstance);
+              save_register_state.DefineSafepoint();
+
+              // Make sure the return value is preserved across the live
+              // register restoring pop all.
+              return_val = kReturnRegister0;
+              if (node->register_snapshot().live_registers.has(return_val)) {
+                DCHECK(!node->register_snapshot().live_registers.has(
+                    kScratchRegister));
+                __ movq(kScratchRegister, return_val);
+                return_val = kScratchRegister;
+              }
+            }
 
-          __ Push(object);
-          __ Move(kContextRegister, masm->native_context().object());
-          __ CallRuntime(Runtime::kTryMigrateInstance);
-          save_register_state.DefineSafepoint();
+            // On failure, the returned value is zero
+            __ cmpl(return_val, Immediate(0));
+            __ j(equal, *continue_label);
+
+            // The migrated object is returned on success, retry the map check.
+            __ Move(object, return_val);
+            // Manually load the map pointer without uncompressing it.
+            __ Cmp(FieldOperand(object, HeapObject::kMapOffset),
+                   node->maps().at(map_index));
+            __ j(equal, *done);
+            __ jmp(*continue_label);
+          },
+          // If this is the last map to check, we should deopt if we fail.
+          // This is safe to do, since {eager_deopt_info} is ZoneAllocated.
+          (last_map ? ZoneLabelRef::UnsafeFromLabelPointer(
+                          eager_deopt_info()->deopt_entry_label())
+                    : continue_label),
+          done, object, i, this);
+    } else if (last_map) {
+      // If it is the last map and it is not a migration target, we should deopt
+      // if the check fails.
+      __ j(not_equal, eager_deopt_info()->deopt_entry_label());
+    }
 
-          // Make sure the return value is preserved across the live register
-          // restoring pop all.
-          return_val = kReturnRegister0;
-          if (node->register_snapshot().live_registers.has(return_val)) {
-            DCHECK(!node->register_snapshot().live_registers.has(
-                kScratchRegister));
-            __ movq(kScratchRegister, return_val);
-            return_val = kScratchRegister;
-          }
-        }
+    if (!last_map) {
+      // We don't need to bind the label for the last map.
+      __ j(equal, *done);
+      __ bind(*continue_label);
+    }
+  }
 
-        // On failure, the returned value is zero
-        __ cmpl(return_val, Immediate(0));
-        __ j(equal, &deopt_info->deopt_entry_label);
-
-        // The migrated object is returned on success, retry the map check.
-        __ Move(object, return_val);
-        // Manually load the map pointer without uncompressing it.
-        __ Cmp(FieldOperand(object, HeapObject::kMapOffset),
-               node->map().object());
-        __ j(equal, *migration_done);
-        __ jmp(&deopt_info->deopt_entry_label);
-      },
-      migration_done, object, this, eager_deopt_info());
-  __ bind(*migration_done);
+  __ bind(*done);
 }
 void CheckMapsWithMigration::PrintParams(
     std::ostream& os, MaglevGraphLabeller* graph_labeller) const {
-  os << "(" << *map().object() << ")";
+  os << "(";
+  size_t map_count = maps().size();
+  if (map_count > 0) {
+    for (size_t i = 0; i < map_count - 1; ++i) {
+      os << maps().at(i) << ", ";
+    }
+    os << maps().at(map_count - 1);
+  }
+  os << ")";
 }
 
 void CheckJSArrayBounds::AllocateVreg(MaglevVregAllocationState* vreg_state) {
@@ -1447,6 +1403,20 @@
   __ EmitEagerDeoptIf(above_equal, DeoptimizeReason::kOutOfBounds, this);
 }
 
+void CheckInt32Condition::AllocateVreg(MaglevVregAllocationState* vreg_state) {
+  UseRegister(left_input());
+  UseRegister(right_input());
+}
+void CheckInt32Condition::GenerateCode(MaglevAssembler* masm,
+                                       const ProcessingState& state) {
+  __ cmpq(ToRegister(left_input()), ToRegister(right_input()));
+  __ EmitEagerDeoptIf(NegateCondition(ToCondition(condition_)), reason_, this);
+}
+void CheckInt32Condition::PrintParams(
+    std::ostream& os, MaglevGraphLabeller* graph_labeller) const {
+  os << "(" << condition_ << ")";
+}
+
 void DebugBreak::AllocateVreg(MaglevVregAllocationState* vreg_state) {}
 void DebugBreak::GenerateCode(MaglevAssembler* masm,
                               const ProcessingState& state) {
@@ -1489,10 +1459,10 @@
         static_assert(kThinStringTagBit > 0);
         // Deopt if this isn't a string.
         __ testw(map_tmp, Immediate(kIsNotStringMask));
-        __ j(not_zero, &deopt_info->deopt_entry_label);
+        __ j(not_zero, deopt_info->deopt_entry_label());
         // Deopt if this isn't a thin string.
         __ testb(map_tmp, Immediate(kThinStringTagBit));
-        __ j(zero, &deopt_info->deopt_entry_label);
+        __ j(zero, deopt_info->deopt_entry_label());
         __ LoadTaggedPointerField(
             object, FieldOperand(object, ThinString::kActualOffset));
         if (v8_flags.debug_code) {
@@ -1584,6 +1554,59 @@
   __ bind(*done);
 }
 
+void BuiltinStringFromCharCode::AllocateVreg(
+    MaglevVregAllocationState* vreg_state) {
+  if (code_input().node()->Is<Int32Constant>()) {
+    UseAny(code_input());
+  } else {
+    UseAndClobberRegister(code_input());
+    set_temporaries_needed(1);
+  }
+  DefineAsRegister(vreg_state, this);
+}
+void BuiltinStringFromCharCode::GenerateCode(MaglevAssembler* masm,
+                                             const ProcessingState& state) {
+  Register result_string = ToRegister(result());
+  if (Int32Constant* constant = code_input().node()->TryCast<Int32Constant>()) {
+    int32_t char_code = constant->value();
+    if (0 <= char_code && char_code < String::kMaxOneByteCharCode) {
+      __ LoadSingleCharacterString(result_string, char_code);
+    } else {
+      __ AllocateTwoByteString(register_snapshot(), result_string, 1);
+      __ movw(FieldOperand(result_string, SeqTwoByteString::kHeaderSize),
+              Immediate(char_code & 0xFFFF));
+    }
+  } else {
+    Register char_code = ToRegister(code_input());
+    // We only need a scratch here if {char_code} alias with {result}.
+    // TODO(victorgomes): Add a constraint in the register allocator for this
+    // use case?
+    Register scratch = general_temporaries().PopFirst();
+    __ StringFromCharCode(register_snapshot(), nullptr, result_string,
+                          char_code, scratch);
+  }
+}
+
+void BuiltinStringPrototypeCharCodeAt::AllocateVreg(
+    MaglevVregAllocationState* vreg_state) {
+  UseAndClobberRegister(string_input());
+  UseAndClobberRegister(index_input());
+  DefineAsRegister(vreg_state, this);
+  set_temporaries_needed(1);
+}
+
+void BuiltinStringPrototypeCharCodeAt::GenerateCode(
+    MaglevAssembler* masm, const ProcessingState& state) {
+  Register string = ToRegister(string_input());
+  Register index = ToRegister(index_input());
+  Register scratch = general_temporaries().PopFirst();
+  ZoneLabelRef done(masm);
+  RegisterSnapshot save_registers = register_snapshot();
+  __ StringCharCodeAt(save_registers, ToRegister(result()), string, index,
+                      scratch, *done);
+  __ bind(*done);
+}
+
 void LoadTaggedField::AllocateVreg(MaglevVregAllocationState* vreg_state) {
   UseRegister(object_input());
   DefineAsRegister(vreg_state, this);
@@ -1677,6 +1700,26 @@
                                     FixedDoubleArray::kHeaderSize));
 }
 
+void StoreDoubleField::AllocateVreg(MaglevVregAllocationState* vreg_state) {
+  UseRegister(object_input());
+  UseRegister(value_input());
+  set_temporaries_needed(1);
+}
+void StoreDoubleField::GenerateCode(MaglevAssembler* masm,
+                                    const ProcessingState& state) {
+  Register tmp = general_temporaries().PopFirst();
+  Register object = ToRegister(object_input());
+  DoubleRegister value = ToDoubleRegister(value_input());
+
+  __ AssertNotSmi(object);
+  __ DecompressAnyTagged(tmp, FieldOperand(object, offset()));
+  __ AssertNotSmi(tmp);
+  __ Movsd(FieldOperand(tmp, HeapNumber::kValueOffset), value);
+}
+void StoreDoubleField::PrintParams(std::ostream& os,
+                                   MaglevGraphLabeller* graph_labeller) const {
+  os << "(" << std::hex << offset() << std::dec << ")";
+}
 void StoreTaggedFieldNoWriteBarrier::AllocateVreg(
     MaglevVregAllocationState* vreg_state) {
   UseRegister(object_input());
@@ -1695,6 +1738,64 @@
   os << "(" << std::hex << offset() << std::dec << ")";
 }
 
+void StoreMap::AllocateVreg(MaglevVregAllocationState* vreg_state) {
+  UseFixed(object_input(), WriteBarrierDescriptor::ObjectRegister());
+}
+void StoreMap::GenerateCode(MaglevAssembler* masm,
+                            const ProcessingState& state) {
+  // TODO(leszeks): Consider making this an arbitrary register and push/popping
+  // in the deferred path.
+  Register object = WriteBarrierDescriptor::ObjectRegister();
+  DCHECK_EQ(object, ToRegister(object_input()));
+
+  __ AssertNotSmi(object);
+  Register value = kScratchRegister;
+  __ Move(value, map_.object());
+  __ StoreTaggedField(FieldOperand(object, HeapObject::kMapOffset),
+                      kScratchRegister);
+
+  ZoneLabelRef done(masm);
+  DeferredCodeInfo* deferred_write_barrier = __ PushDeferredCode(
+      [](MaglevAssembler* masm, ZoneLabelRef done, Register value,
+         Register object, StoreMap* node) {
+        ASM_CODE_COMMENT_STRING(masm, "Write barrier slow path");
+        __ CheckPageFlag(
+            value, kScratchRegister,
+            MemoryChunk::kPointersToHereAreInterestingOrInSharedHeapMask, zero,
+            *done);
+
+        Register slot_reg = WriteBarrierDescriptor::SlotAddressRegister();
+        RegList saved;
+        if (node->register_snapshot().live_registers.has(slot_reg)) {
+          saved.set(slot_reg);
+        }
+
+        __ PushAll(saved);
+        __ leaq(slot_reg, FieldOperand(object, HeapObject::kMapOffset));
+
+        SaveFPRegsMode const save_fp_mode =
+            !node->register_snapshot().live_double_registers.is_empty()
+                ? SaveFPRegsMode::kSave
+                : SaveFPRegsMode::kIgnore;
+
+        __ CallRecordWriteStub(object, slot_reg, save_fp_mode);
+
+        __ PopAll(saved);
+        __ jmp(*done);
+      },
+      done, value, object, this);
+
+  __ JumpIfSmi(value, *done);
+  __ CheckPageFlag(object, kScratchRegister,
+                   MemoryChunk::kPointersFromHereAreInterestingMask, not_zero,
+                   &deferred_write_barrier->deferred_code_label);
+  __ bind(*done);
+}
+void StoreMap::PrintParams(std::ostream& os,
+                           MaglevGraphLabeller* graph_labeller) const {
+  os << "(" << *map_.object() << ")";
+}
+
 void StoreTaggedFieldWithWriteBarrier::AllocateVreg(
     MaglevVregAllocationState* vreg_state) {
   UseFixed(object_input(), WriteBarrierDescriptor::ObjectRegister());
@@ -1820,7 +1921,7 @@
   DCHECK_EQ(ToRegister(value_input()), D::GetRegisterParameter(D::kValue));
   __ Move(D::GetRegisterParameter(D::kName), name().object());
   __ Move(D::GetRegisterParameter(D::kSlot),
-          Smi::FromInt(feedback().slot.ToInt()));
+          TaggedIndex::FromIntptr(feedback().index()));
   __ Move(D::GetRegisterParameter(D::kVector), feedback().vector);
   __ CallBuiltin(Builtin::kStoreIC);
   masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
@@ -1838,17 +1939,44 @@
                                 const ProcessingState& state) {
   Register object = ToRegister(object_input());
   if (v8_flags.debug_code) {
-    // Use return register as temporary.
+    // Use return register as temporary. Push it in case it aliases the object
+    // register.
     Register tmp = ToRegister(result());
+    __ Push(tmp);
     // Check if {object} is a string.
     __ AssertNotSmi(object);
     __ LoadMap(tmp, object);
     __ CmpInstanceTypeRange(tmp, tmp, FIRST_STRING_TYPE, LAST_STRING_TYPE);
     __ Check(below_equal, AbortReason::kUnexpectedValue);
+    __ Pop(tmp);
   }
   __ movl(ToRegister(result()), FieldOperand(object, String::kLengthOffset));
 }
 
+void StringAt::AllocateVreg(MaglevVregAllocationState* vreg_state) {
+  UseAndClobberRegister(string_input());
+  UseAndClobberRegister(index_input());
+  DefineAsRegister(vreg_state, this);
+  set_temporaries_needed(1);
+}
+void StringAt::GenerateCode(MaglevAssembler* masm,
+                            const ProcessingState& state) {
+  Register result_string = ToRegister(result());
+  Register string = ToRegister(string_input());
+  Register index = ToRegister(index_input());
+  Register scratch = general_temporaries().PopFirst();
+  Register char_code = string;
+
+  ZoneLabelRef done(masm);
+  Label cached_one_byte_string;
+
+  RegisterSnapshot save_registers = register_snapshot();
+  __ StringCharCodeAt(save_registers, char_code, string, index, scratch,
+                      &cached_one_byte_string);
+  __ StringFromCharCode(save_registers, &cached_one_byte_string, result_string,
+                        char_code, scratch);
+}
+
 void DefineNamedOwnGeneric::AllocateVreg(
     MaglevVregAllocationState* vreg_state) {
   using D = CallInterfaceDescriptorFor<Builtin::kDefineNamedOwnIC>::type;
@@ -1865,7 +1993,7 @@
   DCHECK_EQ(ToRegister(value_input()), D::GetRegisterParameter(D::kValue));
   __ Move(D::GetRegisterParameter(D::kName), name().object());
   __ Move(D::GetRegisterParameter(D::kSlot),
-          Smi::FromInt(feedback().slot.ToInt()));
+          TaggedIndex::FromIntptr(feedback().index()));
   __ Move(D::GetRegisterParameter(D::kVector), feedback().vector);
   __ CallBuiltin(Builtin::kDefineNamedOwnIC);
   masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
@@ -1891,7 +2019,7 @@
   DCHECK_EQ(ToRegister(key_input()), D::GetRegisterParameter(D::kName));
   DCHECK_EQ(ToRegister(value_input()), D::GetRegisterParameter(D::kValue));
   __ Move(D::GetRegisterParameter(D::kSlot),
-          Smi::FromInt(feedback().slot.ToInt()));
+          TaggedIndex::FromIntptr(feedback().index()));
   __ Move(D::GetRegisterParameter(D::kVector), feedback().vector);
   __ CallBuiltin(Builtin::kKeyedStoreIC);
   masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
@@ -1914,7 +2042,7 @@
   DCHECK_EQ(ToRegister(key_input()), D::GetRegisterParameter(D::kName));
   DCHECK_EQ(ToRegister(value_input()), D::GetRegisterParameter(D::kValue));
   __ Move(D::GetRegisterParameter(D::kSlot),
-          Smi::FromInt(feedback().slot.ToInt()));
+          TaggedIndex::FromIntptr(feedback().index()));
   __ Move(D::GetRegisterParameter(D::kVector), feedback().vector);
   __ CallBuiltin(Builtin::kDefineKeyedOwnIC);
   masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
@@ -1937,7 +2065,7 @@
   DCHECK_EQ(ToRegister(value_input()), D::GetRegisterParameter(D::kValue));
   DCHECK_EQ(ToRegister(name_input()), D::GetRegisterParameter(D::kName));
   __ Move(D::GetRegisterParameter(D::kSlot),
-          Smi::FromInt(feedback().slot.ToInt()));
+          TaggedIndex::FromIntptr(feedback().index()));
   __ Move(D::GetRegisterParameter(D::kVector), feedback().vector);
   __ CallBuiltin(Builtin::kStoreInArrayLiteralIC);
   masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
@@ -2186,6 +2314,94 @@
   __ bind(&end);
 }
 
+void Int32ModulusWithOverflow::AllocateVreg(
+    MaglevVregAllocationState* vreg_state) {
+  UseRegister(left_input());
+  UseRegister(right_input());
+  DefineAsFixed(vreg_state, this, rdx);
+  // rax,rdx are clobbered by div.
+  RequireSpecificTemporary(rax);
+  RequireSpecificTemporary(rdx);
+}
+
+void Int32ModulusWithOverflow::GenerateCode(MaglevAssembler* masm,
+                                            const ProcessingState& state) {
+  // Using same algorithm as in EffectControlLinearizer:
+  //   if rhs <= 0 then
+  //     rhs = -rhs
+  //     deopt if rhs == 0
+  //   if lhs < 0 then
+  //     let lhs_abs = -lsh in
+  //     let res = lhs_abs % rhs in
+  //     deopt if res == 0
+  //     -res
+  //   else
+  //     let msk = rhs - 1 in
+  //     if rhs & msk == 0 then
+  //       lhs & msk
+  //     else
+  //       lhs % rhs
+
+  DCHECK(general_temporaries().has(rax));
+  DCHECK(general_temporaries().has(rdx));
+  Register left = ToRegister(left_input());
+  Register right = ToRegister(right_input());
+
+  ZoneLabelRef done(masm);
+  ZoneLabelRef rhs_checked(masm);
+
+  __ cmpl(right, Immediate(0));
+  __ JumpToDeferredIf(
+      less_equal,
+      [](MaglevAssembler* masm, ZoneLabelRef rhs_checked, Register right,
+         Int32ModulusWithOverflow* node) {
+        __ negl(right);
+        __ testl(right, right);
+        __ EmitEagerDeoptIf(equal, DeoptimizeReason::kDivisionByZero, node);
+        __ jmp(*rhs_checked);
+      },
+      rhs_checked, right, this);
+  __ bind(*rhs_checked);
+
+  __ cmpl(left, Immediate(0));
+  __ JumpToDeferredIf(
+      less,
+      [](MaglevAssembler* masm, ZoneLabelRef done, Register left,
+         Register right, Int32ModulusWithOverflow* node) {
+        __ negl(left);
+        __ movl(rax, left);
+        __ xorl(rdx, rdx);
+        __ divl(right);
+        __ testl(rdx, rdx);
+        // TODO(victorgomes): This ideally should be kMinusZero, but Maglev only
+        // allows one deopt reason per IR.
+        __ EmitEagerDeoptIf(equal, DeoptimizeReason::kDivisionByZero, node);
+        __ negl(rdx);
+        __ jmp(*done);
+      },
+      done, left, right, this);
+
+  Label right_not_power_of_2;
+  Register mask = rax;
+  __ leal(mask, Operand(right, -1));
+  __ testl(right, mask);
+  __ j(not_zero, &right_not_power_of_2, Label::kNear);
+
+  // {right} is power of 2.
+  __ andl(mask, left);
+  __ movl(ToRegister(result()), mask);
+  __ jmp(*done, Label::kNear);
+
+  __ bind(&right_not_power_of_2);
+  __ movl(rax, left);
+  __ xorl(rdx, rdx);
+  __ divl(right);
+  // Result is implicitly written to rdx.
+  DCHECK_EQ(ToRegister(result()), rdx);
+
+  __ bind(*done);
+}
+
 void Int32DivideWithOverflow::AllocateVreg(
     MaglevVregAllocationState* vreg_state) {
   UseRegister(left_input());
@@ -2539,6 +2755,18 @@
   __ SmiToInt32(value);
 }
 
+void UnsafeSmiUntag::AllocateVreg(MaglevVregAllocationState* vreg_state) {
+  UseRegister(input());
+  DefineSameAsFirst(vreg_state, this);
+}
+
+void UnsafeSmiUntag::GenerateCode(MaglevAssembler* masm,
+                                  const ProcessingState& state) {
+  Register value = ToRegister(input());
+  __ AssertSmi(value);
+  __ SmiToInt32(value);
+}
+
 void CheckedSmiTag::AllocateVreg(MaglevVregAllocationState* vreg_state) {
   UseRegister(input());
   DefineSameAsFirst(vreg_state, this);
@@ -2597,7 +2825,7 @@
   // call might trash it.
   RegisterSnapshot save_registers = register_snapshot();
   save_registers.live_double_registers.set(value);
-  AllocateRaw(masm, save_registers, object, HeapNumber::kSize);
+  __ Allocate(save_registers, object, HeapNumber::kSize);
   __ LoadRoot(kScratchRegister, RootIndex::kHeapNumberMap);
   __ StoreTaggedField(FieldOperand(object, HeapObject::kMapOffset),
                       kScratchRegister);
@@ -2691,6 +2919,28 @@
   }
 }
 
+void ToBoolean::AllocateVreg(MaglevVregAllocationState* vreg_state) {
+  UseRegister(value());
+  DefineAsRegister(vreg_state, this);
+}
+void ToBoolean::GenerateCode(MaglevAssembler* masm,
+                             const ProcessingState& state) {
+  Register object = ToRegister(value());
+  Register return_value = ToRegister(result());
+  Label done;
+  Zone* zone = masm->compilation_info()->zone();
+  ZoneLabelRef object_is_true(zone), object_is_false(zone);
+  // TODO(leszeks): We're likely to be calling this on an existing boolean --
+  // maybe that's a case we should fast-path here and re-use that boolean value?
+  __ ToBoolean(object, object_is_true, object_is_false, true);
+  __ bind(*object_is_true);
+  __ LoadRoot(return_value, RootIndex::kTrueValue);
+  __ jmp(&done, Label::kNear);
+  __ bind(*object_is_false);
+  __ LoadRoot(return_value, RootIndex::kFalseValue);
+  __ bind(&done);
+}
+
 void ToBooleanLogicalNot::AllocateVreg(MaglevVregAllocationState* vreg_state) {
   UseRegister(value());
   DefineAsRegister(vreg_state, this);
@@ -2702,7 +2952,7 @@
   Label done;
   Zone* zone = masm->compilation_info()->zone();
   ZoneLabelRef object_is_true(zone), object_is_false(zone);
-  ToBoolean(masm, object, object_is_true, object_is_false, true);
+  __ ToBoolean(object, object_is_true, object_is_false, true);
   __ bind(*object_is_true);
   __ LoadRoot(return_value, RootIndex::kFalseValue);
   __ jmp(&done, Label::kNear);
@@ -3044,20 +3294,35 @@
 }
 
 void Call::AllocateVreg(MaglevVregAllocationState* vreg_state) {
-  UseFixed(function(), CallTrampolineDescriptor::GetRegisterParameter(
-                           CallTrampolineDescriptor::kFunction));
-  UseFixed(context(), kContextRegister);
-  for (int i = 0; i < num_args(); i++) {
+  // TODO(leszeks): Consider splitting Call into with- and without-feedback
+  // opcodes, rather than checking for feedback validity.
+  if (feedback_.IsValid()) {
+    using D = CallTrampoline_WithFeedbackDescriptor;
+    UseFixed(function(), D::GetRegisterParameter(D::kFunction));
+    UseFixed(arg(0), D::GetRegisterParameter(D::kReceiver));
+  } else {
+    using D = CallTrampolineDescriptor;
+    UseFixed(function(), D::GetRegisterParameter(D::kFunction));
+    UseAny(arg(0));
+  }
+  for (int i = 1; i < num_args(); i++) {
     UseAny(arg(i));
   }
+  UseFixed(context(), kContextRegister);
   DefineAsFixed(vreg_state, this, kReturnRegister0);
 }
 void Call::GenerateCode(MaglevAssembler* masm, const ProcessingState& state) {
   // TODO(leszeks): Port the nice Sparkplug CallBuiltin helper.
-
-  DCHECK_EQ(ToRegister(function()),
-            CallTrampolineDescriptor::GetRegisterParameter(
-                CallTrampolineDescriptor::kFunction));
+#ifdef DEBUG
+  if (feedback_.IsValid()) {
+    using D = CallTrampoline_WithFeedbackDescriptor;
+    DCHECK_EQ(ToRegister(function()), D::GetRegisterParameter(D::kFunction));
+    DCHECK_EQ(ToRegister(arg(0)), D::GetRegisterParameter(D::kReceiver));
+  } else {
+    using D = CallTrampolineDescriptor;
+    DCHECK_EQ(ToRegister(function()), D::GetRegisterParameter(D::kFunction));
+  }
+#endif
   DCHECK_EQ(ToRegister(context()), kContextRegister);
 
   for (int i = num_args() - 1; i >= 0; --i) {
@@ -3065,26 +3330,120 @@
   }
 
   uint32_t arg_count = num_args();
-  __ Move(CallTrampolineDescriptor::GetRegisterParameter(
-              CallTrampolineDescriptor::kActualArgumentsCount),
-          Immediate(arg_count));
+  if (feedback_.IsValid()) {
+    DCHECK_EQ(TargetType::kAny, target_type_);
+    using D = CallTrampoline_WithFeedbackDescriptor;
+    __ Move(D::GetRegisterParameter(D::kActualArgumentsCount),
+            Immediate(arg_count));
+    __ Move(D::GetRegisterParameter(D::kFeedbackVector), feedback().vector);
+    __ Move(D::GetRegisterParameter(D::kSlot), Immediate(feedback().index()));
+
+    switch (receiver_mode_) {
+      case ConvertReceiverMode::kNullOrUndefined:
+        __ CallBuiltin(Builtin::kCall_ReceiverIsNullOrUndefined_WithFeedback);
+        break;
+      case ConvertReceiverMode::kNotNullOrUndefined:
+        __ CallBuiltin(
+            Builtin::kCall_ReceiverIsNotNullOrUndefined_WithFeedback);
+        break;
+      case ConvertReceiverMode::kAny:
+        __ CallBuiltin(Builtin::kCall_ReceiverIsAny_WithFeedback);
+        break;
+    }
+  } else if (target_type_ == TargetType::kAny) {
+    using D = CallTrampolineDescriptor;
+    __ Move(D::GetRegisterParameter(D::kActualArgumentsCount),
+            Immediate(arg_count));
+
+    switch (receiver_mode_) {
+      case ConvertReceiverMode::kNullOrUndefined:
+        __ CallBuiltin(Builtin::kCall_ReceiverIsNullOrUndefined);
+        break;
+      case ConvertReceiverMode::kNotNullOrUndefined:
+        __ CallBuiltin(Builtin::kCall_ReceiverIsNotNullOrUndefined);
+        break;
+      case ConvertReceiverMode::kAny:
+        __ CallBuiltin(Builtin::kCall_ReceiverIsAny);
+        break;
+    }
+  } else {
+    DCHECK_EQ(TargetType::kJSFunction, target_type_);
+    using D = CallTrampolineDescriptor;
+    __ Move(D::GetRegisterParameter(D::kActualArgumentsCount),
+            Immediate(arg_count));
+
+    switch (receiver_mode_) {
+      case ConvertReceiverMode::kNullOrUndefined:
+        __ CallBuiltin(Builtin::kCallFunction_ReceiverIsNullOrUndefined);
+        break;
+      case ConvertReceiverMode::kNotNullOrUndefined:
+        __ CallBuiltin(Builtin::kCallFunction_ReceiverIsNotNullOrUndefined);
+        break;
+      case ConvertReceiverMode::kAny:
+        __ CallBuiltin(Builtin::kCallFunction_ReceiverIsAny);
+        break;
+    }
+  }
 
-  // TODO(leszeks): This doesn't collect feedback yet, either pass in the
-  // feedback vector by Handle.
-  switch (receiver_mode_) {
-    case ConvertReceiverMode::kNullOrUndefined:
-      __ CallBuiltin(Builtin::kCall_ReceiverIsNullOrUndefined);
-      break;
-    case ConvertReceiverMode::kNotNullOrUndefined:
-      __ CallBuiltin(Builtin::kCall_ReceiverIsNotNullOrUndefined);
+  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
+}
+void Call::PrintParams(std::ostream& os,
+                       MaglevGraphLabeller* graph_labeller) const {
+  os << "(" << receiver_mode_ << ", ";
+  switch (target_type_) {
+    case TargetType::kJSFunction:
+      os << "JSFunction";
       break;
-    case ConvertReceiverMode::kAny:
-      __ CallBuiltin(Builtin::kCall_ReceiverIsAny);
+    case TargetType::kAny:
+      os << "Any";
       break;
   }
+  os << ")";
+}
 
+void CallKnownJSFunction::AllocateVreg(MaglevVregAllocationState* vreg_state) {
+  UseAny(receiver());
+  for (int i = 0; i < num_args(); i++) {
+    UseAny(arg(i));
+  }
+  DefineAsFixed(vreg_state, this, kReturnRegister0);
+}
+void CallKnownJSFunction::GenerateCode(MaglevAssembler* masm,
+                                       const ProcessingState& state) {
+  int expected_parameter_count =
+      shared_function_info().internal_formal_parameter_count_with_receiver();
+  int actual_parameter_count = num_args() + 1;
+  if (actual_parameter_count < expected_parameter_count) {
+    int number_of_undefineds =
+        expected_parameter_count - actual_parameter_count;
+    __ LoadRoot(kScratchRegister, RootIndex::kUndefinedValue);
+    for (int i = 0; i < number_of_undefineds; i++) {
+      __ Push(kScratchRegister);
+    }
+  }
+  for (int i = num_args() - 1; i >= 0; --i) {
+    __ PushInput(arg(i));
+  }
+  __ PushInput(receiver());
+  __ Move(kContextRegister, function_.context().object());
+  __ Move(kJavaScriptCallTargetRegister, function_.object());
+  __ LoadRoot(kJavaScriptCallNewTargetRegister, RootIndex::kUndefinedValue);
+  __ Move(kJavaScriptCallArgCountRegister, Immediate(actual_parameter_count));
+  if (shared_function_info().HasBuiltinId()) {
+    __ CallBuiltin(shared_function_info().builtin_id());
+  } else {
+    __ AssertCallableFunction(kJavaScriptCallTargetRegister);
+    __ LoadTaggedPointerField(
+        kJavaScriptCallCodeStartRegister,
+        FieldOperand(kJavaScriptCallTargetRegister, JSFunction::kCodeOffset));
+    __ CallCodeTObject(kJavaScriptCallCodeStartRegister);
+  }
   masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
 }
+void CallKnownJSFunction::PrintParams(
+    std::ostream& os, MaglevGraphLabeller* graph_labeller) const {
+  os << "(" << function_.object() << ")";
+}
 
 void Construct::AllocateVreg(MaglevVregAllocationState* vreg_state) {
   using D = Construct_WithFeedbackDescriptor;
@@ -3106,6 +3465,8 @@
   for (int i = num_args() - 1; i >= 0; --i) {
     __ PushInput(arg(i));
   }
+  static_assert(D::GetStackParameterIndex(D::kFeedbackVector) == 0);
+  static_assert(D::GetStackParameterCount() == 1);
   __ Push(feedback().vector);
 
   uint32_t arg_count = num_args();
@@ -3246,7 +3607,8 @@
 }
 
 void CallWithSpread::AllocateVreg(MaglevVregAllocationState* vreg_state) {
-  using D = CallInterfaceDescriptorFor<Builtin::kCallWithSpread>::type;
+  using D =
+      CallInterfaceDescriptorFor<Builtin::kCallWithSpread_WithFeedback>::type;
   UseFixed(function(), D::GetRegisterParameter(D::kTarget));
   UseFixed(context(), kContextRegister);
   for (int i = 0; i < num_args() - 1; i++) {
@@ -3257,22 +3619,31 @@
 }
 void CallWithSpread::GenerateCode(MaglevAssembler* masm,
                                   const ProcessingState& state) {
-  using D = CallInterfaceDescriptorFor<Builtin::kCallWithSpread>::type;
+  using D =
+      CallInterfaceDescriptorFor<Builtin::kCallWithSpread_WithFeedback>::type;
   DCHECK_EQ(ToRegister(function()), D::GetRegisterParameter(D::kTarget));
+  DCHECK_EQ(ToRegister(spread()), D::GetRegisterParameter(D::kSpread));
   DCHECK_EQ(ToRegister(context()), kContextRegister);
   // Push other arguments (other than the spread) to the stack.
   int argc_no_spread = num_args() - 1;
   for (int i = argc_no_spread - 1; i >= 0; --i) {
     __ PushInput(arg(i));
   }
+  static_assert(D::GetStackParameterIndex(D::kReceiver) == 0);
+  static_assert(D::GetStackParameterCount() == 1);
+  __ PushInput(arg(0));
+
   __ Move(D::GetRegisterParameter(D::kArgumentsCount),
           Immediate(argc_no_spread));
-  __ CallBuiltin(Builtin::kCallWithSpread);
+  __ Move(D::GetRegisterParameter(D::kFeedbackVector), feedback().vector);
+  __ Move(D::GetRegisterParameter(D::kSlot), Immediate(feedback().index()));
+  __ CallBuiltin(Builtin::kCallWithSpread_WithFeedback);
   masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
 }
 
 void ConstructWithSpread::AllocateVreg(MaglevVregAllocationState* vreg_state) {
-  using D = CallInterfaceDescriptorFor<Builtin::kConstructWithSpread>::type;
+  using D = CallInterfaceDescriptorFor<
+      Builtin::kConstructWithSpread_WithFeedback>::type;
   UseFixed(function(), D::GetRegisterParameter(D::kTarget));
   UseFixed(new_target(), D::GetRegisterParameter(D::kNewTarget));
   UseFixed(context(), kContextRegister);
@@ -3284,7 +3655,8 @@
 }
 void ConstructWithSpread::GenerateCode(MaglevAssembler* masm,
                                        const ProcessingState& state) {
-  using D = CallInterfaceDescriptorFor<Builtin::kConstructWithSpread>::type;
+  using D = CallInterfaceDescriptorFor<
+      Builtin::kConstructWithSpread_WithFeedback>::type;
   DCHECK_EQ(ToRegister(function()), D::GetRegisterParameter(D::kTarget));
   DCHECK_EQ(ToRegister(new_target()), D::GetRegisterParameter(D::kNewTarget));
   DCHECK_EQ(ToRegister(context()), kContextRegister);
@@ -3293,12 +3665,56 @@
   for (int i = argc_no_spread - 1; i >= 0; --i) {
     __ PushInput(arg(i));
   }
+  static_assert(D::GetStackParameterIndex(D::kFeedbackVector) == 0);
+  static_assert(D::GetStackParameterCount() == 1);
+  __ Push(feedback().vector);
+
   __ Move(D::GetRegisterParameter(D::kActualArgumentsCount),
           Immediate(argc_no_spread));
-  __ CallBuiltin(Builtin::kConstructWithSpread);
+  __ Move(D::GetRegisterParameter(D::kSlot), Immediate(feedback().index()));
+  __ CallBuiltin(Builtin::kConstructWithSpread_WithFeedback);
   masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
 }
 
+void ConvertReceiver::AllocateVreg(MaglevVregAllocationState* vreg_state) {
+  using D = CallInterfaceDescriptorFor<Builtin::kToObject>::type;
+  UseFixed(receiver_input(), D::GetRegisterParameter(D::kInput));
+  set_temporaries_needed(1);
+  DefineAsFixed(vreg_state, this, kReturnRegister0);
+}
+void ConvertReceiver::GenerateCode(MaglevAssembler* masm,
+                                   const ProcessingState& state) {
+  Label convert_to_object, done;
+  Register receiver = ToRegister(receiver_input());
+  Register scratch = general_temporaries().first();
+  __ JumpIfSmi(receiver, &convert_to_object, Label::kNear);
+  static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+  __ CmpObjectType(receiver, FIRST_JS_RECEIVER_TYPE, scratch);
+  __ j(above_equal, &done);
+
+  if (mode_ != ConvertReceiverMode::kNotNullOrUndefined) {
+    Label convert_global_proxy;
+    __ JumpIfRoot(receiver, RootIndex::kUndefinedValue, &convert_global_proxy,
+                  Label::kNear);
+    __ JumpIfNotRoot(receiver, RootIndex::kNullValue, &convert_to_object,
+                     Label::kNear);
+    __ bind(&convert_global_proxy);
+    {
+      // Patch receiver to global proxy.
+      __ Move(ToRegister(result()),
+              target_.native_context().global_proxy_object().object());
+    }
+    __ jmp(&done);
+  }
+
+  __ bind(&convert_to_object);
+  // ToObject needs to be ran with the target context installed.
+  __ Move(kContextRegister, target_.context().object());
+  __ CallBuiltin(Builtin::kToObject);
+  masm->DefineExceptionHandlerAndLazyDeoptPoint(this);
+  __ bind(&done);
+}
+
 void IncreaseInterruptBudget::AllocateVreg(
     MaglevVregAllocationState* vreg_state) {
   set_temporaries_needed(1);
@@ -3591,22 +4007,7 @@
       // TODO(v8:7700): Consider making the snapshot location
       // configurable.
       RegisterSnapshot snapshot = node->register_snapshot();
-      detail::DeepForEachInput(
-          node->eager_deopt_info(),
-          [&](ValueNode* node, interpreter::Register reg,
-              InputLocation* input) {
-            if (!input->IsAnyRegister()) return;
-            if (input->IsDoubleRegister()) {
-              snapshot.live_double_registers.set(
-                  input->AssignedDoubleRegister());
-            } else {
-              snapshot.live_registers.set(input->AssignedGeneralRegister());
-              if (node->is_tagged()) {
-                snapshot.live_tagged_registers.set(
-                    input->AssignedGeneralRegister());
-              }
-            }
-          });
+      AddDeoptRegistersToSnapshot(&snapshot, node->eager_deopt_info());
       DCHECK(!snapshot.live_registers.has(maybe_target_code));
       SaveRegisterStateForCall save_register_state(masm, snapshot);
       __ Move(kContextRegister, masm->native_context().object());
@@ -3778,8 +4179,8 @@
   ZoneLabelRef false_label =
       ZoneLabelRef::UnsafeFromLabelPointer(if_false()->label());
   bool fallthrough_when_true = (if_true() == state.next_block());
-  ToBoolean(masm, ToRegister(condition_input()), true_label, false_label,
-            fallthrough_when_true);
+  __ ToBoolean(ToRegister(condition_input()), true_label, false_label,
+               fallthrough_when_true);
 }
 
 }  // namespace maglev
diff -r -u --color up/v8/src/maglev/maglev-ir.h nw/v8/src/maglev/maglev-ir.h
--- up/v8/src/maglev/maglev-ir.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/maglev/maglev-ir.h	2023-01-19 16:46:36.298109548 -0500
@@ -12,6 +12,7 @@
 #include "src/base/threaded-list.h"
 #include "src/codegen/label.h"
 #include "src/codegen/reglist.h"
+#include "src/codegen/source-position.h"
 #include "src/common/globals.h"
 #include "src/common/operation.h"
 #include "src/compiler/backend/instruction.h"
@@ -24,6 +25,7 @@
 #include "src/objects/smi.h"
 #include "src/roots/roots.h"
 #include "src/utils/utils.h"
+#include "src/zone/zone-handle-set.h"
 #include "src/zone/zone.h"
 
 namespace v8 {
@@ -75,7 +77,7 @@
   V(Int32SubtractWithOverflow)         \
   V(Int32MultiplyWithOverflow)         \
   V(Int32DivideWithOverflow)           \
-  /*V(Int32ModulusWithOverflow)*/      \
+  V(Int32ModulusWithOverflow)          \
   /*V(Int32ExponentiateWithOverflow)*/ \
   V(Int32BitwiseAnd)                   \
   V(Int32BitwiseOr)                    \
@@ -118,13 +120,19 @@
   V(RootConstant)                   \
   V(SmiConstant)
 
+#define INLINE_BUILTIN_NODE_LIST(V) \
+  V(BuiltinStringFromCharCode)      \
+  V(BuiltinStringPrototypeCharCodeAt)
+
 #define VALUE_NODE_LIST(V)         \
   V(Call)                          \
   V(CallBuiltin)                   \
   V(CallRuntime)                   \
   V(CallWithSpread)                \
+  V(CallKnownJSFunction)           \
   V(Construct)                     \
   V(ConstructWithSpread)           \
+  V(ConvertReceiver)               \
   V(CreateEmptyArrayLiteral)       \
   V(CreateArrayLiteral)            \
   V(CreateShallowArrayLiteral)     \
@@ -162,6 +170,7 @@
   V(CheckedSmiTag)                 \
   V(UnsafeSmiTag)                  \
   V(CheckedSmiUntag)               \
+  V(UnsafeSmiUntag)                \
   V(CheckedInternalizedString)     \
   V(CheckedObjectToIndex)          \
   V(ChangeInt32ToFloat64)          \
@@ -170,7 +179,9 @@
   V(CheckedFloat64Unbox)           \
   V(LogicalNot)                    \
   V(SetPendingMessage)             \
+  V(StringAt)                      \
   V(StringLength)                  \
+  V(ToBoolean)                     \
   V(ToBooleanLogicalNot)           \
   V(TaggedEqual)                   \
   V(TaggedNotEqual)                \
@@ -184,7 +195,8 @@
   CONSTANT_VALUE_NODE_LIST(V)      \
   INT32_OPERATIONS_NODE_LIST(V)    \
   FLOAT64_OPERATIONS_NODE_LIST(V)  \
-  GENERIC_OPERATIONS_NODE_LIST(V)
+  GENERIC_OPERATIONS_NODE_LIST(V)  \
+  INLINE_BUILTIN_NODE_LIST(V)
 
 #define GAP_MOVE_NODE_LIST(V) \
   V(ConstantGapMove)          \
@@ -192,18 +204,22 @@
 
 #define NODE_LIST(V)                  \
   V(AssertInt32)                      \
-  V(CheckMaps)                        \
-  V(CheckSmi)                         \
-  V(CheckNumber)                      \
   V(CheckHeapObject)                  \
-  V(CheckSymbol)                      \
-  V(CheckString)                      \
-  V(CheckMapsWithMigration)           \
+  V(CheckInt32Condition)              \
   V(CheckJSArrayBounds)               \
   V(CheckJSObjectElementsBounds)      \
+  V(CheckMaps)                        \
+  V(CheckMapsWithMigration)           \
+  V(CheckNumber)                      \
+  V(CheckSmi)                         \
+  V(CheckString)                      \
+  V(CheckSymbol)                      \
+  V(CheckValue)                       \
   V(DebugBreak)                       \
   V(GeneratorStore)                   \
   V(JumpLoopPrologue)                 \
+  V(StoreMap)                         \
+  V(StoreDoubleField)                 \
   V(StoreTaggedFieldNoWriteBarrier)   \
   V(StoreTaggedFieldWithWriteBarrier) \
   V(IncreaseInterruptBudget)          \
@@ -351,16 +367,6 @@
 NODE_BASE_LIST(DEF_FORWARD_DECLARATION)
 #undef DEF_FORWARD_DECLARATION
 
-#define DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()          \
-  void AllocateVreg(MaglevVregAllocationState*);               \
-  void GenerateCode(MaglevAssembler*, const ProcessingState&); \
-  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
-
-#define DECL_NODE_INTERFACE()                                  \
-  void AllocateVreg(MaglevVregAllocationState*);               \
-  void GenerateCode(MaglevAssembler*, const ProcessingState&); \
-  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
-
 using NodeIdT = uint32_t;
 static constexpr uint32_t kInvalidNodeId = 0;
 static constexpr uint32_t kFirstValidNodeId = 1;
@@ -616,6 +622,12 @@
     operand_ = location;
   }
 
+  // We use USED_AT_START to indicate that the input will be clobbered.
+  bool Cloberred() {
+    DCHECK(operand_.IsUnallocated());
+    return compiler::UnallocatedOperand::cast(operand_).IsUsedAtStart();
+  }
+
   template <typename... Args>
   void SetConstant(Args&&... args) {
     DCHECK(operand_.IsUnallocated());
@@ -662,32 +674,131 @@
   ValueNode* node_;
 };
 
-class CheckpointedInterpreterState {
+class InterpretedDeoptFrame;
+class BuiltinContinuationDeoptFrame;
+class DeoptFrame {
+ public:
+  enum class FrameType {
+    kInterpretedFrame,
+    kBuiltinContinuationFrame,
+  };
+
+  FrameType type() const { return type_; }
+  const DeoptFrame* parent() const { return parent_; }
+
+  inline const InterpretedDeoptFrame& as_interpreted() const;
+  inline const BuiltinContinuationDeoptFrame& as_builtin_continuation() const;
+
+ protected:
+  struct InterpretedFrameData {
+    const MaglevCompilationUnit& unit;
+    const CompactInterpreterFrameState* frame_state;
+    BytecodeOffset bytecode_position;
+    SourcePosition source_position;
+  };
+  struct BuiltinContinuationFrameData {
+    Builtin builtin_id;
+    base::Vector<ValueNode*> parameters;
+    ValueNode* context;
+  };
+
+  DeoptFrame(InterpretedFrameData data, const DeoptFrame* parent)
+      : interpreted_frame_data_(data),
+        type_(FrameType::kInterpretedFrame),
+        parent_(parent) {}
+  DeoptFrame(BuiltinContinuationFrameData data, const DeoptFrame* parent)
+      : builtin_continuation_frame_data_(data),
+        type_(FrameType::kBuiltinContinuationFrame),
+        parent_(parent) {}
+
+  union {
+    const InterpretedFrameData interpreted_frame_data_;
+    const BuiltinContinuationFrameData builtin_continuation_frame_data_;
+  };
+  FrameType type_;
+  const DeoptFrame* parent_;
+};
+
+class InterpretedDeoptFrame : public DeoptFrame {
  public:
-  CheckpointedInterpreterState() = default;
-  CheckpointedInterpreterState(BytecodeOffset bytecode_position,
-                               const CompactInterpreterFrameState* state,
-                               const CheckpointedInterpreterState* parent)
-      : bytecode_position(bytecode_position),
-        register_frame(state),
-        parent(parent) {}
-
-  BytecodeOffset bytecode_position = BytecodeOffset::None();
-  const CompactInterpreterFrameState* register_frame = nullptr;
-  const CheckpointedInterpreterState* parent = nullptr;
+  InterpretedDeoptFrame(const MaglevCompilationUnit& unit,
+                        const CompactInterpreterFrameState* frame_state,
+                        BytecodeOffset bytecode_position,
+                        SourcePosition source_position,
+                        const DeoptFrame* parent)
+      : DeoptFrame(InterpretedFrameData{unit, frame_state, bytecode_position,
+                                        source_position},
+                   parent) {}
+
+  const MaglevCompilationUnit& unit() const {
+    return interpreted_frame_data_.unit;
+  }
+  const CompactInterpreterFrameState* frame_state() const {
+    return interpreted_frame_data_.frame_state;
+  }
+  BytecodeOffset bytecode_position() const {
+    return interpreted_frame_data_.bytecode_position;
+  }
+  SourcePosition source_position() const {
+    return interpreted_frame_data_.source_position;
+  }
 };
 
+// Make sure storing/passing deopt frames by value doesn't truncate them.
+static_assert(sizeof(InterpretedDeoptFrame) == sizeof(DeoptFrame));
+
+inline const InterpretedDeoptFrame& DeoptFrame::as_interpreted() const {
+  DCHECK_EQ(type(), FrameType::kInterpretedFrame);
+  return static_cast<const InterpretedDeoptFrame&>(*this);
+}
+
+class BuiltinContinuationDeoptFrame : public DeoptFrame {
+ public:
+  BuiltinContinuationDeoptFrame(Builtin builtin_id,
+                                base::Vector<ValueNode*> parameters,
+                                ValueNode* context, const DeoptFrame* parent)
+      : DeoptFrame(
+            BuiltinContinuationFrameData{builtin_id, parameters, context},
+            parent) {}
+
+  const Builtin& builtin_id() const {
+    return builtin_continuation_frame_data_.builtin_id;
+  }
+  base::Vector<ValueNode*> parameters() const {
+    return builtin_continuation_frame_data_.parameters;
+  }
+  ValueNode* context() const {
+    return builtin_continuation_frame_data_.context;
+  }
+};
+
+// Make sure storing/passing deopt frames by value doesn't truncate them.
+static_assert(sizeof(BuiltinContinuationDeoptFrame) == sizeof(DeoptFrame));
+
+inline const BuiltinContinuationDeoptFrame&
+DeoptFrame::as_builtin_continuation() const {
+  DCHECK_EQ(type(), FrameType::kBuiltinContinuationFrame);
+  return static_cast<const BuiltinContinuationDeoptFrame&>(*this);
+}
+
 class DeoptInfo {
  protected:
-  DeoptInfo(Zone* zone, const MaglevCompilationUnit& compilation_unit,
-            CheckpointedInterpreterState checkpoint);
+  DeoptInfo(Zone* zone, DeoptFrame top_frame);
 
  public:
-  const MaglevCompilationUnit& unit;
-  CheckpointedInterpreterState state;
-  InputLocation* input_locations = nullptr;
-  Label deopt_entry_label;
-  int translation_index = -1;
+  const DeoptFrame& top_frame() const { return top_frame_; }
+
+  InputLocation* input_locations() const { return input_locations_; }
+  Label* deopt_entry_label() { return &deopt_entry_label_; }
+
+  int translation_index() const { return translation_index_; }
+  void set_translation_index(int index) { translation_index_ = index; }
+
+ private:
+  const DeoptFrame top_frame_;
+  InputLocation* const input_locations_;
+  Label deopt_entry_label_;
+  int translation_index_ = -1;
 };
 
 struct RegisterSnapshot {
@@ -698,24 +809,41 @@
 
 class EagerDeoptInfo : public DeoptInfo {
  public:
-  EagerDeoptInfo(Zone* zone, const MaglevCompilationUnit& compilation_unit,
-                 CheckpointedInterpreterState checkpoint)
-      : DeoptInfo(zone, compilation_unit, checkpoint) {}
-  DeoptimizeReason reason = DeoptimizeReason::kUnknown;
+  EagerDeoptInfo(Zone* zone, DeoptFrame&& top_frame)
+      : DeoptInfo(zone, std::move(top_frame)) {}
+
+  DeoptimizeReason reason() const { return reason_; }
+  void set_reason(DeoptimizeReason reason) { reason_ = reason; }
+
+ private:
+  DeoptimizeReason reason_ = DeoptimizeReason::kUnknown;
 };
 
 class LazyDeoptInfo : public DeoptInfo {
  public:
-  LazyDeoptInfo(Zone* zone, const MaglevCompilationUnit& compilation_unit,
-                CheckpointedInterpreterState checkpoint)
-      : DeoptInfo(zone, compilation_unit, checkpoint) {}
+  LazyDeoptInfo(Zone* zone, DeoptFrame&& top_frame)
+      : DeoptInfo(zone, std::move(top_frame)) {}
+
+  interpreter::Register result_location() const { return result_location_; }
+  int result_size() const { return result_size_; }
 
   bool IsResultRegister(interpreter::Register reg) const;
+  void SetResultLocation(interpreter::Register result_location,
+                         int result_size) {
+    DCHECK(result_location.is_valid());
+    DCHECK(!result_location_.is_valid());
+    result_location_ = result_location;
+    result_size_ = result_size;
+  }
 
-  int deopting_call_return_pc = -1;
-  interpreter::Register result_location =
+  int deopting_call_return_pc() const { return deopting_call_return_pc_; }
+  void set_deopting_call_return_pc(int pc) { deopting_call_return_pc_ = pc; }
+
+ private:
+  int deopting_call_return_pc_ = -1;
+  interpreter::Register result_location_ =
       interpreter::Register::invalid_value();
-  int result_size = 1;
+  int result_size_ = 1;
 };
 
 class ExceptionHandlerInfo {
@@ -813,16 +941,14 @@
   }
 
   template <class Derived, typename... Args>
-  static Derived* New(Zone* zone, const MaglevCompilationUnit& compilation_unit,
-                      CheckpointedInterpreterState checkpoint, Args&&... args) {
+  static Derived* New(Zone* zone, DeoptFrame&& deopt_frame, Args&&... args) {
     Derived* node = New<Derived>(zone, std::forward<Args>(args)...);
     if constexpr (Derived::kProperties.can_eager_deopt()) {
       new (node->eager_deopt_info())
-          EagerDeoptInfo(zone, compilation_unit, checkpoint);
+          EagerDeoptInfo(zone, std::move(deopt_frame));
     } else {
       static_assert(Derived::kProperties.can_lazy_deopt());
-      new (node->lazy_deopt_info())
-          LazyDeoptInfo(zone, compilation_unit, checkpoint);
+      new (node->lazy_deopt_info()) LazyDeoptInfo(zone, std::move(deopt_frame));
     }
     return node;
   }
@@ -1442,7 +1568,9 @@
                                  const compiler::FeedbackSource& feedback)
       : Base(bitfield), feedback_(feedback) {}
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
   const compiler::FeedbackSource feedback_;
 };
@@ -1466,7 +1594,9 @@
                          const compiler::FeedbackSource& feedback)
       : Base(bitfield), feedback_(feedback) {}
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
   const compiler::FeedbackSource feedback_;
 };
@@ -1478,7 +1608,9 @@
    public:                                                            \
     Name(uint64_t bitfield, const compiler::FeedbackSource& feedback) \
         : Base(bitfield, feedback) {}                                 \
-    DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()                     \
+    void AllocateVreg(MaglevVregAllocationState*);                    \
+    void GenerateCode(MaglevAssembler*, const ProcessingState&);      \
+    void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}    \
   };
 
 #define DEF_UNARY_WITH_FEEDBACK_NODE(Name) \
@@ -1512,13 +1644,15 @@
   void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
-#define DEF_OPERATION_NODE(Name, Super, OpName)           \
-  class Name : public Super<Name, Operation::k##OpName> { \
-    using Base = Super<Name, Operation::k##OpName>;       \
-                                                          \
-   public:                                                \
-    explicit Name(uint64_t bitfield) : Base(bitfield) {}  \
-    DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()         \
+#define DEF_OPERATION_NODE(Name, Super, OpName)                    \
+  class Name : public Super<Name, Operation::k##OpName> {          \
+    using Base = Super<Name, Operation::k##OpName>;                \
+                                                                   \
+   public:                                                         \
+    explicit Name(uint64_t bitfield) : Base(bitfield) {}           \
+    void AllocateVreg(MaglevVregAllocationState*);                 \
+    void GenerateCode(MaglevAssembler*, const ProcessingState&);   \
+    void PrintParams(std::ostream&, MaglevGraphLabeller*) const {} \
   };
 
 #define DEF_INT32_BINARY_WITH_OVERFLOW_NODE(Name)                            \
@@ -1528,7 +1662,7 @@
 DEF_INT32_BINARY_WITH_OVERFLOW_NODE(Subtract)
 DEF_INT32_BINARY_WITH_OVERFLOW_NODE(Multiply)
 DEF_INT32_BINARY_WITH_OVERFLOW_NODE(Divide)
-// DEF_INT32_BINARY_WITH_OVERFLOW_NODE(Modulus)
+DEF_INT32_BINARY_WITH_OVERFLOW_NODE(Modulus)
 // DEF_INT32_BINARY_WITH_OVERFLOW_NODE(Exponentiate)
 #undef DEF_INT32_BINARY_WITH_OVERFLOW_NODE
 
@@ -1551,13 +1685,15 @@
   void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
-#define DEF_OPERATION_NODE(Name, Super, OpName)           \
-  class Name : public Super<Name, Operation::k##OpName> { \
-    using Base = Super<Name, Operation::k##OpName>;       \
-                                                          \
-   public:                                                \
-    explicit Name(uint64_t bitfield) : Base(bitfield) {}  \
-    DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()         \
+#define DEF_OPERATION_NODE(Name, Super, OpName)                    \
+  class Name : public Super<Name, Operation::k##OpName> {          \
+    using Base = Super<Name, Operation::k##OpName>;                \
+                                                                   \
+   public:                                                         \
+    explicit Name(uint64_t bitfield) : Base(bitfield) {}           \
+    void AllocateVreg(MaglevVregAllocationState*);                 \
+    void GenerateCode(MaglevAssembler*, const ProcessingState&);   \
+    void PrintParams(std::ostream&, MaglevGraphLabeller*) const {} \
   };
 
 #define DEF_INT32_BINARY_NODE(Name) \
@@ -1586,16 +1722,20 @@
  protected:
   explicit Int32CompareNode(uint64_t bitfield) : Base(bitfield) {}
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
-#define DEF_OPERATION_NODE(Name, Super, OpName)           \
-  class Name : public Super<Name, Operation::k##OpName> { \
-    using Base = Super<Name, Operation::k##OpName>;       \
-                                                          \
-   public:                                                \
-    explicit Name(uint64_t bitfield) : Base(bitfield) {}  \
-    DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()         \
+#define DEF_OPERATION_NODE(Name, Super, OpName)                    \
+  class Name : public Super<Name, Operation::k##OpName> {          \
+    using Base = Super<Name, Operation::k##OpName>;                \
+                                                                   \
+   public:                                                         \
+    explicit Name(uint64_t bitfield) : Base(bitfield) {}           \
+    void AllocateVreg(MaglevVregAllocationState*);                 \
+    void GenerateCode(MaglevAssembler*, const ProcessingState&);   \
+    void PrintParams(std::ostream&, MaglevGraphLabeller*) const {} \
   };
 
 #define DEF_INT32_COMPARE_NODE(Name) \
@@ -1628,13 +1768,15 @@
   void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
-#define DEF_OPERATION_NODE(Name, Super, OpName)           \
-  class Name : public Super<Name, Operation::k##OpName> { \
-    using Base = Super<Name, Operation::k##OpName>;       \
-                                                          \
-   public:                                                \
-    explicit Name(uint64_t bitfield) : Base(bitfield) {}  \
-    DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()         \
+#define DEF_OPERATION_NODE(Name, Super, OpName)                    \
+  class Name : public Super<Name, Operation::k##OpName> {          \
+    using Base = Super<Name, Operation::k##OpName>;                \
+                                                                   \
+   public:                                                         \
+    explicit Name(uint64_t bitfield) : Base(bitfield) {}           \
+    void AllocateVreg(MaglevVregAllocationState*);                 \
+    void GenerateCode(MaglevAssembler*, const ProcessingState&);   \
+    void PrintParams(std::ostream&, MaglevGraphLabeller*) const {} \
   };
 
 #define DEF_FLOAT64_BINARY_NODE(Name) \
@@ -1666,16 +1808,20 @@
  protected:
   explicit Float64CompareNode(uint64_t bitfield) : Base(bitfield) {}
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
-#define DEF_OPERATION_NODE(Name, Super, OpName)           \
-  class Name : public Super<Name, Operation::k##OpName> { \
-    using Base = Super<Name, Operation::k##OpName>;       \
-                                                          \
-   public:                                                \
-    explicit Name(uint64_t bitfield) : Base(bitfield) {}  \
-    DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()         \
+#define DEF_OPERATION_NODE(Name, Super, OpName)                    \
+  class Name : public Super<Name, Operation::k##OpName> {          \
+    using Base = Super<Name, Operation::k##OpName>;                \
+                                                                   \
+   public:                                                         \
+    explicit Name(uint64_t bitfield) : Base(bitfield) {}           \
+    void AllocateVreg(MaglevVregAllocationState*);                 \
+    void GenerateCode(MaglevAssembler*, const ProcessingState&);   \
+    void PrintParams(std::ostream&, MaglevGraphLabeller*) const {} \
   };
 
 #define DEF_FLOAT64_COMPARE_NODE(Name) \
@@ -1701,7 +1847,9 @@
 
   Input& input() { return Node::input(0); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 // Input must guarantee to fit in a Smi.
@@ -1715,7 +1863,9 @@
 
   Input& input() { return Node::input(0); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class CheckedSmiUntag : public FixedInputValueNodeT<1, CheckedSmiUntag> {
@@ -1730,7 +1880,25 @@
 
   Input& input() { return Node::input(0); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
+};
+
+class UnsafeSmiUntag : public FixedInputValueNodeT<1, UnsafeSmiUntag> {
+  using Base = FixedInputValueNodeT<1, UnsafeSmiUntag>;
+
+ public:
+  explicit UnsafeSmiUntag(uint64_t bitfield) : Base(bitfield) {}
+
+  static constexpr OpProperties kProperties =
+      OpProperties::Int32() | OpProperties::ConversionNode();
+
+  Input& input() { return Node::input(0); }
+
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class Int32Constant : public FixedInputValueNodeT<0, Int32Constant> {
@@ -1748,7 +1916,9 @@
 
   bool ToBoolean(LocalIsolate* local_isolate) const { return value_ != 0; }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
   void DoLoadToRegister(MaglevAssembler*, OutputRegister);
   Handle<Object> DoReify(LocalIsolate* isolate);
@@ -1774,7 +1944,9 @@
     return value_ != 0.0 && !std::isnan(value_);
   }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
   void DoLoadToRegister(MaglevAssembler*, OutputRegister);
   Handle<Object> DoReify(LocalIsolate* isolate);
@@ -1794,7 +1966,9 @@
 
   Input& input() { return Node::input(0); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class ChangeInt32ToFloat64
@@ -1809,7 +1983,9 @@
 
   Input& input() { return Node::input(0); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class CheckedTruncateFloat64ToInt32
@@ -1825,7 +2001,9 @@
 
   Input& input() { return Node::input(0); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class CheckedFloat64Unbox
@@ -1841,7 +2019,9 @@
 
   Input& input() { return Node::input(0); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class LogicalNot : public FixedInputValueNodeT<1, LogicalNot> {
@@ -1852,7 +2032,9 @@
 
   Input& value() { return Node::input(0); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class SetPendingMessage : public FixedInputValueNodeT<1, SetPendingMessage> {
@@ -1863,7 +2045,22 @@
 
   Input& value() { return Node::input(0); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
+};
+
+class ToBoolean : public FixedInputValueNodeT<1, ToBoolean> {
+  using Base = FixedInputValueNodeT<1, ToBoolean>;
+
+ public:
+  explicit ToBoolean(uint64_t bitfield) : Base(bitfield) {}
+
+  Input& value() { return Node::input(0); }
+
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class ToBooleanLogicalNot
@@ -1875,7 +2072,9 @@
 
   Input& value() { return Node::input(0); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class TaggedEqual : public FixedInputValueNodeT<2, TaggedEqual> {
@@ -1887,7 +2086,9 @@
   Input& lhs() { return Node::input(0); }
   Input& rhs() { return Node::input(1); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class TaggedNotEqual : public FixedInputValueNodeT<2, TaggedNotEqual> {
@@ -1899,7 +2100,9 @@
   Input& lhs() { return Node::input(0); }
   Input& rhs() { return Node::input(1); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class TestInstanceOf : public FixedInputValueNodeT<3, TestInstanceOf> {
@@ -1917,7 +2120,9 @@
   Input& callable() { return input(2); }
   compiler::FeedbackSource feedback() const { return feedback_; }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   const compiler::FeedbackSource feedback_;
@@ -1931,7 +2136,9 @@
 
   Input& value() { return Node::input(0); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class TestTypeOf : public FixedInputValueNodeT<1, TestTypeOf> {
@@ -1944,7 +2151,9 @@
 
   Input& value() { return Node::input(0); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   interpreter::TestTypeOfFlags::LiteralFlag literal_;
@@ -1962,7 +2171,9 @@
   Input& context() { return Node::input(0); }
   Input& value_input() { return Node::input(1); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class ToNumberOrNumeric : public FixedInputValueNodeT<2, ToNumberOrNumeric> {
@@ -1979,7 +2190,9 @@
   Input& value_input() { return Node::input(1); }
   Object::Conversion mode() const { return mode_; }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   const Object::Conversion mode_;
@@ -2001,7 +2214,9 @@
 
   LanguageMode mode() const { return mode_; }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   const LanguageMode mode_;
@@ -2043,7 +2258,9 @@
     set_input(i + kFixedInputCount, node);
   }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   const int suspend_id_;
@@ -2067,7 +2284,9 @@
   static constexpr OpProperties kProperties =
       OpProperties::NeedsRegisterSnapshot() | OpProperties::EagerDeopt();
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   // For OSR.
@@ -2094,7 +2313,9 @@
 
   int ReturnCount() const { return 2; }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   const compiler::FeedbackSource feedback_;
@@ -2117,7 +2338,9 @@
   Input& cache_type() { return Node::input(3); }
   Input& cache_index() { return Node::input(4); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   const compiler::FeedbackSource feedback_;
@@ -2143,7 +2366,9 @@
   int call_slot() const { return call_slot_; }
   Handle<FeedbackVector> feedback() const { return feedback_; }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   const int load_slot_;
@@ -2158,7 +2383,9 @@
  public:
   explicit GetSecondReturnedValue(uint64_t bitfield) : Base(bitfield) {}
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class ToObject : public FixedInputValueNodeT<2, ToObject> {
@@ -2173,7 +2400,9 @@
   Input& context() { return Node::input(0); }
   Input& value_input() { return Node::input(1); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class ToString : public FixedInputValueNodeT<2, ToString> {
@@ -2188,7 +2417,9 @@
   Input& context() { return Node::input(0); }
   Input& value_input() { return Node::input(1); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class GeneratorRestoreRegister
@@ -2202,7 +2433,9 @@
   Input& array_input() { return input(0); }
   int index() const { return index_; }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   const int index_;
@@ -2217,7 +2450,9 @@
 
   interpreter::Register source() const { return source_; }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   const interpreter::Register source_;
@@ -2237,7 +2472,9 @@
 
   Register input() const { return input_; }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   const Register input_;
@@ -2258,7 +2495,9 @@
     return value_ != Smi::FromInt(0);
   }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
   void DoLoadToRegister(MaglevAssembler*, OutputRegister);
   Handle<Object> DoReify(LocalIsolate* isolate);
@@ -2282,7 +2521,9 @@
 
   bool IsTheHole() const { return object_.IsTheHole(); }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
   compiler::HeapObjectRef object() { return object_; }
 
@@ -2308,7 +2549,9 @@
 
   RootIndex index() const { return index_; }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
   void DoLoadToRegister(MaglevAssembler*, OutputRegister);
   Handle<Object> DoReify(LocalIsolate* isolate);
@@ -2332,7 +2575,9 @@
   static constexpr OpProperties kProperties =
       OpProperties::GenericRuntimeOrBuiltinCall();
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   const compiler::FeedbackSource feedback_;
@@ -2359,7 +2604,9 @@
   static constexpr OpProperties kProperties =
       OpProperties::Call() | OpProperties::Throw() | OpProperties::LazyDeopt();
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   const compiler::HeapObjectRef constant_elements_;
@@ -2388,7 +2635,9 @@
   static constexpr OpProperties kProperties =
       OpProperties::GenericRuntimeOrBuiltinCall();
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   const compiler::HeapObjectRef constant_elements_;
@@ -2420,7 +2669,9 @@
   static constexpr OpProperties kProperties =
       OpProperties::Call() | OpProperties::Throw() | OpProperties::LazyDeopt();
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   const compiler::ObjectBoilerplateDescriptionRef boilerplate_descriptor_;
@@ -2440,7 +2691,9 @@
 
   compiler::MapRef map() { return map_; }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   const compiler::MapRef map_;
@@ -2472,7 +2725,9 @@
   static constexpr OpProperties kProperties =
       OpProperties::GenericRuntimeOrBuiltinCall();
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   const compiler::ObjectBoilerplateDescriptionRef boilerplate_descriptor_;
@@ -2503,7 +2758,9 @@
   static constexpr OpProperties kProperties =
       OpProperties::GenericRuntimeOrBuiltinCall();
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   const compiler::ScopeInfoRef scope_info_;
@@ -2533,7 +2790,9 @@
   static constexpr OpProperties kProperties =
       OpProperties::GenericRuntimeOrBuiltinCall();
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   const compiler::SharedFunctionInfoRef shared_function_info_;
@@ -2558,7 +2817,9 @@
   // The implementation currently calls runtime.
   static constexpr OpProperties kProperties = OpProperties::Call();
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   compiler::StringRef pattern_;
@@ -2590,7 +2851,9 @@
   // The implementation currently calls runtime.
   static constexpr OpProperties kProperties = OpProperties::Call();
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   const compiler::SharedFunctionInfoRef shared_function_info_;
@@ -2618,7 +2881,9 @@
   Input& left_input() { return input(0); }
   Input& right_input() { return input(1); }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   AssertCondition condition_;
@@ -2631,25 +2896,48 @@
   using Base = FixedInputNodeT<1, CheckMaps>;
 
  public:
-  explicit CheckMaps(uint64_t bitfield, const compiler::MapRef& map,
+  explicit CheckMaps(uint64_t bitfield, const ZoneHandleSet<Map>& maps,
                      CheckType check_type)
-      : Base(bitfield), map_(map), check_type_(check_type) {
-    DCHECK(!map.is_migration_target());
-  }
+      : Base(bitfield), maps_(maps), check_type_(check_type) {}
 
   static constexpr OpProperties kProperties = OpProperties::EagerDeopt();
 
-  compiler::MapRef map() const { return map_; }
+  const ZoneHandleSet<Map>& maps() const { return maps_; }
 
   static constexpr int kReceiverIndex = 0;
   Input& receiver_input() { return input(kReceiverIndex); }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
-  const compiler::MapRef map_;
+  const ZoneHandleSet<Map> maps_;
   const CheckType check_type_;
 };
+
+class CheckValue : public FixedInputNodeT<1, CheckValue> {
+  using Base = FixedInputNodeT<1, CheckValue>;
+
+ public:
+  explicit CheckValue(uint64_t bitfield, const compiler::HeapObjectRef& value)
+      : Base(bitfield), value_(value) {}
+
+  static constexpr OpProperties kProperties = OpProperties::EagerDeopt();
+
+  compiler::HeapObjectRef value() const { return value_; }
+
+  static constexpr int kTargetIndex = 0;
+  Input& target_input() { return input(kTargetIndex); }
+
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
+
+ private:
+  const compiler::HeapObjectRef value_;
+};
+
 class CheckSmi : public FixedInputNodeT<1, CheckSmi> {
   using Base = FixedInputNodeT<1, CheckSmi>;
 
@@ -2661,7 +2949,9 @@
   static constexpr int kReceiverIndex = 0;
   Input& receiver_input() { return input(kReceiverIndex); }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 };
 
 class CheckNumber : public FixedInputNodeT<1, CheckNumber> {
@@ -2677,7 +2967,9 @@
   Input& receiver_input() { return input(kReceiverIndex); }
   Object::Conversion mode() const { return mode_; }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   const Object::Conversion mode_;
@@ -2694,7 +2986,9 @@
   static constexpr int kReceiverIndex = 0;
   Input& receiver_input() { return input(kReceiverIndex); }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 };
 
 class CheckSymbol : public FixedInputNodeT<1, CheckSymbol> {
@@ -2709,7 +3003,9 @@
   static constexpr int kReceiverIndex = 0;
   Input& receiver_input() { return input(kReceiverIndex); }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   const CheckType check_type_;
@@ -2727,7 +3023,9 @@
   static constexpr int kReceiverIndex = 0;
   Input& receiver_input() { return input(kReceiverIndex); }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   const CheckType check_type_;
@@ -2739,24 +3037,24 @@
 
  public:
   explicit CheckMapsWithMigration(uint64_t bitfield,
-                                  const compiler::MapRef& map,
+                                  const ZoneHandleSet<Map>& maps,
                                   CheckType check_type)
-      : Base(bitfield), map_(map), check_type_(check_type) {
-    DCHECK(map.is_migration_target());
-  }
+      : Base(bitfield), maps_(maps), check_type_(check_type) {}
 
   static constexpr OpProperties kProperties =
       OpProperties::EagerDeopt() | OpProperties::DeferredCall();
 
-  compiler::MapRef map() const { return map_; }
+  const ZoneHandleSet<Map>& maps() const { return maps_; }
 
   static constexpr int kReceiverIndex = 0;
   Input& receiver_input() { return input(kReceiverIndex); }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
-  const compiler::MapRef map_;
+  const ZoneHandleSet<Map> maps_;
   const CheckType check_type_;
 };
 
@@ -2773,7 +3071,33 @@
   Input& receiver_input() { return input(kReceiverIndex); }
   Input& index_input() { return input(kIndexIndex); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
+};
+
+class CheckInt32Condition : public FixedInputNodeT<2, CheckInt32Condition> {
+  using Base = FixedInputNodeT<2, CheckInt32Condition>;
+
+ public:
+  explicit CheckInt32Condition(uint64_t bitfield, AssertCondition condition,
+                               DeoptimizeReason reason)
+      : Base(bitfield), condition_(condition), reason_(reason) {}
+
+  static constexpr OpProperties kProperties = OpProperties::EagerDeopt();
+
+  static constexpr int kLeftIndex = 0;
+  static constexpr int kRightIndex = 1;
+  Input& left_input() { return input(kLeftIndex); }
+  Input& right_input() { return input(kRightIndex); }
+
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
+
+ private:
+  AssertCondition condition_;
+  DeoptimizeReason reason_;
 };
 
 class CheckJSObjectElementsBounds
@@ -2790,7 +3114,9 @@
   Input& receiver_input() { return input(kReceiverIndex); }
   Input& index_input() { return input(kIndexIndex); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class DebugBreak : public FixedInputNodeT<0, DebugBreak> {
@@ -2799,7 +3125,9 @@
  public:
   explicit DebugBreak(uint64_t bitfield) : Base(bitfield) {}
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class CheckedInternalizedString
@@ -2819,7 +3147,9 @@
   static constexpr int kObjectIndex = 0;
   Input& object_input() { return Node::input(kObjectIndex); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   const CheckType check_type_;
@@ -2839,7 +3169,9 @@
   static constexpr int kObjectIndex = 0;
   Input& object_input() { return Node::input(kObjectIndex); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class GetTemplateObject : public FixedInputValueNodeT<1, GetTemplateObject> {
@@ -2865,13 +3197,53 @@
   }
   compiler::FeedbackSource feedback() const { return feedback_; }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   compiler::SharedFunctionInfoRef shared_function_info_;
   const compiler::FeedbackSource feedback_;
 };
 
+class BuiltinStringFromCharCode
+    : public FixedInputValueNodeT<1, BuiltinStringFromCharCode> {
+  using Base = FixedInputValueNodeT<1, BuiltinStringFromCharCode>;
+
+ public:
+  explicit BuiltinStringFromCharCode(uint64_t bitfield) : Base(bitfield) {}
+
+  static constexpr OpProperties kProperties = OpProperties::DeferredCall();
+
+  Input& code_input() { return input(0); }
+
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
+};
+
+class BuiltinStringPrototypeCharCodeAt
+    : public FixedInputValueNodeT<2, BuiltinStringPrototypeCharCodeAt> {
+  using Base = FixedInputValueNodeT<2, BuiltinStringPrototypeCharCodeAt>;
+
+ public:
+  explicit BuiltinStringPrototypeCharCodeAt(uint64_t bitfield)
+      : Base(bitfield) {}
+
+  static constexpr OpProperties kProperties = OpProperties::Reading() |
+                                              OpProperties::DeferredCall() |
+                                              OpProperties::Int32();
+
+  static constexpr int kStringIndex = 0;
+  static constexpr int kIndexIndex = 1;
+  Input& string_input() { return input(kStringIndex); }
+  Input& index_input() { return input(kIndexIndex); }
+
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
+};
+
 class LoadTaggedField : public FixedInputValueNodeT<1, LoadTaggedField> {
   using Base = FixedInputValueNodeT<1, LoadTaggedField>;
 
@@ -2886,7 +3258,9 @@
   static constexpr int kObjectIndex = 0;
   Input& object_input() { return input(kObjectIndex); }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   const int offset_;
@@ -2907,7 +3281,9 @@
   static constexpr int kObjectIndex = 0;
   Input& object_input() { return input(kObjectIndex); }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   const int offset_;
@@ -2926,7 +3302,9 @@
   Input& object_input() { return input(kObjectIndex); }
   Input& index_input() { return input(kIndexIndex); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class LoadDoubleElement : public FixedInputValueNodeT<2, LoadDoubleElement> {
@@ -2943,7 +3321,33 @@
   Input& object_input() { return input(kObjectIndex); }
   Input& index_input() { return input(kIndexIndex); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
+};
+
+class StoreDoubleField : public FixedInputNodeT<2, StoreDoubleField> {
+  using Base = FixedInputNodeT<2, StoreDoubleField>;
+
+ public:
+  explicit StoreDoubleField(uint64_t bitfield, int offset)
+      : Base(bitfield), offset_(offset) {}
+
+  static constexpr OpProperties kProperties = OpProperties::Writing();
+
+  int offset() const { return offset_; }
+
+  static constexpr int kObjectIndex = 0;
+  static constexpr int kValueIndex = 1;
+  Input& object_input() { return input(kObjectIndex); }
+  Input& value_input() { return input(kValueIndex); }
+
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
+
+ private:
+  const int offset_;
 };
 
 class StoreTaggedFieldNoWriteBarrier
@@ -2963,12 +3367,35 @@
   Input& object_input() { return input(kObjectIndex); }
   Input& value_input() { return input(kValueIndex); }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   const int offset_;
 };
 
+class StoreMap : public FixedInputNodeT<1, StoreMap> {
+  using Base = FixedInputNodeT<1, StoreMap>;
+
+ public:
+  explicit StoreMap(uint64_t bitfield, compiler::MapRef& map)
+      : Base(bitfield), map_(map) {}
+
+  static constexpr OpProperties kProperties =
+      OpProperties::Writing() | OpProperties::DeferredCall();
+
+  static constexpr int kObjectIndex = 0;
+  Input& object_input() { return input(kObjectIndex); }
+
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
+
+ private:
+  const compiler::MapRef map_;
+};
+
 class StoreTaggedFieldWithWriteBarrier
     : public FixedInputNodeT<2, StoreTaggedFieldWithWriteBarrier> {
   using Base = FixedInputNodeT<2, StoreTaggedFieldWithWriteBarrier>;
@@ -2987,7 +3414,9 @@
   Input& object_input() { return input(kObjectIndex); }
   Input& value_input() { return input(kValueIndex); }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   const int offset_;
@@ -3014,7 +3443,9 @@
 
   Input& context() { return input(0); }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   const compiler::NameRef name_;
@@ -3039,7 +3470,9 @@
   Input& context() { return input(0); }
   Input& value() { return input(1); }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   const compiler::NameRef name_;
@@ -3065,7 +3498,9 @@
   Input& context() { return input(kContextIndex); }
   Input& object_input() { return input(kObjectIndex); }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   const compiler::NameRef name_;
@@ -3095,7 +3530,9 @@
   Input& receiver() { return input(kReceiverIndex); }
   Input& lookup_start_object() { return input(kLookupStartObjectIndex); }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   const compiler::NameRef name_;
@@ -3123,13 +3560,34 @@
   Input& object_input() { return input(kObjectIndex); }
   Input& value_input() { return input(kValueIndex); }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   const compiler::NameRef name_;
   const compiler::FeedbackSource feedback_;
 };
 
+class StringAt : public FixedInputValueNodeT<2, StringAt> {
+  using Base = FixedInputValueNodeT<2, StringAt>;
+
+ public:
+  explicit StringAt(uint64_t bitfield) : Base(bitfield) {}
+
+  static constexpr OpProperties kProperties =
+      OpProperties::Reading() | OpProperties::DeferredCall();
+
+  static constexpr int kStringIndex = 0;
+  static constexpr int kIndexIndex = 1;
+  Input& string_input() { return input(kStringIndex); }
+  Input& index_input() { return input(kIndexIndex); }
+
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
+};
+
 class StringLength : public FixedInputValueNodeT<1, StringLength> {
   using Base = FixedInputValueNodeT<1, StringLength>;
 
@@ -3142,7 +3600,9 @@
   static constexpr int kObjectIndex = 0;
   Input& object_input() { return input(kObjectIndex); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class DefineNamedOwnGeneric
@@ -3168,7 +3628,9 @@
   Input& object_input() { return input(kObjectIndex); }
   Input& value_input() { return input(kValueIndex); }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   const compiler::NameRef name_;
@@ -3198,7 +3660,9 @@
   Input& name_input() { return input(kNameIndex); }
   Input& value_input() { return input(kValueIndex); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   const compiler::FeedbackSource feedback_;
@@ -3224,7 +3688,9 @@
   Input& object_input() { return input(kObjectIndex); }
   Input& key_input() { return input(kKeyIndex); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   const compiler::FeedbackSource feedback_;
@@ -3252,7 +3718,9 @@
   Input& key_input() { return input(kKeyIndex); }
   Input& value_input() { return input(kValueIndex); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   const compiler::FeedbackSource feedback_;
@@ -3281,7 +3749,9 @@
   Input& key_input() { return input(kKeyIndex); }
   Input& value_input() { return input(kValueIndex); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   const compiler::FeedbackSource feedback_;
@@ -3298,7 +3768,9 @@
   compiler::AllocatedOperand source() const { return source_; }
   compiler::AllocatedOperand target() const { return target_; }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   compiler::AllocatedOperand source_;
@@ -3316,7 +3788,9 @@
   compiler::AllocatedOperand target() const { return target_; }
   ValueNode* node() const { return node_; }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   ValueNode* node_;
@@ -3345,7 +3819,9 @@
 
   bool is_exception_phi() const { return input_count() == 0; }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
   void AllocateVregInPostProcess(MaglevVregAllocationState*);
 
  private:
@@ -3362,6 +3838,8 @@
   using Base = ValueNodeT<Call>;
 
  public:
+  enum class TargetType { kJSFunction, kAny };
+
   // We assume function and context as fixed inputs.
   static constexpr int kFunctionIndex = 0;
   static constexpr int kContextIndex = 1;
@@ -3373,9 +3851,13 @@
 
   // This ctor is used when for variable input counts.
   // Inputs must be initialized manually.
-  Call(uint64_t bitfield, ConvertReceiverMode mode, ValueNode* function,
+  Call(uint64_t bitfield, ConvertReceiverMode mode, TargetType target_type,
+       const compiler::FeedbackSource& feedback, ValueNode* function,
        ValueNode* context)
-      : Base(bitfield), receiver_mode_(mode) {
+      : Base(bitfield),
+        receiver_mode_(mode),
+        target_type_(target_type),
+        feedback_(feedback) {
     set_input(kFunctionIndex, function);
     set_input(kContextIndex, context);
   }
@@ -3391,11 +3873,16 @@
   void set_arg(int i, ValueNode* node) {
     set_input(i + kFixedInputCount, node);
   }
+  compiler::FeedbackSource feedback() const { return feedback_; }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   ConvertReceiverMode receiver_mode_;
+  TargetType target_type_;
+  const compiler::FeedbackSource feedback_;
 };
 
 class Construct : public ValueNodeT<Construct> {
@@ -3437,7 +3924,9 @@
   }
   compiler::FeedbackSource feedback() const { return feedback_; }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   const compiler::FeedbackSource feedback_;
@@ -3520,7 +4009,9 @@
     return Builtins::CallInterfaceDescriptorFor(builtin_).GetReturnCount();
   }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   void PassFeedbackSlotOnStack(MaglevAssembler*);
@@ -3564,7 +4055,9 @@
     return Runtime::FunctionForId(function_id())->result_size;
   }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   Runtime::FunctionId function_id_;
@@ -3581,8 +4074,9 @@
 
   // This ctor is used when for variable input counts.
   // Inputs must be initialized manually.
-  CallWithSpread(uint64_t bitfield, ValueNode* function, ValueNode* context)
-      : Base(bitfield) {
+  CallWithSpread(uint64_t bitfield, compiler::FeedbackSource feedback,
+                 ValueNode* function, ValueNode* context)
+      : Base(bitfield), feedback_(feedback) {
     set_input(kFunctionIndex, function);
     set_input(kContextIndex, context);
   }
@@ -3602,8 +4096,56 @@
     // Spread is the last argument/input.
     return input(input_count() - 1);
   }
+  compiler::FeedbackSource feedback() const { return feedback_; }
+
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+ private:
+  const compiler::FeedbackSource feedback_;
+};
+
+class CallKnownJSFunction : public ValueNodeT<CallKnownJSFunction> {
+  using Base = ValueNodeT<CallKnownJSFunction>;
+
+ public:
+  // We assume function and context as fixed inputs.
+  static constexpr int kReceiverIndex = 0;
+  static constexpr int kFixedInputCount = 1;
+
+  // We need enough inputs to have these fixed inputs plus the maximum arguments
+  // to a function call.
+  static_assert(kMaxInputs >= kFixedInputCount + Code::kMaxArguments);
+
+  // This ctor is used when for variable input counts.
+  // Inputs must be initialized manually.
+  CallKnownJSFunction(uint64_t bitfield, const compiler::JSFunctionRef function,
+                      ValueNode* receiver)
+      : Base(bitfield), function_(function) {
+    set_input(kReceiverIndex, receiver);
+  }
+
+  static constexpr OpProperties kProperties = OpProperties::JSCall();
+
+  Input& receiver() { return input(kReceiverIndex); }
+  const Input& receiver() const { return input(kReceiverIndex); }
+  int num_args() const { return input_count() - kFixedInputCount; }
+  Input& arg(int i) { return input(i + kFixedInputCount); }
+  void set_arg(int i, ValueNode* node) {
+    set_input(i + kFixedInputCount, node);
+  }
+
+  compiler::SharedFunctionInfoRef shared_function_info() const {
+    return function_.shared();
+  }
+
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
+
+ private:
+  const compiler::JSFunctionRef function_;
 };
 
 class ConstructWithSpread : public ValueNodeT<ConstructWithSpread> {
@@ -3618,9 +4160,10 @@
 
   // This ctor is used when for variable input counts.
   // Inputs must be initialized manually.
-  ConstructWithSpread(uint64_t bitfield, ValueNode* function,
-                      ValueNode* new_target, ValueNode* context)
-      : Base(bitfield) {
+  ConstructWithSpread(uint64_t bitfield, compiler::FeedbackSource feedback,
+                      ValueNode* function, ValueNode* new_target,
+                      ValueNode* context)
+      : Base(bitfield), feedback_(feedback) {
     set_input(kFunctionIndex, function);
     set_input(kNewTargetIndex, new_target);
     set_input(kContextIndex, context);
@@ -3643,8 +4186,37 @@
     // Spread is the last argument/input.
     return input(input_count() - 1);
   }
+  compiler::FeedbackSource feedback() const { return feedback_; }
+
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
+
+ private:
+  const compiler::FeedbackSource feedback_;
+};
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+class ConvertReceiver : public FixedInputValueNodeT<1, ConvertReceiver> {
+  using Base = FixedInputValueNodeT<1, ConvertReceiver>;
+
+ public:
+  explicit ConvertReceiver(uint64_t bitfield,
+                           const compiler::JSFunctionRef target,
+                           ConvertReceiverMode mode)
+      : Base(bitfield), target_(target), mode_(mode) {}
+
+  Input& receiver_input() { return input(0); }
+
+  // The implementation currently calls runtime.
+  static constexpr OpProperties kProperties = OpProperties::JSCall();
+
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
+
+ private:
+  const compiler::JSFunctionRef target_;
+  ConvertReceiverMode mode_;
 };
 
 class IncreaseInterruptBudget
@@ -3659,7 +4231,9 @@
 
   int amount() const { return amount_; }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   const int amount_;
@@ -3683,7 +4257,9 @@
 
   int amount() const { return amount_; }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   const int amount_;
@@ -3705,7 +4281,9 @@
 
   Input& value() { return Node::input(0); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   const compiler::NameRef name_;
@@ -3723,7 +4301,9 @@
 
   Input& value() { return Node::input(0); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class ThrowSuperAlreadyCalledIfNotHole
@@ -3739,7 +4319,9 @@
 
   Input& value() { return Node::input(0); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class ThrowIfNotSuperConstructor
@@ -3755,7 +4337,9 @@
   Input& constructor() { return Node::input(0); }
   Input& function() { return Node::input(1); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class ControlNode : public NodeBase {
@@ -3906,7 +4490,9 @@
   Jump(uint64_t bitfield, BasicBlockRef* target_refs)
       : Base(bitfield, target_refs) {}
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class JumpLoop : public UnconditionalControlNodeT<JumpLoop> {
@@ -3919,7 +4505,9 @@
   explicit JumpLoop(uint64_t bitfield, BasicBlockRef* ref)
       : Base(bitfield, ref) {}
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
   base::Vector<Input> used_nodes() { return used_node_locations_; }
   void set_used_nodes(base::Vector<Input> locations) {
@@ -3938,7 +4526,9 @@
                          MaglevCompilationUnit* unit)
       : Base(bitfield, target_refs), unit_(unit) {}
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
   const MaglevCompilationUnit* unit() const { return unit_; }
 
@@ -3953,7 +4543,9 @@
   explicit JumpFromInlined(uint64_t bitfield, BasicBlockRef* target_refs)
       : Base(bitfield, target_refs) {}
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class Abort : public TerminalControlNode {
@@ -3965,7 +4557,9 @@
 
   AbortReason reason() const { return reason_; }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   const AbortReason reason_;
@@ -3979,7 +4573,9 @@
 
   Input& value_input() { return input(0); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class Deopt : public TerminalControlNode {
@@ -3993,7 +4589,9 @@
 
   DeoptimizeReason reason() const { return reason_; }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   DeoptimizeReason reason_;
@@ -4029,7 +4627,9 @@
 
   Input& value() { return input(0); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 
  private:
   const int value_base_;
@@ -4051,7 +4651,9 @@
   RootIndex root_index() { return root_index_; }
   Input& condition_input() { return input(0); }
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   RootIndex root_index_;
@@ -4069,7 +4671,9 @@
 
   Input& condition_input() { return input(0); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class BranchIfJSReceiver : public BranchControlNodeT<1, BranchIfJSReceiver> {
@@ -4082,7 +4686,9 @@
 
   Input& condition_input() { return input(0); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class BranchIfToBooleanTrue
@@ -4098,7 +4704,9 @@
 
   Input& condition_input() { return input(0); }
 
-  DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const {}
 };
 
 class BranchIfInt32Compare
@@ -4116,7 +4724,9 @@
                                 BasicBlockRef* if_false_refs)
       : Base(bitfield, if_true_refs, if_false_refs), operation_(operation) {}
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   Operation operation_;
@@ -4137,7 +4747,9 @@
                                   BasicBlockRef* if_false_refs)
       : Base(bitfield, if_true_refs, if_false_refs), operation_(operation) {}
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   Operation operation_;
@@ -4158,15 +4770,14 @@
                                     BasicBlockRef* if_false_refs)
       : Base(bitfield, if_true_refs, if_false_refs), operation_(operation) {}
 
-  DECL_NODE_INTERFACE()
+  void AllocateVreg(MaglevVregAllocationState*);
+  void GenerateCode(MaglevAssembler*, const ProcessingState&);
+  void PrintParams(std::ostream&, MaglevGraphLabeller*) const;
 
  private:
   Operation operation_;
 };
 
-#undef DECL_NODE_INTERFACE_WITH_EMPTY_PRINT_PARAMS
-#undef DECL_NODE_INTERFACE
-
 }  // namespace maglev
 }  // namespace internal
 }  // namespace v8
diff -r -u --color up/v8/src/maglev/maglev-regalloc.cc nw/v8/src/maglev/maglev-regalloc.cc
--- up/v8/src/maglev/maglev-regalloc.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/maglev/maglev-regalloc.cc	2023-01-19 16:46:36.298109548 -0500
@@ -12,6 +12,7 @@
 #include "src/codegen/register.h"
 #include "src/codegen/reglist.h"
 #include "src/compiler/backend/instruction.h"
+#include "src/maglev/maglev-code-gen-state.h"
 #include "src/maglev/maglev-compilation-info.h"
 #include "src/maglev/maglev-compilation-unit.h"
 #include "src/maglev/maglev-graph-labeller.h"
@@ -330,8 +331,8 @@
       } else {
         InitializeRegisterValues(block->state()->register_state());
       }
-    } else if (block->is_empty_block()) {
-      InitializeRegisterValues(block->empty_block_register_state());
+    } else if (block->is_edge_split_block()) {
+      InitializeRegisterValues(block->edge_split_block_register_state());
     }
 
     if (v8_flags.trace_maglev_regalloc) {
@@ -394,7 +395,8 @@
         // (the first one by default) that is marked with the
         // virtual_accumulator and force kReturnRegister0. This corresponds to
         // the exception message object.
-        Phi* phi = block->phis()->first();
+        Phi::List::Iterator phi_it = block->phis()->begin();
+        Phi* phi = *phi_it;
         DCHECK_EQ(phi->input_count(), 0);
         if (phi->owner() == interpreter::Register::virtual_accumulator() &&
             !phi->is_dead()) {
@@ -405,6 +407,31 @@
                                     << phi->result().operand() << std::endl;
           }
         }
+        // The receiver is the next phi after the accumulator (or the first phi
+        // if there is no accumulator).
+        if (phi->owner() == interpreter::Register::virtual_accumulator()) {
+          ++phi_it;
+          phi = *phi_it;
+        }
+        DCHECK(phi->owner().is_receiver());
+        // The receiver is a special case for a fairly silly reason:
+        // OptimizedFrame::Summarize requires the receiver (and the function)
+        // to be in a stack slot, since it's value must be available even
+        // though we're not deoptimizing (and thus register states are not
+        // available).
+        //
+        // TODO(leszeks):
+        // For inlined functions / nested graph generation, this a) doesn't
+        // work (there's no receiver stack slot); and b) isn't necessary
+        // (Summarize only looks at noninlined functions).
+        phi->Spill(compiler::AllocatedOperand(
+            compiler::AllocatedOperand::STACK_SLOT,
+            MachineRepresentation::kTagged,
+            (StandardFrameConstants::kExpressionsOffset -
+             UnoptimizedFrameConstants::kRegisterFileFromFp) /
+                    kSystemPointerSize +
+                interpreter::Register::receiver().index()));
+        phi->result().SetAllocated(phi->spill_slot());
       }
       // Secondly try to assign the phi to a free register.
       for (Phi* phi : *block->phis()) {
@@ -504,8 +531,7 @@
 void StraightForwardRegisterAllocator::UpdateUse(
     const EagerDeoptInfo& deopt_info) {
   detail::DeepForEachInput(
-      &deopt_info,
-      [&](ValueNode* node, interpreter::Register reg, InputLocation* input) {
+      &deopt_info, [&](ValueNode* node, InputLocation* input) {
         if (v8_flags.trace_maglev_regalloc) {
           printing_visitor_->os()
               << "- using " << PrintNodeLabel(graph_labeller(), node) << "\n";
@@ -522,8 +548,7 @@
 void StraightForwardRegisterAllocator::UpdateUse(
     const LazyDeoptInfo& deopt_info) {
   detail::DeepForEachInput(
-      &deopt_info,
-      [&](ValueNode* node, interpreter::Register reg, InputLocation* input) {
+      &deopt_info, [&](ValueNode* node, InputLocation* input) {
         if (v8_flags.trace_maglev_regalloc) {
           printing_visitor_->os()
               << "- using " << PrintNodeLabel(graph_labeller(), node) << "\n";
@@ -713,8 +738,6 @@
     RegisterFrameState<RegisterT>& registers, RegisterT reg) {
   // The register should not already be free.
   DCHECK(!registers.free().has(reg));
-  // We are only allowed to allocated blocked registers at the end.
-  DCHECK(!registers.is_blocked(reg));
 
   ValueNode* node = registers.GetValue(reg);
 
@@ -759,7 +782,7 @@
 
 void StraightForwardRegisterAllocator::InitializeBranchTargetPhis(
     int predecessor_id, BasicBlock* target) {
-  DCHECK(!target->is_empty_block());
+  DCHECK(!target->is_edge_split_block());
 
   if (!target->has_phi()) return;
 
@@ -788,7 +811,7 @@
     // Not a fall-through branch, copy the state over.
     return InitializeBranchTargetRegisterValues(control_node, target);
   }
-  if (target->is_empty_block()) {
+  if (target->is_edge_split_block()) {
     return InitializeEmptyBlockRegisterValues(control_node, target);
   }
 
@@ -1044,6 +1067,21 @@
   }
 }
 
+void StraightForwardRegisterAllocator::MarkAsClobbered(
+    ValueNode* node, const compiler::AllocatedOperand& location) {
+  if (node->use_double_register()) {
+    DoubleRegister reg = location.GetDoubleRegister();
+    DCHECK(double_registers_.is_blocked(reg));
+    DropRegisterValue(reg);
+    double_registers_.AddToFree(reg);
+  } else {
+    Register reg = location.GetRegister();
+    DCHECK(general_registers_.is_blocked(reg));
+    DropRegisterValue(reg);
+    general_registers_.AddToFree(reg);
+  }
+}
+
 void StraightForwardRegisterAllocator::AssignArbitraryRegisterInput(
     Input& input) {
   // Already assigned in AssignFixedInput
@@ -1074,9 +1112,15 @@
         node->use_double_register()
             ? double_registers_.ChooseInputRegister(node)
             : general_registers_.ChooseInputRegister(node);
+    if (input.Cloberred()) {
+      MarkAsClobbered(node, location);
+    }
     input.SetAllocated(location);
   } else {
     compiler::AllocatedOperand allocation = AllocateRegister(node);
+    if (input.Cloberred()) {
+      MarkAsClobbered(node, allocation);
+    }
     input.SetAllocated(allocation);
     DCHECK_NE(location, allocation);
     AddMoveBeforeCurrentNode(node, location, allocation);
@@ -1122,14 +1166,14 @@
     if (input.operand().IsRegister()) {
       Register reg =
           compiler::AllocatedOperand::cast(input.operand()).GetRegister();
-      if (general_registers_.GetValue(reg) != input.node()) {
+      if (general_registers_.GetValueMaybeFreeButBlocked(reg) != input.node()) {
         FATAL("Input node n%d is not in expected register %s",
               graph_labeller()->NodeId(input.node()), RegisterName(reg));
       }
     } else if (input.operand().IsDoubleRegister()) {
       DoubleRegister reg =
           compiler::AllocatedOperand::cast(input.operand()).GetDoubleRegister();
-      if (double_registers_.GetValue(reg) != input.node()) {
+      if (double_registers_.GetValueMaybeFreeButBlocked(reg) != input.node()) {
         FATAL("Input node n%d is not in expected register %s",
               graph_labeller()->NodeId(input.node()), RegisterName(reg));
       }
@@ -1259,6 +1303,17 @@
   });
   snapshot.live_registers = general_registers_.used();
   snapshot.live_double_registers = double_registers_.used();
+  // If a value node, then the result register is removed from the snapshot.
+  if (ValueNode* value_node = node->TryCast<ValueNode>()) {
+    if (value_node->use_double_register()) {
+      snapshot.live_double_registers.clear(
+          ToDoubleRegister(value_node->result()));
+    } else {
+      Register reg = ToRegister(value_node->result());
+      snapshot.live_registers.clear(reg);
+      snapshot.live_tagged_registers.clear(reg);
+    }
+  }
   node->set_register_snapshot(snapshot);
 }
 
@@ -1328,6 +1383,7 @@
   RegisterFrameState<RegisterT>& registers = GetRegisterFrameState<RegisterT>();
   RegisterT best = PickRegisterToFree<RegisterT>(registers.blocked());
   DCHECK(best.is_valid());
+  DCHECK(!registers.is_blocked(best));
   DropRegisterValue(registers, best);
   registers.AddToFree(best);
   return best;
@@ -1405,6 +1461,7 @@
                                       node->GetMachineRepresentation(),
                                       reg.code());
   } else {
+    DCHECK(!registers.is_blocked(reg));
     DropRegisterValue(registers, reg);
   }
 #ifdef DEBUG
@@ -1646,7 +1703,7 @@
 
 void StraightForwardRegisterAllocator::InitializeEmptyBlockRegisterValues(
     ControlNode* source, BasicBlock* target) {
-  DCHECK(target->is_empty_block());
+  DCHECK(target->is_edge_split_block());
   MergePointRegisterState* register_state =
       compilation_info_->zone()->New<MergePointRegisterState>();
 
@@ -1662,13 +1719,13 @@
   };
   ForEachMergePointRegisterState(*register_state, init);
 
-  target->set_empty_block_register_state(register_state);
+  target->set_edge_split_block_register_state(register_state);
 }
 
 void StraightForwardRegisterAllocator::MergeRegisterValues(ControlNode* control,
                                                            BasicBlock* target,
                                                            int predecessor_id) {
-  if (target->is_empty_block()) {
+  if (target->is_edge_split_block()) {
     return InitializeEmptyBlockRegisterValues(control, target);
   }
 
diff -r -u --color up/v8/src/maglev/maglev-regalloc.h nw/v8/src/maglev/maglev-regalloc.h
--- up/v8/src/maglev/maglev-regalloc.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/maglev/maglev-regalloc.h	2023-01-19 16:46:36.298109548 -0500
@@ -20,6 +20,26 @@
 class MaglevPrintingVisitor;
 class MergePointRegisterState;
 
+// Represents the state of the register frame during register allocation,
+// including current register values, and the state of each register.
+//
+// Register state encodes two orthogonal concepts:
+//
+//   1. Used/free registers: Which registers currently hold a valid value,
+//   2. Blocked/unblocked registers: Which registers can be modified during the
+//      current allocation.
+//
+// The combination of these encodes values in different states:
+//
+//  Free + unblocked: Completely unused registers which can be used for
+//                    anything.
+//  Used + unblocked: Live values that can be spilled if there is register
+//                    pressure.
+//  Used + blocked:   Values that are in a register and are used as an input in
+//                    the current allocation.
+//  Free + blocked:   Unused registers that are reserved as temporaries, or
+//                    inputs that will get clobbered during the execution of the
+//                    node being allocated.
 template <typename RegisterT>
 class RegisterFrameState {
  public:
@@ -85,6 +105,19 @@
     DCHECK_NOT_NULL(node);
     return node;
   }
+#ifdef DEBUG
+  // Like GetValue, but allow reading freed registers as long as they were also
+  // blocked. This allows us to DCHECK expected register state against node
+  // state, even if that node is dead or clobbered by the end of the current
+  // allocation.
+  ValueNode* GetValueMaybeFreeButBlocked(RegisterT reg) const {
+    DCHECK(!free_.has(reg) || blocked_.has(reg));
+    ValueNode* node = values_[reg.code()];
+    DCHECK_NOT_NULL(node);
+    return node;
+  }
+#endif
+
   RegTList blocked() const { return blocked_; }
   void block(RegisterT reg) { blocked_.set(reg); }
   void unblock(RegisterT reg) { blocked_.clear(reg); }
@@ -135,6 +168,9 @@
   void UpdateUse(const EagerDeoptInfo& deopt_info);
   void UpdateUse(const LazyDeoptInfo& deopt_info);
 
+  void MarkAsClobbered(ValueNode* node,
+                       const compiler::AllocatedOperand& location);
+
   void AllocateControlNode(ControlNode* node, BasicBlock* block);
   void AllocateNode(Node* node);
   void AllocateNodeResult(ValueNode* node);
diff -r -u --color up/v8/src/numbers/math-random.cc nw/v8/src/numbers/math-random.cc
--- up/v8/src/numbers/math-random.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/numbers/math-random.cc	2023-01-19 16:46:36.298109548 -0500
@@ -44,8 +44,8 @@
   // sequence.
   if (state.s0 == 0 && state.s1 == 0) {
     uint64_t seed;
-    if (FLAG_random_seed != 0) {
-      seed = FLAG_random_seed;
+    if (v8_flags.random_seed != 0) {
+      seed = v8_flags.random_seed;
     } else {
       isolate->random_number_generator()->NextBytes(&seed, sizeof(seed));
     }
Only in nw/v8/src: nwjc.cc
diff -r -u --color up/v8/src/objects/backing-store.cc nw/v8/src/objects/backing-store.cc
--- up/v8/src/objects/backing-store.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/backing-store.cc	2023-01-19 16:46:36.298109548 -0500
@@ -164,6 +164,7 @@
       is_shared_(shared == SharedFlag::kShared),
       is_resizable_by_js_(resizable == ResizableFlag::kResizable),
       is_wasm_memory_(is_wasm_memory),
+      is_nodejs_(false),
       holds_shared_ptr_to_allocator_(false),
       free_on_destruct_(free_on_destruct),
       has_guard_regions_(has_guard_regions),
@@ -222,6 +223,17 @@
     Clear();
     return;
   }
+
+  if (is_nodejs_) {
+    // JSArrayBuffer backing store. Deallocate through the embedder's allocator.
+    auto allocator = reinterpret_cast<v8::ArrayBuffer::Allocator*>(
+        get_v8_api_array_buffer_allocator());
+    TRACE_BS("BSn:free   bs=%p mem=%p (length=%zu, capacity=%zu)\n", this,
+             buffer_start_, byte_length(), byte_capacity_);
+    allocator->Free(buffer_start_, byte_length_, v8::ArrayBuffer::Allocator::AllocationMode::kNodeJS);
+    Clear();
+    return;
+  }
   if (free_on_destruct_) {
     // JSArrayBuffer backing store. Deallocate through the embedder's allocator.
     auto allocator = get_v8_api_array_buffer_allocator();
@@ -714,7 +726,7 @@
 
 std::unique_ptr<BackingStore> BackingStore::WrapAllocation(
     Isolate* isolate, void* allocation_base, size_t allocation_length,
-    SharedFlag shared, bool free_on_destruct) {
+    SharedFlag shared, bool free_on_destruct, bool is_nodejs) {
   auto result = new BackingStore(allocation_base,               // start
                                  allocation_length,             // length
                                  allocation_length,             // max length
@@ -727,6 +739,7 @@
                                  false,             // custom_deleter
                                  false);            // empty_deleter
   result->SetAllocatorFromIsolate(isolate);
+  result->is_nodejs_ = is_nodejs;
   TRACE_BS("BS:wrap   bs=%p mem=%p (length=%zu)\n", result,
            result->buffer_start(), result->byte_length());
   return std::unique_ptr<BackingStore>(result);
@@ -735,7 +748,7 @@
 std::unique_ptr<BackingStore> BackingStore::WrapAllocation(
     void* allocation_base, size_t allocation_length,
     v8::BackingStore::DeleterCallback deleter, void* deleter_data,
-    SharedFlag shared) {
+    SharedFlag shared, bool is_nodejs) {
   bool is_empty_deleter = (deleter == v8::BackingStore::EmptyDeleter);
   auto result = new BackingStore(allocation_base,               // start
                                  allocation_length,             // length
@@ -748,6 +761,7 @@
                                  false,              // has_guard_regions
                                  true,               // custom_deleter
                                  is_empty_deleter);  // empty_deleter
+  result->is_nodejs_ = is_nodejs;
   result->type_specific_data_.deleter = {deleter, deleter_data};
   TRACE_BS("BS:wrap   bs=%p mem=%p (length=%zu)\n", result,
            result->buffer_start(), result->byte_length());
diff -r -u --color up/v8/src/objects/backing-store.h nw/v8/src/objects/backing-store.h
--- up/v8/src/objects/backing-store.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/backing-store.h	2023-01-19 16:46:36.298109548 -0500
@@ -76,12 +76,13 @@
                                                       void* allocation_base,
                                                       size_t allocation_length,
                                                       SharedFlag shared,
-                                                      bool free_on_destruct);
+                                                      bool free_on_destruct,
+                                                      bool is_nodejs = false);
 
   static std::unique_ptr<BackingStore> WrapAllocation(
       void* allocation_base, size_t allocation_length,
       v8::BackingStore::DeleterCallback deleter, void* deleter_data,
-      SharedFlag shared);
+      SharedFlag shared, bool is_nodejs = false);
 
   // Create an empty backing store.
   static std::unique_ptr<BackingStore> EmptyBackingStore(SharedFlag shared);
@@ -97,6 +98,7 @@
   bool is_shared() const { return is_shared_; }
   bool is_resizable_by_js() const { return is_resizable_by_js_; }
   bool is_wasm_memory() const { return is_wasm_memory_; }
+  bool is_node_js() const { return is_nodejs_; }
   bool has_guard_regions() const { return has_guard_regions_; }
   bool free_on_destruct() const { return free_on_destruct_; }
 
@@ -116,6 +118,7 @@
            buffer_start_ != nullptr;
   }
 
+  void set_nodejs(bool nodejs) { is_nodejs_ = nodejs; }
   // Wrapper around ArrayBuffer::Allocator::Reallocate.
   bool Reallocate(Isolate* isolate, size_t new_byte_length);
 
@@ -226,6 +229,7 @@
   // Backing stores for (Resizable|GrowableShared)ArrayBuffer
   bool is_resizable_by_js_ : 1;
   bool is_wasm_memory_ : 1;
+  bool is_nodejs_ : 1;
   bool holds_shared_ptr_to_allocator_ : 1;
   bool free_on_destruct_ : 1;
   bool has_guard_regions_ : 1;
diff -r -u --color up/v8/src/objects/bigint.cc nw/v8/src/objects/bigint.cc
--- up/v8/src/objects/bigint.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/bigint.cc	2023-01-19 16:46:36.298109548 -0500
@@ -1683,6 +1683,29 @@
   return 0;
 }
 
+int32_t MutableBigInt_AbsoluteModAndCanonicalize(Address result_addr,
+                                                 Address x_addr,
+                                                 Address y_addr) {
+  BigInt x = BigInt::cast(Object(x_addr));
+  BigInt y = BigInt::cast(Object(y_addr));
+  MutableBigInt result = MutableBigInt::cast(Object(result_addr));
+
+  Isolate* isolate;
+  if (!GetIsolateFromHeapObject(x, &isolate)) {
+    // We should always get the isolate from the BigInt.
+    UNREACHABLE();
+  }
+
+  bigint::Status status = isolate->bigint_processor()->Modulo(
+      GetRWDigits(result), GetDigits(x), GetDigits(y));
+  if (status == bigint::Status::kInterrupted) {
+    return 1;
+  }
+
+  MutableBigInt::Canonicalize(result);
+  return 0;
+}
+
 void MutableBigInt_BitwiseAndPosPosAndCanonicalize(Address result_addr,
                                                    Address x_addr,
                                                    Address y_addr) {
diff -r -u --color up/v8/src/objects/bigint.h nw/v8/src/objects/bigint.h
--- up/v8/src/objects/bigint.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/bigint.h	2023-01-19 16:46:36.298109548 -0500
@@ -32,6 +32,9 @@
 int32_t MutableBigInt_AbsoluteDivAndCanonicalize(Address result_addr,
                                                  Address x_addr,
                                                  Address y_addr);
+int32_t MutableBigInt_AbsoluteModAndCanonicalize(Address result_addr,
+                                                 Address x_addr,
+                                                 Address y_addr);
 void MutableBigInt_BitwiseAndPosPosAndCanonicalize(Address result_addr,
                                                    Address x_addr,
                                                    Address y_addr);
diff -r -u --color up/v8/src/objects/call-site-info-inl.h nw/v8/src/objects/call-site-info-inl.h
--- up/v8/src/objects/call-site-info-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/call-site-info-inl.h	2023-01-19 16:46:36.298109548 -0500
@@ -35,7 +35,7 @@
   HeapObject value = TorqueGeneratedClass::code_object(cage_base);
   // The |code_object| field can contain many types of objects, but only CodeT
   // values have to be converted to Code.
-  if (V8_REMOVE_BUILTINS_CODE_OBJECTS) {
+  if (V8_EXTERNAL_CODE_SPACE_BOOL) {
     // In this mode the callers are fine with CodeT result.
     return value;
   }
diff -r -u --color up/v8/src/objects/code-inl.h nw/v8/src/objects/code-inl.h
--- up/v8/src/objects/code-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/code-inl.h	2023-01-19 16:46:36.308942878 -0500
@@ -51,7 +51,7 @@
   InstanceType instance_type = map(cage_base).instance_type();
   if (InstanceTypeChecker::IsCode(instance_type)) {
     return GetCode().InstructionSize();
-  } else if (V8_REMOVE_BUILTINS_CODE_OBJECTS &&
+  } else if (V8_EXTERNAL_CODE_SPACE_BOOL &&
              InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
     return GetCodeT().InstructionSize();
   } else {
@@ -66,7 +66,7 @@
   if (InstanceTypeChecker::IsCode(instance_type)) {
     DCHECK_NE(GetCode().kind(), CodeKind::BASELINE);
     return GetCode().source_position_table(cage_base);
-  } else if (V8_REMOVE_BUILTINS_CODE_OBJECTS &&
+  } else if (V8_EXTERNAL_CODE_SPACE_BOOL &&
              InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
     CodeT codet = GetCodeT();
     if (codet.is_off_heap_trampoline()) {
@@ -84,7 +84,7 @@
   InstanceType instance_type = map(cage_base).instance_type();
   if (InstanceTypeChecker::IsCode(instance_type)) {
     return GetCode().SourcePositionTable(cage_base, sfi);
-  } else if (V8_REMOVE_BUILTINS_CODE_OBJECTS &&
+  } else if (V8_EXTERNAL_CODE_SPACE_BOOL &&
              InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
     CodeT codet = GetCodeT();
     if (codet.is_off_heap_trampoline()) {
@@ -101,7 +101,7 @@
   InstanceType instance_type = map(cage_base).instance_type();
   if (InstanceTypeChecker::IsCode(instance_type)) {
     return GetCode().SizeIncludingMetadata(cage_base);
-  } else if (V8_REMOVE_BUILTINS_CODE_OBJECTS &&
+  } else if (V8_EXTERNAL_CODE_SPACE_BOOL &&
              InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
     CodeT codet = GetCodeT();
     return codet.is_off_heap_trampoline()
@@ -117,7 +117,7 @@
   InstanceType instance_type = map(cage_base).instance_type();
   if (InstanceTypeChecker::IsCode(instance_type)) {
     return GetCode().InstructionStart();
-  } else if (V8_REMOVE_BUILTINS_CODE_OBJECTS &&
+  } else if (V8_EXTERNAL_CODE_SPACE_BOOL &&
              InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
     return GetCodeT().InstructionStart();
   } else {
@@ -130,7 +130,7 @@
   InstanceType instance_type = map(cage_base).instance_type();
   if (InstanceTypeChecker::IsCode(instance_type)) {
     return GetCode().InstructionEnd();
-  } else if (V8_REMOVE_BUILTINS_CODE_OBJECTS &&
+  } else if (V8_EXTERNAL_CODE_SPACE_BOOL &&
              InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
     return GetCodeT().InstructionEnd();
   } else {
@@ -145,7 +145,7 @@
   InstanceType instance_type = map(cage_base).instance_type();
   if (InstanceTypeChecker::IsCode(instance_type)) {
     return GetCode().contains(isolate, inner_pointer);
-  } else if (V8_REMOVE_BUILTINS_CODE_OBJECTS &&
+  } else if (V8_EXTERNAL_CODE_SPACE_BOOL &&
              InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
     return GetCodeT().contains(isolate, inner_pointer);
   } else {
@@ -159,7 +159,7 @@
   InstanceType instance_type = map(cage_base).instance_type();
   if (InstanceTypeChecker::IsCode(instance_type)) {
     return GetCode().kind();
-  } else if (V8_REMOVE_BUILTINS_CODE_OBJECTS &&
+  } else if (V8_EXTERNAL_CODE_SPACE_BOOL &&
              InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
     return GetCodeT().kind();
   } else {
@@ -172,7 +172,7 @@
   InstanceType instance_type = map(cage_base).instance_type();
   if (InstanceTypeChecker::IsCode(instance_type)) {
     return GetCode().builtin_id();
-  } else if (V8_REMOVE_BUILTINS_CODE_OBJECTS &&
+  } else if (V8_EXTERNAL_CODE_SPACE_BOOL &&
              InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
     return GetCodeT().builtin_id();
   } else {
@@ -185,7 +185,7 @@
   InstanceType instance_type = map(cage_base).instance_type();
   if (InstanceTypeChecker::IsCode(instance_type)) {
     return GetCode().is_off_heap_trampoline();
-  } else if (V8_REMOVE_BUILTINS_CODE_OBJECTS &&
+  } else if (V8_EXTERNAL_CODE_SPACE_BOOL &&
              InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
     return GetCodeT().is_off_heap_trampoline();
   } else {
@@ -199,7 +199,7 @@
   InstanceType instance_type = map(cage_base).instance_type();
   if (InstanceTypeChecker::IsCode(instance_type)) {
     return GetCode().GetBuiltinCatchPrediction();
-  } else if (V8_REMOVE_BUILTINS_CODE_OBJECTS &&
+  } else if (V8_EXTERNAL_CODE_SPACE_BOOL &&
              InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
     return GetCodeT().GetBuiltinCatchPrediction();
   } else {
@@ -212,7 +212,7 @@
 }
 
 bool AbstractCode::IsCodeT(PtrComprCageBase cage_base) const {
-  CHECK(V8_REMOVE_BUILTINS_CODE_OBJECTS);
+  CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
   return HeapObject::IsCodeT(cage_base);
 }
 
@@ -223,7 +223,7 @@
 Code AbstractCode::GetCode() { return Code::cast(*this); }
 
 CodeT AbstractCode::GetCodeT() {
-  CHECK(V8_REMOVE_BUILTINS_CODE_OBJECTS);
+  CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
   return CodeT::cast(*this);
 }
 
@@ -235,7 +235,7 @@
   InstanceType instance_type = map(cage_base).instance_type();
   if (InstanceTypeChecker::IsCode(instance_type)) {
     return GetCode();
-  } else if (V8_REMOVE_BUILTINS_CODE_OBJECTS &&
+  } else if (V8_EXTERNAL_CODE_SPACE_BOOL &&
              InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
     CodeT codet = GetCodeT();
     DCHECK(!codet.is_off_heap_trampoline());
@@ -249,7 +249,7 @@
   InstanceType instance_type = map(cage_base).instance_type();
   if (InstanceTypeChecker::IsCode(instance_type)) {
     return i::ToCodeT(GetCode());
-  } else if (V8_REMOVE_BUILTINS_CODE_OBJECTS &&
+  } else if (V8_EXTERNAL_CODE_SPACE_BOOL &&
              InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
     return GetCodeT();
   } else {
@@ -412,19 +412,32 @@
 
 inline Code FromCodeT(CodeT code) {
 #ifdef V8_EXTERNAL_CODE_SPACE
-  DCHECK_IMPLIES(V8_REMOVE_BUILTINS_CODE_OBJECTS,
-                 !code.is_off_heap_trampoline());
-  return code.code();
+  DCHECK(!code.is_off_heap_trampoline());
+  // Compute the Code object pointer from the code entry point.
+  Address ptr = code.code_entry_point() - Code::kHeaderSize + kHeapObjectTag;
+  return Code::cast(Object(ptr));
+#else
+  return code;
+#endif
+}
+
+inline Code FromCodeT(CodeT code, PtrComprCageBase code_cage_base,
+                      RelaxedLoadTag tag) {
+#ifdef V8_EXTERNAL_CODE_SPACE
+  DCHECK(!code.is_off_heap_trampoline());
+  // Since the code entry point field is not aligned we can't load it atomically
+  // and use for Code object pointer calculation. So, we load and decompress
+  // the code field.
+  return code.code(code_cage_base, tag);
 #else
   return code;
 #endif
 }
 
-inline Code FromCodeT(CodeT code, RelaxedLoadTag) {
+inline Code FromCodeT(CodeT code, Isolate* isolate, RelaxedLoadTag tag) {
 #ifdef V8_EXTERNAL_CODE_SPACE
-  DCHECK_IMPLIES(V8_REMOVE_BUILTINS_CODE_OBJECTS,
-                 !code.is_off_heap_trampoline());
-  return code.code(kRelaxedLoad);
+  PtrComprCageBase code_cage_base(isolate->code_cage_base());
+  return FromCodeT(code, code_cage_base, tag);
 #else
   return code;
 #endif
@@ -439,7 +452,7 @@
 }
 
 inline AbstractCode ToAbstractCode(CodeT code) {
-  if (V8_REMOVE_BUILTINS_CODE_OBJECTS) {
+  if (V8_EXTERNAL_CODE_SPACE_BOOL) {
     return AbstractCode::cast(code);
   }
   return AbstractCode::cast(FromCodeT(code));
@@ -447,7 +460,7 @@
 
 inline Handle<AbstractCode> ToAbstractCode(Handle<CodeT> code,
                                            Isolate* isolate) {
-  if (V8_REMOVE_BUILTINS_CODE_OBJECTS) {
+  if (V8_EXTERNAL_CODE_SPACE_BOOL) {
     return Handle<AbstractCode>::cast(code);
   }
   return Handle<AbstractCode>::cast(FromCodeT(code, isolate));
@@ -526,7 +539,7 @@
 
 AbstractCode CodeLookupResult::ToAbstractCode() const {
   DCHECK(IsFound());
-  if (V8_REMOVE_BUILTINS_CODE_OBJECTS) {
+  if (V8_EXTERNAL_CODE_SPACE_BOOL) {
     return IsCodeDataContainer() ? AbstractCode::cast(code_data_container())
                                  : AbstractCode::cast(code());
   }
@@ -810,7 +823,7 @@
 #endif
 
 ByteArray Code::unchecked_relocation_info() const {
-  PtrComprCageBase cage_base = main_cage_base();
+  PtrComprCageBase cage_base = main_cage_base(kRelaxedLoad);
   return ByteArray::unchecked_cast(
       TaggedField<HeapObject, kRelocationInfoOffset>::load(cage_base, *this));
 }
@@ -862,7 +875,7 @@
 bool CodeDataContainer::contains(Isolate* isolate, Address inner_pointer) {
   if (is_off_heap_trampoline()) {
     if (OffHeapBuiltinContains(isolate, inner_pointer)) return true;
-    if (V8_REMOVE_BUILTINS_CODE_OBJECTS) return false;
+    if (V8_EXTERNAL_CODE_SPACE_BOOL) return false;
   }
   return code().contains(isolate, inner_pointer);
 }
@@ -1002,12 +1015,22 @@
 }
 
 inline bool Code::has_tagged_outgoing_params() const {
+#if V8_ENABLE_WEBASSEMBLY
+  return CodeKindHasTaggedOutgoingParams(kind()) &&
+         builtin_id() != Builtin::kWasmCompileLazy;
+#else
   return CodeKindHasTaggedOutgoingParams(kind());
+#endif
 }
 
 #ifdef V8_EXTERNAL_CODE_SPACE
 inline bool CodeDataContainer::has_tagged_outgoing_params() const {
+#if V8_ENABLE_WEBASSEMBLY
+  return CodeKindHasTaggedOutgoingParams(kind()) &&
+         builtin_id() != Builtin::kWasmCompileLazy;
+#else
   return CodeKindHasTaggedOutgoingParams(kind());
+#endif
 }
 #endif
 
@@ -1097,7 +1120,7 @@
 }
 
 inline bool Code::is_off_heap_trampoline() const {
-  if (V8_REMOVE_BUILTINS_CODE_OBJECTS) return false;
+  if (V8_EXTERNAL_CODE_SPACE_BOOL) return false;
   const uint32_t flags = RELAXED_READ_UINT32_FIELD(*this, kFlagsOffset);
   return IsOffHeapTrampoline::decode(flags);
 }
@@ -1418,18 +1441,6 @@
 RELAXED_INT32_ACCESSORS(CodeDataContainer, kind_specific_flags,
                         kKindSpecificFlagsOffset)
 
-#if defined(V8_TARGET_LITTLE_ENDIAN)
-static_assert(!V8_EXTERNAL_CODE_SPACE_BOOL ||
-                  (CodeDataContainer::kCodeCageBaseUpper32BitsOffset ==
-                   CodeDataContainer::kCodeOffset + kTaggedSize),
-              "CodeDataContainer::code field layout requires updating "
-              "for little endian architectures");
-#elif defined(V8_TARGET_BIG_ENDIAN)
-static_assert(!V8_EXTERNAL_CODE_SPACE_BOOL,
-              "CodeDataContainer::code field layout requires updating "
-              "for big endian architectures");
-#endif
-
 Object CodeDataContainer::raw_code() const {
   PtrComprCageBase cage_base = code_cage_base();
   return CodeDataContainer::raw_code(cage_base);
@@ -1454,7 +1465,7 @@
 }
 
 Object CodeDataContainer::raw_code(RelaxedLoadTag tag) const {
-  PtrComprCageBase cage_base = code_cage_base(tag);
+  PtrComprCageBase cage_base = code_cage_base();
   return CodeDataContainer::raw_code(cage_base, tag);
 }
 
@@ -1472,46 +1483,13 @@
 
 PtrComprCageBase CodeDataContainer::code_cage_base() const {
 #ifdef V8_EXTERNAL_CODE_SPACE
-  // TODO(v8:10391): consider protecting this value with the sandbox.
-  Address code_cage_base_hi =
-      ReadField<Tagged_t>(kCodeCageBaseUpper32BitsOffset);
-  return PtrComprCageBase(code_cage_base_hi << 32);
+  Isolate* isolate = GetIsolateFromWritableObject(*this);
+  return PtrComprCageBase(isolate->code_cage_base());
 #else
   return GetPtrComprCageBase(*this);
 #endif
 }
 
-void CodeDataContainer::set_code_cage_base(Address code_cage_base) {
-#ifdef V8_EXTERNAL_CODE_SPACE
-  Tagged_t code_cage_base_hi = static_cast<Tagged_t>(code_cage_base >> 32);
-  WriteField<Tagged_t>(kCodeCageBaseUpper32BitsOffset, code_cage_base_hi);
-#else
-  UNREACHABLE();
-#endif
-}
-
-PtrComprCageBase CodeDataContainer::code_cage_base(RelaxedLoadTag) const {
-#ifdef V8_EXTERNAL_CODE_SPACE
-  // TODO(v8:10391): consider protecting this value with the sandbox.
-  Address code_cage_base_hi =
-      Relaxed_ReadField<Tagged_t>(kCodeCageBaseUpper32BitsOffset);
-  return PtrComprCageBase(code_cage_base_hi << 32);
-#else
-  return GetPtrComprCageBase(*this);
-#endif
-}
-
-void CodeDataContainer::set_code_cage_base(Address code_cage_base,
-                                           RelaxedStoreTag) {
-#ifdef V8_EXTERNAL_CODE_SPACE
-  Tagged_t code_cage_base_hi = static_cast<Tagged_t>(code_cage_base >> 32);
-  Relaxed_WriteField<Tagged_t>(kCodeCageBaseUpper32BitsOffset,
-                               code_cage_base_hi);
-#else
-  UNREACHABLE();
-#endif
-}
-
 Code CodeDataContainer::code() const {
   PtrComprCageBase cage_base = code_cage_base();
   return CodeDataContainer::code(cage_base);
@@ -1519,13 +1497,13 @@
 Code CodeDataContainer::code(PtrComprCageBase cage_base) const {
   CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
 #ifdef V8_EXTERNAL_CODE_SPACE
-  DCHECK_IMPLIES(V8_REMOVE_BUILTINS_CODE_OBJECTS, !is_off_heap_trampoline());
+  DCHECK(!is_off_heap_trampoline());
 #endif
   return Code::cast(raw_code(cage_base));
 }
 
 Code CodeDataContainer::code(RelaxedLoadTag tag) const {
-  PtrComprCageBase cage_base = code_cage_base(tag);
+  PtrComprCageBase cage_base = code_cage_base();
   return CodeDataContainer::code(cage_base, tag);
 }
 
@@ -1558,7 +1536,7 @@
 
 void CodeDataContainer::SetEntryPointForOffHeapBuiltin(
     Isolate* isolate_for_sandbox, Address entry) {
-  CHECK(V8_REMOVE_BUILTINS_CODE_OBJECTS);
+  CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
 #ifdef V8_EXTERNAL_CODE_SPACE
   DCHECK(is_off_heap_trampoline());
 #endif
diff -r -u --color up/v8/src/objects/code.h nw/v8/src/objects/code.h
--- up/v8/src/objects/code.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/code.h	2023-01-19 16:46:36.308942878 -0500
@@ -74,15 +74,11 @@
   // When V8_EXTERNAL_CODE_SPACE is enabled, Code objects are allocated in
   // a separate pointer compression cage instead of the cage where all the
   // other objects are allocated.
-  // This field contains code cage base value which is used for decompressing
-  // the reference to respective Code. Basically, |code_cage_base| and |code|
-  // fields together form a full pointer. The reason why they are split is that
-  // the code field must also support atomic access and the word alignment of
-  // the full value is not guaranteed.
+  // This helper method returns code cage base value which is used for
+  // decompressing the reference to respective Code. It loads the Isolate from
+  // the page header (since the CodeDataContainer objects are always writable)
+  // and then the code cage base value from there.
   inline PtrComprCageBase code_cage_base() const;
-  inline void set_code_cage_base(Address code_cage_base);
-  inline PtrComprCageBase code_cage_base(RelaxedLoadTag) const;
-  inline void set_code_cage_base(Address code_cage_base, RelaxedStoreTag);
 
   // Cached value of code().InstructionStart().
   // Available only when V8_EXTERNAL_CODE_SPACE is defined.
@@ -252,8 +248,6 @@
   V(kCodeOffset, V8_EXTERNAL_CODE_SPACE_BOOL ? kTaggedSize : 0)     \
   V(kCodePointerFieldsStrongEndOffset, 0)                           \
   /* Raw data fields. */                                            \
-  V(kCodeCageBaseUpper32BitsOffset,                                 \
-    V8_EXTERNAL_CODE_SPACE_BOOL ? kTaggedSize : 0)                  \
   V(kCodeEntryPointOffset,                                          \
     V8_EXTERNAL_CODE_SPACE_BOOL ? kSystemPointerSize : 0)           \
   V(kFlagsOffset, V8_EXTERNAL_CODE_SPACE_BOOL ? kUInt16Size : 0)    \
@@ -1016,20 +1010,16 @@
 inline CodeT ToCodeT(Code code);
 inline Handle<CodeT> ToCodeT(Handle<Code> code, Isolate* isolate);
 inline Code FromCodeT(CodeT code);
-inline Code FromCodeT(CodeT code, RelaxedLoadTag);
-inline Code FromCodeT(CodeT code, AcquireLoadTag);
-inline Code FromCodeT(CodeT code, PtrComprCageBase);
+inline Code FromCodeT(CodeT code, Isolate* isolate, RelaxedLoadTag);
 inline Code FromCodeT(CodeT code, PtrComprCageBase, RelaxedLoadTag);
-inline Code FromCodeT(CodeT code, PtrComprCageBase, AcquireLoadTag);
 inline Handle<Code> FromCodeT(Handle<CodeT> code, Isolate* isolate);
 inline AbstractCode ToAbstractCode(CodeT code);
 inline Handle<AbstractCode> ToAbstractCode(Handle<CodeT> code,
                                            Isolate* isolate);
 inline CodeDataContainer CodeDataContainerFromCodeT(CodeT code);
 
-// AbsractCode is an helper wrapper around {Code | BytecodeArray} or
-// {Code | CodeDataContainer | BytecodeArray} depending on whether the
-// V8_REMOVE_BUILTINS_CODE_OBJECTS is disabled or not.
+// AbsractCode is a helper wrapper around {Code|CodeDataContainer|BytecodeArray}
+// when V8_EXTERNAL_CODE_SPACE is enabled or {Code|BytecodeArray} otherwise.
 // Note that when V8_EXTERNAL_CODE_SPACE is enabled then the same abstract code
 // can be represented either by Code object or by respective CodeDataContainer
 // object.
diff -r -u --color up/v8/src/objects/contexts.cc nw/v8/src/objects/contexts.cc
--- up/v8/src/objects/contexts.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/contexts.cc	2023-01-19 16:46:36.308942878 -0500
@@ -276,7 +276,6 @@
         // TODO(v8:5405): Replace this check with a DCHECK when resolution of
         // of synthetic variables does not go through this code path.
         if (ScopeInfo::VariableIsSynthetic(*name)) {
-          DCHECK(context->IsWithContext());
           maybe = Just(ABSENT);
         } else {
           LookupIterator it(isolate, object, name, object);
diff -r -u --color up/v8/src/objects/descriptor-array.h nw/v8/src/objects/descriptor-array.h
--- up/v8/src/objects/descriptor-array.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/descriptor-array.h	2023-01-19 16:46:36.308942878 -0500
@@ -65,10 +65,10 @@
 
   void ClearEnumCache();
   inline void CopyEnumCacheFrom(DescriptorArray array);
-  static void InitializeOrChangeEnumCache(Handle<DescriptorArray> descriptors,
-                                          Isolate* isolate,
-                                          Handle<FixedArray> keys,
-                                          Handle<FixedArray> indices);
+  static void InitializeOrChangeEnumCache(
+      Handle<DescriptorArray> descriptors, Isolate* isolate,
+      Handle<FixedArray> keys, Handle<FixedArray> indices,
+      AllocationType allocation_if_initialize);
 
   // Accessors for fetching instance descriptor at descriptor number.
   inline Name GetKey(InternalIndex descriptor_number) const;
diff -r -u --color up/v8/src/objects/elements-kind.cc nw/v8/src/objects/elements-kind.cc
--- up/v8/src/objects/elements-kind.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/elements-kind.cc	2023-01-19 16:46:36.308942878 -0500
@@ -12,62 +12,65 @@
 namespace v8 {
 namespace internal {
 
-int ElementsKindToShiftSize(ElementsKind elements_kind) {
-  switch (elements_kind) {
-    case UINT8_ELEMENTS:
-    case INT8_ELEMENTS:
-    case UINT8_CLAMPED_ELEMENTS:
-    case RAB_GSAB_UINT8_ELEMENTS:
-    case RAB_GSAB_INT8_ELEMENTS:
-    case RAB_GSAB_UINT8_CLAMPED_ELEMENTS:
+namespace {
+constexpr size_t size_to_shift(size_t size) {
+  switch (size) {
+    case 1:
       return 0;
-    case UINT16_ELEMENTS:
-    case INT16_ELEMENTS:
-    case RAB_GSAB_UINT16_ELEMENTS:
-    case RAB_GSAB_INT16_ELEMENTS:
+    case 2:
       return 1;
-    case UINT32_ELEMENTS:
-    case INT32_ELEMENTS:
-    case FLOAT32_ELEMENTS:
-    case RAB_GSAB_UINT32_ELEMENTS:
-    case RAB_GSAB_INT32_ELEMENTS:
-    case RAB_GSAB_FLOAT32_ELEMENTS:
+    case 4:
       return 2;
-    case PACKED_DOUBLE_ELEMENTS:
-    case HOLEY_DOUBLE_ELEMENTS:
-    case FLOAT64_ELEMENTS:
-    case BIGINT64_ELEMENTS:
-    case BIGUINT64_ELEMENTS:
-    case RAB_GSAB_FLOAT64_ELEMENTS:
-    case RAB_GSAB_BIGINT64_ELEMENTS:
-    case RAB_GSAB_BIGUINT64_ELEMENTS:
+    case 8:
       return 3;
-    case PACKED_SMI_ELEMENTS:
-    case PACKED_ELEMENTS:
-    case PACKED_FROZEN_ELEMENTS:
-    case PACKED_SEALED_ELEMENTS:
-    case PACKED_NONEXTENSIBLE_ELEMENTS:
-    case HOLEY_SMI_ELEMENTS:
-    case HOLEY_ELEMENTS:
-    case HOLEY_FROZEN_ELEMENTS:
-    case HOLEY_SEALED_ELEMENTS:
-    case HOLEY_NONEXTENSIBLE_ELEMENTS:
-    case DICTIONARY_ELEMENTS:
-    case FAST_SLOPPY_ARGUMENTS_ELEMENTS:
-    case SLOW_SLOPPY_ARGUMENTS_ELEMENTS:
-    case FAST_STRING_WRAPPER_ELEMENTS:
-    case SLOW_STRING_WRAPPER_ELEMENTS:
-    case SHARED_ARRAY_ELEMENTS:
-      return kTaggedSizeLog2;
-    case WASM_ARRAY_ELEMENTS:
-    case NO_ELEMENTS:
+    default:
       UNREACHABLE();
   }
-  UNREACHABLE();
 }
+}  // namespace
 
-int ElementsKindToByteSize(ElementsKind elements_kind) {
-  return 1 << ElementsKindToShiftSize(elements_kind);
+constexpr uint8_t kTypedArrayAndRabGsabTypedArrayElementsKindShifts[] = {
+#define SHIFT(Type, type, TYPE, ctype) size_to_shift(sizeof(ctype)),
+    TYPED_ARRAYS(SHIFT) RAB_GSAB_TYPED_ARRAYS(SHIFT)
+#undef SHIFT
+};
+
+constexpr uint8_t kTypedArrayAndRabGsabTypedArrayElementsKindSizes[] = {
+#define SIZE(Type, type, TYPE, ctype) sizeof(ctype),
+    TYPED_ARRAYS(SIZE) RAB_GSAB_TYPED_ARRAYS(SIZE)
+#undef SIZE
+};
+
+#define VERIFY_SHIFT(Type, type, TYPE, ctype)                          \
+  static_assert(                                                       \
+      kTypedArrayAndRabGsabTypedArrayElementsKindShifts                \
+              [ElementsKind::TYPE##_ELEMENTS -                         \
+               ElementsKind::FIRST_FIXED_TYPED_ARRAY_ELEMENTS_KIND] == \
+          ElementsKindToShiftSize(ElementsKind::TYPE##_ELEMENTS),      \
+      "Shift of ElementsKind::" #TYPE                                  \
+      "_ELEMENTS does not match in static table");
+TYPED_ARRAYS(VERIFY_SHIFT)
+RAB_GSAB_TYPED_ARRAYS(VERIFY_SHIFT)
+#undef VERIFY_SHIFT
+
+#define VERIFY_SIZE(Type, type, TYPE, ctype)                           \
+  static_assert(                                                       \
+      kTypedArrayAndRabGsabTypedArrayElementsKindSizes                 \
+              [ElementsKind::TYPE##_ELEMENTS -                         \
+               ElementsKind::FIRST_FIXED_TYPED_ARRAY_ELEMENTS_KIND] == \
+          ElementsKindToByteSize(ElementsKind::TYPE##_ELEMENTS),       \
+      "Size of ElementsKind::" #TYPE                                   \
+      "_ELEMENTS does not match in static table");
+TYPED_ARRAYS(VERIFY_SIZE)
+RAB_GSAB_TYPED_ARRAYS(VERIFY_SIZE)
+#undef VERIFY_SIZE
+
+const uint8_t* TypedArrayAndRabGsabTypedArrayElementsKindShifts() {
+  return &kTypedArrayAndRabGsabTypedArrayElementsKindShifts[0];
+}
+
+const uint8_t* TypedArrayAndRabGsabTypedArrayElementsKindSizes() {
+  return &kTypedArrayAndRabGsabTypedArrayElementsKindSizes[0];
 }
 
 int GetDefaultHeaderSizeForElementsKind(ElementsKind elements_kind) {
diff -r -u --color up/v8/src/objects/elements-kind.h nw/v8/src/objects/elements-kind.h
--- up/v8/src/objects/elements-kind.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/elements-kind.h	2023-01-19 16:46:36.308942878 -0500
@@ -169,8 +169,64 @@
 static_assert((1 << kFastElementsKindBits) > LAST_FAST_ELEMENTS_KIND);
 static_assert((1 << (kFastElementsKindBits - 1)) <= LAST_FAST_ELEMENTS_KIND);
 
-V8_EXPORT_PRIVATE int ElementsKindToShiftSize(ElementsKind elements_kind);
-V8_EXPORT_PRIVATE int ElementsKindToByteSize(ElementsKind elements_kind);
+const uint8_t* TypedArrayAndRabGsabTypedArrayElementsKindShifts();
+const uint8_t* TypedArrayAndRabGsabTypedArrayElementsKindSizes();
+inline constexpr int ElementsKindToShiftSize(ElementsKind elements_kind) {
+  switch (elements_kind) {
+    case UINT8_ELEMENTS:
+    case INT8_ELEMENTS:
+    case UINT8_CLAMPED_ELEMENTS:
+    case RAB_GSAB_UINT8_ELEMENTS:
+    case RAB_GSAB_INT8_ELEMENTS:
+    case RAB_GSAB_UINT8_CLAMPED_ELEMENTS:
+      return 0;
+    case UINT16_ELEMENTS:
+    case INT16_ELEMENTS:
+    case RAB_GSAB_UINT16_ELEMENTS:
+    case RAB_GSAB_INT16_ELEMENTS:
+      return 1;
+    case UINT32_ELEMENTS:
+    case INT32_ELEMENTS:
+    case FLOAT32_ELEMENTS:
+    case RAB_GSAB_UINT32_ELEMENTS:
+    case RAB_GSAB_INT32_ELEMENTS:
+    case RAB_GSAB_FLOAT32_ELEMENTS:
+      return 2;
+    case PACKED_DOUBLE_ELEMENTS:
+    case HOLEY_DOUBLE_ELEMENTS:
+    case FLOAT64_ELEMENTS:
+    case BIGINT64_ELEMENTS:
+    case BIGUINT64_ELEMENTS:
+    case RAB_GSAB_FLOAT64_ELEMENTS:
+    case RAB_GSAB_BIGINT64_ELEMENTS:
+    case RAB_GSAB_BIGUINT64_ELEMENTS:
+      return 3;
+    case PACKED_SMI_ELEMENTS:
+    case PACKED_ELEMENTS:
+    case PACKED_FROZEN_ELEMENTS:
+    case PACKED_SEALED_ELEMENTS:
+    case PACKED_NONEXTENSIBLE_ELEMENTS:
+    case HOLEY_SMI_ELEMENTS:
+    case HOLEY_ELEMENTS:
+    case HOLEY_FROZEN_ELEMENTS:
+    case HOLEY_SEALED_ELEMENTS:
+    case HOLEY_NONEXTENSIBLE_ELEMENTS:
+    case DICTIONARY_ELEMENTS:
+    case FAST_SLOPPY_ARGUMENTS_ELEMENTS:
+    case SLOW_SLOPPY_ARGUMENTS_ELEMENTS:
+    case FAST_STRING_WRAPPER_ELEMENTS:
+    case SLOW_STRING_WRAPPER_ELEMENTS:
+    case SHARED_ARRAY_ELEMENTS:
+      return kTaggedSizeLog2;
+    case WASM_ARRAY_ELEMENTS:
+    case NO_ELEMENTS:
+      CONSTEXPR_UNREACHABLE();
+  }
+  CONSTEXPR_UNREACHABLE();
+}
+inline constexpr int ElementsKindToByteSize(ElementsKind elements_kind) {
+  return 1 << ElementsKindToShiftSize(elements_kind);
+}
 int GetDefaultHeaderSizeForElementsKind(ElementsKind elements_kind);
 const char* ElementsKindToString(ElementsKind kind);
 
diff -r -u --color up/v8/src/objects/elements.cc nw/v8/src/objects/elements.cc
--- up/v8/src/objects/elements.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/elements.cc	2023-01-19 16:46:36.308942878 -0500
@@ -3974,10 +3974,8 @@
       CHECK(!out_of_bounds);
       Handle<JSTypedArray> source_ta = Handle<JSTypedArray>::cast(source);
       ElementsKind source_kind = source_ta->GetElementsKind();
-      bool source_is_bigint =
-          source_kind == BIGINT64_ELEMENTS || source_kind == BIGUINT64_ELEMENTS;
-      bool target_is_bigint =
-          Kind == BIGINT64_ELEMENTS || Kind == BIGUINT64_ELEMENTS;
+      bool source_is_bigint = IsBigIntTypedArrayElementsKind(source_kind);
+      bool target_is_bigint = IsBigIntTypedArrayElementsKind(Kind);
       // If we have to copy more elements than we have in the source, we need to
       // do special handling and conversion; that happens in the slow case.
       if (source_is_bigint == target_is_bigint && !source_ta->WasDetached() &&
diff -r -u --color up/v8/src/objects/feedback-vector.cc nw/v8/src/objects/feedback-vector.cc
--- up/v8/src/objects/feedback-vector.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/feedback-vector.cc	2023-01-19 16:46:36.308942878 -0500
@@ -438,7 +438,7 @@
     return;
   }
 
-  Code code = FromCodeT(CodeT::cast(slot->GetHeapObject()));
+  CodeT code = CodeT::cast(slot->GetHeapObject());
   if (code.marked_for_deoptimization()) {
     Deoptimizer::TraceEvictFromOptimizedCodeCache(shared, reason);
     ClearOptimizedCode();
diff -r -u --color up/v8/src/objects/hash-table-inl.h nw/v8/src/objects/hash-table-inl.h
--- up/v8/src/objects/hash-table-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/hash-table-inl.h	2023-01-19 16:46:36.308942878 -0500
@@ -168,6 +168,7 @@
   uint32_t count = 1;
   Object undefined = roots.undefined_value();
   Object the_hole = roots.the_hole_value();
+  DCHECK_EQ(Shape::Hash(roots, key), static_cast<uint32_t>(hash));
   // EnsureCapacity will guarantee the hash table is never full.
   for (InternalIndex entry = FirstProbe(hash, capacity);;
        entry = NextProbe(entry, count++, capacity)) {
diff -r -u --color up/v8/src/objects/heap-object.h nw/v8/src/objects/heap-object.h
--- up/v8/src/objects/heap-object.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/heap-object.h	2023-01-19 16:46:36.308942878 -0500
@@ -48,6 +48,10 @@
   inline void set_map_no_write_barrier(Map value,
                                        RelaxedStoreTag = kRelaxedStore);
   inline void set_map_no_write_barrier(Map value, ReleaseStoreTag);
+  inline void set_map_safe_transition_no_write_barrier(
+      Map value, RelaxedStoreTag = kRelaxedStore);
+  inline void set_map_safe_transition_no_write_barrier(Map value,
+                                                       ReleaseStoreTag);
 
   // Access the map using acquire load and release store.
   DECL_ACQUIRE_GETTER(map, Map)
diff -r -u --color up/v8/src/objects/instance-type-inl.h nw/v8/src/objects/instance-type-inl.h
--- up/v8/src/objects/instance-type-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/instance-type-inl.h	2023-01-19 16:46:36.308942878 -0500
@@ -81,8 +81,7 @@
 
 V8_INLINE constexpr bool IsAbstractCode(InstanceType instance_type) {
   return IsBytecodeArray(instance_type) || IsCode(instance_type) ||
-         (V8_REMOVE_BUILTINS_CODE_OBJECTS &&
-          IsCodeDataContainer(instance_type));
+         (V8_EXTERNAL_CODE_SPACE_BOOL && IsCodeDataContainer(instance_type));
 }
 
 V8_INLINE constexpr bool IsFreeSpaceOrFiller(InstanceType instance_type) {
diff -r -u --color up/v8/src/objects/instance-type.h nw/v8/src/objects/instance-type.h
--- up/v8/src/objects/instance-type.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/instance-type.h	2023-01-19 16:46:36.308942878 -0500
@@ -315,8 +315,6 @@
   V(_, BreakPointMap, break_point_map, BreakPoint)                             \
   V(_, BreakPointInfoMap, break_point_info_map, BreakPointInfo)                \
   V(_, BytecodeArrayMap, bytecode_array_map, BytecodeArray)                    \
-  V(_, CachedTemplateObjectMap, cached_template_object_map,                    \
-    CachedTemplateObject)                                                      \
   V(_, CellMap, cell_map, Cell)                                                \
   V(_, WeakCellMap, weak_cell_map, WeakCell)                                   \
   V(_, CodeMap, code_map, Code)                                                \
diff -r -u --color up/v8/src/objects/js-array-buffer-inl.h nw/v8/src/objects/js-array-buffer-inl.h
--- up/v8/src/objects/js-array-buffer-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/js-array-buffer-inl.h	2023-01-19 16:46:36.308942878 -0500
@@ -64,7 +64,16 @@
     // BackingStore).
     DCHECK_EQ(0, byte_length());
 
-    return GetBackingStore()->byte_length(std::memory_order_seq_cst);
+    // If the byte length is read after the JSArrayBuffer object is allocated
+    // but before it's attached to the backing store, GetBackingStore returns
+    // nullptr. This is rare, but can happen e.g., when memory measurements
+    // are enabled (via performance.measureMemory()).
+    auto backing_store = GetBackingStore();
+    if (!backing_store) {
+      return 0;
+    }
+
+    return backing_store->byte_length(std::memory_order_seq_cst);
   }
   return byte_length();
 }
@@ -147,6 +156,8 @@
   }
 }
 
+ACCESSORS(JSArrayBuffer, detach_key, Object, kDetachKeyOffset)
+
 void JSArrayBuffer::set_bit_field(uint32_t bits) {
   RELAXED_WRITE_UINT32_FIELD(*this, kBitFieldOffset, bits);
 }
@@ -168,6 +179,8 @@
                     JSArrayBuffer::IsSharedBit)
 BIT_FIELD_ACCESSORS(JSArrayBuffer, bit_field, is_resizable_by_js,
                     JSArrayBuffer::IsResizableByJsBit)
+BIT_FIELD_ACCESSORS(JSArrayBuffer, bit_field, is_node_js,
+                    JSArrayBuffer::IsNodejsBit)
 
 bool JSArrayBuffer::IsEmpty() const {
   auto backing_store = GetBackingStore();
diff -r -u --color up/v8/src/objects/js-array-buffer.cc nw/v8/src/objects/js-array-buffer.cc
--- up/v8/src/objects/js-array-buffer.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/js-array-buffer.cc	2023-01-19 16:46:36.308942878 -0500
@@ -44,8 +44,10 @@
 }  // anonymous namespace
 
 void JSArrayBuffer::Setup(SharedFlag shared, ResizableFlag resizable,
-                          std::shared_ptr<BackingStore> backing_store) {
+                          std::shared_ptr<BackingStore> backing_store,
+                          Isolate* isolate) {
   clear_padding();
+  set_detach_key(ReadOnlyRoots(isolate).undefined_value());
   set_bit_field(0);
   set_is_shared(shared == SharedFlag::kShared);
   set_is_resizable_by_js(resizable == ResizableFlag::kResizable);
@@ -55,14 +57,14 @@
   }
   set_extension(nullptr);
   if (!backing_store) {
-    set_backing_store(GetIsolate(), EmptyBackingStoreBuffer());
+    set_backing_store(isolate, EmptyBackingStoreBuffer());
     set_byte_length(0);
     set_max_byte_length(0);
   } else {
     Attach(std::move(backing_store));
   }
   if (shared == SharedFlag::kShared) {
-    GetIsolate()->CountUsage(
+    isolate->CountUsage(
         v8::Isolate::UseCounterFeature::kSharedArrayBufferConstructed);
   }
 }
@@ -102,17 +104,45 @@
   isolate->heap()->AppendArrayBufferExtension(*this, extension);
 }
 
-void JSArrayBuffer::Detach(bool force_for_wasm_memory) {
-  if (was_detached()) return;
+Maybe<bool> JSArrayBuffer::Detach(Handle<JSArrayBuffer> buffer,
+                                  bool force_for_wasm_memory,
+                                  Handle<Object> maybe_key) {
+  Isolate* const isolate = buffer->GetIsolate();
+
+  Handle<Object> detach_key = handle(buffer->detach_key(), isolate);
+
+  bool key_mismatch = false;
+
+  if (!detach_key->IsUndefined(isolate)) {
+    key_mismatch = maybe_key.is_null() || !maybe_key->StrictEquals(*detach_key);
+  } else {
+    // Detach key is undefined; allow not passing maybe_key but disallow passing
+    // something else than undefined.
+    key_mismatch =
+        !maybe_key.is_null() && !maybe_key->StrictEquals(*detach_key);
+  }
+  if (key_mismatch) {
+    THROW_NEW_ERROR_RETURN_VALUE(
+        isolate,
+        NewTypeError(MessageTemplate::kArrayBufferDetachKeyDoesntMatch),
+        Nothing<bool>());
+  }
+
+  if (buffer->was_detached()) return Just(true);
 
   if (force_for_wasm_memory) {
     // Skip the is_detachable() check.
-  } else if (!is_detachable()) {
+  } else if (!buffer->is_detachable()) {
     // Not detachable, do nothing.
-    return;
+    return Just(true);
   }
 
-  Isolate* const isolate = GetIsolate();
+  buffer->DetachInternal(force_for_wasm_memory, isolate);
+  return Just(true);
+}
+
+void JSArrayBuffer::DetachInternal(bool force_for_wasm_memory,
+                                   Isolate* isolate) {
   ArrayBufferExtension* extension = this->extension();
 
   if (extension) {
@@ -248,7 +278,7 @@
 
   // Attach the backing store to the array buffer.
   array_buffer->Setup(SharedFlag::kNotShared, ResizableFlag::kNotResizable,
-                      std::move(backing_store));
+                      std::move(backing_store), isolate);
 
   // Clear the elements of the typed array.
   self->set_elements(ReadOnlyRoots(isolate).empty_byte_array());
diff -r -u --color up/v8/src/objects/js-array-buffer.h nw/v8/src/objects/js-array-buffer.h
--- up/v8/src/objects/js-array-buffer.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/js-array-buffer.h	2023-01-19 16:46:36.308942878 -0500
@@ -6,6 +6,7 @@
 #define V8_OBJECTS_JS_ARRAY_BUFFER_H_
 
 #include "include/v8-typed-array.h"
+#include "src/handles/maybe-handles.h"
 #include "src/objects/backing-store.h"
 #include "src/objects/js-objects.h"
 #include "torque-generated/bit-fields.h"
@@ -70,6 +71,7 @@
 
   // [was_detached]: true => the buffer was previously detached.
   DECL_BOOLEAN_ACCESSORS(was_detached)
+  DECL_BOOLEAN_ACCESSORS(is_node_js)
 
   // [is_asmjs_memory]: true => this buffer was once used as asm.js memory.
   DECL_BOOLEAN_ACCESSORS(is_asmjs_memory)
@@ -89,11 +91,14 @@
   // An ArrayBuffer with a size greater than zero is never empty.
   DECL_GETTER(IsEmpty, bool)
 
+  DECL_ACCESSORS(detach_key, Object)
+
   // Initializes the fields of the ArrayBuffer. The provided backing_store can
   // be nullptr. If it is not nullptr, then the function registers it with
   // src/heap/array-buffer-tracker.h.
   V8_EXPORT_PRIVATE void Setup(SharedFlag shared, ResizableFlag resizable,
-                               std::shared_ptr<BackingStore> backing_store);
+                               std::shared_ptr<BackingStore> backing_store,
+                               Isolate* isolate);
 
   // Attaches the backing store to an already constructed empty ArrayBuffer.
   // This is intended to be used only in ArrayBufferConstructor builtin.
@@ -108,7 +113,9 @@
   // of growing the underlying memory object. The {force_for_wasm_memory} flag
   // is used by the implementation of Wasm memory growth in order to bypass the
   // non-detachable check.
-  V8_EXPORT_PRIVATE void Detach(bool force_for_wasm_memory = false);
+  V8_EXPORT_PRIVATE V8_WARN_UNUSED_RESULT static Maybe<bool> Detach(
+      Handle<JSArrayBuffer> buffer, bool force_for_wasm_memory = false,
+      Handle<Object> key = Handle<Object>());
 
   // Get a reference to backing store of this array buffer, if there is a
   // backing store. Returns nullptr if there is no backing store (e.g. detached
@@ -151,7 +158,7 @@
   DECL_PRINTER(JSArrayBuffer)
   DECL_VERIFIER(JSArrayBuffer)
 
-  static constexpr int kEndOfTaggedFieldsOffset = JSObject::kHeaderSize;
+  static constexpr int kEndOfTaggedFieldsOffset = kRawByteLengthOffset;
 
   static const int kSizeWithEmbedderFields =
       kHeaderSize +
@@ -160,6 +167,8 @@
   class BodyDescriptor;
 
  private:
+  void DetachInternal(bool force_for_wasm_memory, Isolate* isolate);
+
   inline ArrayBufferExtension** extension_location() const;
 
 #if V8_COMPRESS_POINTERS
diff -r -u --color up/v8/src/objects/js-array-buffer.tq nw/v8/src/objects/js-array-buffer.tq
--- up/v8/src/objects/js-array-buffer.tq	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/js-array-buffer.tq	2023-01-19 16:46:36.308942878 -0500
@@ -9,9 +9,11 @@
   is_asm_js_memory: bool: 1 bit;
   is_shared: bool: 1 bit;
   is_resizable_by_js: bool: 1 bit;
+  is_nodejs: bool: 1 bit;
 }
 
 extern class JSArrayBuffer extends JSObjectWithEmbedderSlots {
+  detach_key: Object;
   // A BoundedSize if the sandbox is enabled
   raw_byte_length: uintptr;
   // A BoundedSize if the sandbox is enabled
diff -r -u --color up/v8/src/objects/js-array-inl.h nw/v8/src/objects/js-array-inl.h
--- up/v8/src/objects/js-array-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/js-array-inl.h	2023-01-19 16:46:36.308942878 -0500
@@ -19,6 +19,7 @@
 
 TQ_OBJECT_CONSTRUCTORS_IMPL(JSArray)
 TQ_OBJECT_CONSTRUCTORS_IMPL(JSArrayIterator)
+TQ_OBJECT_CONSTRUCTORS_IMPL(TemplateLiteralObject)
 
 DEF_GETTER(JSArray, length, Object) {
   return TaggedField<Object, kLengthOffset>::load(cage_base, *this);
@@ -73,13 +74,7 @@
   set_raw_kind(static_cast<int>(kind));
 }
 
-// static
-void TemplateLiteralObject::SetRaw(Handle<JSArray> template_object,
-                                   Handle<JSArray> raw_object) {
-  TaggedField<Object, kRawFieldOffset>::store(*template_object, *raw_object);
-  CONDITIONAL_WRITE_BARRIER(*template_object, kRawFieldOffset, *raw_object,
-                            UPDATE_WRITE_BARRIER);
-}
+CAST_ACCESSOR(TemplateLiteralObject)
 
 }  // namespace internal
 }  // namespace v8
diff -r -u --color up/v8/src/objects/js-array.h nw/v8/src/objects/js-array.h
--- up/v8/src/objects/js-array.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/js-array.h	2023-01-19 16:46:36.308942878 -0500
@@ -160,11 +160,14 @@
 };
 
 // Helper class for JSArrays that are template literal objects
-class TemplateLiteralObject {
+class TemplateLiteralObject
+    : public TorqueGeneratedTemplateLiteralObject<TemplateLiteralObject,
+                                                  JSArray> {
  public:
-  static const int kRawFieldOffset = JSArray::kLengthOffset + kTaggedSize;
-  static inline void SetRaw(Handle<JSArray> template_object,
-                            Handle<JSArray> raw_object);
+  DECL_CAST(TemplateLiteralObject)
+
+ private:
+  TQ_OBJECT_CONSTRUCTORS(TemplateLiteralObject)
 };
 
 }  // namespace internal
diff -r -u --color up/v8/src/objects/js-array.tq nw/v8/src/objects/js-array.tq
--- up/v8/src/objects/js-array.tq	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/js-array.tq	2023-01-19 16:46:36.308942878 -0500
@@ -56,6 +56,14 @@
   length: Number;
 }
 
+@hasSameInstanceTypeAsParent
+@doNotGenerateCast
+extern class TemplateLiteralObject extends JSArray {
+  raw: JSArray;
+  function_literal_id: Smi;
+  slot_id: Smi;
+}
+
 @doNotGenerateCast
 extern class JSArrayConstructor extends JSFunction
     generates 'TNode<JSFunction>';
diff -r -u --color up/v8/src/objects/js-function.cc nw/v8/src/objects/js-function.cc
--- up/v8/src/objects/js-function.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/js-function.cc	2023-01-19 16:46:36.319776208 -0500
@@ -6,6 +6,7 @@
 
 #include "src/baseline/baseline-batch-compiler.h"
 #include "src/codegen/compiler.h"
+#include "src/common/globals.h"
 #include "src/diagnostics/code-tracer.h"
 #include "src/execution/isolate.h"
 #include "src/execution/tiering-manager.h"
@@ -1241,6 +1242,13 @@
     return NativeCodeFunctionSourceString(shared_info);
   }
 
+  //NWJS#6061: moved here or it will crash when trying to print
+  //function as a class
+  // Check if we have source code for the {function}.
+  if (!shared_info->HasSourceCode()) {
+    return NativeCodeFunctionSourceString(shared_info);
+  }
+
   // Check if we should print {function} as a class.
   Handle<Object> maybe_class_positions = JSReceiver::GetDataProperty(
       isolate, function, isolate->factory()->class_positions_symbol());
@@ -1255,11 +1263,6 @@
                                             end_position);
   }
 
-  // Check if we have source code for the {function}.
-  if (!shared_info->HasSourceCode()) {
-    return NativeCodeFunctionSourceString(shared_info);
-  }
-
   // If this function was compiled from asm.js, use the recorded offset
   // information.
 #if V8_ENABLE_WEBASSEMBLY
diff -r -u --color up/v8/src/objects/js-regexp.h nw/v8/src/objects/js-regexp.h
--- up/v8/src/objects/js-regexp.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/js-regexp.h	2023-01-19 16:46:36.319776208 -0500
@@ -100,7 +100,7 @@
       return {};
     }
     if (f.value() == RegExpFlag::kUnicodeSets &&
-        !FLAG_harmony_regexp_unicode_sets) {
+        !v8_flags.harmony_regexp_unicode_sets) {
       return {};
     }
     return f;
diff -r -u --color up/v8/src/objects/js-temporal-objects.cc nw/v8/src/objects/js-temporal-objects.cc
--- up/v8/src/objects/js-temporal-objects.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/js-temporal-objects.cc	2023-01-19 16:46:36.319776208 -0500
@@ -4610,13 +4610,12 @@
 
 DEFINE_LAZY_LEAKY_OBJECT_GETTER(CalendarMap, GetCalendarMap)
 
-// #sec-temporal-isbuiltincalendar
-bool IsBuiltinCalendar(Isolate* isolate, const std::string& id) {
-  return GetCalendarMap()->Contains(id);
-}
-
 bool IsBuiltinCalendar(Isolate* isolate, Handle<String> id) {
-  return IsBuiltinCalendar(isolate, id->ToCString().get());
+  // 1. Let calendars be AvailableCalendars().
+  // 2. If calendars contains the ASCII-lowercase of id, return true.
+  // 3. Return false.
+  id = Intl::ConvertToLower(isolate, id).ToHandleChecked();
+  return GetCalendarMap()->Contains(id->ToCString().get());
 }
 
 Handle<String> CalendarIdentifier(Isolate* isolate, int32_t index) {
@@ -4625,6 +4624,7 @@
 }
 
 int32_t CalendarIndex(Isolate* isolate, Handle<String> id) {
+  id = Intl::ConvertToLower(isolate, id).ToHandleChecked();
   return GetCalendarMap()->Index(id->ToCString().get());
 }
 
@@ -4645,9 +4645,24 @@
 
 // #sec-temporal-isbuiltincalendar
 bool IsBuiltinCalendar(Isolate* isolate, Handle<String> id) {
-  // 1. If id is not "iso8601", return false.
-  // 2. Return true
-  return isolate->factory()->iso8601_string()->Equals(*id);
+  // Note: For build without intl support, the only item in AvailableCalendars()
+  // is "iso8601".
+  // 1. Let calendars be AvailableCalendars().
+  // 2. If calendars contains the ASCII-lowercase of id, return true.
+  // 3. Return false.
+
+  // Fast path
+  if (isolate->factory()->iso8601_string()->Equals(*id)) return true;
+  if (id->length() != 7) return false;
+  id = String::Flatten(isolate, id);
+
+  DisallowGarbageCollection no_gc;
+  const String::FlatContent& flat = id->GetFlatContent(no_gc);
+  // Return true if id is case insensitive equals to "iso8601".
+  return AsciiAlphaToLower(flat.Get(0)) == 'i' &&
+         AsciiAlphaToLower(flat.Get(1)) == 's' &&
+         AsciiAlphaToLower(flat.Get(2)) == 'o' && flat.Get(3) == '8' &&
+         flat.Get(4) == '6' && flat.Get(5) == '0' && flat.Get(6) == '1';
 }
 
 int32_t CalendarIndex(Isolate* isolate, Handle<String> id) { return 0; }
diff -r -u --color up/v8/src/objects/keys.cc nw/v8/src/objects/keys.cc
--- up/v8/src/objects/keys.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/keys.cc	2023-01-19 16:46:36.319776208 -0500
@@ -312,7 +312,7 @@
   map.SetEnumLength(0);
 }
 
-bool CheckAndInitalizeEmptyEnumCache(JSReceiver object) {
+bool CheckAndInitializeEmptyEnumCache(JSReceiver object) {
   if (object.map().EnumLength() == kInvalidEnumCacheSentinel) {
     TrySettingEmptyEnumCache(object);
   }
@@ -342,7 +342,7 @@
         only_own_has_simple_elements_ = false;
       }
     }
-    bool has_no_properties = CheckAndInitalizeEmptyEnumCache(current);
+    bool has_no_properties = CheckAndInitializeEmptyEnumCache(current);
     if (has_no_properties) continue;
     last_prototype = current;
     has_empty_prototype_ = false;
@@ -368,7 +368,7 @@
   return isolate->factory()->CopyFixedArrayUpTo(array, length);
 }
 
-// Initializes and directly returns the enume cache. Users of this function
+// Initializes and directly returns the enum cache. Users of this function
 // have to make sure to never directly leak the enum cache.
 Handle<FixedArray> GetFastEnumPropertyKeys(Isolate* isolate,
                                            Handle<JSObject> object) {
@@ -398,51 +398,8 @@
     return ReduceFixedArrayTo(isolate, keys, enum_length);
   }
 
-  Handle<DescriptorArray> descriptors =
-      Handle<DescriptorArray>(map->instance_descriptors(isolate), isolate);
-  isolate->counters()->enum_cache_misses()->Increment();
-
-  // Create the keys array.
-  int index = 0;
-  bool fields_only = true;
-  keys = isolate->factory()->NewFixedArray(enum_length);
-  for (InternalIndex i : map->IterateOwnDescriptors()) {
-    DisallowGarbageCollection no_gc;
-    PropertyDetails details = descriptors->GetDetails(i);
-    if (details.IsDontEnum()) continue;
-    Object key = descriptors->GetKey(i);
-    if (key.IsSymbol()) continue;
-    keys->set(index, key);
-    if (details.location() != PropertyLocation::kField) fields_only = false;
-    index++;
-  }
-  DCHECK_EQ(index, keys->length());
-
-  // Optionally also create the indices array.
-  Handle<FixedArray> indices = isolate->factory()->empty_fixed_array();
-  if (fields_only) {
-    indices = isolate->factory()->NewFixedArray(enum_length);
-    index = 0;
-    for (InternalIndex i : map->IterateOwnDescriptors()) {
-      DisallowGarbageCollection no_gc;
-      PropertyDetails details = descriptors->GetDetails(i);
-      if (details.IsDontEnum()) continue;
-      Object key = descriptors->GetKey(i);
-      if (key.IsSymbol()) continue;
-      DCHECK_EQ(PropertyKind::kData, details.kind());
-      DCHECK_EQ(PropertyLocation::kField, details.location());
-      FieldIndex field_index = FieldIndex::ForDescriptor(*map, i);
-      indices->set(index, Smi::FromInt(field_index.GetLoadByFieldIndex()));
-      index++;
-    }
-    DCHECK_EQ(index, indices->length());
-  }
-
-  DescriptorArray::InitializeOrChangeEnumCache(descriptors, isolate, keys,
-                                               indices);
-  if (map->OnlyHasSimpleProperties()) map->SetEnumLength(enum_length);
-
-  return keys;
+  return FastKeyAccumulator::InitializeFastPropertyEnumCache(isolate, map,
+                                                             enum_length);
 }
 
 template <bool fast_properties>
@@ -451,7 +408,6 @@
                                                GetKeysConversion convert,
                                                bool skip_indices) {
   Handle<FixedArray> keys;
-  ElementsAccessor* accessor = object->GetElementsAccessor();
   if (fast_properties) {
     keys = GetFastEnumPropertyKeys(isolate, object);
   } else {
@@ -463,6 +419,7 @@
   if (skip_indices) {
     result = keys;
   } else {
+    ElementsAccessor* accessor = object->GetElementsAccessor(isolate);
     result = accessor->PrependElementIndices(isolate, object, keys, convert,
                                              ONLY_ENUMERABLE);
   }
@@ -518,7 +475,7 @@
   if (enum_length == kInvalidEnumCacheSentinel) {
     Handle<FixedArray> keys;
     // Try initializing the enum cache and return own properties.
-    if (GetOwnKeysWithUninitializedEnumCache().ToHandle(&keys)) {
+    if (GetOwnKeysWithUninitializedEnumLength().ToHandle(&keys)) {
       if (v8_flags.trace_for_in_enumerate) {
         PrintF("| strings=%d symbols=0 elements=0 || prototypes>=1 ||\n",
                keys->length());
@@ -534,10 +491,70 @@
                                       skip_indices_);
 }
 
+// static
+Handle<FixedArray> FastKeyAccumulator::InitializeFastPropertyEnumCache(
+    Isolate* isolate, Handle<Map> map, int enum_length,
+    AllocationType allocation) {
+  DCHECK_EQ(kInvalidEnumCacheSentinel, map->EnumLength());
+  DCHECK_GT(enum_length, 0);
+  DCHECK_EQ(enum_length, map->NumberOfEnumerableProperties());
+  DCHECK(!map->is_dictionary_map());
+
+  Handle<DescriptorArray> descriptors =
+      Handle<DescriptorArray>(map->instance_descriptors(isolate), isolate);
+
+  // The enum cache should have been a hit if the number of enumerable
+  // properties is fewer than what's already in the cache.
+  DCHECK_LT(descriptors->enum_cache().keys().length(), enum_length);
+  isolate->counters()->enum_cache_misses()->Increment();
+
+  // Create the keys array.
+  int index = 0;
+  bool fields_only = true;
+  Handle<FixedArray> keys =
+      isolate->factory()->NewFixedArray(enum_length, allocation);
+  for (InternalIndex i : map->IterateOwnDescriptors()) {
+    DisallowGarbageCollection no_gc;
+    PropertyDetails details = descriptors->GetDetails(i);
+    if (details.IsDontEnum()) continue;
+    Object key = descriptors->GetKey(i);
+    if (key.IsSymbol()) continue;
+    keys->set(index, key);
+    if (details.location() != PropertyLocation::kField) fields_only = false;
+    index++;
+  }
+  DCHECK_EQ(index, keys->length());
+
+  // Optionally also create the indices array.
+  Handle<FixedArray> indices = isolate->factory()->empty_fixed_array();
+  if (fields_only) {
+    indices = isolate->factory()->NewFixedArray(enum_length, allocation);
+    index = 0;
+    for (InternalIndex i : map->IterateOwnDescriptors()) {
+      DisallowGarbageCollection no_gc;
+      PropertyDetails details = descriptors->GetDetails(i);
+      if (details.IsDontEnum()) continue;
+      Object key = descriptors->GetKey(i);
+      if (key.IsSymbol()) continue;
+      DCHECK_EQ(PropertyKind::kData, details.kind());
+      DCHECK_EQ(PropertyLocation::kField, details.location());
+      FieldIndex field_index = FieldIndex::ForDescriptor(*map, i);
+      indices->set(index, Smi::FromInt(field_index.GetLoadByFieldIndex()));
+      index++;
+    }
+    DCHECK_EQ(index, indices->length());
+  }
+
+  DescriptorArray::InitializeOrChangeEnumCache(descriptors, isolate, keys,
+                                               indices, allocation);
+  if (map->OnlyHasSimpleProperties()) map->SetEnumLength(enum_length);
+  return keys;
+}
+
 MaybeHandle<FixedArray>
-FastKeyAccumulator::GetOwnKeysWithUninitializedEnumCache() {
+FastKeyAccumulator::GetOwnKeysWithUninitializedEnumLength() {
   Handle<JSObject> object = Handle<JSObject>::cast(receiver_);
-  // Uninitalized enum cache
+  // Uninitialized enum length
   Map map = object->map();
   if (object->elements() != ReadOnlyRoots(isolate_).empty_fixed_array() &&
       object->elements() !=
diff -r -u --color up/v8/src/objects/keys.h nw/v8/src/objects/keys.h
--- up/v8/src/objects/keys.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/keys.h	2023-01-19 16:46:36.319776208 -0500
@@ -174,7 +174,7 @@
 };
 
 // The FastKeyAccumulator handles the cases where there are no elements on the
-// prototype chain and forwords the complex/slow cases to the normal
+// prototype chain and forwards the complex/slow cases to the normal
 // KeyAccumulator. This significantly speeds up the cases where the OWN_ONLY
 // case where we do not have to walk the prototype chain.
 class FastKeyAccumulator {
@@ -200,6 +200,19 @@
   MaybeHandle<FixedArray> GetKeys(
       GetKeysConversion convert = GetKeysConversion::kKeepNumbers);
 
+  // Initialize the the enum cache for a map with all of the following:
+  //   - uninitialized enum length
+  //   - fast properties (i.e. !is_dictionary_map())
+  //   - has >0 enumerable own properties
+  //
+  // The number of enumerable properties is passed in as an optimization, for
+  // when the caller has already computed it.
+  //
+  // Returns the keys.
+  static Handle<FixedArray> InitializeFastPropertyEnumCache(
+      Isolate* isolate, Handle<Map> map, int enum_length,
+      AllocationType allocation = AllocationType::kOld);
+
  private:
   void Prepare();
   MaybeHandle<FixedArray> GetKeysFast(GetKeysConversion convert);
@@ -207,7 +220,7 @@
   MaybeHandle<FixedArray> GetKeysWithPrototypeInfoCache(
       GetKeysConversion convert);
 
-  MaybeHandle<FixedArray> GetOwnKeysWithUninitializedEnumCache();
+  MaybeHandle<FixedArray> GetOwnKeysWithUninitializedEnumLength();
 
   bool MayHaveElements(JSReceiver receiver);
   bool TryPrototypeInfoCache(Handle<JSReceiver> receiver);
diff -r -u --color up/v8/src/objects/lookup.cc nw/v8/src/objects/lookup.cc
--- up/v8/src/objects/lookup.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/lookup.cc	2023-01-19 16:46:36.319776208 -0500
@@ -1496,8 +1496,8 @@
   // TODO(jgruber): Support other string kinds.
   Map string_map = string.map(isolate, kAcquireLoad);
   InstanceType type = string_map.instance_type();
-  if (!(InstanceTypeChecker::IsInternalizedString(type)) ||
-      InstanceTypeChecker::IsThinString(type)) {
+  if (!(InstanceTypeChecker::IsInternalizedString(type) ||
+        InstanceTypeChecker::IsThinString(type))) {
     return kGaveUp;
   }
 
diff -r -u --color up/v8/src/objects/object-list-macros.h nw/v8/src/objects/object-list-macros.h
--- up/v8/src/objects/object-list-macros.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/object-list-macros.h	2023-01-19 16:46:36.330609538 -0500
@@ -15,7 +15,6 @@
 class AccessCheckInfo;
 class AllocationSite;
 class ByteArray;
-class CachedTemplateObject;
 class Cell;
 class ClosureFeedbackCellArray;
 class ConsString;
@@ -238,6 +237,7 @@
   V(SyntheticModule)                            \
   V(TemplateInfo)                               \
   V(TemplateList)                               \
+  V(TemplateLiteralObject)                      \
   V(ThinString)                                 \
   V(TransitionArray)                            \
   V(UncompiledData)                             \
diff -r -u --color up/v8/src/objects/objects-body-descriptors-inl.h nw/v8/src/objects/objects-body-descriptors-inl.h
--- up/v8/src/objects/objects-body-descriptors-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/objects-body-descriptors-inl.h	2023-01-19 16:46:36.330609538 -0500
@@ -32,7 +32,7 @@
 #include "src/objects/source-text-module.h"
 #include "src/objects/swiss-name-dictionary-inl.h"
 #include "src/objects/synthetic-module.h"
-#include "src/objects/template-objects.h"
+#include "src/objects/template-objects-inl.h"
 #include "src/objects/torque-defined-classes-inl.h"
 #include "src/objects/transitions.h"
 #include "src/objects/turbofan-types-inl.h"
diff -r -u --color up/v8/src/objects/objects-definitions.h nw/v8/src/objects/objects-definitions.h
--- up/v8/src/objects/objects-definitions.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/objects-definitions.h	2023-01-19 16:46:36.330609538 -0500
@@ -152,8 +152,6 @@
     async_generator_request)                                                   \
   V(_, BREAK_POINT_TYPE, BreakPoint, break_point)                              \
   V(_, BREAK_POINT_INFO_TYPE, BreakPointInfo, break_point_info)                \
-  V(_, CACHED_TEMPLATE_OBJECT_TYPE, CachedTemplateObject,                      \
-    cached_template_object)                                                    \
   V(_, CALL_SITE_INFO_TYPE, CallSiteInfo, call_site_info)                      \
   V(_, CLASS_POSITIONS_TYPE, ClassPositions, class_positions)                  \
   V(_, DEBUG_INFO_TYPE, DebugInfo, debug_info)                                 \
diff -r -u --color up/v8/src/objects/objects-inl.h nw/v8/src/objects/objects-inl.h
--- up/v8/src/objects/objects-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/objects-inl.h	2023-01-19 16:46:36.330609538 -0500
@@ -43,6 +43,7 @@
 #include "src/objects/tagged-impl-inl.h"
 #include "src/objects/tagged-index.h"
 #include "src/objects/templates.h"
+#include "src/roots/roots.h"
 #include "src/sandbox/bounded-size-inl.h"
 #include "src/sandbox/external-pointer-inl.h"
 #include "src/sandbox/sandboxed-pointer-inl.h"
@@ -263,26 +264,22 @@
 
 DEF_GETTER(HeapObject, IsSeqOneByteString, bool) {
   if (!IsString(cage_base)) return false;
-  return StringShape(String::cast(*this).map(cage_base)).IsSequential() &&
-         String::cast(*this).IsOneByteRepresentation(cage_base);
+  return StringShape(String::cast(*this).map(cage_base)).IsSequentialOneByte();
 }
 
 DEF_GETTER(HeapObject, IsSeqTwoByteString, bool) {
   if (!IsString(cage_base)) return false;
-  return StringShape(String::cast(*this).map(cage_base)).IsSequential() &&
-         String::cast(*this).IsTwoByteRepresentation(cage_base);
+  return StringShape(String::cast(*this).map(cage_base)).IsSequentialTwoByte();
 }
 
 DEF_GETTER(HeapObject, IsExternalOneByteString, bool) {
   if (!IsString(cage_base)) return false;
-  return StringShape(String::cast(*this).map(cage_base)).IsExternal() &&
-         String::cast(*this).IsOneByteRepresentation(cage_base);
+  return StringShape(String::cast(*this).map(cage_base)).IsExternalOneByte();
 }
 
 DEF_GETTER(HeapObject, IsExternalTwoByteString, bool) {
   if (!IsString(cage_base)) return false;
-  return StringShape(String::cast(*this).map(cage_base)).IsExternal() &&
-         String::cast(*this).IsTwoByteRepresentation(cage_base);
+  return StringShape(String::cast(*this).map(cage_base)).IsExternalTwoByte();
 }
 
 bool Object::IsNumber() const {
@@ -308,6 +305,10 @@
   return IsNumber(cage_base) || IsBigInt(cage_base);
 }
 
+DEF_GETTER(HeapObject, IsTemplateLiteralObject, bool) {
+  return IsJSArray(cage_base);
+}
+
 DEF_GETTER(HeapObject, IsArrayList, bool) {
   return map(cage_base) ==
          GetReadOnlyRoots(cage_base).unchecked_array_list_map();
@@ -821,6 +822,18 @@
                                   VerificationMode::kSafeMapTransition);
 }
 
+void HeapObject::set_map_safe_transition_no_write_barrier(Map value,
+                                                          RelaxedStoreTag tag) {
+  set_map<EmitWriteBarrier::kNo>(value, kRelaxedStore,
+                                 VerificationMode::kSafeMapTransition);
+}
+
+void HeapObject::set_map_safe_transition_no_write_barrier(Map value,
+                                                          ReleaseStoreTag tag) {
+  set_map<EmitWriteBarrier::kNo>(value, kReleaseStore,
+                                 VerificationMode::kSafeMapTransition);
+}
+
 // Unsafe accessor omitting write barrier.
 void HeapObject::set_map_no_write_barrier(Map value, RelaxedStoreTag tag) {
   set_map<EmitWriteBarrier::kNo>(value, kRelaxedStore,
@@ -1152,6 +1165,9 @@
   } else if (InstanceTypeChecker::IsScopeInfo(instance_type)) {
     uint32_t hash = ScopeInfo::cast(object).Hash();
     return Smi::FromInt(hash & Smi::kMaxValue);
+  } else if (InstanceTypeChecker::IsScript(instance_type)) {
+    int id = Script::cast(object).id();
+    return Smi::FromInt(ComputeUnseededHash(id) & Smi::kMaxValue);
   }
   DCHECK(object.IsJSReceiver());
   return object;
diff -r -u --color up/v8/src/objects/objects.cc nw/v8/src/objects/objects.cc
--- up/v8/src/objects/objects.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/objects.cc	2023-01-19 16:46:36.330609538 -0500
@@ -2802,19 +2802,22 @@
   if (it->IsElement() && receiver->IsJSObject(isolate) &&
       JSObject::cast(*receiver).HasTypedArrayOrRabGsabTypedArrayElements(
           isolate)) {
+    auto receiver_ta = Handle<JSTypedArray>::cast(receiver);
     ElementsKind elements_kind = JSObject::cast(*receiver).GetElementsKind();
     if (IsBigIntTypedArrayElementsKind(elements_kind)) {
       ASSIGN_RETURN_ON_EXCEPTION_VALUE(isolate, to_assign,
                                        BigInt::FromObject(isolate, value),
                                        Nothing<bool>());
-      if (Handle<JSTypedArray>::cast(receiver)->IsDetachedOrOutOfBounds()) {
+      if (V8_UNLIKELY(receiver_ta->IsDetachedOrOutOfBounds() ||
+                      it->index() >= receiver_ta->GetLength())) {
         return Just(true);
       }
     } else if (!value->IsNumber() && !value->IsUndefined(isolate)) {
       ASSIGN_RETURN_ON_EXCEPTION_VALUE(isolate, to_assign,
                                        Object::ToNumber(isolate, value),
                                        Nothing<bool>());
-      if (Handle<JSTypedArray>::cast(receiver)->IsDetachedOrOutOfBounds()) {
+      if (V8_UNLIKELY(receiver_ta->IsDetachedOrOutOfBounds() ||
+                      it->index() >= receiver_ta->GetLength())) {
         return Just(true);
       }
     }
@@ -4449,10 +4452,12 @@
 // static
 void DescriptorArray::InitializeOrChangeEnumCache(
     Handle<DescriptorArray> descriptors, Isolate* isolate,
-    Handle<FixedArray> keys, Handle<FixedArray> indices) {
+    Handle<FixedArray> keys, Handle<FixedArray> indices,
+    AllocationType allocation_if_initialize) {
   EnumCache enum_cache = descriptors->enum_cache();
   if (enum_cache == ReadOnlyRoots(isolate).empty_enum_cache()) {
-    enum_cache = *isolate->factory()->NewEnumCache(keys, indices);
+    enum_cache = *isolate->factory()->NewEnumCache(keys, indices,
+                                                   allocation_if_initialize);
     descriptors->set_enum_cache(enum_cache);
   } else {
     enum_cache.set_keys(*keys);
diff -r -u --color up/v8/src/objects/objects.h nw/v8/src/objects/objects.h
--- up/v8/src/objects/objects.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/objects.h	2023-01-19 16:46:36.330609538 -0500
@@ -44,6 +44,7 @@
 //     - JSReceiver  (suitable for property access)
 //       - JSObject
 //         - JSArray
+//           - TemplateLiteralObject
 //         - JSArrayBuffer
 //         - JSArrayBufferView
 //           - JSTypedArray
@@ -181,7 +182,6 @@
 //       - DebugInfo
 //       - BreakPoint
 //       - BreakPointInfo
-//       - CachedTemplateObject
 //       - CallSiteInfo
 //       - CodeCache
 //       - PropertyDescriptorObject
diff -r -u --color up/v8/src/objects/scope-info.cc nw/v8/src/objects/scope-info.cc
--- up/v8/src/objects/scope-info.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/scope-info.cc	2023-01-19 16:46:36.330609538 -0500
@@ -22,11 +22,10 @@
 namespace internal {
 
 #ifdef DEBUG
-bool ScopeInfo::Equals(ScopeInfo other,
-                       bool ignore_position_and_module_info) const {
+bool ScopeInfo::Equals(ScopeInfo other, bool is_live_edit_compare) const {
   if (length() != other.length()) return false;
   for (int index = 0; index < length(); ++index) {
-    if (ignore_position_and_module_info && HasPositionInfo() &&
+    if (is_live_edit_compare && HasPositionInfo() &&
         index >= PositionInfoIndex() && index <= PositionInfoIndex() + 1) {
       continue;
     }
@@ -44,12 +43,12 @@
           return false;
         }
       } else if (entry.IsScopeInfo()) {
-        if (!ScopeInfo::cast(entry).Equals(ScopeInfo::cast(other_entry),
-                                           ignore_position_and_module_info)) {
+        if (!is_live_edit_compare && !ScopeInfo::cast(entry).Equals(
+                                         ScopeInfo::cast(other_entry), false)) {
           return false;
         }
       } else if (entry.IsSourceTextModuleInfo()) {
-        if (!ignore_position_and_module_info &&
+        if (!is_live_edit_compare &&
             !SourceTextModuleInfo::cast(entry).Equals(
                 SourceTextModuleInfo::cast(other_entry))) {
           return false;
diff -r -u --color up/v8/src/objects/scope-info.h nw/v8/src/objects/scope-info.h
--- up/v8/src/objects/scope-info.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/scope-info.h	2023-01-19 16:46:36.330609538 -0500
@@ -253,8 +253,14 @@
   bool IsReplModeScope() const;
 
 #ifdef DEBUG
-  bool Equals(ScopeInfo other,
-              bool ignore_position_and_module_info = false) const;
+  // For LiveEdit we ignore:
+  //   - position info: "unchanged" functions are allowed to move in a script
+  //   - module info: SourceTextModuleInfo::Equals compares exact FixedArray
+  //     addresses which will never match for separate instances.
+  //   - outer scope info: LiveEdit already analyses outer scopes of unchanged
+  //     functions. Also checking it here will break in really subtle cases
+  //     e.g. changing a let to a const in an outer function, which is fine.
+  bool Equals(ScopeInfo other, bool is_live_edit_compare = false) const;
 #endif
 
   template <typename IsolateT>
diff -r -u --color up/v8/src/objects/shared-function-info-inl.h nw/v8/src/objects/shared-function-info-inl.h
--- up/v8/src/objects/shared-function-info-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/shared-function-info-inl.h	2023-01-19 16:46:36.330609538 -0500
@@ -637,6 +637,13 @@
   }
   if (!data.IsBytecodeArray()) return false;
 
+  Object script_obj = script();
+  if (!script_obj.IsUndefined()) {
+    Script script = Script::cast(script_obj);
+    if (script.source().IsUndefined())
+      return false;
+  }
+
   if (IsStressFlushingEnabled(code_flush_mode)) return true;
 
   BytecodeArray bytecode = BytecodeArray::cast(data);
diff -r -u --color up/v8/src/objects/shared-function-info.cc nw/v8/src/objects/shared-function-info.cc
--- up/v8/src/objects/shared-function-info.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/shared-function-info.cc	2023-01-19 16:46:36.330609538 -0500
@@ -347,7 +347,7 @@
     std::function<void(HeapObject object, ObjectSlot slot, HeapObject target)>
         gc_notify_updated_slot) {
   DisallowGarbageCollection no_gc;
-  if (is_compiled()) {
+  if (HasFeedbackMetadata()) {
     if (v8_flags.trace_flush_bytecode) {
       CodeTracer::Scope scope(GetIsolate()->GetCodeTracer());
       PrintF(scope.file(), "[discarding compiled metadata for ");
@@ -386,6 +386,17 @@
   int start_position = shared_info->StartPosition();
   int end_position = shared_info->EndPosition();
 
+  MaybeHandle<UncompiledData> data;
+  if (!shared_info->HasUncompiledDataWithPreparseData()) {
+    // Create a new UncompiledData, without pre-parsed scope.
+    data = isolate->factory()->NewUncompiledDataWithoutPreparseData(
+        inferred_name_val, start_position, end_position);
+  }
+
+  // If the GC runs after changing one but not both fields below, it could see
+  // the SharedFunctionInfo in an unexpected state.
+  DisallowGarbageCollection no_gc;
+
   shared_info->DiscardCompiledMetadata(isolate);
 
   // Replace compiled data with a new UncompiledData object.
@@ -393,14 +404,12 @@
     // If this is uncompiled data with a pre-parsed scope data, we can just
     // clear out the scope data and keep the uncompiled data.
     shared_info->ClearPreparseData();
+    DCHECK(data.is_null());
   } else {
-    // Create a new UncompiledData, without pre-parsed scope, and update the
-    // function data to point to it. Use the raw function data setter to avoid
-    // validity checks, since we're performing the unusual task of decompiling.
-    Handle<UncompiledData> data =
-        isolate->factory()->NewUncompiledDataWithoutPreparseData(
-            inferred_name_val, start_position, end_position);
-    shared_info->set_function_data(*data, kReleaseStore);
+    // Update the function data to point to the UncompiledData without preparse
+    // data created above. Use the raw function data setter to avoid validity
+    // checks, since we're performing the unusual task of decompiling.
+    shared_info->set_function_data(*data.ToHandleChecked(), kReleaseStore);
   }
 }
 
diff -r -u --color up/v8/src/objects/string-forwarding-table.cc nw/v8/src/objects/string-forwarding-table.cc
--- up/v8/src/objects/string-forwarding-table.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/string-forwarding-table.cc	2023-01-19 16:46:36.330609538 -0500
@@ -190,7 +190,7 @@
   constexpr bool is_one_byte =
       std::is_base_of_v<v8::String::ExternalOneByteStringResource, T>;
 
-  DCHECK_IMPLIES(!FLAG_always_use_string_forwarding_table,
+  DCHECK_IMPLIES(!v8_flags.always_use_string_forwarding_table,
                  string.InSharedHeap());
   int index = next_free_index_++;
   uint32_t index_in_block;
diff -r -u --color up/v8/src/objects/string-table.cc nw/v8/src/objects/string-table.cc
--- up/v8/src/objects/string-table.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/string-table.cc	2023-01-19 16:46:36.341442866 -0500
@@ -424,7 +424,7 @@
       // is another thread migrated the string to internalized already.
       // Migrations to thin are impossible, as we only call this method on table
       // misses inside the critical section.
-      string_->set_map_no_write_barrier(*internalized_map);
+      string_->set_map_safe_transition_no_write_barrier(*internalized_map);
       DCHECK(string_->IsInternalizedString());
       return string_;
     }
diff -r -u --color up/v8/src/objects/swiss-hash-table-helpers.h nw/v8/src/objects/swiss-hash-table-helpers.h
--- up/v8/src/objects/swiss-hash-table-helpers.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/swiss-hash-table-helpers.h	2023-01-19 16:46:36.341442866 -0500
@@ -219,22 +219,6 @@
 }
 
 #if V8_SWISS_TABLE_HAVE_SSE2_HOST
-// https://github.com/abseil/abseil-cpp/issues/209
-// https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87853
-// _mm_cmpgt_epi8 is broken under GCC with -funsigned-char
-// Work around this by using the portable implementation of Group
-// when using -funsigned-char under GCC.
-inline __m128i _mm_cmpgt_epi8_fixed(__m128i a, __m128i b) {
-#if defined(__GNUC__) && !defined(__clang__)
-  if (std::is_unsigned<char>::value) {
-    const __m128i mask = _mm_set1_epi8(0x80);
-    const __m128i diff = _mm_subs_epi8(b, a);
-    return _mm_cmpeq_epi8(_mm_and_si128(diff, mask), mask);
-  }
-#endif
-  return _mm_cmpgt_epi8(a, b);
-}
-
 struct GroupSse2Impl {
   static constexpr size_t kWidth = 16;  // the number of slots per group
 
@@ -260,33 +244,6 @@
 #endif
   }
 
-  // Returns a bitmask representing the positions of empty or deleted slots.
-  BitMask<uint32_t, kWidth> MatchEmptyOrDeleted() const {
-    auto special = _mm_set1_epi8(kSentinel);
-    return BitMask<uint32_t, kWidth>(
-        _mm_movemask_epi8(_mm_cmpgt_epi8_fixed(special, ctrl)));
-  }
-
-  // Returns the number of trailing empty or deleted elements in the group.
-  uint32_t CountLeadingEmptyOrDeleted() const {
-    auto special = _mm_set1_epi8(kSentinel);
-    return base::bits::CountTrailingZerosNonZero(
-        _mm_movemask_epi8(_mm_cmpgt_epi8_fixed(special, ctrl)) + 1);
-  }
-
-  void ConvertSpecialToEmptyAndFullToDeleted(ctrl_t* dst) const {
-    auto msbs = _mm_set1_epi8(static_cast<char>(-128));
-    auto x126 = _mm_set1_epi8(126);
-#if V8_SWISS_TABLE_HAVE_SSSE3_HOST
-    auto res = _mm_or_si128(_mm_shuffle_epi8(x126, ctrl), msbs);
-#else
-    auto zero = _mm_setzero_si128();
-    auto special_mask = _mm_cmpgt_epi8_fixed(zero, ctrl);
-    auto res = _mm_or_si128(msbs, _mm_andnot_si128(special_mask, x126));
-#endif
-    _mm_storeu_si128(reinterpret_cast<__m128i*>(dst), res);
-  }
-
   __m128i ctrl;
 };
 #endif  // V8_SWISS_TABLE_HAVE_SSE2_HOST
@@ -314,26 +271,6 @@
     return Match(static_cast<h2_t>(kEmpty));
   }
 
-  // Returns a bitmask representing the positions of empty or deleted slots.
-  BitMask<uint32_t, kWidth> MatchEmptyOrDeleted() const {
-    return BitMask<uint32_t, kWidth>(MatchEmptyOrDeletedMask());
-  }
-
-  // Returns the number of trailing empty or deleted elements in the group.
-  uint32_t CountLeadingEmptyOrDeleted() const {
-    return base::bits::CountTrailingZerosNonZero(MatchEmptyOrDeletedMask() + 1);
-  }
-
-  void ConvertSpecialToEmptyAndFullToDeleted(ctrl_t* dst) const {
-    for (size_t i = 0; i < kWidth; i++) {
-      if (ctrl_[i] < 0) {
-        dst[i] = kEmpty;
-      } else {
-        dst[i] = kDeleted;
-      }
-    }
-  }
-
  private:
   uint32_t MatchEmptyOrDeletedMask() const {
     uint32_t mask = 0;
@@ -382,26 +319,6 @@
     return BitMask<uint64_t, kWidth, 3>((ctrl & (~ctrl << 6)) & kMsbs);
   }
 
-  // Returns a bitmask representing the positions of empty or deleted slots.
-  BitMask<uint64_t, kWidth, 3> MatchEmptyOrDeleted() const {
-    return BitMask<uint64_t, kWidth, 3>((ctrl & (~ctrl << 7)) & kMsbs);
-  }
-
-  // Returns the number of trailing empty or deleted elements in the group.
-  uint32_t CountLeadingEmptyOrDeleted() const {
-    constexpr uint64_t gaps = 0x00FEFEFEFEFEFEFEULL;
-    return (base::bits::CountTrailingZerosNonZero(
-                ((~ctrl & (ctrl >> 7)) | gaps) + 1) +
-            7) >>
-           3;
-  }
-
-  void ConvertSpecialToEmptyAndFullToDeleted(ctrl_t* dst) const {
-    auto x = ctrl & kMsbs;
-    auto res = (~x + (x >> 7)) & ~kLsbs;
-    base::WriteLittleEndianValue(reinterpret_cast<uint64_t*>(dst), res);
-  }
-
   uint64_t ctrl;
 };
 
diff -r -u --color up/v8/src/objects/template-objects-inl.h nw/v8/src/objects/template-objects-inl.h
--- up/v8/src/objects/template-objects-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/template-objects-inl.h	2023-01-19 16:46:36.341442866 -0500
@@ -18,7 +18,6 @@
 #include "torque-generated/src/objects/template-objects-tq-inl.inc"
 
 TQ_OBJECT_CONSTRUCTORS_IMPL(TemplateObjectDescription)
-TQ_OBJECT_CONSTRUCTORS_IMPL(CachedTemplateObject)
 
 }  // namespace internal
 }  // namespace v8
diff -r -u --color up/v8/src/objects/template-objects.cc nw/v8/src/objects/template-objects.cc
--- up/v8/src/objects/template-objects.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/template-objects.cc	2023-01-19 16:46:36.341442866 -0500
@@ -7,6 +7,7 @@
 #include "src/base/functional.h"
 #include "src/execution/isolate.h"
 #include "src/heap/factory.h"
+#include "src/objects/js-array.h"
 #include "src/objects/objects-inl.h"
 #include "src/objects/property-descriptor.h"
 #include "src/objects/template-objects-inl.h"
@@ -14,31 +15,69 @@
 namespace v8 {
 namespace internal {
 
+namespace {
+bool CachedTemplateMatches(Isolate* isolate, NativeContext native_context,
+                           JSArray entry, int function_literal_id, int slot_id,
+                           DisallowGarbageCollection& no_gc) {
+  if (native_context.is_js_array_template_literal_object_map(
+          entry.map(isolate))) {
+    TemplateLiteralObject template_object = TemplateLiteralObject::cast(entry);
+    return template_object.function_literal_id() == function_literal_id &&
+           template_object.slot_id() == slot_id;
+  }
+
+  Handle<JSArray> entry_handle(entry, isolate);
+  Smi cached_function_literal_id = Smi::cast(*JSReceiver::GetDataProperty(
+      isolate, entry_handle,
+      isolate->factory()->template_literal_function_literal_id_symbol()));
+  if (cached_function_literal_id.value() != function_literal_id) return false;
+
+  Smi cached_slot_id = Smi::cast(*JSReceiver::GetDataProperty(
+      isolate, entry_handle,
+      isolate->factory()->template_literal_slot_id_symbol()));
+  if (cached_slot_id.value() != slot_id) return false;
+
+  return true;
+}
+}  // namespace
+
 // static
 Handle<JSArray> TemplateObjectDescription::GetTemplateObject(
     Isolate* isolate, Handle<NativeContext> native_context,
     Handle<TemplateObjectDescription> description,
     Handle<SharedFunctionInfo> shared_info, int slot_id) {
-  uint32_t hash = shared_info->Hash();
+  int function_literal_id = shared_info->function_literal_id();
 
   // Check the template weakmap to see if the template object already exists.
-  Handle<EphemeronHashTable> template_weakmap;
+  Handle<Script> script(Script::cast(shared_info->script(isolate)), isolate);
+  int32_t hash =
+      EphemeronHashTable::ShapeT::Hash(ReadOnlyRoots(isolate), script);
+  MaybeHandle<ArrayList> maybe_cached_templates;
 
-  if (native_context->template_weakmap().IsUndefined(isolate)) {
-    template_weakmap = EphemeronHashTable::New(isolate, 1);
-  } else {
+  if (!native_context->template_weakmap().IsUndefined(isolate)) {
     DisallowGarbageCollection no_gc;
+    // The no_gc keeps this safe, and gcmole is confused because
+    // CachedTemplateMatches calls JSReceiver::GetDataProperty.
+    DisableGCMole no_gcmole;
     ReadOnlyRoots roots(isolate);
-    template_weakmap = handle(
-        EphemeronHashTable::cast(native_context->template_weakmap()), isolate);
-    Object maybe_cached_template = template_weakmap->Lookup(shared_info, hash);
-    while (!maybe_cached_template.IsTheHole(roots)) {
-      CachedTemplateObject cached_template =
-          CachedTemplateObject::cast(maybe_cached_template);
-      if (cached_template.slot_id() == slot_id) {
-        return handle(cached_template.template_object(), isolate);
+    EphemeronHashTable template_weakmap =
+        EphemeronHashTable::cast(native_context->template_weakmap());
+    Object cached_templates_lookup =
+        template_weakmap.Lookup(isolate, script, hash);
+    if (!cached_templates_lookup.IsTheHole(roots)) {
+      ArrayList cached_templates = ArrayList::cast(cached_templates_lookup);
+      maybe_cached_templates = handle(cached_templates, isolate);
+
+      // Linear search over the cached template array list for a template
+      // object matching the given function_literal_id + slot_id.
+      // TODO(leszeks): Consider keeping this list sorted for faster lookup.
+      for (int i = 0; i < cached_templates.Length(); i++) {
+        JSArray template_object = JSArray::cast(cached_templates.Get(i));
+        if (CachedTemplateMatches(isolate, *native_context, template_object,
+                                  function_literal_id, slot_id, no_gc)) {
+          return handle(template_object, isolate);
+        }
       }
-      maybe_cached_template = cached_template.next();
     }
   }
 
@@ -46,36 +85,43 @@
   Handle<FixedArray> raw_strings(description->raw_strings(), isolate);
   Handle<FixedArray> cooked_strings(description->cooked_strings(), isolate);
   Handle<JSArray> template_object =
-      isolate->factory()->NewJSArrayForTemplateLiteralArray(cooked_strings,
-                                                            raw_strings);
-
-  // Insert the template object into the template weakmap.
-  Handle<HeapObject> previous_cached_templates = handle(
-      HeapObject::cast(template_weakmap->Lookup(shared_info, hash)), isolate);
-  Handle<CachedTemplateObject> cached_template = CachedTemplateObject::New(
-      isolate, slot_id, template_object, previous_cached_templates);
-  template_weakmap = EphemeronHashTable::Put(
-      isolate, template_weakmap, shared_info, cached_template, hash);
-  native_context->set_template_weakmap(*template_weakmap);
+      isolate->factory()->NewJSArrayForTemplateLiteralArray(
+          cooked_strings, raw_strings, function_literal_id, slot_id);
 
-  return template_object;
-}
+  // Insert the template object into the cached template array list.
+  Handle<ArrayList> cached_templates;
+  if (!maybe_cached_templates.ToHandle(&cached_templates)) {
+    cached_templates = isolate->factory()->NewArrayList(1);
+  }
+  cached_templates = ArrayList::Add(isolate, cached_templates, template_object);
 
-Handle<CachedTemplateObject> CachedTemplateObject::New(
-    Isolate* isolate, int slot_id, Handle<JSArray> template_object,
-    Handle<HeapObject> next) {
-  DCHECK(next->IsCachedTemplateObject() || next->IsTheHole());
-  Handle<CachedTemplateObject> result_handle =
-      Handle<CachedTemplateObject>::cast(isolate->factory()->NewStruct(
-          CACHED_TEMPLATE_OBJECT_TYPE, AllocationType::kOld));
-  {
-    DisallowGarbageCollection no_gc;
-    auto result = *result_handle;
-    result.set_slot_id(slot_id);
-    result.set_template_object(*template_object);
-    result.set_next(*next);
+  // Compare the cached_templates to the original maybe_cached_templates loaded
+  // from the weakmap -- if it doesn't match, we need to update the weakmap.
+  Handle<ArrayList> old_cached_templates;
+  if (!maybe_cached_templates.ToHandle(&old_cached_templates) ||
+      *old_cached_templates != *cached_templates) {
+    HeapObject maybe_template_weakmap = native_context->template_weakmap();
+    Handle<EphemeronHashTable> template_weakmap;
+    if (maybe_template_weakmap.IsUndefined()) {
+      template_weakmap = EphemeronHashTable::New(isolate, 1);
+    } else {
+      template_weakmap =
+          handle(EphemeronHashTable::cast(maybe_template_weakmap), isolate);
+    }
+    template_weakmap = EphemeronHashTable::Put(isolate, template_weakmap,
+                                               script, cached_templates, hash);
+    native_context->set_template_weakmap(*template_weakmap);
   }
-  return result_handle;
+
+  // Check that the list is in the appropriate location on the weakmap, and
+  // that the appropriate entry is in the right location in this list.
+  DCHECK_EQ(EphemeronHashTable::cast(native_context->template_weakmap())
+                .Lookup(isolate, script, hash),
+            *cached_templates);
+  DCHECK_EQ(cached_templates->Get(cached_templates->Length() - 1),
+            *template_object);
+
+  return template_object;
 }
 
 }  // namespace internal
diff -r -u --color up/v8/src/objects/template-objects.h nw/v8/src/objects/template-objects.h
--- up/v8/src/objects/template-objects.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/template-objects.h	2023-01-19 16:46:36.341442866 -0500
@@ -19,21 +19,6 @@
 
 #include "torque-generated/src/objects/template-objects-tq.inc"
 
-// CachedTemplateObject is a tuple used to cache a TemplateObject that has been
-// created. All the CachedTemplateObject's for a given SharedFunctionInfo form a
-// linked list via the next fields.
-class CachedTemplateObject final
-    : public TorqueGeneratedCachedTemplateObject<CachedTemplateObject, Struct> {
- public:
-  static Handle<CachedTemplateObject> New(Isolate* isolate, int slot_id,
-                                          Handle<JSArray> template_object,
-                                          Handle<HeapObject> next);
-
-  using BodyDescriptor = StructBodyDescriptor;
-
-  TQ_OBJECT_CONSTRUCTORS(CachedTemplateObject)
-};
-
 // TemplateObjectDescription is a tuple of raw strings and cooked strings for
 // tagged template literals. Used to communicate with the runtime for template
 // object creation within the {Runtime_GetTemplateObject} method.
diff -r -u --color up/v8/src/objects/template-objects.tq nw/v8/src/objects/template-objects.tq
--- up/v8/src/objects/template-objects.tq	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/template-objects.tq	2023-01-19 16:46:36.341442866 -0500
@@ -2,12 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-extern class CachedTemplateObject extends Struct {
-  slot_id: Smi;
-  template_object: JSArray;
-  next: CachedTemplateObject|TheHole;
-}
-
 extern class TemplateObjectDescription extends Struct {
   raw_strings: FixedArray;
   cooked_strings: FixedArray;
diff -r -u --color up/v8/src/objects/turbofan-types.tq nw/v8/src/objects/turbofan-types.tq
--- up/v8/src/objects/turbofan-types.tq	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/turbofan-types.tq	2023-01-19 16:46:36.341442866 -0500
@@ -54,6 +54,7 @@
 
 bitfield struct TurbofanTypeHighBits extends uint32 {
   sandboxed_pointer: bool: 1 bit;
+  machine: bool: 1 bit;
 }
 
 @export
@@ -249,9 +250,9 @@
     return Undefined;
   }
 
-  Print('Type assertion failed! (value/expectedType/nodeId)');
-  Print(value);
-  Print(expectedType);
-  Print(nodeId);
+  PrintErr('Type assertion failed! (value/expectedType/nodeId)');
+  PrintErr(value);
+  PrintErr(expectedType);
+  PrintErr(nodeId);
   unreachable;
 }
diff -r -u --color up/v8/src/objects/value-serializer.cc nw/v8/src/objects/value-serializer.cc
--- up/v8/src/objects/value-serializer.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/value-serializer.cc	2023-01-19 16:46:36.341442866 -0500
@@ -616,13 +616,8 @@
 #if V8_ENABLE_WEBASSEMBLY
     case WASM_MODULE_OBJECT_TYPE:
       return WriteWasmModule(Handle<WasmModuleObject>::cast(receiver));
-    case WASM_MEMORY_OBJECT_TYPE: {
-      auto enabled_features = wasm::WasmFeatures::FromIsolate(isolate_);
-      if (enabled_features.has_threads()) {
-        return WriteWasmMemory(Handle<WasmMemoryObject>::cast(receiver));
-      }
-      break;
-    }
+    case WASM_MEMORY_OBJECT_TYPE:
+      return WriteWasmMemory(Handle<WasmMemoryObject>::cast(receiver));
 #endif  // V8_ENABLE_WEBASSEMBLY
     default:
       break;
@@ -1282,7 +1277,7 @@
   // same end state and result.
   auto previous_position = position_;
   Maybe<T> maybe_expected_value = ReadVarintLoop<T>();
-  if (FLAG_fuzzing && maybe_expected_value.IsNothing()) {
+  if (v8_flags.fuzzing && maybe_expected_value.IsNothing()) {
     return maybe_expected_value;
   }
   T expected_value = maybe_expected_value.ToChecked();
@@ -1671,7 +1666,7 @@
     return {};
   }
   // Length is also checked in ReadRawBytes.
-  DCHECK_IMPLIES(!FLAG_fuzzing,
+  DCHECK_IMPLIES(!v8_flags.fuzzing,
                  byte_length <= static_cast<uint32_t>(
                                     std::numeric_limits<int32_t>::max()));
   if (!ReadRawBytes(byte_length).To(&bytes)) {
@@ -2228,11 +2223,6 @@
 MaybeHandle<WasmMemoryObject> ValueDeserializer::ReadWasmMemory() {
   uint32_t id = next_id_++;
 
-  auto enabled_features = wasm::WasmFeatures::FromIsolate(isolate_);
-  if (!enabled_features.has_threads()) {
-    return MaybeHandle<WasmMemoryObject>();
-  }
-
   int32_t maximum_pages;
   if (!ReadZigZag<int32_t>().To(&maximum_pages)) {
     return MaybeHandle<WasmMemoryObject>();
diff -r -u --color up/v8/src/objects/visitors.h nw/v8/src/objects/visitors.h
--- up/v8/src/objects/visitors.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/objects/visitors.h	2023-01-19 16:46:36.341442866 -0500
@@ -30,6 +30,7 @@
   V(kHandleScope, "(Handle scope)")                     \
   V(kBuiltins, "(Builtins)")                            \
   V(kGlobalHandles, "(Global handles)")                 \
+  V(kTracedHandles, "(Traced handles)")                 \
   V(kEternalHandles, "(Eternal handles)")               \
   V(kThreadManager, "(Thread manager)")                 \
   V(kStrongRoots, "(Strong roots)")                     \
diff -r -u --color up/v8/src/profiler/heap-snapshot-generator.cc nw/v8/src/profiler/heap-snapshot-generator.cc
--- up/v8/src/profiler/heap-snapshot-generator.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/profiler/heap-snapshot-generator.cc	2023-01-19 16:46:36.352276199 -0500
@@ -38,6 +38,7 @@
 #include "src/profiler/allocation-tracker.h"
 #include "src/profiler/heap-profiler.h"
 #include "src/profiler/heap-snapshot-generator-inl.h"
+#include "src/profiler/output-stream-writer.h"
 
 namespace v8 {
 namespace internal {
@@ -1480,7 +1481,7 @@
   CodeT code = shared.GetCode();
   // Don't try to get the Code object from Code-less embedded builtin.
   HeapObject maybe_code_obj =
-      V8_REMOVE_BUILTINS_CODE_OBJECTS && code.is_off_heap_trampoline()
+      V8_EXTERNAL_CODE_SPACE_BOOL && code.is_off_heap_trampoline()
           ? HeapObject::cast(code)
           : FromCodeT(code);
   if (name[0] != '\0') {
@@ -1557,7 +1558,7 @@
   if (V8_EXTERNAL_CODE_SPACE_BOOL) {
     TagObject(code, names_->GetFormatted("(%s builtin handle)", name));
   }
-  if (!V8_REMOVE_BUILTINS_CODE_OBJECTS || !code.is_off_heap_trampoline()) {
+  if (!V8_EXTERNAL_CODE_SPACE_BOOL || !code.is_off_heap_trampoline()) {
     TagObject(FromCodeT(code), names_->GetFormatted("(%s builtin)", name));
   }
 }
@@ -2450,6 +2451,7 @@
   Isolate* isolate = Isolate::FromHeap(heap_);
   GlobalObjectsEnumerator enumerator(isolate);
   isolate->global_handles()->IterateAllRoots(&enumerator);
+  isolate->traced_handles()->Iterate(&enumerator);
   for (int i = 0, l = enumerator.count(); i < l; ++i) {
     Handle<JSGlobalObject> obj = enumerator.at(i);
     const char* tag = global_object_name_resolver_->GetName(
@@ -2461,7 +2463,7 @@
 }
 
 void V8HeapExplorer::MakeGlobalObjectTagMap(
-    const SafepointScope& safepoint_scope) {
+    const IsolateSafepointScope& safepoint_scope) {
   for (const auto& pair : global_object_tag_pairs_) {
     global_object_tag_map_.emplace(*pair.first, pair.second);
   }
@@ -2721,7 +2723,7 @@
   heap_->CollectAllAvailableGarbage(GarbageCollectionReason::kHeapProfiler);
 
   NullContextForSnapshotScope null_context_scope(isolate);
-  SafepointScope scope(heap_);
+  IsolateSafepointScope scope(heap_);
   v8_heap_explorer_.MakeGlobalObjectTagMap(scope);
   handle_scope.reset();
 
@@ -2787,111 +2789,6 @@
          dom_explorer_.IterateAndExtractReferences(this);
 }
 
-template<int bytes> struct MaxDecimalDigitsIn;
-template <>
-struct MaxDecimalDigitsIn<1> {
-  static const int kSigned = 3;
-  static const int kUnsigned = 3;
-};
-template<> struct MaxDecimalDigitsIn<4> {
-  static const int kSigned = 11;
-  static const int kUnsigned = 10;
-};
-template<> struct MaxDecimalDigitsIn<8> {
-  static const int kSigned = 20;
-  static const int kUnsigned = 20;
-};
-
-class OutputStreamWriter {
- public:
-  explicit OutputStreamWriter(v8::OutputStream* stream)
-      : stream_(stream),
-        chunk_size_(stream->GetChunkSize()),
-        chunk_(chunk_size_),
-        chunk_pos_(0),
-        aborted_(false) {
-    DCHECK_GT(chunk_size_, 0);
-  }
-  bool aborted() { return aborted_; }
-  void AddCharacter(char c) {
-    DCHECK_NE(c, '\0');
-    DCHECK(chunk_pos_ < chunk_size_);
-    chunk_[chunk_pos_++] = c;
-    MaybeWriteChunk();
-  }
-  void AddString(const char* s) {
-    size_t len = strlen(s);
-    DCHECK_GE(kMaxInt, len);
-    AddSubstring(s, static_cast<int>(len));
-  }
-  void AddSubstring(const char* s, int n) {
-    if (n <= 0) return;
-    DCHECK_LE(n, strlen(s));
-    const char* s_end = s + n;
-    while (s < s_end) {
-      int s_chunk_size =
-          std::min(chunk_size_ - chunk_pos_, static_cast<int>(s_end - s));
-      DCHECK_GT(s_chunk_size, 0);
-      MemCopy(chunk_.begin() + chunk_pos_, s, s_chunk_size);
-      s += s_chunk_size;
-      chunk_pos_ += s_chunk_size;
-      MaybeWriteChunk();
-    }
-  }
-  void AddNumber(unsigned n) { AddNumberImpl<unsigned>(n, "%u"); }
-  void Finalize() {
-    if (aborted_) return;
-    DCHECK(chunk_pos_ < chunk_size_);
-    if (chunk_pos_ != 0) {
-      WriteChunk();
-    }
-    stream_->EndOfStream();
-  }
-
- private:
-  template<typename T>
-  void AddNumberImpl(T n, const char* format) {
-    // Buffer for the longest value plus trailing \0
-    static const int kMaxNumberSize =
-        MaxDecimalDigitsIn<sizeof(T)>::kUnsigned + 1;
-    if (chunk_size_ - chunk_pos_ >= kMaxNumberSize) {
-      int result = SNPrintF(
-          chunk_.SubVector(chunk_pos_, chunk_size_), format, n);
-      DCHECK_NE(result, -1);
-      chunk_pos_ += result;
-      MaybeWriteChunk();
-    } else {
-      base::EmbeddedVector<char, kMaxNumberSize> buffer;
-      int result = SNPrintF(buffer, format, n);
-      USE(result);
-      DCHECK_NE(result, -1);
-      AddString(buffer.begin());
-    }
-  }
-  void MaybeWriteChunk() {
-    DCHECK(chunk_pos_ <= chunk_size_);
-    if (chunk_pos_ == chunk_size_) {
-      WriteChunk();
-    }
-  }
-  void WriteChunk() {
-    if (aborted_) return;
-    if (stream_->WriteAsciiChunk(chunk_.begin(), chunk_pos_) ==
-        v8::OutputStream::kAbort)
-      aborted_ = true;
-    chunk_pos_ = 0;
-  }
-
-  v8::OutputStream* stream_;
-  int chunk_size_;
-  base::ScopedVector<char> chunk_;
-  int chunk_pos_;
-  bool aborted_;
-};
-
-
-// type, name|index, to_node.
-const int HeapSnapshotJSONSerializer::kEdgeFieldsCount = 3;
 // type, name, id, self_size, edge_count, trace_node_id, detachedness.
 const int HeapSnapshotJSONSerializer::kNodeFieldsCount = 7;
 
diff -r -u --color up/v8/src/profiler/heap-snapshot-generator.h nw/v8/src/profiler/heap-snapshot-generator.h
--- up/v8/src/profiler/heap-snapshot-generator.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/profiler/heap-snapshot-generator.h	2023-01-19 16:46:36.352276199 -0500
@@ -36,6 +36,7 @@
 class HeapProfiler;
 class HeapSnapshot;
 class HeapSnapshotGenerator;
+class IsolateSafepointScope;
 class JSArrayBuffer;
 class JSCollection;
 class JSGeneratorObject;
@@ -43,7 +44,6 @@
 class JSGlobalProxy;
 class JSPromise;
 class JSWeakCollection;
-class SafepointScope;
 
 struct SourceLocation {
   SourceLocation(int entry_index, int scriptId, int line, int col)
@@ -390,7 +390,7 @@
   uint32_t EstimateObjectsCount();
   bool IterateAndExtractReferences(HeapSnapshotGenerator* generator);
   void CollectGlobalObjectsTags();
-  void MakeGlobalObjectTagMap(const SafepointScope& safepoint_scope);
+  void MakeGlobalObjectTagMap(const IsolateSafepointScope& safepoint_scope);
   void TagBuiltinCodeObject(CodeT code, const char* name);
   HeapEntry* AddEntry(Address address,
                       HeapEntry::Type type,
Only in nw/v8/src/profiler: output-stream-writer.h
diff -r -u --color up/v8/src/profiler/profile-generator.cc nw/v8/src/profiler/profile-generator.cc
--- up/v8/src/profiler/profile-generator.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/profiler/profile-generator.cc	2023-01-19 16:46:36.352276199 -0500
@@ -5,12 +5,14 @@
 #include "src/profiler/profile-generator.h"
 
 #include <algorithm>
+#include <vector>
 
 #include "include/v8-profiler.h"
 #include "src/base/lazy-instance.h"
 #include "src/codegen/source-position.h"
 #include "src/objects/shared-function-info-inl.h"
 #include "src/profiler/cpu-profiler.h"
+#include "src/profiler/output-stream-writer.h"
 #include "src/profiler/profile-generator-inl.h"
 #include "src/profiler/profiler-stats.h"
 #include "src/tracing/trace-event.h"
@@ -762,6 +764,160 @@
                               "ProfileChunk", id_, "data", std::move(value));
 }
 
+namespace {
+
+void FlattenNodesTree(const v8::CpuProfileNode* node,
+                      std::vector<const v8::CpuProfileNode*>* nodes) {
+  nodes->emplace_back(node);
+  const int childrenCount = node->GetChildrenCount();
+  for (int i = 0; i < childrenCount; i++)
+    FlattenNodesTree(node->GetChild(i), nodes);
+}
+
+}  // namespace
+
+void CpuProfileJSONSerializer::Serialize(v8::OutputStream* stream) {
+  DCHECK_NULL(writer_);
+  writer_ = new OutputStreamWriter(stream);
+  SerializeImpl();
+  delete writer_;
+  writer_ = nullptr;
+}
+
+void CpuProfileJSONSerializer::SerializePositionTicks(
+    const v8::CpuProfileNode* node, int lineCount) {
+  std::vector<v8::CpuProfileNode::LineTick> entries(lineCount);
+  if (node->GetLineTicks(&entries[0], lineCount)) {
+    for (int i = 0; i < lineCount; i++) {
+      writer_->AddCharacter('{');
+      writer_->AddString("\"line\":");
+      writer_->AddNumber(entries[i].line);
+      writer_->AddString(",\"ticks\":");
+      writer_->AddNumber(entries[i].hit_count);
+      writer_->AddCharacter('}');
+      if (i != (lineCount - 1)) writer_->AddCharacter(',');
+    }
+  }
+}
+
+void CpuProfileJSONSerializer::SerializeCallFrame(
+    const v8::CpuProfileNode* node) {
+  writer_->AddString("\"functionName\":\"");
+  writer_->AddString(node->GetFunctionNameStr());
+  writer_->AddString("\",\"lineNumber\":");
+  writer_->AddNumber(node->GetLineNumber() - 1);
+  writer_->AddString(",\"columnNumber\":");
+  writer_->AddNumber(node->GetColumnNumber() - 1);
+  writer_->AddString(",\"scriptId\":");
+  writer_->AddNumber(node->GetScriptId());
+  writer_->AddString(",\"url\":\"");
+  writer_->AddString(node->GetScriptResourceNameStr());
+  writer_->AddCharacter('"');
+}
+
+void CpuProfileJSONSerializer::SerializeChildren(const v8::CpuProfileNode* node,
+                                                 int childrenCount) {
+  for (int i = 0; i < childrenCount; i++) {
+    writer_->AddNumber(node->GetChild(i)->GetNodeId());
+    if (i != (childrenCount - 1)) writer_->AddCharacter(',');
+  }
+}
+
+void CpuProfileJSONSerializer::SerializeNode(const v8::CpuProfileNode* node) {
+  writer_->AddCharacter('{');
+  writer_->AddString("\"id\":");
+  writer_->AddNumber(node->GetNodeId());
+
+  writer_->AddString(",\"hitCount\":");
+  writer_->AddNumber(node->GetHitCount());
+
+  writer_->AddString(",\"callFrame\":{");
+  SerializeCallFrame(node);
+  writer_->AddCharacter('}');
+
+  const int childrenCount = node->GetChildrenCount();
+  if (childrenCount) {
+    writer_->AddString(",\"children\":[");
+    SerializeChildren(node, childrenCount);
+    writer_->AddCharacter(']');
+  }
+
+  const char* deoptReason = node->GetBailoutReason();
+  if (deoptReason && deoptReason[0] && strcmp(deoptReason, "no reason")) {
+    writer_->AddString(",\"deoptReason\":\"");
+    writer_->AddString(deoptReason);
+    writer_->AddCharacter('"');
+  }
+
+  unsigned lineCount = node->GetHitLineCount();
+  if (lineCount) {
+    writer_->AddString(",\"positionTicks\":[");
+    SerializePositionTicks(node, lineCount);
+    writer_->AddCharacter(']');
+  }
+  writer_->AddCharacter('}');
+}
+
+void CpuProfileJSONSerializer::SerializeNodes() {
+  std::vector<const v8::CpuProfileNode*> nodes;
+  FlattenNodesTree(
+      reinterpret_cast<const v8::CpuProfileNode*>(profile_->top_down()->root()),
+      &nodes);
+
+  for (size_t i = 0; i < nodes.size(); i++) {
+    SerializeNode(nodes.at(i));
+    if (writer_->aborted()) return;
+    if (i != (nodes.size() - 1)) writer_->AddCharacter(',');
+  }
+}
+
+void CpuProfileJSONSerializer::SerializeTimeDeltas() {
+  int count = profile_->samples_count();
+  uint64_t lastTime = profile_->start_time().since_origin().InMicroseconds();
+  for (int i = 0; i < count; i++) {
+    uint64_t ts = profile_->sample(i).timestamp.since_origin().InMicroseconds();
+    writer_->AddNumber(static_cast<int>(ts - lastTime));
+    if (i != (count - 1)) writer_->AddString(",");
+    lastTime = ts;
+  }
+}
+
+void CpuProfileJSONSerializer::SerializeSamples() {
+  int count = profile_->samples_count();
+  for (int i = 0; i < count; i++) {
+    writer_->AddNumber(profile_->sample(i).node->id());
+    if (i != (count - 1)) writer_->AddString(",");
+  }
+}
+
+void CpuProfileJSONSerializer::SerializeImpl() {
+  writer_->AddCharacter('{');
+  writer_->AddString("\"nodes\":[");
+  SerializeNodes();
+  writer_->AddString("]");
+
+  writer_->AddString(",\"startTime\":");
+  writer_->AddNumber(static_cast<unsigned>(
+      profile_->start_time().since_origin().InMicroseconds()));
+
+  writer_->AddString(",\"endTime\":");
+  writer_->AddNumber(static_cast<unsigned>(
+      profile_->end_time().since_origin().InMicroseconds()));
+
+  writer_->AddString(",\"samples\":[");
+  SerializeSamples();
+  if (writer_->aborted()) return;
+  writer_->AddCharacter(']');
+
+  writer_->AddString(",\"timeDeltas\":[");
+  SerializeTimeDeltas();
+  if (writer_->aborted()) return;
+  writer_->AddString("]");
+
+  writer_->AddCharacter('}');
+  writer_->Finalize();
+}
+
 void CpuProfile::Print() const {
   base::OS::Print("[Top down]:\n");
   top_down_.Print();
diff -r -u --color up/v8/src/profiler/profile-generator.h nw/v8/src/profiler/profile-generator.h
--- up/v8/src/profiler/profile-generator.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/profiler/profile-generator.h	2023-01-19 16:46:36.352276199 -0500
@@ -19,6 +19,7 @@
 #include "src/builtins/builtins.h"
 #include "src/execution/vm-state.h"
 #include "src/logging/code-events.h"
+#include "src/profiler/output-stream-writer.h"
 #include "src/profiler/strings-storage.h"
 #include "src/utils/allocation.h"
 
@@ -596,6 +597,31 @@
   Isolate* isolate_;
 };
 
+class CpuProfileJSONSerializer {
+ public:
+  explicit CpuProfileJSONSerializer(CpuProfile* profile)
+      : profile_(profile), writer_(nullptr) {}
+  CpuProfileJSONSerializer(const CpuProfileJSONSerializer&) = delete;
+  CpuProfileJSONSerializer& operator=(const CpuProfileJSONSerializer&) = delete;
+  void Serialize(v8::OutputStream* stream);
+
+ private:
+  void SerializePositionTicks(const v8::CpuProfileNode* node, int lineCount);
+  void SerializeCallFrame(const v8::CpuProfileNode* node);
+  void SerializeChildren(const v8::CpuProfileNode* node, int childrenCount);
+  void SerializeNode(const v8::CpuProfileNode* node);
+  void SerializeNodes();
+  void SerializeSamples();
+  void SerializeTimeDeltas();
+  void SerializeImpl();
+
+  static const int kEdgeFieldsCount;
+  static const int kNodeFieldsCount;
+
+  CpuProfile* profile_;
+  OutputStreamWriter* writer_;
+};
+
 }  // namespace internal
 }  // namespace v8
 
diff -r -u --color up/v8/src/profiler/tracing-cpu-profiler.cc nw/v8/src/profiler/tracing-cpu-profiler.cc
--- up/v8/src/profiler/tracing-cpu-profiler.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/profiler/tracing-cpu-profiler.cc	2023-01-19 16:46:36.352276199 -0500
@@ -19,6 +19,7 @@
 
 TracingCpuProfilerImpl::~TracingCpuProfilerImpl() {
   StopProfiling();
+  if (V8::GetCurrentPlatform())
   V8::GetCurrentPlatform()->GetTracingController()->RemoveTraceStateObserver(
       this);
 }
diff -r -u --color up/v8/src/regexp/gen-regexp-special-case.cc nw/v8/src/regexp/gen-regexp-special-case.cc
--- up/v8/src/regexp/gen-regexp-special-case.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/regexp/gen-regexp-special-case.cc	2023-01-19 16:46:36.352276199 -0500
@@ -9,6 +9,7 @@
 
 #include "src/base/strings.h"
 #include "src/regexp/special-case.h"
+#include "unicode/usetiter.h"
 
 namespace v8 {
 namespace internal {
@@ -126,6 +127,52 @@
   PrintSet(out, "SpecialAddSet", special_add);
 }
 
+void PrintUnicodeSpecial(std::ofstream& out) {
+  icu::UnicodeSet non_simple_folding;
+  icu::UnicodeSet current;
+  UErrorCode status = U_ZERO_ERROR;
+  // Look at all characters except white spaces.
+  icu::UnicodeSet interestingCP(u"[^[:White_Space:]]", status);
+  CHECK_EQ(status, U_ZERO_ERROR);
+  icu::UnicodeSetIterator iter(interestingCP);
+  while (iter.next()) {
+    UChar32 c = iter.getCodepoint();
+    current.set(c, c);
+    current.closeOver(USET_CASE_INSENSITIVE).removeAllStrings();
+    CHECK(!current.isBogus());
+    // Remove characters from the closeover that have a simple case folding.
+    icu::UnicodeSet toRemove;
+    icu::UnicodeSetIterator closeOverIter(current);
+    while (closeOverIter.next()) {
+      UChar32 closeOverChar = closeOverIter.getCodepoint();
+      UChar32 closeOverSCF = u_foldCase(closeOverChar, U_FOLD_CASE_DEFAULT);
+      if (closeOverChar != closeOverSCF) {
+        toRemove.add(closeOverChar);
+      }
+    }
+    CHECK(!toRemove.isBogus());
+    current.removeAll(toRemove);
+
+    // The current character and its simple case folding are also always OK.
+    UChar32 scf = u_foldCase(c, U_FOLD_CASE_DEFAULT);
+    current.remove(c);
+    current.remove(scf);
+
+    // If there are any characters remaining, they were added due to full case
+    // foldings and shouldn't match the current charcter according to the spec.
+    if (!current.isEmpty()) {
+      // Ensure that the character doesn't have a simple case folding.
+      // Otherwise the current approach of simply removing the character from
+      // the set before calling closeOver won't work.
+      CHECK_EQ(c, scf);
+      non_simple_folding.add(c);
+    }
+  }
+  CHECK(!non_simple_folding.isBogus());
+
+  PrintSet(out, "UnicodeNonSimpleCloseOverSet", non_simple_folding);
+}
+
 void WriteHeader(const char* header_filename) {
   std::ofstream out(header_filename);
   out << std::hex << std::setfill('0') << std::setw(4);
@@ -146,6 +193,7 @@
       << "namespace internal {\n\n";
 
   PrintSpecial(out);
+  PrintUnicodeSpecial(out);
 
   out << "\n"
       << "}  // namespace internal\n"
diff -r -u --color up/v8/src/regexp/regexp-ast.h nw/v8/src/regexp/regexp-ast.h
--- up/v8/src/regexp/regexp-ast.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/regexp/regexp-ast.h	2023-01-19 16:46:36.363109529 -0500
@@ -10,6 +10,9 @@
 #include "src/zone/zone-containers.h"
 #include "src/zone/zone-list.h"
 #include "src/zone/zone.h"
+#ifdef V8_INTL_SUPPORT
+#include "unicode/uniset.h"
+#endif  // V8_INTL_SUPPORT
 
 namespace v8 {
 namespace internal {
@@ -130,6 +133,12 @@
   static void AddUnicodeCaseEquivalents(ZoneList<CharacterRange>* ranges,
                                         Zone* zone);
 
+#ifdef V8_INTL_SUPPORT
+  // Creates the closeOver of the given UnicodeSet, removing all
+  // characters/strings that can't be derived via simple case folding.
+  static void UnicodeSimpleCloseOver(icu::UnicodeSet& set);
+#endif  // V8_INTL_SUPPORT
+
   bool Contains(base::uc32 i) const { return from_ <= i && i <= to_; }
   base::uc32 from() const { return from_; }
   base::uc32 to() const { return to_; }
diff -r -u --color up/v8/src/regexp/regexp-compiler-tonode.cc nw/v8/src/regexp/regexp-compiler-tonode.cc
--- up/v8/src/regexp/regexp-compiler-tonode.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/regexp/regexp-compiler-tonode.cc	2023-01-19 16:46:36.363109529 -0500
@@ -421,15 +421,26 @@
 
 }  // namespace
 
-// TODO(pthier, v8:11935): We use this method to implement
-// MaybeSimpleCaseFolding
-// TODO(v8:11935): Change to permalink once proposal is in stage 4.
-// https://arai-a.github.io/ecma262-compare/snapshot.html?pr=2418#sec-maybesimplecasefolding
-// which is slightly different. The main difference is that we retain original
-// characters and add case equivalents, whereas according to the spec original
-// characters should be replaced with their case equivalent.
-// This shouldn't make a difference for correctness, but we could potentially
-// create smaller character classes for unicode sets.
+#ifdef V8_INTL_SUPPORT
+// static
+void CharacterRange::UnicodeSimpleCloseOver(icu::UnicodeSet& set) {
+  // Remove characters for which closeOver() adds full-case-folding equivalents
+  // because we should work only with simple case folding mappings.
+  icu::UnicodeSet non_simple = icu::UnicodeSet(set);
+  non_simple.retainAll(RegExpCaseFolding::UnicodeNonSimpleCloseOverSet());
+  set.removeAll(non_simple);
+
+  set.closeOver(USET_CASE_INSENSITIVE);
+  // Full case folding maps single characters to multiple characters.
+  // Those are represented as strings in the set. Remove them so that
+  // we end up with only simple and common case mappings.
+  set.removeAllStrings();
+
+  // Add characters that have non-simple case foldings again (they match
+  // themselves).
+  set.addAll(non_simple);
+}
+#endif  // V8_INTL_SUPPORT
 
 // static
 void CharacterRange::AddUnicodeCaseEquivalents(ZoneList<CharacterRange>* ranges,
@@ -452,11 +463,8 @@
   }
   // Clear the ranges list without freeing the backing store.
   ranges->Rewind(0);
-  set.closeOver(USET_CASE_INSENSITIVE);
-  // Full case mapping map single characters to multiple characters.
-  // Those are represented as strings in the set. Remove them so that
-  // we end up with only simple and common case mappings.
-  set.removeAllStrings();
+
+  UnicodeSimpleCloseOver(set);
   for (int i = 0; i < set.getRangeCount(); i++) {
     ranges->Add(Range(set.getRangeStart(i), set.getRangeEnd(i)), zone);
   }
diff -r -u --color up/v8/src/regexp/regexp-parser.cc nw/v8/src/regexp/regexp-parser.cc
--- up/v8/src/regexp/regexp-parser.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/regexp/regexp-parser.cc	2023-01-19 16:46:36.363109529 -0500
@@ -615,6 +615,7 @@
   // Zip to the end to make sure no more input is read.
   current_ = kEndMarker;
   next_pos_ = input_length();
+  has_more_ = false;
   return nullptr;
 }
 
@@ -1619,7 +1620,7 @@
   bool success = ec == U_ZERO_ERROR && !set.isEmpty();
 
   if (success) {
-    if (needs_case_folding) set.closeOver(USET_CASE_INSENSITIVE);
+    if (needs_case_folding) CharacterRange::UnicodeSimpleCloseOver(set);
     set.removeAllStrings();
     if (negate) set.complement();
     for (int i = 0; i < set.getRangeCount(); i++) {
diff -r -u --color up/v8/src/regexp/special-case.h nw/v8/src/regexp/special-case.h
--- up/v8/src/regexp/special-case.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/regexp/special-case.h	2023-01-19 16:46:36.373942859 -0500
@@ -71,11 +71,21 @@
 // another character. Characters that match no other characters in
 // their equivalence class are added to IgnoreSet. Characters that
 // match at least one other character are added to SpecialAddSet.
+//
+// For unicode ignoreCase ("iu" and "iv"),
+// UnicodeSet::closeOver(USET_CASE_INSENSITIVE) adds all characters that are in
+// the same equivalence class. This includes characaters that are in the same
+// equivalence class using full case folding. According to the spec, only
+// simple case folding shall be considered. We therefore create
+// UnicodeNonSimpleCloseOverSet containing all characters for which
+// UnicodeSet::closeOver adds characters that are not simple case folds. This
+// set should be used similar to IgnoreSet described above.
 
 class RegExpCaseFolding final : public AllStatic {
  public:
   static const icu::UnicodeSet& IgnoreSet();
   static const icu::UnicodeSet& SpecialAddSet();
+  static const icu::UnicodeSet& UnicodeNonSimpleCloseOverSet();
 
   // This implements ECMAScript 2020 21.2.2.8.2 (Runtime Semantics:
   // Canonicalize) step 3, which is used to determine whether
diff -r -u --color up/v8/src/roots/roots.h nw/v8/src/roots/roots.h
--- up/v8/src/roots/roots.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/roots/roots.h	2023-01-19 16:46:36.373942859 -0500
@@ -330,8 +330,8 @@
   V(HeapObject, current_microtask, CurrentMicrotask)                        \
   /* KeepDuringJob set for JS WeakRefs */                                   \
   V(HeapObject, weak_refs_keep_during_job, WeakRefsKeepDuringJob)           \
-  V(Object, pending_optimize_for_test_bytecode,                             \
-    PendingOptimizeForTestBytecode)                                         \
+  V(Object, functions_marked_for_manual_optimization,                       \
+    FunctionsMarkedForManualOptimization)                                   \
   V(ArrayList, basic_block_profiling_data, BasicBlockProfilingData)         \
   V(WeakArrayList, shared_wasm_memories, SharedWasmMemories)                \
   /* EphemeronHashTable for debug scopes (local debug evaluate) */          \
diff -r -u --color up/v8/src/runtime/runtime-debug.cc nw/v8/src/runtime/runtime-debug.cc
--- up/v8/src/runtime/runtime-debug.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/runtime/runtime-debug.cc	2023-01-19 16:46:36.373942859 -0500
@@ -18,6 +18,7 @@
 #include "src/objects/js-collection-inl.h"
 #include "src/objects/js-generator-inl.h"
 #include "src/objects/js-promise-inl.h"
+#include "src/objects/js-weak-refs-inl.h"
 #include "src/runtime/runtime-utils.h"
 #include "src/runtime/runtime.h"
 #include "src/snapshot/embedded/embedded-data.h"
@@ -300,6 +301,13 @@
         isolate, result,
         isolate->factory()->NewStringFromAsciiChecked("[[PrimitiveValue]]"),
         handle(js_value->value(), isolate));
+  } else if (object->IsJSWeakRef()) {
+    Handle<JSWeakRef> js_weak_ref = Handle<JSWeakRef>::cast(object);
+
+    result = ArrayList::Add(
+        isolate, result,
+        isolate->factory()->NewStringFromAsciiChecked("[[WeakRefTarget]]"),
+        handle(js_weak_ref->target(), isolate));
   } else if (object->IsJSArrayBuffer()) {
     Handle<JSArrayBuffer> js_array_buffer = Handle<JSArrayBuffer>::cast(object);
     if (js_array_buffer->was_detached()) {
diff -r -u --color up/v8/src/runtime/runtime-intl.cc nw/v8/src/runtime/runtime-intl.cc
--- up/v8/src/runtime/runtime-intl.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/runtime/runtime-intl.cc	2023-01-19 16:46:36.373942859 -0500
@@ -57,5 +57,17 @@
   RETURN_RESULT_OR_FAILURE(isolate, Intl::ConvertToUpper(isolate, s));
 }
 
+RUNTIME_FUNCTION(Runtime_StringToLocaleLowerCase) {
+  HandleScope scope(isolate);
+  DCHECK_EQ(args.length(), 2);
+  Handle<String> s = args.at<String>(0);
+  Handle<Object> locale = args.at<Object>(1);
+
+  isolate->CountUsage(v8::Isolate::UseCounterFeature::kStringToLocaleLowerCase);
+
+  RETURN_RESULT_OR_FAILURE(
+      isolate, Intl::StringLocaleConvertCase(isolate, s, false, locale));
+}
+
 }  // namespace internal
 }  // namespace v8
diff -r -u --color up/v8/src/runtime/runtime-scopes.cc nw/v8/src/runtime/runtime-scopes.cc
--- up/v8/src/runtime/runtime-scopes.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/runtime/runtime-scopes.cc	2023-01-19 16:46:36.373942859 -0500
@@ -234,10 +234,17 @@
   // context and not the declaration context.
   Handle<Context> context(isolate->context().declaration_context(), isolate);
 
+  // For debug-evaluate we always use sloppy eval, in which case context could
+  // also be a module context. As module contexts re-use the extension slot
+  // we need to check for this.
+  const bool is_debug_evaluate_in_module =
+      isolate->context().IsDebugEvaluateContext() && context->IsModuleContext();
+
   DCHECK(context->IsFunctionContext() || context->IsNativeContext() ||
          context->IsScriptContext() || context->IsEvalContext() ||
          (context->IsBlockContext() &&
-          context->scope_info().is_declaration_scope()));
+          context->scope_info().is_declaration_scope()) ||
+         is_debug_evaluate_in_module);
 
   bool is_var = value->IsUndefined(isolate);
   DCHECK_IMPLIES(!is_var, value->IsJSFunction());
@@ -288,10 +295,11 @@
 
     object = Handle<JSObject>::cast(holder);
 
-  } else if (context->has_extension()) {
+  } else if (context->has_extension() && !is_debug_evaluate_in_module) {
     object = handle(context->extension_object(), isolate);
     DCHECK(object->IsJSContextExtensionObject());
-  } else if (context->scope_info().HasContextExtensionSlot()) {
+  } else if (context->scope_info().HasContextExtensionSlot() &&
+             !is_debug_evaluate_in_module) {
     // Sloppy varblock and function contexts might not have an extension object
     // yet. Sloppy eval will never have an extension object, as vars are hoisted
     // out, and lets are known statically.
diff -r -u --color up/v8/src/runtime/runtime-test.cc nw/v8/src/runtime/runtime-test.cc
--- up/v8/src/runtime/runtime-test.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/runtime/runtime-test.cc	2023-01-19 16:46:36.373942859 -0500
@@ -2,6 +2,8 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
+#include <stdio.h>
+
 #include <fstream>
 #include <memory>
 
@@ -247,14 +249,8 @@
 
 namespace {
 
-template <CodeKind code_kind>
-bool CanOptimizeFunction(Handle<JSFunction> function, Isolate* isolate,
-                         IsCompiledScope* is_compiled_scope);
-
-template <>
-bool CanOptimizeFunction<CodeKind::TURBOFAN>(
-    Handle<JSFunction> function, Isolate* isolate,
-    IsCompiledScope* is_compiled_scope) {
+bool CanOptimizeFunction(CodeKind target_kind, Handle<JSFunction> function,
+                         Isolate* isolate, IsCompiledScope* is_compiled_scope) {
   // The following conditions were lifted (in part) from the DCHECK inside
   // JSFunction::MarkForOptimization().
 
@@ -282,40 +278,23 @@
   }
 
   if (v8_flags.testing_d8_test_runner) {
-    PendingOptimizationTable::MarkedForOptimization(isolate, function);
+    ManualOptimizationTable::CheckMarkedForManualOptimization(isolate,
+                                                              *function);
   }
 
-  CodeKind kind = CodeKindForTopTier();
-  if (function->HasAvailableOptimizedCode() ||
-      function->HasAvailableCodeKind(kind)) {
+  if (function->HasAvailableCodeKind(target_kind) ||
+      function->HasAvailableHigherTierCodeThan(target_kind) ||
+      IsInProgress(function->tiering_state())) {
     DCHECK(function->HasAttachedOptimizedCode() ||
            function->ChecksTieringState());
-    if (v8_flags.testing_d8_test_runner) {
-      PendingOptimizationTable::FunctionWasOptimized(isolate, function);
-    }
     return false;
   }
 
   return true;
 }
 
-#ifdef V8_ENABLE_MAGLEV
-template <>
-bool CanOptimizeFunction<CodeKind::MAGLEV>(Handle<JSFunction> function,
-                                           Isolate* isolate,
-                                           IsCompiledScope* is_compiled_scope) {
-  if (!v8_flags.maglev) return false;
-
-  CHECK(!IsAsmWasmFunction(isolate, *function));
-
-  // TODO(v8:7700): Disabled optimization due to deopts?
-  // TODO(v8:7700): Already cached?
-
-  return function->GetActiveTier() < CodeKind::MAGLEV;
-}
-#endif  // V8_ENABLE_MAGLEV
-
-Object OptimizeFunctionOnNextCall(RuntimeArguments& args, Isolate* isolate) {
+Object OptimizeFunctionOnNextCall(RuntimeArguments& args, Isolate* isolate,
+                                  CodeKind target_kind) {
   if (args.length() != 1 && args.length() != 2) {
     return CrashUnlessFuzzing(isolate);
   }
@@ -324,11 +303,10 @@
   if (!function_object->IsJSFunction()) return CrashUnlessFuzzing(isolate);
   Handle<JSFunction> function = Handle<JSFunction>::cast(function_object);
 
-  static constexpr CodeKind kCodeKind = CodeKind::TURBOFAN;
-
   IsCompiledScope is_compiled_scope(
       function->shared().is_compiled_scope(isolate));
-  if (!CanOptimizeFunction<kCodeKind>(function, isolate, &is_compiled_scope)) {
+  if (!CanOptimizeFunction(target_kind, function, isolate,
+                           &is_compiled_scope)) {
     return ReadOnlyRoots(isolate).undefined_value();
   }
 
@@ -354,37 +332,31 @@
     function->set_code(codet);
   }
 
-  TraceManualRecompile(*function, kCodeKind, concurrency_mode);
+  TraceManualRecompile(*function, target_kind, concurrency_mode);
   JSFunction::EnsureFeedbackVector(isolate, function, &is_compiled_scope);
-  function->MarkForOptimization(isolate, CodeKind::TURBOFAN, concurrency_mode);
+  function->MarkForOptimization(isolate, target_kind, concurrency_mode);
 
   return ReadOnlyRoots(isolate).undefined_value();
 }
 
-bool EnsureFeedbackVector(Isolate* isolate, Handle<JSFunction> function) {
+bool EnsureCompiledAndFeedbackVector(Isolate* isolate,
+                                     Handle<JSFunction> function,
+                                     IsCompiledScope* is_compiled_scope) {
   // Check function allows lazy compilation.
   if (!function->shared().allows_lazy_compilation()) return false;
 
-  if (function->has_feedback_vector()) return true;
-
   // If function isn't compiled, compile it now.
-  IsCompiledScope is_compiled_scope(
-      function->shared().is_compiled_scope(function->GetIsolate()));
-  // If the JSFunction isn't compiled but it has a initialized feedback cell
-  // then no need to compile. CompileLazy builtin would handle these cases by
-  // installing the code from SFI. Calling compile here may cause another
-  // optimization if v8_flags.always_turbofan is set.
-  bool needs_compilation =
-      !function->is_compiled() && !function->has_closure_feedback_cell_array();
-  if (needs_compilation &&
+  *is_compiled_scope =
+      function->shared().is_compiled_scope(function->GetIsolate());
+  if (!is_compiled_scope->is_compiled() &&
       !Compiler::Compile(isolate, function, Compiler::CLEAR_EXCEPTION,
-                         &is_compiled_scope)) {
+                         is_compiled_scope)) {
     return false;
   }
 
   // Ensure function has a feedback vector to hold type feedback for
   // optimization.
-  JSFunction::EnsureFeedbackVector(isolate, function, &is_compiled_scope);
+  JSFunction::EnsureFeedbackVector(isolate, function, is_compiled_scope);
   return true;
 }
 
@@ -506,27 +478,7 @@
 #ifdef V8_ENABLE_MAGLEV
 RUNTIME_FUNCTION(Runtime_OptimizeMaglevOnNextCall) {
   HandleScope scope(isolate);
-  DCHECK_EQ(args.length(), 1);
-  Handle<JSFunction> function = args.at<JSFunction>(0);
-
-  static constexpr CodeKind kCodeKind = CodeKind::MAGLEV;
-
-  IsCompiledScope is_compiled_scope(
-      function->shared().is_compiled_scope(isolate));
-  if (!CanOptimizeFunction<kCodeKind>(function, isolate, &is_compiled_scope)) {
-    return ReadOnlyRoots(isolate).undefined_value();
-  }
-  DCHECK(is_compiled_scope.is_compiled());
-  DCHECK(function->is_compiled());
-
-  // TODO(v8:7700): Support concurrent compiles.
-  const ConcurrencyMode concurrency_mode = ConcurrencyMode::kSynchronous;
-
-  TraceManualRecompile(*function, kCodeKind, concurrency_mode);
-  JSFunction::EnsureFeedbackVector(isolate, function, &is_compiled_scope);
-  function->MarkForOptimization(isolate, kCodeKind, concurrency_mode);
-
-  return ReadOnlyRoots(isolate).undefined_value();
+  return OptimizeFunctionOnNextCall(args, isolate, CodeKind::MAGLEV);
 }
 #else
 RUNTIME_FUNCTION(Runtime_OptimizeMaglevOnNextCall) {
@@ -538,14 +490,19 @@
 // TODO(jgruber): Rename to OptimizeTurbofanOnNextCall.
 RUNTIME_FUNCTION(Runtime_OptimizeFunctionOnNextCall) {
   HandleScope scope(isolate);
-  return OptimizeFunctionOnNextCall(args, isolate);
+  return OptimizeFunctionOnNextCall(args, isolate, CodeKind::TURBOFAN);
 }
 
 RUNTIME_FUNCTION(Runtime_EnsureFeedbackVectorForFunction) {
   HandleScope scope(isolate);
   DCHECK_EQ(1, args.length());
   Handle<JSFunction> function = args.at<JSFunction>(0);
-  EnsureFeedbackVector(isolate, function);
+  if (function->has_feedback_vector()) {
+    return ReadOnlyRoots(isolate).undefined_value();
+  }
+
+  IsCompiledScope is_compiled_scope;
+  EnsureCompiledAndFeedbackVector(isolate, function, &is_compiled_scope);
   return ReadOnlyRoots(isolate).undefined_value();
 }
 
@@ -556,23 +513,13 @@
   }
   Handle<JSFunction> function = args.at<JSFunction>(0);
 
-  bool allow_heuristic_optimization = false;
-  if (args.length() == 2) {
-    Handle<Object> sync_object = args.at(1);
-    if (!sync_object->IsString()) return CrashUnlessFuzzing(isolate);
-    Handle<String> sync = Handle<String>::cast(sync_object);
-    if (sync->IsOneByteEqualTo(
-            base::StaticCharVector("allow heuristic optimization"))) {
-      allow_heuristic_optimization = true;
-    }
-  }
-
-  if (!EnsureFeedbackVector(isolate, function)) {
+  IsCompiledScope is_compiled_scope;
+  if (!EnsureCompiledAndFeedbackVector(isolate, function, &is_compiled_scope)) {
     return CrashUnlessFuzzing(isolate);
   }
 
-  // If optimization is disabled for the function, return without making it
-  // pending optimize for test.
+  // If optimization is disabled for the function, return without marking it for
+  // manual optimization
   if (function->shared().optimization_disabled() &&
       function->shared().disabled_optimization_reason() ==
           BailoutReason::kNeverOptimize) {
@@ -584,8 +531,8 @@
   // Hold onto the bytecode array between marking and optimization to ensure
   // it's not flushed.
   if (v8_flags.testing_d8_test_runner) {
-    PendingOptimizationTable::PreparedForOptimization(
-        isolate, function, allow_heuristic_optimization);
+    ManualOptimizationTable::MarkFunctionForManualOptimization(
+        isolate, function, &is_compiled_scope);
   }
 
   return ReadOnlyRoots(isolate).undefined_value();
@@ -673,17 +620,14 @@
   }
 
   if (v8_flags.testing_d8_test_runner) {
-    PendingOptimizationTable::MarkedForOptimization(isolate, function);
+    ManualOptimizationTable::CheckMarkedForManualOptimization(isolate,
+                                                              *function);
   }
 
   if (function->HasAvailableOptimizedCode()) {
     DCHECK(function->HasAttachedOptimizedCode() ||
            function->ChecksTieringState());
-    // If function is already optimized, remove the bytecode array from the
-    // pending optimize for test table and return.
-    if (v8_flags.testing_d8_test_runner) {
-      PendingOptimizationTable::FunctionWasOptimized(isolate, function);
-    }
+    // If function is already optimized, return.
     return ReadOnlyRoots(isolate).undefined_value();
   }
 
@@ -847,6 +791,9 @@
   if (function->ActiveTierIsIgnition()) {
     status |= static_cast<int>(OptimizationStatus::kInterpreted);
   }
+  if (!function->is_compiled()) {
+    status |= static_cast<int>(OptimizationStatus::kIsLazy);
+  }
 
   // Additionally, detect activations of this frame on the stack, and report the
   // status of the topmost frame.
@@ -907,6 +854,20 @@
   return ReadOnlyRoots(isolate).undefined_value();
 }
 
+RUNTIME_FUNCTION(Runtime_ForceFlush) {
+  HandleScope scope(isolate);
+  if (args.length() != 1) return CrashUnlessFuzzing(isolate);
+
+  Handle<Object> function_object = args.at(0);
+  if (!function_object->IsJSFunction()) return CrashUnlessFuzzing(isolate);
+  Handle<JSFunction> function = Handle<JSFunction>::cast(function_object);
+
+  SharedFunctionInfo::DiscardCompiled(
+      isolate, handle(function->shared(isolate), isolate));
+  function->ResetIfCodeFlushed();
+  return ReadOnlyRoots(isolate).undefined_value();
+}
+
 static void ReturnNull(const v8::FunctionCallbackInfo<v8::Value>& args) {
   args.GetReturnValue().SetNull();
 }
@@ -1100,8 +1061,7 @@
   return ReadOnlyRoots(isolate).undefined_value();
 }
 
-static void DebugPrintImpl(MaybeObject maybe_object) {
-  StdoutStream os;
+static void DebugPrintImpl(MaybeObject maybe_object, std::ostream& os) {
   if (maybe_object->IsCleared()) {
     os << "[weak cleared]";
   } else {
@@ -1126,10 +1086,21 @@
 
 RUNTIME_FUNCTION(Runtime_DebugPrint) {
   SealHandleScope shs(isolate);
-  DCHECK_EQ(1, args.length());
+
+  // This is exposed to tests / fuzzers; handle variable arguments gracefully.
+  std::unique_ptr<std::ostream> output_stream(new StdoutStream());
+  if (args.length() >= 2) {
+    // Args: object, stream.
+    if (args[1].IsSmi()) {
+      int output_int = Smi::cast(args[1]).value();
+      if (output_int == fileno(stderr)) {
+        output_stream.reset(new StderrStream());
+      }
+    }
+  }
 
   MaybeObject maybe_object(*args.address_of_arg_at(0));
-  DebugPrintImpl(maybe_object);
+  DebugPrintImpl(maybe_object, *output_stream.get());
   return args[0];
 }
 
@@ -1144,7 +1115,7 @@
     size_t pointer;
     if (object.ToIntegerIndex(&pointer)) {
       MaybeObject from_pointer(static_cast<Address>(pointer));
-      DebugPrintImpl(from_pointer);
+      DebugPrintImpl(from_pointer, os);
     }
   }
   // We don't allow the converted pointer to leak out to JavaScript.
@@ -1201,13 +1172,28 @@
 // very slowly for very deeply nested ConsStrings.  For debugging use only.
 RUNTIME_FUNCTION(Runtime_GlobalPrint) {
   SealHandleScope shs(isolate);
-  DCHECK_EQ(1, args.length());
+
+  // This is exposed to tests / fuzzers; handle variable arguments gracefully.
+  FILE* output_stream = stdout;
+  if (args.length() >= 2) {
+    // Args: object, stream.
+    if (args[1].IsSmi()) {
+      int output_int = Smi::cast(args[1]).value();
+      if (output_int == fileno(stderr)) {
+        output_stream = stderr;
+      }
+    }
+  }
+
+  if (!args[0].IsString()) {
+    return args[0];
+  }
 
   auto string = String::cast(args[0]);
   StringCharacterStream stream(string);
   while (stream.HasMore()) {
     uint16_t character = stream.GetNext();
-    PrintF("%c", character);
+    PrintF(output_stream, "%c", character);
   }
   return string;
 }
diff -r -u --color up/v8/src/runtime/runtime-typedarray.cc nw/v8/src/runtime/runtime-typedarray.cc
--- up/v8/src/runtime/runtime-typedarray.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/runtime/runtime-typedarray.cc	2023-01-19 16:46:36.373942859 -0500
@@ -16,8 +16,25 @@
 
 RUNTIME_FUNCTION(Runtime_ArrayBufferDetach) {
   HandleScope scope(isolate);
-  DCHECK_EQ(1, args.length());
+  // This runtime function is exposed in ClusterFuzz and as such has to
+  // support arbitrary arguments.
+  if (args.length() < 1 || !args.at(0)->IsJSArrayBuffer()) {
+    THROW_NEW_ERROR_RETURN_FAILURE(
+        isolate, NewTypeError(MessageTemplate::kNotTypedArray));
+  }
+  Handle<JSArrayBuffer> array_buffer = Handle<JSArrayBuffer>::cast(args.at(0));
+  constexpr bool kForceForWasmMemory = false;
+  MAYBE_RETURN(JSArrayBuffer::Detach(array_buffer, kForceForWasmMemory,
+                                     args.atOrUndefined(isolate, 1)),
+               ReadOnlyRoots(isolate).exception());
+  return ReadOnlyRoots(isolate).undefined_value();
+}
+
+RUNTIME_FUNCTION(Runtime_ArrayBufferSetDetachKey) {
+  HandleScope scope(isolate);
+  DCHECK_EQ(2, args.length());
   Handle<Object> argument = args.at(0);
+  Handle<Object> key = args.at(1);
   // This runtime function is exposed in ClusterFuzz and as such has to
   // support arbitrary arguments.
   if (!argument->IsJSArrayBuffer()) {
@@ -25,7 +42,7 @@
         isolate, NewTypeError(MessageTemplate::kNotTypedArray));
   }
   Handle<JSArrayBuffer> array_buffer = Handle<JSArrayBuffer>::cast(argument);
-  array_buffer->Detach();
+  array_buffer->set_detach_key(*key);
   return ReadOnlyRoots(isolate).undefined_value();
 }
 
diff -r -u --color up/v8/src/runtime/runtime-wasm.cc nw/v8/src/runtime/runtime-wasm.cc
--- up/v8/src/runtime/runtime-wasm.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/runtime/runtime-wasm.cc	2023-01-19 16:46:36.373942859 -0500
@@ -238,29 +238,52 @@
 
 RUNTIME_FUNCTION(Runtime_WasmCompileLazy) {
   ClearThreadInWasmScope wasm_flag(isolate);
+  DisallowHeapAllocation no_gc;
   HandleScope scope(isolate);
-  DCHECK_EQ(3, args.length());
-  Handle<WasmInstanceObject> instance(WasmInstanceObject::cast(args[0]),
-                                      isolate);
+  DCHECK_EQ(2, args.length());
+  WasmInstanceObject instance = WasmInstanceObject::cast(args[0]);
   int func_index = args.smi_value_at(1);
 
-  // Save the native_module on the stack, where the GC will use it to scan
-  // WasmCompileLazy stack frames.
-  wasm::NativeModule** native_module_stack_slot =
-      reinterpret_cast<wasm::NativeModule**>(args.address_of_arg_at(2));
-  *native_module_stack_slot = instance->module_object().native_module();
-
   DCHECK(isolate->context().is_null());
-  isolate->set_context(instance->native_context());
+  isolate->set_context(instance.native_context());
   bool success = wasm::CompileLazy(isolate, instance, func_index);
   if (!success) {
+    DCHECK(v8_flags.wasm_lazy_validation);
+    AllowHeapAllocation throwing_unwinds_the_stack;
     wasm::ThrowLazyCompilationError(
-        isolate, instance->module_object().native_module(), func_index);
+        isolate, instance.module_object().native_module(), func_index);
     DCHECK(isolate->has_pending_exception());
     return ReadOnlyRoots{isolate}.exception();
   }
 
-  return Smi::FromInt(wasm::JumpTableOffset(instance->module(), func_index));
+  return Smi::FromInt(wasm::JumpTableOffset(instance.module(), func_index));
+}
+
+RUNTIME_FUNCTION(Runtime_WasmAllocateFeedbackVector) {
+  ClearThreadInWasmScope wasm_flag(isolate);
+  DCHECK(v8_flags.wasm_speculative_inlining);
+  HandleScope scope(isolate);
+  DCHECK_EQ(3, args.length());
+  Handle<WasmInstanceObject> instance(WasmInstanceObject::cast(args[0]),
+                                      isolate);
+  int declared_func_index = args.smi_value_at(1);
+  wasm::NativeModule** native_module_stack_slot =
+      reinterpret_cast<wasm::NativeModule**>(args.address_of_arg_at(2));
+  wasm::NativeModule* native_module = instance->module_object().native_module();
+  // We have to save the native_module on the stack, in case the allocation
+  // triggers a GC and we need the module to scan LiftoffSetupFrame stack frame.
+  *native_module_stack_slot = native_module;
+
+  DCHECK(isolate->context().is_null());
+  isolate->set_context(instance->native_context());
+
+  const wasm::WasmModule* module = native_module->module();
+  int func_index = declared_func_index + module->num_imported_functions;
+  Handle<FixedArray> vector = isolate->factory()->NewFixedArrayWithZeroes(
+      NumFeedbackSlots(module, func_index));
+  DCHECK_EQ(instance->feedback_vectors().get(declared_func_index), Smi::zero());
+  instance->feedback_vectors().set(declared_func_index, *vector);
+  return *vector;
 }
 
 namespace {
diff -r -u --color up/v8/src/runtime/runtime.h nw/v8/src/runtime/runtime.h
--- up/v8/src/runtime/runtime.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/runtime/runtime.h	2023-01-19 16:46:36.373942859 -0500
@@ -202,6 +202,7 @@
   F(FormatList, 2, 1)                 \
   F(FormatListToParts, 2, 1)          \
   F(StringToLowerCaseIntl, 1, 1)      \
+  F(StringToLocaleLowerCase, 2, 1)    \
   F(StringToUpperCaseIntl, 1, 1)  // End of macro.
 #else
 #define FOR_EACH_INTRINSIC_INTL(F, I)
@@ -496,7 +497,7 @@
   F(ConstructSlicedString, 2, 1)              \
   F(ConstructThinString, 1, 1)                \
   F(CurrentFrameIsTurbofan, 0, 1)             \
-  F(DebugPrint, 1, 1)                         \
+  F(DebugPrint, -1, 1)                        \
   F(DebugPrintPtr, 1, 1)                      \
   F(DebugTrace, 0, 1)                         \
   F(DebugTrackRetainingPath, -1, 1)           \
@@ -507,12 +508,13 @@
   F(EnableCodeLoggingForTesting, 0, 1)        \
   F(EnsureFeedbackVectorForFunction, 1, 1)    \
   F(FinalizeOptimization, 0, 1)               \
+  F(ForceFlush, 1, 1)                         \
   F(GetCallable, 0, 1)                        \
   F(GetInitializerFunction, 1, 1)             \
   F(GetOptimizationStatus, 1, 1)              \
   F(GetUndetectable, 0, 1)                    \
   F(GetWeakCollectionSize, 1, 1)              \
-  F(GlobalPrint, 1, 1)                        \
+  F(GlobalPrint, -1, 1)                       \
   F(HasDictionaryElements, 1, 1)              \
   F(HasDoubleElements, 1, 1)                  \
   F(HasElementsInALargeObjectSpace, 1, 1)     \
@@ -590,7 +592,8 @@
   I(DeoptimizeNow, 0, 1)
 
 #define FOR_EACH_INTRINSIC_TYPEDARRAY(F, I)    \
-  F(ArrayBufferDetach, 1, 1)                   \
+  F(ArrayBufferDetach, -1, 1)                  \
+  F(ArrayBufferSetDetachKey, 2, 1)             \
   F(GrowableSharedArrayBufferByteLength, 1, 1) \
   F(TypedArrayCopyElements, 3, 1)              \
   F(TypedArrayGetBuffer, 1, 1)                 \
@@ -617,7 +620,8 @@
   F(WasmTableGrow, 3, 1)              \
   F(WasmTableFill, 5, 1)              \
   F(WasmJSToWasmObject, 3, 1)         \
-  F(WasmCompileLazy, 3, 1)            \
+  F(WasmCompileLazy, 2, 1)            \
+  F(WasmAllocateFeedbackVector, 3, 1) \
   F(WasmCompileWrapper, 2, 1)         \
   F(WasmTriggerTierUp, 1, 1)          \
   F(WasmDebugBreak, 0, 1)             \
@@ -939,6 +943,7 @@
   kBaseline = 1 << 15,
   kTopmostFrameIsInterpreted = 1 << 16,
   kTopmostFrameIsBaseline = 1 << 17,
+  kIsLazy = 1 << 18,
 };
 
 }  // namespace internal
diff -r -u --color up/v8/src/sandbox/external-pointer-table-inl.h nw/v8/src/sandbox/external-pointer-table-inl.h
--- up/v8/src/sandbox/external-pointer-table-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/sandbox/external-pointer-table-inl.h	2023-01-19 16:46:36.373942859 -0500
@@ -6,6 +6,7 @@
 #define V8_SANDBOX_EXTERNAL_POINTER_TABLE_INL_H_
 
 #include "src/base/atomicops.h"
+#include "src/common/assert-scope.h"
 #include "src/sandbox/external-pointer-table.h"
 #include "src/sandbox/external-pointer.h"
 #include "src/utils/allocation.h"
@@ -75,6 +76,13 @@
     Isolate* isolate, Address initial_value, ExternalPointerTag tag) {
   DCHECK(is_initialized());
 
+  // We currently don't want entry allocation to trigger garbage collection as
+  // this may cause seemingly harmless pointer field assignments to trigger
+  // garbage collection. This is especially true for lazily-initialized
+  // external pointer slots which will typically only allocate the external
+  // pointer table entry when the pointer is first set to a non-null value.
+  DisallowGarbageCollection no_gc;
+
   Freelist freelist;
   bool success = false;
   while (!success) {
@@ -144,7 +152,14 @@
 void ExternalPointerTable::Mark(ExternalPointerHandle handle,
                                 Address handle_location) {
   static_assert(sizeof(base::Atomic64) == sizeof(Address));
-  DCHECK_EQ(handle, *reinterpret_cast<ExternalPointerHandle*>(handle_location));
+  // The handle_location must contain the given handle. The only exception to
+  // this is when the handle is zero, which means that it hasn't yet been
+  // initialized. In that case, the handle may be initialized between the
+  // caller loading it and this DCHECK loading it again, in which case the two
+  // values would not be the same. This scenario is unproblematic though as the
+  // new entry will already be marked as alive as it has just been allocated.
+  DCHECK(handle == kNullExternalPointerHandle ||
+         handle == *reinterpret_cast<ExternalPointerHandle*>(handle_location));
 
   uint32_t index = HandleToIndex(handle);
 
diff -r -u --color up/v8/src/sandbox/external-pointer-table.cc nw/v8/src/sandbox/external-pointer-table.cc
--- up/v8/src/sandbox/external-pointer-table.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/sandbox/external-pointer-table.cc	2023-01-19 16:46:36.373942859 -0500
@@ -315,18 +315,6 @@
 
   set_capacity(new_capacity);
 
-  // Schedule GC when the table's utilization crosses one of these thresholds.
-  constexpr double kGCThresholds[] = {0.5, 0.75, 0.9, 0.95, 0.99};
-  constexpr double kMaxCapacity = static_cast<double>(kMaxExternalPointers);
-  double old_utilization = static_cast<double>(old_capacity) / kMaxCapacity;
-  double new_utilization = static_cast<double>(new_capacity) / kMaxCapacity;
-  for (double threshold : kGCThresholds) {
-    if (old_utilization < threshold && new_utilization >= threshold) {
-      isolate->heap()->ReportExternalMemoryPressure();
-      break;
-    }
-  }
-
   // Build freelist bottom to top, which might be more cache friendly.
   uint32_t start = std::max<uint32_t>(old_capacity, 1);  // Skip entry zero
   uint32_t last = new_capacity - 1;
diff -r -u --color up/v8/src/snapshot/code-serializer.cc nw/v8/src/snapshot/code-serializer.cc
--- up/v8/src/snapshot/code-serializer.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/snapshot/code-serializer.cc	2023-01-19 16:46:36.373942859 -0500
@@ -466,6 +466,7 @@
   if (!maybe_result.ToHandle(&result)) {
     // Deserializing may fail if the reservations cannot be fulfilled.
     if (v8_flags.profile_deserialization) PrintF("[Deserializing failed]\n");
+    cached_data->Reject();
     return MaybeHandle<SharedFunctionInfo>();
   }
   BaselineBatchCompileIfSparkplugCompiled(isolate,
@@ -737,7 +738,7 @@
     SerializedCodeSanityCheckResult* rejection_result) {
   DisallowGarbageCollection no_gc;
   SerializedCodeData scd(cached_data);
-  *rejection_result = scd.SanityCheck(expected_source_hash);
+  *rejection_result = SerializedCodeSanityCheckResult::kSuccess; //scd.SanityCheck(expected_source_hash);
   if (*rejection_result != SerializedCodeSanityCheckResult::kSuccess) {
     cached_data->Reject();
     return SerializedCodeData(nullptr, 0);
diff -r -u --color up/v8/src/snapshot/context-serializer.cc nw/v8/src/snapshot/context-serializer.cc
--- up/v8/src/snapshot/context-serializer.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/snapshot/context-serializer.cc	2023-01-19 16:46:36.384776190 -0500
@@ -203,12 +203,11 @@
 }
 
 bool ContextSerializer::ShouldBeInTheStartupObjectCache(HeapObject o) {
-  // Scripts should be referred only through shared function infos.  We can't
-  // allow them to be part of the context snapshot because they contain a
-  // unique ID, and deserializing several context snapshots containing script
-  // would cause dupes.
-  DCHECK(!o.IsScript());
-  return o.IsName() || o.IsSharedFunctionInfo() || o.IsHeapNumber() ||
+  // We can't allow scripts to be part of the context snapshot because they
+  // contain a unique ID, and deserializing several context snapshots containing
+  // script would cause dupes.
+  return o.IsName() || o.IsScript() || o.IsSharedFunctionInfo() ||
+         o.IsHeapNumber() ||
          (V8_EXTERNAL_CODE_SPACE_BOOL && o.IsCodeDataContainer()) ||
          o.IsCode() || o.IsScopeInfo() || o.IsAccessorInfo() ||
          o.IsTemplateInfo() || o.IsClassPositions() ||
diff -r -u --color up/v8/src/snapshot/deserializer.cc nw/v8/src/snapshot/deserializer.cc
--- up/v8/src/snapshot/deserializer.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/snapshot/deserializer.cc	2023-01-19 16:46:36.384776190 -0500
@@ -210,7 +210,9 @@
 #ifdef DEBUG
   num_api_references_ = GetNumApiReferences(isolate);
 #endif  // DEBUG
-  CHECK_EQ(magic_number_, SerializedData::kMagicNumber);
+  bool ret =
+    (magic_number_ == SerializedData::kMagicNumber);
+  valid_ = ret;
 }
 
 template <typename IsolateT>
@@ -354,10 +356,12 @@
 
 }  // namespace
 
-template <typename IsolateT>
-void Deserializer<IsolateT>::PostProcessNewJSReceiver(
-    Map map, Handle<JSReceiver> obj, InstanceType instance_type,
-    SnapshotSpace space) {
+// Should be called only on the main thread (not thread safe).
+template <>
+void Deserializer<Isolate>::PostProcessNewJSReceiver(Map map,
+                                                     Handle<JSReceiver> obj,
+                                                     InstanceType instance_type,
+                                                     SnapshotSpace space) {
   DCHECK_EQ(map.instance_type(), instance_type);
 
   if (InstanceTypeChecker::IsJSDataView(instance_type)) {
@@ -408,11 +412,18 @@
       ResizableFlag resizable = bs && bs->is_resizable_by_js()
                                     ? ResizableFlag::kResizable
                                     : ResizableFlag::kNotResizable;
-      buffer.Setup(shared, resizable, bs);
+      buffer.Setup(shared, resizable, bs, main_thread_isolate());
     }
   }
 }
 
+template <>
+void Deserializer<LocalIsolate>::PostProcessNewJSReceiver(
+    Map map, Handle<JSReceiver> obj, InstanceType instance_type,
+    SnapshotSpace space) {
+  UNREACHABLE();
+}
+
 template <typename IsolateT>
 void Deserializer<IsolateT>::PostProcessNewObject(Handle<Map> map,
                                                   Handle<HeapObject> obj,
@@ -490,11 +501,10 @@
   } else if (V8_EXTERNAL_CODE_SPACE_BOOL &&
              InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
     auto code_data_container = CodeDataContainer::cast(raw_obj);
-    code_data_container.set_code_cage_base(isolate()->code_cage_base());
     code_data_container.init_code_entry_point(main_thread_isolate(),
                                               kNullAddress);
 #ifdef V8_EXTERNAL_CODE_SPACE
-    if (V8_REMOVE_BUILTINS_CODE_OBJECTS &&
+    if (V8_EXTERNAL_CODE_SPACE_BOOL &&
         code_data_container.is_off_heap_trampoline()) {
       Address entry = OffHeapInstructionStart(code_data_container,
                                               code_data_container.builtin_id());
@@ -575,8 +585,6 @@
   switch (space) {
     case SnapshotSpace::kCode:
       return AllocationType::kCode;
-    case SnapshotSpace::kMap:
-      return AllocationType::kMap;
     case SnapshotSpace::kOld:
       return AllocationType::kOld;
     case SnapshotSpace::kReadOnlyHeap:
@@ -863,11 +871,12 @@
 #define CASE_R32(byte_code) CASE_R16(byte_code) : case CASE_R16(byte_code + 16)
 
 // This generates a case range for all the spaces.
-#define CASE_RANGE_ALL_SPACES(bytecode)                           \
-  SpaceEncoder<bytecode>::Encode(SnapshotSpace::kOld)             \
-      : case SpaceEncoder<bytecode>::Encode(SnapshotSpace::kCode) \
-      : case SpaceEncoder<bytecode>::Encode(SnapshotSpace::kMap)  \
-      : case SpaceEncoder<bytecode>::Encode(SnapshotSpace::kReadOnlyHeap)
+// clang-format off
+#define CASE_RANGE_ALL_SPACES(bytecode)                               \
+  SpaceEncoder<bytecode>::Encode(SnapshotSpace::kOld):                \
+    case SpaceEncoder<bytecode>::Encode(SnapshotSpace::kCode):        \
+    case SpaceEncoder<bytecode>::Encode(SnapshotSpace::kReadOnlyHeap)
+// clang-format on
 
 template <typename IsolateT>
 void Deserializer<IsolateT>::ReadData(Handle<HeapObject> object,
diff -r -u --color up/v8/src/snapshot/deserializer.h nw/v8/src/snapshot/deserializer.h
--- up/v8/src/snapshot/deserializer.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/snapshot/deserializer.h	2023-01-19 16:46:36.384776190 -0500
@@ -112,6 +112,8 @@
 
   Handle<HeapObject> ReadObject();
 
+  bool valid_ = true;
+
  private:
   friend class DeserializerRelocInfoVisitor;
   // A circular queue of hot objects. This is added to in the same order as in
@@ -265,11 +267,6 @@
 #endif  // DEBUG
 };
 
-extern template class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
-    Deserializer<Isolate>;
-extern template class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
-    Deserializer<LocalIsolate>;
-
 enum class DeserializingUserCodeOption {
   kNotDeserializingUserCode,
   kIsDeserializingUserCode
diff -r -u --color up/v8/src/snapshot/object-deserializer.cc nw/v8/src/snapshot/object-deserializer.cc
--- up/v8/src/snapshot/object-deserializer.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/snapshot/object-deserializer.cc	2023-01-19 16:46:36.384776190 -0500
@@ -34,6 +34,8 @@
 
 MaybeHandle<HeapObject> ObjectDeserializer::Deserialize() {
   DCHECK(deserializing_user_code());
+  if (!valid_)
+    return MaybeHandle<HeapObject>();
   HandleScope scope(isolate());
   Handle<HeapObject> result;
   {
diff -r -u --color up/v8/src/snapshot/references.h nw/v8/src/snapshot/references.h
--- up/v8/src/snapshot/references.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/snapshot/references.h	2023-01-19 16:46:36.384776190 -0500
@@ -17,10 +17,9 @@
   kReadOnlyHeap,
   kOld,
   kCode,
-  kMap,
 };
 static constexpr int kNumberOfSnapshotSpaces =
-    static_cast<int>(SnapshotSpace::kMap) + 1;
+    static_cast<int>(SnapshotSpace::kCode) + 1;
 
 class SerializerReference {
  private:
diff -r -u --color up/v8/src/snapshot/serializer-deserializer.h nw/v8/src/snapshot/serializer-deserializer.h
--- up/v8/src/snapshot/serializer-deserializer.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/snapshot/serializer-deserializer.h	2023-01-19 16:46:36.384776190 -0500
@@ -65,7 +65,7 @@
   // The static assert below will trigger when the number of preallocated spaces
   // changed. If that happens, update the kNewObject and kBackref bytecode
   // ranges in the comments below.
-  static_assert(4 == kNumberOfSnapshotSpaces);
+  static_assert(3 == kNumberOfSnapshotSpaces);
 
   // First 32 root array items.
   static const int kRootArrayConstantsCount = 0x20;
@@ -86,7 +86,7 @@
     // 0x00..0x03  Allocate new object, in specified space.
     kNewObject = 0x00,
     // Reference to previously allocated object.
-    kBackref = 0x04,
+    kBackref = 0x03,
     // Reference to an object in the read only heap.
     kReadOnlyHeapRef,
     // Object in the startup object cache.
diff -r -u --color up/v8/src/snapshot/serializer.cc nw/v8/src/snapshot/serializer.cc
--- up/v8/src/snapshot/serializer.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/snapshot/serializer.cc	2023-01-19 16:46:36.384776190 -0500
@@ -760,8 +760,6 @@
       return SnapshotSpace::kCode;
     } else if (ReadOnlyHeap::Contains(object)) {
       return SnapshotSpace::kReadOnlyHeap;
-    } else if (object.IsMap()) {
-      return SnapshotSpace::kMap;
     } else {
       return SnapshotSpace::kOld;
     }
@@ -783,13 +781,13 @@
       // detail and isn't relevant to the snapshot.
       case NEW_LO_SPACE:
       case LO_SPACE:
+      // Shared objects are currently encoded as 'old' snapshot objects. This
+      // basically duplicates shared heap objects for each isolate again.
+      case SHARED_SPACE:
+      case SHARED_LO_SPACE:
         return SnapshotSpace::kOld;
       case CODE_SPACE:
         return SnapshotSpace::kCode;
-      case MAP_SPACE:
-        return SnapshotSpace::kMap;
-      case SHARED_SPACE:
-      case SHARED_LO_SPACE:
       case CODE_LO_SPACE:
       case RO_SPACE:
         UNREACHABLE();
@@ -1228,16 +1226,13 @@
           sizeof(field_value), field_value);
     } else if (V8_EXTERNAL_CODE_SPACE_BOOL &&
                object_->IsCodeDataContainer(cage_base)) {
-      // code_cage_base and code_entry_point fields contain raw values that
-      // will be recomputed after deserialization, so write zeros to keep the
-      // snapshot deterministic.
-      CHECK_EQ(CodeDataContainer::kCodeCageBaseUpper32BitsOffset + kTaggedSize,
-               CodeDataContainer::kCodeEntryPointOffset);
-      static byte field_value[kTaggedSize + kSystemPointerSize] = {0};
-      OutputRawWithCustomField(
-          sink_, object_start, base, bytes_to_output,
-          CodeDataContainer::kCodeCageBaseUpper32BitsOffset,
-          sizeof(field_value), field_value);
+      // code_entry_point field contains a raw value that will be recomputed
+      // after deserialization, so write zeros to keep the snapshot
+      // deterministic.
+      static byte field_value[kSystemPointerSize] = {0};
+      OutputRawWithCustomField(sink_, object_start, base, bytes_to_output,
+                               CodeDataContainer::kCodeEntryPointOffset,
+                               sizeof(field_value), field_value);
     } else if (object_->IsSeqString()) {
       // SeqStrings may contain padding. Serialize the padding bytes as 0s to
       // make the snapshot content deterministic.
diff -r -u --color up/v8/src/snapshot/snapshot.cc nw/v8/src/snapshot/snapshot.cc
--- up/v8/src/snapshot/snapshot.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/snapshot/snapshot.cc	2023-01-19 16:46:36.384776190 -0500
@@ -301,7 +301,8 @@
   // PendingOptimizeTable also contains BytecodeArray, we need to clear the
   // recompilable code same as above.
   ReadOnlyRoots roots(isolate);
-  isolate->heap()->SetPendingOptimizeForTestBytecode(roots.undefined_value());
+  isolate->heap()->SetFunctionsMarkedForManualOptimization(
+      roots.undefined_value());
 }
 
 // static
@@ -315,7 +316,10 @@
 
   // Test serialization.
   {
-    GlobalSafepointScope global_safepoint(isolate);
+    SafepointKind safepoint_kind = isolate->has_shared_heap()
+                                       ? SafepointKind::kGlobal
+                                       : SafepointKind::kIsolate;
+    SafepointScope safepoint_scope(isolate, safepoint_kind);
     DisallowGarbageCollection no_gc;
 
     Snapshot::SerializerFlags flags(
@@ -325,7 +329,7 @@
              ? Snapshot::kReconstructReadOnlyAndSharedObjectCachesForTesting
              : 0));
     serialized_data = Snapshot::Create(isolate, *default_context,
-                                       global_safepoint, no_gc, flags);
+                                       safepoint_scope, no_gc, flags);
     auto_delete_serialized_data.reset(serialized_data.data);
   }
 
@@ -369,14 +373,14 @@
     Isolate* isolate, std::vector<Context>* contexts,
     const std::vector<SerializeInternalFieldsCallback>&
         embedder_fields_serializers,
-    const GlobalSafepointScope& global_safepoint,
+    const SafepointScope& safepoint_scope,
     const DisallowGarbageCollection& no_gc, SerializerFlags flags) {
   TRACE_EVENT0("v8", "V8.SnapshotCreate");
   DCHECK_EQ(contexts->size(), embedder_fields_serializers.size());
   DCHECK_GT(contexts->size(), 0);
   HandleScope scope(isolate);
 
-  // The GlobalSafepointScope ensures we are in a safepoint scope so that the
+  // The HeapSafepointScope ensures we are in a safepoint scope so that the
   // string table is safe to iterate. Unlike mksnapshot, embedders may have
   // background threads running.
 
@@ -453,13 +457,13 @@
 
 // static
 v8::StartupData Snapshot::Create(Isolate* isolate, Context default_context,
-                                 const GlobalSafepointScope& global_safepoint,
+                                 const SafepointScope& safepoint_scope,
                                  const DisallowGarbageCollection& no_gc,
                                  SerializerFlags flags) {
   std::vector<Context> contexts{default_context};
   std::vector<SerializeInternalFieldsCallback> callbacks{{}};
-  return Snapshot::Create(isolate, &contexts, callbacks, global_safepoint,
-                          no_gc, flags);
+  return Snapshot::Create(isolate, &contexts, callbacks, safepoint_scope, no_gc,
+                          flags);
 }
 
 v8::StartupData SnapshotImpl::CreateSnapshotBlob(
diff -r -u --color up/v8/src/snapshot/snapshot.h nw/v8/src/snapshot/snapshot.h
--- up/v8/src/snapshot/snapshot.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/snapshot/snapshot.h	2023-01-19 16:46:36.384776190 -0500
@@ -15,10 +15,10 @@
 namespace internal {
 
 class Context;
-class GlobalSafepointScope;
 class Isolate;
-class SnapshotData;
 class JSGlobalProxy;
+class SafepointScope;
+class SnapshotData;
 
 class Snapshot : public AllStatic {
  public:
@@ -69,14 +69,14 @@
       Isolate* isolate, std::vector<Context>* contexts,
       const std::vector<SerializeInternalFieldsCallback>&
           embedder_fields_serializers,
-      const GlobalSafepointScope& global_safepoint,
+      const SafepointScope& safepoint_scope,
       const DisallowGarbageCollection& no_gc,
       SerializerFlags flags = kDefaultSerializerFlags);
 
   // Convenience helper for the above when only serializing a single context.
   static v8::StartupData Create(
       Isolate* isolate, Context default_context,
-      const GlobalSafepointScope& global_safepoint,
+      const SafepointScope& safepoint_scope,
       const DisallowGarbageCollection& no_gc,
       SerializerFlags flags = kDefaultSerializerFlags);
 
diff -r -u --color up/v8/src/snapshot/startup-serializer.cc nw/v8/src/snapshot/startup-serializer.cc
--- up/v8/src/snapshot/startup-serializer.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/snapshot/startup-serializer.cc	2023-01-19 16:46:36.384776190 -0500
@@ -241,6 +241,7 @@
 
 bool SerializedHandleChecker::CheckGlobalAndEternalHandles() {
   isolate_->global_handles()->IterateAllRoots(this);
+  isolate_->traced_handles()->Iterate(this);
   isolate_->eternal_handles()->IterateAllRoots(this);
   return ok_;
 }
diff -r -u --color up/v8/src/trap-handler/handler-inside-posix.cc nw/v8/src/trap-handler/handler-inside-posix.cc
--- up/v8/src/trap-handler/handler-inside-posix.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/trap-handler/handler-inside-posix.cc	2023-01-19 16:46:36.406442851 -0500
@@ -47,6 +47,8 @@
 namespace internal {
 namespace trap_handler {
 
+#if V8_TRAP_HANDLER_SUPPORTED
+
 #if V8_OS_LINUX
 #define CONTEXT_REG(reg, REG) &uc->uc_mcontext.gregs[REG_##REG]
 #elif V8_OS_DARWIN
@@ -181,6 +183,8 @@
   // TryHandleSignal modifies context to change where we return to.
 }
 
+#endif
+
 }  // namespace trap_handler
 }  // namespace internal
 }  // namespace v8
diff -r -u --color up/v8/src/trap-handler/handler-inside-win.cc nw/v8/src/trap-handler/handler-inside-win.cc
--- up/v8/src/trap-handler/handler-inside-win.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/trap-handler/handler-inside-win.cc	2023-01-19 16:46:36.406442851 -0500
@@ -38,6 +38,8 @@
 namespace internal {
 namespace trap_handler {
 
+#if V8_TRAP_HANDLER_SUPPORTED
+
 // The below struct needed to access the offset in the Thread Environment Block
 // to see if the thread local storage for the thread has been allocated yet.
 //
@@ -129,6 +131,8 @@
   return EXCEPTION_CONTINUE_SEARCH;
 }
 
+#endif
+
 }  // namespace trap_handler
 }  // namespace internal
 }  // namespace v8
diff -r -u --color up/v8/src/trap-handler/handler-outside-simulator.cc nw/v8/src/trap-handler/handler-outside-simulator.cc
--- up/v8/src/trap-handler/handler-outside-simulator.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/trap-handler/handler-outside-simulator.cc	2023-01-19 16:46:36.406442851 -0500
@@ -4,6 +4,9 @@
 
 #include "include/v8config.h"
 #include "src/trap-handler/trap-handler-simulator.h"
+#include "src/trap-handler/trap-handler.h"
+
+#if V8_TRAP_HANDLER_SUPPORTED
 
 #if V8_OS_DARWIN
 #define SYMBOL(name) "_" #name
@@ -11,17 +14,6 @@
 #define SYMBOL(name) #name
 #endif  // !V8_OS_DARWIN
 
-#if defined(_MSC_VER) && !defined(__clang__)
-// MSVC does not accept inline assembly
-#include <intrin.h>
-extern "C" uintptr_t ProbeMemory(uintptr_t address, uintptr_t pc) {
-  // @pc parameter is unused.
-  // This intrinsic guarantees that a load from address will be done.
-  __iso_volatile_load8(reinterpret_cast<char*>(address));
-  return 0;
-}
-extern "C" void v8_probe_memory_continuation() {}
-#else
 // Define the ProbeMemory function declared in trap-handler-simulators.h.
 asm(
     ".globl " SYMBOL(ProbeMemory) "                 \n"
@@ -46,4 +38,5 @@
     SYMBOL(v8_probe_memory_continuation) ":         \n"
     // If the trap handler continues here, it wrote the landing pad in %rax.
     "  ret                                          \n");
+
 #endif
diff -r -u --color up/v8/src/trap-handler/trap-handler.h nw/v8/src/trap-handler/trap-handler.h
--- up/v8/src/trap-handler/trap-handler.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/trap-handler/trap-handler.h	2023-01-19 16:46:36.406442851 -0500
@@ -26,8 +26,14 @@
 #elif V8_TARGET_ARCH_ARM64 && V8_HOST_ARCH_ARM64 && V8_OS_DARWIN
 #define V8_TRAP_HANDLER_SUPPORTED true
 // Arm64 simulator on x64 on Linux, Mac, or Windows.
+//
+// The simulator case uses some inline assembly code, which cannot be
+// compiled with MSVC, so don't enable the trap handler in that case.
+// (MSVC #defines _MSC_VER, but so does Clang when targeting Windows, hence
+// the check for __clang__.)
 #elif V8_TARGET_ARCH_ARM64 && V8_HOST_ARCH_X64 && \
-    (V8_OS_LINUX || V8_OS_DARWIN || V8_OS_WIN)
+    (V8_OS_LINUX || V8_OS_DARWIN || V8_OS_WIN) && \
+    (!defined(_MSC_VER) || defined(__clang__))
 #define V8_TRAP_HANDLER_VIA_SIMULATOR
 #define V8_TRAP_HANDLER_SUPPORTED true
 // Everything else is unsupported.
@@ -35,6 +41,14 @@
 #define V8_TRAP_HANDLER_SUPPORTED false
 #endif
 
+#if V8_OS_ANDROID && V8_TRAP_HANDLER_SUPPORTED
+// It would require some careful security review before the trap handler
+// can be enabled on Android.  Android may do unexpected things with signal
+// handling and crash reporting that could open up security holes in V8's
+// trap handling.
+#error "The V8 trap handler should not be enabled on Android"
+#endif
+
 // Setup for shared library export.
 #if defined(BUILDING_V8_SHARED) && defined(V8_OS_WIN)
 #define TH_EXPORT_PRIVATE __declspec(dllexport)
diff -r -u --color up/v8/src/utils/allocation.cc nw/v8/src/utils/allocation.cc
--- up/v8/src/utils/allocation.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/utils/allocation.cc	2023-01-19 16:46:36.406442851 -0500
@@ -129,6 +129,16 @@
   return result;
 }
 
+base::AllocationResult<void*> AllocAtLeastWithRetry(size_t size) {
+  base::AllocationResult<char*> result = {nullptr, 0u};
+  for (int i = 0; i < kAllocationTries; ++i) {
+    result = base::AllocateAtLeast<char>(size);
+    if (V8_LIKELY(result.ptr != nullptr)) break;
+    OnCriticalMemoryPressure();
+  }
+  return {result.ptr, result.count};
+}
+
 void* AlignedAllocWithRetry(size_t size, size_t alignment) {
   void* result = nullptr;
   for (int i = 0; i < kAllocationTries; ++i) {
@@ -156,7 +166,7 @@
   DCHECK_NOT_NULL(page_allocator);
   DCHECK_EQ(hint, AlignedAddress(hint, alignment));
   DCHECK(IsAligned(size, page_allocator->AllocatePageSize()));
-  if (FLAG_randomize_all_allocations) {
+  if (v8_flags.randomize_all_allocations) {
     hint = AlignedAddress(page_allocator->GetRandomMmapAddr(), alignment);
   }
   void* result = nullptr;
diff -r -u --color up/v8/src/utils/allocation.h nw/v8/src/utils/allocation.h
--- up/v8/src/utils/allocation.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/utils/allocation.h	2023-01-19 16:46:36.406442851 -0500
@@ -94,6 +94,10 @@
 // Call free to release memory allocated with this function.
 void* AllocWithRetry(size_t size, MallocFn = base::Malloc);
 
+// Performs a malloc, with retry logic on failure. Returns nullptr on failure.
+// Call free to release memory allocated with this function.
+base::AllocationResult<void*> AllocAtLeastWithRetry(size_t size);
+
 V8_EXPORT_PRIVATE void* AlignedAllocWithRetry(size_t size, size_t alignment);
 V8_EXPORT_PRIVATE void AlignedFree(void* ptr);
 
diff -r -u --color up/v8/src/utils/ostreams.h nw/v8/src/utils/ostreams.h
--- up/v8/src/utils/ostreams.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/utils/ostreams.h	2023-01-19 16:46:36.406442851 -0500
@@ -80,6 +80,8 @@
   StdoutStream() : std::ostream(&stream_) {}
 
  private:
+  friend class StderrStream;
+
   static V8_EXPORT_PRIVATE base::RecursiveMutex* GetStdoutMutex();
 
   AndroidLogStream stream_;
@@ -91,12 +93,21 @@
   StdoutStream() : OFStream(stdout) {}
 
  private:
+  friend class StderrStream;
   static V8_EXPORT_PRIVATE base::RecursiveMutex* GetStdoutMutex();
 
   base::RecursiveMutexGuard mutex_guard_{GetStdoutMutex()};
 };
 #endif
 
+class StderrStream : public OFStream {
+ public:
+  StderrStream() : OFStream(stderr) {}
+
+ private:
+  base::RecursiveMutexGuard mutex_guard_{StdoutStream::GetStdoutMutex()};
+};
+
 // Wrappers to disambiguate uint16_t and base::uc16.
 struct AsUC16 {
   explicit AsUC16(uint16_t v) : value(v) {}
Only in nw/v8/src: v8.gyp
diff -r -u --color up/v8/src/wasm/baseline/arm/liftoff-assembler-arm.h nw/v8/src/wasm/baseline/arm/liftoff-assembler-arm.h
--- up/v8/src/wasm/baseline/arm/liftoff-assembler-arm.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/baseline/arm/liftoff-assembler-arm.h	2023-01-19 16:46:36.417276181 -0500
@@ -464,6 +464,23 @@
   return offset;
 }
 
+void LiftoffAssembler::CallFrameSetupStub(int declared_function_index) {
+  // TODO(jkummerow): Enable this check when we have C++20.
+  // static_assert(std::find(std::begin(wasm::kGpParamRegisters),
+  //                         std::end(wasm::kGpParamRegisters),
+  //                         kLiftoffFrameSetupFunctionReg) ==
+  //                         std::end(wasm::kGpParamRegisters));
+
+  // On ARM, we must push at least {lr} before calling the stub, otherwise
+  // it would get clobbered with no possibility to recover it.
+  Register scratch = r7;
+  mov(scratch, Operand(StackFrame::TypeToMarker(StackFrame::WASM)));
+  PushCommonFrame(scratch);
+  LoadConstant(LiftoffRegister(kLiftoffFrameSetupFunctionReg),
+               WasmValue(declared_function_index));
+  CallRuntimeStub(WasmCode::kWasmLiftoffFrameSetup);
+}
+
 void LiftoffAssembler::PrepareTailCall(int num_callee_stack_params,
                                        int stack_param_delta) {
   UseScratchRegisterScope temps(this);
@@ -496,6 +513,10 @@
   // pushed as part of frame construction, so we don't need to allocate memory
   // for them anymore.
   int frame_size = GetTotalFrameSize() - 2 * kSystemPointerSize;
+  // The frame setup builtin also pushes the feedback vector.
+  if (v8_flags.wasm_speculative_inlining) {
+    frame_size -= kSystemPointerSize;
+  }
 
   PatchingAssembler patching_assembler(AssemblerOptions{},
                                        buffer_start_ + offset,
diff -r -u --color up/v8/src/wasm/baseline/arm64/liftoff-assembler-arm64.h nw/v8/src/wasm/baseline/arm64/liftoff-assembler-arm64.h
--- up/v8/src/wasm/baseline/arm64/liftoff-assembler-arm64.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/baseline/arm64/liftoff-assembler-arm64.h	2023-01-19 16:46:36.417276181 -0500
@@ -58,10 +58,9 @@
 //  -1   | StackFrame::WASM   |
 //  -2   |     instance       |
 //  -3   |     feedback vector|
-//  -4   |     tiering budget |
 //  -----+--------------------+---------------------------
-//  -5   |     slot 0         |   ^
-//  -6   |     slot 1         |   |
+//  -4   |     slot 0         |   ^
+//  -5   |     slot 1         |   |
 //       |                    | Frame slots
 //       |                    |   |
 //       |                    |   v
@@ -214,10 +213,10 @@
                         LiftoffRegister src) {
   // AnyTrue does not depend on the number of lanes, so we can use V4S for all.
   UseScratchRegisterScope scope(assm);
-  VRegister temp = scope.AcquireV(kFormatS);
-  assm->Umaxv(temp, src.fp().V4S());
-  assm->Umov(dst.gp().W(), temp, 0);
-  assm->Cmp(dst.gp().W(), 0);
+  VRegister temp = scope.AcquireV(kFormat4S);
+  assm->Umaxp(temp, src.fp().V4S(), src.fp().V4S());
+  assm->Fmov(dst.gp().X(), temp.D());
+  assm->Cmp(dst.gp().X(), 0);
   assm->Cset(dst.gp().W(), ne);
 }
 
@@ -244,6 +243,22 @@
   return offset;
 }
 
+void LiftoffAssembler::CallFrameSetupStub(int declared_function_index) {
+  // TODO(jkummerow): Enable this check when we have C++20.
+  // static_assert(std::find(std::begin(wasm::kGpParamRegisters),
+  //                         std::end(wasm::kGpParamRegisters),
+  //                         kLiftoffFrameSetupFunctionReg) ==
+  //                         std::end(wasm::kGpParamRegisters));
+
+  // On ARM64, we must push at least {lr} before calling the stub, otherwise
+  // it would get clobbered with no possibility to recover it. So just set
+  // up the frame here.
+  EnterFrame(StackFrame::WASM);
+  LoadConstant(LiftoffRegister(kLiftoffFrameSetupFunctionReg),
+               WasmValue(declared_function_index));
+  CallRuntimeStub(WasmCode::kWasmLiftoffFrameSetup);
+}
+
 void LiftoffAssembler::PrepareTailCall(int num_callee_stack_params,
                                        int stack_param_delta) {
   UseScratchRegisterScope temps(this);
@@ -283,27 +298,18 @@
   // The frame_size includes the frame marker. The frame marker has already been
   // pushed on the stack though, so we don't need to allocate memory for it
   // anymore.
-  int initial_frame_size = GetTotalFrameSize() - 2 * kSystemPointerSize;
-  int frame_size = initial_frame_size;
+  int frame_size = GetTotalFrameSize() - 2 * kSystemPointerSize;
 
   static_assert(kStackSlotSize == kXRegSize,
                 "kStackSlotSize must equal kXRegSize");
+
   // The stack pointer is required to be quadword aligned.
   // Misalignment will cause a stack alignment fault.
-  frame_size = RoundUp(frame_size, kQuadWordSizeInBytes);
-  if (!IsImmAddSub(frame_size)) {
-    // Round the stack to a page to try to fit a add/sub immediate.
-    frame_size = RoundUp(frame_size, 0x1000);
-    if (!IsImmAddSub(frame_size)) {
-      // Stack greater than 4M! Because this is a quite improbable case, we
-      // just fallback to TurboFan.
-      bailout(kOtherReason, "Stack too big");
-      return;
-    }
-  }
-  if (frame_size > initial_frame_size) {
-    // Record the padding, as it is needed for GC offsets later.
-    max_used_spill_offset_ += (frame_size - initial_frame_size);
+  int misalignment = frame_size % kQuadWordSizeInBytes;
+  if (misalignment) {
+    int padding = kQuadWordSizeInBytes - misalignment;
+    frame_size += padding;
+    max_used_spill_offset_ += padding;
   }
 }
 
@@ -313,11 +319,15 @@
   // pushed as part of frame construction, so we don't need to allocate memory
   // for them anymore.
   int frame_size = GetTotalFrameSize() - 2 * kSystemPointerSize;
+  // The frame setup builtin also pushes the feedback vector, and an unused
+  // slot for alignment.
+  if (v8_flags.wasm_speculative_inlining) {
+    frame_size = std::max(frame_size - 2 * kSystemPointerSize, 0);
+  }
 
   // The stack pointer is required to be quadword aligned.
   // Misalignment will cause a stack alignment fault.
   DCHECK_EQ(frame_size, RoundUp(frame_size, kQuadWordSizeInBytes));
-  DCHECK(IsImmAddSub(frame_size));
 
   PatchingAssembler patching_assembler(AssemblerOptions{},
                                        buffer_start_ + offset, 1);
@@ -325,6 +335,7 @@
   if (V8_LIKELY(frame_size < 4 * KB)) {
     // This is the standard case for small frames: just subtract from SP and be
     // done with it.
+    DCHECK(IsImmAddSub(frame_size));
     patching_assembler.PatchSubSp(frame_size);
     return;
   }
@@ -1580,7 +1591,12 @@
     case kRtt:
       DCHECK(rhs.is_valid());
       DCHECK(liftoff_cond == kEqual || liftoff_cond == kUnequal);
-      V8_FALLTHROUGH;
+#if defined(V8_COMPRESS_POINTERS)
+      Cmp(lhs.W(), rhs.W());
+#else
+      Cmp(lhs.X(), rhs.X());
+#endif
+      break;
     case kI64:
       if (rhs.is_valid()) {
         Cmp(lhs.X(), rhs.X());
diff -r -u --color up/v8/src/wasm/baseline/ia32/liftoff-assembler-ia32.h nw/v8/src/wasm/baseline/ia32/liftoff-assembler-ia32.h
--- up/v8/src/wasm/baseline/ia32/liftoff-assembler-ia32.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/baseline/ia32/liftoff-assembler-ia32.h	2023-01-19 16:46:36.417276181 -0500
@@ -205,6 +205,18 @@
   return offset;
 }
 
+void LiftoffAssembler::CallFrameSetupStub(int declared_function_index) {
+  // TODO(jkummerow): Enable this check when we have C++20.
+  // static_assert(std::find(std::begin(wasm::kGpParamRegisters),
+  //                         std::end(wasm::kGpParamRegisters),
+  //                         kLiftoffFrameSetupFunctionReg) ==
+  //                         std::end(wasm::kGpParamRegisters));
+
+  LoadConstant(LiftoffRegister(kLiftoffFrameSetupFunctionReg),
+               WasmValue(declared_function_index));
+  CallRuntimeStub(WasmCode::kWasmLiftoffFrameSetup);
+}
+
 void LiftoffAssembler::PrepareTailCall(int num_callee_stack_params,
                                        int stack_param_delta) {
   // Push the return address and frame pointer to complete the stack frame.
@@ -234,6 +246,10 @@
   // pushed as part of frame construction, so we don't need to allocate memory
   // for them anymore.
   int frame_size = GetTotalFrameSize() - 2 * kSystemPointerSize;
+  // The frame setup builtin also pushes the feedback vector.
+  if (v8_flags.wasm_speculative_inlining) {
+    frame_size -= kSystemPointerSize;
+  }
   DCHECK_EQ(0, frame_size % kSystemPointerSize);
 
   // We can't run out of space when patching, just pass anything big enough to
diff -r -u --color up/v8/src/wasm/baseline/liftoff-assembler-defs.h nw/v8/src/wasm/baseline/liftoff-assembler-defs.h
--- up/v8/src/wasm/baseline/liftoff-assembler-defs.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/baseline/liftoff-assembler-defs.h	2023-01-19 16:46:36.417276181 -0500
@@ -21,6 +21,9 @@
 constexpr DoubleRegList kLiftoffAssemblerFpCacheRegs = {xmm0, xmm1, xmm2, xmm3,
                                                         xmm4, xmm5, xmm6};
 
+// For the "WasmLiftoffFrameSetup" builtin.
+constexpr Register kLiftoffFrameSetupFunctionReg = edi;
+
 #elif V8_TARGET_ARCH_X64
 
 // r10: kScratchRegister (MacroAssembler)
@@ -33,6 +36,9 @@
 constexpr DoubleRegList kLiftoffAssemblerFpCacheRegs = {xmm0, xmm1, xmm2, xmm3,
                                                         xmm4, xmm5, xmm6, xmm7};
 
+// For the "WasmLiftoffFrameSetup" builtin.
+constexpr Register kLiftoffFrameSetupFunctionReg = r12;
+
 #elif V8_TARGET_ARCH_MIPS
 
 constexpr RegList kLiftoffAssemblerGpCacheRegs = {a0, a1, a2, a3, t0, t1, t2,
@@ -49,6 +55,9 @@
 constexpr DoubleRegList kLiftoffAssemblerFpCacheRegs = {
     f0, f2, f4, f6, f8, f10, f12, f14, f16, f18, f20, f22, f24, f26};
 
+// For the "WasmLiftoffFrameSetup" builtin.
+constexpr Register kLiftoffFrameSetupFunctionReg = t0;
+
 #elif V8_TARGET_ARCH_LOONG64
 
 // t6-t8 and s3-s4: scratch registers, s6: root
@@ -61,6 +70,9 @@
     f0,  f1,  f2,  f3,  f4,  f5,  f6,  f7,  f8,  f9,  f10, f11, f12, f13, f14,
     f15, f16, f17, f18, f19, f20, f21, f22, f23, f24, f25, f26, f27, f28};
 
+// For the "WasmLiftoffFrameSetup" builtin.
+constexpr Register kLiftoffFrameSetupFunctionReg = t0;
+
 #elif V8_TARGET_ARCH_ARM
 
 // r10: root, r11: fp, r12: ip, r13: sp, r14: lr, r15: pc.
@@ -71,6 +83,9 @@
 constexpr DoubleRegList kLiftoffAssemblerFpCacheRegs = {
     d0, d1, d2, d3, d4, d5, d6, d7, d8, d9, d10, d11, d12};
 
+// For the "WasmLiftoffFrameSetup" builtin.
+constexpr Register kLiftoffFrameSetupFunctionReg = r4;
+
 #elif V8_TARGET_ARCH_ARM64
 
 // x16: ip0, x17: ip1, x18: platform register, x26: root, x28: base, x29: fp,
@@ -84,6 +99,9 @@
     d0,  d1,  d2,  d3,  d4,  d5,  d6,  d7,  d8,  d9,  d10, d11, d12, d13, d14,
     d16, d17, d18, d19, d20, d21, d22, d23, d24, d25, d26, d27, d28, d29};
 
+// For the "WasmLiftoffFrameSetup" builtin.
+constexpr Register kLiftoffFrameSetupFunctionReg = x8;
+
 #elif V8_TARGET_ARCH_S390X
 
 constexpr RegList kLiftoffAssemblerGpCacheRegs = {r2, r3, r4, r5,
@@ -92,14 +110,20 @@
 constexpr DoubleRegList kLiftoffAssemblerFpCacheRegs = {
     d0, d1, d2, d3, d4, d5, d6, d7, d8, d9, d10, d11, d12};
 
+// For the "WasmLiftoffFrameSetup" builtin.
+constexpr Register kLiftoffFrameSetupFunctionReg = r7;
+
 #elif V8_TARGET_ARCH_PPC64
 
-constexpr RegList kLiftoffAssemblerGpCacheRegs = {r3, r4, r5,  r6,  r7,
-                                                  r8, r9, r10, r11, cp};
+constexpr RegList kLiftoffAssemblerGpCacheRegs = {r3, r4,  r5,  r6,  r7, r8,
+                                                  r9, r10, r11, r15, cp};
 
 constexpr DoubleRegList kLiftoffAssemblerFpCacheRegs = {
     d0, d1, d2, d3, d4, d5, d6, d7, d8, d9, d10, d11, d12};
 
+// For the "WasmLiftoffFrameSetup" builtin.
+constexpr Register kLiftoffFrameSetupFunctionReg = r15;
+
 #elif V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64
 // Any change of kLiftoffAssemblerGpCacheRegs also need to update
 // kPushedGpRegs in frame-constants-riscv.h
@@ -113,6 +137,9 @@
 constexpr DoubleRegList kLiftoffAssemblerFpCacheRegs = {
     ft1, ft2, ft3, ft4, ft5, ft6, ft7, fa0,  fa1, fa2,
     fa3, fa4, fa5, fa6, fa7, ft8, ft9, ft10, ft11};
+
+// For the "WasmLiftoffFrameSetup" builtin.
+constexpr Register kLiftoffFrameSetupFunctionReg = t0;
 #else
 
 constexpr RegList kLiftoffAssemblerGpCacheRegs = RegList::FromBits(0xff);
@@ -121,6 +148,13 @@
     DoubleRegList::FromBits(0xff);
 
 #endif
+
+static_assert(kLiftoffFrameSetupFunctionReg != kWasmInstanceRegister);
+static_assert(kLiftoffFrameSetupFunctionReg != kRootRegister);
+#ifdef V8_COMPRESS_POINTERS_IN_SHARED_CAGE
+static_assert(kLiftoffFrameSetupFunctionReg != kPtrComprCageBaseRegister);
+#endif
+
 }  // namespace wasm
 }  // namespace internal
 }  // namespace v8
diff -r -u --color up/v8/src/wasm/baseline/liftoff-assembler.h nw/v8/src/wasm/baseline/liftoff-assembler.h
--- up/v8/src/wasm/baseline/liftoff-assembler.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/baseline/liftoff-assembler.h	2023-01-19 16:46:36.417276181 -0500
@@ -752,6 +752,7 @@
   // which can later be patched (via {PatchPrepareStackFrame)} when the size of
   // the frame is known.
   inline int PrepareStackFrame();
+  inline void CallFrameSetupStub(int declared_function_index);
   inline void PrepareTailCall(int num_callee_stack_params,
                               int stack_param_delta);
   inline void AlignFrameSize();
diff -r -u --color up/v8/src/wasm/baseline/liftoff-compiler.cc nw/v8/src/wasm/baseline/liftoff-compiler.cc
--- up/v8/src/wasm/baseline/liftoff-compiler.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/baseline/liftoff-compiler.cc	2023-01-19 16:46:36.417276181 -0500
@@ -138,12 +138,6 @@
              : call_desc;
 }
 
-constexpr LiftoffRegList GetGpParamRegisters() {
-  LiftoffRegList registers;
-  for (auto reg : kGpParamRegisters) registers.set(reg);
-  return registers;
-}
-
 constexpr LiftoffCondition GetCompareCondition(WasmOpcode opcode) {
   switch (opcode) {
     case kExprI32Eq:
@@ -346,10 +340,9 @@
 
 class LiftoffCompiler {
  public:
-  // TODO(clemensb): Make this a template parameter.
-  static constexpr Decoder::ValidateFlag validate = Decoder::kBooleanValidation;
+  using ValidationTag = Decoder::BooleanValidationTag;
 
-  using Value = ValueBase<validate>;
+  using Value = ValueBase<ValidationTag>;
 
   struct ElseState {
     MovableLabel label;
@@ -364,7 +357,7 @@
     bool in_handler = false;
   };
 
-  struct Control : public ControlBase<Value, validate> {
+  struct Control : public ControlBase<Value, ValidationTag> {
     std::unique_ptr<ElseState> else_state;
     LiftoffAssembler::CacheState label_state;
     MovableLabel label;
@@ -379,7 +372,7 @@
         : ControlBase(std::forward<Args>(args)...) {}
   };
 
-  using FullDecoder = WasmFullDecoder<validate, LiftoffCompiler>;
+  using FullDecoder = WasmFullDecoder<ValidationTag, LiftoffCompiler>;
   using ValueKindSig = LiftoffAssembler::ValueKindSig;
 
   class MostlySmallValueKindSig : public Signature<ValueKind> {
@@ -881,7 +874,15 @@
 
     __ CodeEntry();
 
-    __ EnterFrame(StackFrame::WASM);
+    if (v8_flags.wasm_speculative_inlining) {
+      CODE_COMMENT("frame setup");
+      int declared_func_index =
+          func_index_ - env_->module->num_imported_functions;
+      DCHECK_GE(declared_func_index, 0);
+      __ CallFrameSetupStub(declared_func_index);
+    } else {
+      __ EnterFrame(StackFrame::WASM);
+    }
     __ set_has_frame(true);
     pc_offset_stack_frame_construction_ = __ PrepareStackFrame();
     // {PrepareStackFrame} is the first platform-specific assembler method.
@@ -901,23 +902,7 @@
                       .AsRegister()));
     USE(kInstanceParameterIndex);
     __ cache_state()->SetInstanceCacheRegister(kWasmInstanceRegister);
-    // Load the feedback vector and cache it in a stack slot.
-    constexpr LiftoffRegList kGpParamRegisters = GetGpParamRegisters();
-    if (v8_flags.wasm_speculative_inlining) {
-      CODE_COMMENT("load feedback vector");
-      int declared_func_index =
-          func_index_ - env_->module->num_imported_functions;
-      DCHECK_GE(declared_func_index, 0);
-      LiftoffRegList pinned = kGpParamRegisters;
-      LiftoffRegister tmp = pinned.set(__ GetUnusedRegister(kGpReg, pinned));
-      __ LoadTaggedPointerFromInstance(
-          tmp.gp(), kWasmInstanceRegister,
-          WASM_INSTANCE_OBJECT_FIELD_OFFSET(FeedbackVectors));
-      __ LoadTaggedPointer(tmp.gp(), tmp.gp(), no_reg,
-                           wasm::ObjectAccess::ElementOffsetInTaggedFixedArray(
-                               declared_func_index));
-      __ Spill(liftoff::kFeedbackVectorOffset, tmp, kPointerKind);
-    }
+
     if (for_debugging_) __ ResetOSRTarget();
 
     if (num_params) {
@@ -1213,7 +1198,7 @@
     SLOW_DCHECK(__ ValidateCacheState());
     CODE_COMMENT(WasmOpcodes::OpcodeName(
         WasmOpcodes::IsPrefixOpcode(opcode)
-            ? decoder->read_prefixed_opcode<Decoder::kFullValidation>(
+            ? decoder->read_prefixed_opcode<Decoder::FullValidationTag>(
                   decoder->pc())
             : opcode));
   }
@@ -1293,9 +1278,8 @@
     return LiftoffRegister(kReturnRegister0);
   }
 
-  void CatchException(FullDecoder* decoder,
-                      const TagIndexImmediate<validate>& imm, Control* block,
-                      base::Vector<Value> values) {
+  void CatchException(FullDecoder* decoder, const TagIndexImmediate& imm,
+                      Control* block, base::Vector<Value> values) {
     DCHECK(block->is_try_catch());
     __ emit_jump(block->label.get());
 
@@ -1842,54 +1826,27 @@
         LiftoffRegList pinned;
         LiftoffRegister ref = pinned.set(__ PopToRegister());
         LiftoffRegister null = __ GetUnusedRegister(kGpReg, pinned);
-        LoadNullValue(null.gp(), pinned);
+        LoadNullValueForCompare(null.gp(), pinned);
         // Prefer to overwrite one of the input registers with the result
         // of the comparison.
         LiftoffRegister dst = __ GetUnusedRegister(kGpReg, {ref, null}, {});
+#if defined(V8_COMPRESS_POINTERS)
+        // As the value in the {null} register is only the tagged pointer part,
+        // we may only compare 32 bits, not the full pointer size.
+        __ emit_i32_set_cond(opcode == kExprRefIsNull ? kEqual : kUnequal,
+                             dst.gp(), ref.gp(), null.gp());
+#else
         __ emit_ptrsize_set_cond(opcode == kExprRefIsNull ? kEqual : kUnequal,
                                  dst.gp(), ref, null);
+#endif
         __ PushRegister(kI32, dst);
         return;
       }
       case kExprExternInternalize:
-        if (!v8_flags.wasm_gc_js_interop) {
-          LiftoffRegList pinned;
-          LiftoffRegister context_reg =
-              pinned.set(__ GetUnusedRegister(kGpReg, pinned));
-          LOAD_TAGGED_PTR_INSTANCE_FIELD(context_reg.gp(), NativeContext,
-                                         pinned);
-          LiftoffAssembler::VarState& extern_value =
-              __ cache_state()->stack_state.back();
-
-          LiftoffAssembler::VarState context(kPointerKind, context_reg, 0);
-
-          CallRuntimeStub(
-              WasmCode::kWasmExternInternalize,
-              MakeSig::Returns(kPointerKind).Params(kPointerKind, kPointerKind),
-              {extern_value, context}, decoder->position());
-          __ DropValues(1);
-          __ PushRegister(kRefNull, LiftoffRegister(kReturnRegister0));
-        }
+        // TODO(7748): Canonicalize heap numbers.
         return;
       case kExprExternExternalize:
-        if (!v8_flags.wasm_gc_js_interop) {
-          LiftoffRegList pinned;
-          LiftoffRegister context_reg =
-              pinned.set(__ GetUnusedRegister(kGpReg, pinned));
-          LOAD_TAGGED_PTR_INSTANCE_FIELD(context_reg.gp(), NativeContext,
-                                         pinned);
-          LiftoffAssembler::VarState& value =
-              __ cache_state()->stack_state.back();
-
-          LiftoffAssembler::VarState context(kPointerKind, context_reg, 0);
-
-          CallRuntimeStub(
-              WasmCode::kWasmExternExternalize,
-              MakeSig::Returns(kPointerKind).Params(kPointerKind, kPointerKind),
-              {value, context}, decoder->position());
-          __ DropValues(1);
-          __ PushRegister(kRefNull, LiftoffRegister(kReturnRegister0));
-        }
+        // This is a no-op.
         return;
       default:
         UNREACHABLE();
@@ -2399,7 +2356,7 @@
   }
 
   void LocalGet(FullDecoder* decoder, Value* result,
-                const IndexImmediate<validate>& imm) {
+                const IndexImmediate& imm) {
     auto local_slot = __ cache_state()->stack_state[imm.index];
     __ cache_state()->stack_state.emplace_back(
         local_slot.kind(), __ NextSpillOffset(local_slot.kind()));
@@ -2463,12 +2420,12 @@
   }
 
   void LocalSet(FullDecoder* decoder, const Value& value,
-                const IndexImmediate<validate>& imm) {
+                const IndexImmediate& imm) {
     LocalSet(imm.index, false);
   }
 
   void LocalTee(FullDecoder* decoder, const Value& value, Value* result,
-                const IndexImmediate<validate>& imm) {
+                const IndexImmediate& imm) {
     LocalSet(imm.index, true);
   }
 
@@ -2525,7 +2482,7 @@
   }
 
   void GlobalGet(FullDecoder* decoder, Value* result,
-                 const GlobalIndexImmediate<validate>& imm) {
+                 const GlobalIndexImmediate& imm) {
     const auto* global = &env_->module->globals[imm.index];
     ValueKind kind = global->type.kind();
     if (!CheckSupportedType(decoder, kind, "global")) {
@@ -2567,7 +2524,7 @@
   }
 
   void GlobalSet(FullDecoder* decoder, const Value&,
-                 const GlobalIndexImmediate<validate>& imm) {
+                 const GlobalIndexImmediate& imm) {
     auto* global = &env_->module->globals[imm.index];
     ValueKind kind = global->type.kind();
     if (!CheckSupportedType(decoder, kind, "global")) {
@@ -2607,7 +2564,7 @@
   }
 
   void TableGet(FullDecoder* decoder, const Value&, Value*,
-                const IndexImmediate<validate>& imm) {
+                const IndexImmediate& imm) {
     LiftoffRegList pinned;
 
     LiftoffRegister table_index_reg =
@@ -2634,7 +2591,7 @@
   }
 
   void TableSet(FullDecoder* decoder, const Value&, const Value&,
-                const IndexImmediate<validate>& imm) {
+                const IndexImmediate& imm) {
     LiftoffRegList pinned;
 
     LiftoffRegister table_index_reg =
@@ -2679,21 +2636,30 @@
     __ AssertUnreachable(AbortReason::kUnexpectedReturnFromWasmTrap);
   }
 
-  void AssertNull(FullDecoder* decoder, const Value& arg, Value* result) {
+  void AssertNullImpl(FullDecoder* decoder, const Value& arg, Value* result,
+                      LiftoffCondition cond) {
     LiftoffRegList pinned;
     LiftoffRegister obj = pinned.set(__ PopToRegister(pinned));
     Label* trap_label =
         AddOutOfLineTrap(decoder, WasmCode::kThrowWasmTrapIllegalCast);
     LiftoffRegister null = __ GetUnusedRegister(kGpReg, pinned);
-    LoadNullValue(null.gp(), pinned);
+    LoadNullValueForCompare(null.gp(), pinned);
     {
       FREEZE_STATE(trapping);
-      __ emit_cond_jump(kUnequal, trap_label, kRefNull, obj.gp(), null.gp(),
+      __ emit_cond_jump(cond, trap_label, kRefNull, obj.gp(), null.gp(),
                         trapping);
     }
     __ PushRegister(kRefNull, obj);
   }
 
+  void AssertNull(FullDecoder* decoder, const Value& arg, Value* result) {
+    AssertNullImpl(decoder, arg, result, kUnequal);
+  }
+
+  void AssertNotNull(FullDecoder* decoder, const Value& arg, Value* result) {
+    AssertNullImpl(decoder, arg, result, kEqual);
+  }
+
   void NopForTestingUnsupportedInLiftoff(FullDecoder* decoder) {
     unsupported(decoder, kOtherReason, "testing opcode");
   }
@@ -2827,7 +2793,7 @@
   // TODO(wasm): Generate a real branch table (like TF TableSwitch).
   void GenerateBrTable(FullDecoder* decoder, LiftoffRegister tmp,
                        LiftoffRegister value, uint32_t min, uint32_t max,
-                       BranchTableIterator<validate>* table_iterator,
+                       BranchTableIterator<ValidationTag>* table_iterator,
                        std::map<uint32_t, MovableLabel>* br_targets,
                        Register tmp1, Register tmp2,
                        const FreezeCacheState& frozen) {
@@ -2855,7 +2821,7 @@
                     tmp1, tmp2, frozen);
   }
 
-  void BrTable(FullDecoder* decoder, const BranchTableImmediate<validate>& imm,
+  void BrTable(FullDecoder* decoder, const BranchTableImmediate& imm,
                const Value& key) {
     LiftoffRegList pinned;
     LiftoffRegister value = pinned.set(__ PopToRegister());
@@ -2866,7 +2832,7 @@
     Register tmp2 = no_reg;
     if (dynamic_tiering()) {
       bool need_temps = false;
-      BranchTableIterator<validate> table_iterator(decoder, imm);
+      BranchTableIterator<ValidationTag> table_iterator(decoder, imm);
       while (table_iterator.has_next()) {
         uint32_t depth = table_iterator.next();
         if (depth == decoder->control_depth() - 1 ||
@@ -2885,13 +2851,13 @@
       // All targets must have the same arity (checked by validation), so
       // we can just sample any of them to find that arity.
       uint32_t ignored_length;
-      uint32_t sample_depth = decoder->read_u32v<Decoder::kNoValidation>(
+      uint32_t sample_depth = decoder->read_u32v<Decoder::NoValidationTag>(
           imm.table, &ignored_length, "first depth");
       __ PrepareForBranch(decoder->control_at(sample_depth)->br_merge()->arity,
                           pinned);
     }
 
-    BranchTableIterator<validate> table_iterator(decoder, imm);
+    BranchTableIterator<ValidationTag> table_iterator(decoder, imm);
     std::map<uint32_t, MovableLabel> br_targets;
 
     if (imm.table_count > 0) {
@@ -3178,8 +3144,8 @@
   }
 
   void LoadMem(FullDecoder* decoder, LoadType type,
-               const MemoryAccessImmediate<validate>& imm,
-               const Value& index_val, Value* result) {
+               const MemoryAccessImmediate& imm, const Value& index_val,
+               Value* result) {
     ValueKind kind = type.value_type().kind();
     DCHECK_EQ(kind, result->type.kind());
     if (!CheckSupportedType(decoder, kind, "load")) return;
@@ -3234,8 +3200,8 @@
 
   void LoadTransform(FullDecoder* decoder, LoadType type,
                      LoadTransformationKind transform,
-                     const MemoryAccessImmediate<validate>& imm,
-                     const Value& index_val, Value* result) {
+                     const MemoryAccessImmediate& imm, const Value& index_val,
+                     Value* result) {
     // LoadTransform requires SIMD support, so check for it here. If
     // unsupported, bailout and let TurboFan lower the code.
     if (!CheckSupportedType(decoder, kS128, "LoadTransform")) {
@@ -3278,7 +3244,7 @@
   }
 
   void LoadLane(FullDecoder* decoder, LoadType type, const Value& _value,
-                const Value& _index, const MemoryAccessImmediate<validate>& imm,
+                const Value& _index, const MemoryAccessImmediate& imm,
                 const uint8_t laneidx, Value* _result) {
     if (!CheckSupportedType(decoder, kS128, "LoadLane")) {
       return;
@@ -3314,8 +3280,8 @@
   }
 
   void StoreMem(FullDecoder* decoder, StoreType type,
-                const MemoryAccessImmediate<validate>& imm,
-                const Value& index_val, const Value& value_val) {
+                const MemoryAccessImmediate& imm, const Value& index_val,
+                const Value& value_val) {
     ValueKind kind = type.value_type().kind();
     DCHECK_EQ(kind, value_val.type.kind());
     if (!CheckSupportedType(decoder, kind, "store")) return;
@@ -3365,8 +3331,8 @@
   }
 
   void StoreLane(FullDecoder* decoder, StoreType type,
-                 const MemoryAccessImmediate<validate>& imm,
-                 const Value& _index, const Value& _value, const uint8_t lane) {
+                 const MemoryAccessImmediate& imm, const Value& _index,
+                 const Value& _value, const uint8_t lane) {
     if (!CheckSupportedType(decoder, kS128, "StoreLane")) return;
     LiftoffRegList pinned;
     LiftoffRegister value = pinned.set(__ PopToRegister());
@@ -3547,15 +3513,14 @@
 
   enum TailCall : bool { kTailCall = true, kNoTailCall = false };
 
-  void CallDirect(FullDecoder* decoder,
-                  const CallFunctionImmediate<validate>& imm,
+  void CallDirect(FullDecoder* decoder, const CallFunctionImmediate& imm,
                   const Value args[], Value[]) {
     CallDirect(decoder, imm, args, nullptr, kNoTailCall);
   }
 
   void CallIndirect(FullDecoder* decoder, const Value& index_val,
-                    const CallIndirectImmediate<validate>& imm,
-                    const Value args[], Value returns[]) {
+                    const CallIndirectImmediate& imm, const Value args[],
+                    Value returns[]) {
     CallIndirect(decoder, index_val, imm, kNoTailCall);
   }
 
@@ -3565,15 +3530,14 @@
     CallRef(decoder, func_ref.type, sig, kNoTailCall);
   }
 
-  void ReturnCall(FullDecoder* decoder,
-                  const CallFunctionImmediate<validate>& imm,
+  void ReturnCall(FullDecoder* decoder, const CallFunctionImmediate& imm,
                   const Value args[]) {
     TierupCheckOnTailCall(decoder);
     CallDirect(decoder, imm, args, nullptr, kTailCall);
   }
 
   void ReturnCallIndirect(FullDecoder* decoder, const Value& index_val,
-                          const CallIndirectImmediate<validate>& imm,
+                          const CallIndirectImmediate& imm,
                           const Value args[]) {
     TierupCheckOnTailCall(decoder);
     CallIndirect(decoder, index_val, imm, kTailCall);
@@ -3603,7 +3567,7 @@
     Register tmp = NeedsTierupCheck(decoder, depth)
                        ? pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp()
                        : no_reg;
-    LoadNullValue(null, pinned);
+    LoadNullValueForCompare(null, pinned);
     {
       FREEZE_STATE(frozen);
       __ emit_cond_jump(kUnequal, &cont_false, ref_object.type.kind(), ref.gp(),
@@ -3633,7 +3597,7 @@
     Register tmp = NeedsTierupCheck(decoder, depth)
                        ? pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp()
                        : no_reg;
-    LoadNullValue(null, pinned);
+    LoadNullValueForCompare(null, pinned);
     {
       FREEZE_STATE(frozen);
       __ emit_cond_jump(kEqual, &cont_false, ref_object.type.kind(), ref.gp(),
@@ -3757,7 +3721,7 @@
     RegClass dst_rc = reg_class_for(kS128);
     LiftoffRegister dst = __ GetUnusedRegister(dst_rc, {});
     (asm_.*emit_fn)(dst, src1, src2, src3);
-    __ PushRegister(kS128, src1);
+    __ PushRegister(kS128, dst);
     return;
   }
 
@@ -4333,8 +4297,7 @@
   }
 
   template <ValueKind src_kind, ValueKind result_kind, typename EmitFn>
-  void EmitSimdExtractLaneOp(EmitFn fn,
-                             const SimdLaneImmediate<validate>& imm) {
+  void EmitSimdExtractLaneOp(EmitFn fn, const SimdLaneImmediate& imm) {
     static constexpr RegClass src_rc = reg_class_for(src_kind);
     static constexpr RegClass result_rc = reg_class_for(result_kind);
     LiftoffRegister lhs = __ PopToRegister();
@@ -4346,8 +4309,7 @@
   }
 
   template <ValueKind src2_kind, typename EmitFn>
-  void EmitSimdReplaceLaneOp(EmitFn fn,
-                             const SimdLaneImmediate<validate>& imm) {
+  void EmitSimdReplaceLaneOp(EmitFn fn, const SimdLaneImmediate& imm) {
     static constexpr RegClass src1_rc = reg_class_for(kS128);
     static constexpr RegClass src2_rc = reg_class_for(src2_kind);
     static constexpr RegClass result_rc = reg_class_for(kS128);
@@ -4371,7 +4333,7 @@
   }
 
   void SimdLaneOp(FullDecoder* decoder, WasmOpcode opcode,
-                  const SimdLaneImmediate<validate>& imm,
+                  const SimdLaneImmediate& imm,
                   const base::Vector<Value> inputs, Value* result) {
     if (!CpuFeatures::SupportsWasmSimd128()) {
       return unsupported(decoder, kSimd, "simd");
@@ -4415,7 +4377,7 @@
     }
   }
 
-  void S128Const(FullDecoder* decoder, const Simd128Immediate<validate>& imm,
+  void S128Const(FullDecoder* decoder, const Simd128Immediate& imm,
                  Value* result) {
     if (!CpuFeatures::SupportsWasmSimd128()) {
       return unsupported(decoder, kSimd, "simd");
@@ -4437,8 +4399,7 @@
     __ PushRegister(kS128, dst);
   }
 
-  void Simd8x16ShuffleOp(FullDecoder* decoder,
-                         const Simd128Immediate<validate>& imm,
+  void Simd8x16ShuffleOp(FullDecoder* decoder, const Simd128Immediate& imm,
                          const Value& input0, const Value& input1,
                          Value* result) {
     if (!CpuFeatures::SupportsWasmSimd128()) {
@@ -4711,7 +4672,7 @@
     __ DropValues(1);
   }
 
-  void Throw(FullDecoder* decoder, const TagIndexImmediate<validate>& imm,
+  void Throw(FullDecoder* decoder, const TagIndexImmediate& imm,
              const base::Vector<Value>& /* args */) {
     LiftoffRegList pinned;
 
@@ -4762,13 +4723,15 @@
                      LiftoffAssembler::VarState{kPointerKind, values_array, 0}},
                     decoder->position());
 
+    RegisterDebugSideTableEntry(decoder, DebugSideTableBuilder::kDidSpill);
+
     int pc_offset = __ pc_offset();
     MaybeOSR();
     EmitLandingPad(decoder, pc_offset);
   }
 
   void AtomicStoreMem(FullDecoder* decoder, StoreType type,
-                      const MemoryAccessImmediate<validate>& imm) {
+                      const MemoryAccessImmediate& imm) {
     LiftoffRegList pinned;
     LiftoffRegister value = pinned.set(__ PopToRegister());
     LiftoffRegister full_index = __ PopToRegister(pinned);
@@ -4791,7 +4754,7 @@
   }
 
   void AtomicLoadMem(FullDecoder* decoder, LoadType type,
-                     const MemoryAccessImmediate<validate>& imm) {
+                     const MemoryAccessImmediate& imm) {
     ValueKind kind = type.value_type().kind();
     LiftoffRegister full_index = __ PopToRegister();
     Register index = BoundsCheckMem(decoder, type.size(), imm.offset,
@@ -4815,7 +4778,7 @@
   }
 
   void AtomicBinop(FullDecoder* decoder, StoreType type,
-                   const MemoryAccessImmediate<validate>& imm,
+                   const MemoryAccessImmediate& imm,
                    void (LiftoffAssembler::*emit_fn)(Register, Register,
                                                      uintptr_t, LiftoffRegister,
                                                      LiftoffRegister,
@@ -4856,7 +4819,7 @@
   }
 
   void AtomicCompareExchange(FullDecoder* decoder, StoreType type,
-                             const MemoryAccessImmediate<validate>& imm) {
+                             const MemoryAccessImmediate& imm) {
 #ifdef V8_TARGET_ARCH_IA32
     // On ia32 we don't have enough registers to first pop all the values off
     // the stack and then start with the code generation. Instead we do the
@@ -4941,45 +4904,81 @@
   }
 
   void AtomicWait(FullDecoder* decoder, ValueKind kind,
-                  const MemoryAccessImmediate<validate>& imm) {
-    LiftoffRegister full_index = __ PeekToRegister(2, {});
-    Register index_reg =
-        BoundsCheckMem(decoder, value_kind_size(kind), imm.offset, full_index,
-                       {}, kDoForceCheck);
-    if (index_reg == no_reg) return;
-    LiftoffRegList pinned{index_reg};
-    AlignmentCheckMem(decoder, value_kind_size(kind), imm.offset, index_reg,
-                      pinned);
+                  const MemoryAccessImmediate& imm) {
+    {
+      LiftoffRegList pinned;
+      LiftoffRegister full_index = __ PeekToRegister(2, pinned);
+      Register index_reg =
+          BoundsCheckMem(decoder, value_kind_size(kind), imm.offset, full_index,
+                         pinned, kDoForceCheck);
+      if (index_reg == no_reg) return;
+      pinned.set(index_reg);
+      AlignmentCheckMem(decoder, value_kind_size(kind), imm.offset, index_reg,
+                        pinned);
+
+      uintptr_t offset = imm.offset;
+      Register index_plus_offset =
+          __ cache_state()->is_used(LiftoffRegister(index_reg))
+              ? pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp()
+              : index_reg;
+      // TODO(clemensb): Skip this if memory is 64 bit.
+      __ emit_ptrsize_zeroextend_i32(index_plus_offset, index_reg);
+      if (offset) {
+        __ emit_ptrsize_addi(index_plus_offset, index_plus_offset, offset);
+      }
+
+      LiftoffAssembler::VarState index =
+          __ cache_state()->stack_state.end()[-3];
+
+      // We replace the index on the value stack with the `index_plus_offset`
+      // calculated above. Thereby the BigInt allocation below does not
+      // overwrite the calculated value by accident.
+      if (full_index != LiftoffRegister(index_plus_offset)) {
+        __ cache_state()->dec_used(full_index);
+        __ cache_state()->inc_used(LiftoffRegister(index_plus_offset));
+      }
+      index.MakeRegister(LiftoffRegister(index_plus_offset));
+    }
+    {
+      // Convert the top value of the stack (the timeout) from I64 to a BigInt,
+      // which we can then pass to the atomic.wait builtin.
+      LiftoffAssembler::VarState i64_timeout =
+          __ cache_state()->stack_state.back();
+      CallRuntimeStub(
+          kNeedI64RegPair ? WasmCode::kI32PairToBigInt : WasmCode::kI64ToBigInt,
+          MakeSig::Returns(kRef).Params(kI64), {i64_timeout},
+          decoder->position());
+      __ DropValues(1);
+      // We put the result on the value stack so that it gets preserved across
+      // a potential GC that may get triggered by the BigInt allocation below.
+      __ PushRegister(kRef, LiftoffRegister(kReturnRegister0));
+    }
 
-    uintptr_t offset = imm.offset;
-    Register index_plus_offset =
-        __ cache_state()->is_used(LiftoffRegister(index_reg))
-            ? pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp()
-            : index_reg;
-    // TODO(clemensb): Skip this if memory is 64 bit.
-    __ emit_ptrsize_zeroextend_i32(index_plus_offset, index_reg);
-    if (offset) {
-      __ emit_ptrsize_addi(index_plus_offset, index_plus_offset, offset);
+    Register expected_reg = no_reg;
+    if (kind == kI32) {
+      expected_reg = __ PeekToRegister(1, {}).gp();
+    } else {
+      LiftoffAssembler::VarState i64_expected =
+          __ cache_state()->stack_state.end()[-2];
+      CallRuntimeStub(
+          kNeedI64RegPair ? WasmCode::kI32PairToBigInt : WasmCode::kI64ToBigInt,
+          MakeSig::Returns(kRef).Params(kI64), {i64_expected},
+          decoder->position());
+      expected_reg = kReturnRegister0;
     }
+    LiftoffRegister expected(expected_reg);
 
     LiftoffAssembler::VarState timeout =
         __ cache_state()->stack_state.end()[-1];
-    LiftoffAssembler::VarState expected_value =
-        __ cache_state()->stack_state.end()[-2];
+    LiftoffAssembler::VarState expected_value(kRef, expected, 0);
     LiftoffAssembler::VarState index = __ cache_state()->stack_state.end()[-3];
 
-    // We have to set the correct register for the index.
-    index.MakeRegister(LiftoffRegister(index_plus_offset));
-
-    static constexpr WasmCode::RuntimeStubId kTargets[2][2]{
-        // 64 bit systems (kNeedI64RegPair == false):
-        {WasmCode::kWasmI64AtomicWait64, WasmCode::kWasmI32AtomicWait64},
-        // 32 bit systems (kNeedI64RegPair == true):
-        {WasmCode::kWasmI64AtomicWait32, WasmCode::kWasmI32AtomicWait32}};
-    auto target = kTargets[kNeedI64RegPair][kind == kI32];
+    auto target = kind == kI32 ? WasmCode::kWasmI32AtomicWait
+                               : WasmCode::kWasmI64AtomicWait;
 
-    CallRuntimeStub(target, MakeSig::Params(kPointerKind, kind, kI64),
-                    {index, expected_value, timeout}, decoder->position());
+    CallRuntimeStub(
+        target, MakeSig::Params(kPointerKind, kind == kI32 ? kI32 : kRef, kRef),
+        {index, expected_value, timeout}, decoder->position());
     // Pop parameters from the value stack.
     __ DropValues(3);
 
@@ -4988,8 +4987,7 @@
     __ PushRegister(kI32, LiftoffRegister(kReturnRegister0));
   }
 
-  void AtomicNotify(FullDecoder* decoder,
-                    const MemoryAccessImmediate<validate>& imm) {
+  void AtomicNotify(FullDecoder* decoder, const MemoryAccessImmediate& imm) {
     LiftoffRegister full_index = __ PeekToRegister(1, {});
     Register index_reg = BoundsCheckMem(decoder, kInt32Size, imm.offset,
                                         full_index, {}, kDoForceCheck);
@@ -5095,8 +5093,8 @@
   V(I64AtomicCompareExchange32U, kI64Store32)
 
   void AtomicOp(FullDecoder* decoder, WasmOpcode opcode,
-                base::Vector<Value> args,
-                const MemoryAccessImmediate<validate>& imm, Value* result) {
+                base::Vector<Value> args, const MemoryAccessImmediate& imm,
+                Value* result) {
     switch (opcode) {
 #define ATOMIC_STORE_OP(name, type)                \
   case wasm::kExpr##name:                          \
@@ -5197,9 +5195,8 @@
     return reg.low();
   }
 
-  void MemoryInit(FullDecoder* decoder,
-                  const MemoryInitImmediate<validate>& imm, const Value&,
-                  const Value&, const Value&) {
+  void MemoryInit(FullDecoder* decoder, const MemoryInitImmediate& imm,
+                  const Value&, const Value&, const Value&) {
     Register mem_offsets_high_word = no_reg;
     LiftoffRegList pinned;
     LiftoffRegister size = pinned.set(__ PopToRegister(pinned));
@@ -5243,7 +5240,7 @@
     __ emit_cond_jump(kEqual, trap_label, kI32, result.gp(), no_reg, trapping);
   }
 
-  void DataDrop(FullDecoder* decoder, const IndexImmediate<validate>& imm) {
+  void DataDrop(FullDecoder* decoder, const IndexImmediate& imm) {
     LiftoffRegList pinned;
 
     Register seg_size_array =
@@ -5265,9 +5262,8 @@
              pinned);
   }
 
-  void MemoryCopy(FullDecoder* decoder,
-                  const MemoryCopyImmediate<validate>& imm, const Value&,
-                  const Value&, const Value&) {
+  void MemoryCopy(FullDecoder* decoder, const MemoryCopyImmediate& imm,
+                  const Value&, const Value&, const Value&) {
     Register mem_offsets_high_word = no_reg;
     LiftoffRegList pinned;
     LiftoffRegister size = pinned.set(
@@ -5306,9 +5302,8 @@
     __ emit_cond_jump(kEqual, trap_label, kI32, result.gp(), no_reg, trapping);
   }
 
-  void MemoryFill(FullDecoder* decoder,
-                  const MemoryIndexImmediate<validate>& imm, const Value&,
-                  const Value&, const Value&) {
+  void MemoryFill(FullDecoder* decoder, const MemoryIndexImmediate& imm,
+                  const Value&, const Value&, const Value&) {
     Register mem_offsets_high_word = no_reg;
     LiftoffRegList pinned;
     LiftoffRegister size = pinned.set(
@@ -5352,7 +5347,7 @@
     __ LoadConstant(reg, WasmValue{static_cast<smi_type>(smi_value)});
   }
 
-  void TableInit(FullDecoder* decoder, const TableInitImmediate<validate>& imm,
+  void TableInit(FullDecoder* decoder, const TableInitImmediate& imm,
                  base::Vector<Value> args) {
     LiftoffRegList pinned;
     LiftoffRegister table_index_reg =
@@ -5382,7 +5377,7 @@
     RegisterDebugSideTableEntry(decoder, DebugSideTableBuilder::kDidSpill);
   }
 
-  void ElemDrop(FullDecoder* decoder, const IndexImmediate<validate>& imm) {
+  void ElemDrop(FullDecoder* decoder, const IndexImmediate& imm) {
     LiftoffRegList pinned;
     Register dropped_elem_segments =
         pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
@@ -5404,7 +5399,7 @@
              StoreType::kI32Store8, pinned);
   }
 
-  void TableCopy(FullDecoder* decoder, const TableCopyImmediate<validate>& imm,
+  void TableCopy(FullDecoder* decoder, const TableCopyImmediate& imm,
                  base::Vector<Value> args) {
     LiftoffRegList pinned;
 
@@ -5435,8 +5430,8 @@
     RegisterDebugSideTableEntry(decoder, DebugSideTableBuilder::kDidSpill);
   }
 
-  void TableGrow(FullDecoder* decoder, const IndexImmediate<validate>& imm,
-                 const Value&, const Value&, Value* result) {
+  void TableGrow(FullDecoder* decoder, const IndexImmediate& imm, const Value&,
+                 const Value&, Value* result) {
     LiftoffRegList pinned;
 
     LiftoffRegister table_index_reg =
@@ -5460,8 +5455,7 @@
     __ PushRegister(kI32, LiftoffRegister(kReturnRegister0));
   }
 
-  void TableSize(FullDecoder* decoder, const IndexImmediate<validate>& imm,
-                 Value*) {
+  void TableSize(FullDecoder* decoder, const IndexImmediate& imm, Value*) {
     // We have to look up instance->tables[table_index].length.
 
     LiftoffRegList pinned;
@@ -5486,8 +5480,8 @@
     __ PushRegister(kI32, LiftoffRegister(result));
   }
 
-  void TableFill(FullDecoder* decoder, const IndexImmediate<validate>& imm,
-                 const Value&, const Value&, const Value&) {
+  void TableFill(FullDecoder* decoder, const IndexImmediate& imm, const Value&,
+                 const Value&, const Value&) {
     LiftoffRegList pinned;
 
     LiftoffRegister table_index_reg =
@@ -5509,9 +5503,8 @@
     RegisterDebugSideTableEntry(decoder, DebugSideTableBuilder::kDidSpill);
   }
 
-  void StructNew(FullDecoder* decoder,
-                 const StructIndexImmediate<validate>& imm, const Value& rtt,
-                 bool initial_values_on_stack) {
+  void StructNew(FullDecoder* decoder, const StructIndexImmediate& imm,
+                 const Value& rtt, bool initial_values_on_stack) {
     LiftoffRegList pinned;
     LiftoffRegister instance_size =
         pinned.set(__ GetUnusedRegister(kGpReg, pinned));
@@ -5554,21 +5547,18 @@
     __ PushRegister(kRef, obj);
   }
 
-  void StructNew(FullDecoder* decoder,
-                 const StructIndexImmediate<validate>& imm, const Value& rtt,
-                 const Value args[], Value* result) {
+  void StructNew(FullDecoder* decoder, const StructIndexImmediate& imm,
+                 const Value& rtt, const Value args[], Value* result) {
     StructNew(decoder, imm, rtt, true);
   }
 
-  void StructNewDefault(FullDecoder* decoder,
-                        const StructIndexImmediate<validate>& imm,
+  void StructNewDefault(FullDecoder* decoder, const StructIndexImmediate& imm,
                         const Value& rtt, Value* result) {
     StructNew(decoder, imm, rtt, false);
   }
 
   void StructGet(FullDecoder* decoder, const Value& struct_obj,
-                 const FieldImmediate<validate>& field, bool is_signed,
-                 Value* result) {
+                 const FieldImmediate& field, bool is_signed, Value* result) {
     const StructType* struct_type = field.struct_imm.struct_type;
     ValueKind field_kind = struct_type->field(field.field_imm.index).kind();
     if (!CheckSupportedType(decoder, field_kind, "field load")) return;
@@ -5584,8 +5574,7 @@
   }
 
   void StructSet(FullDecoder* decoder, const Value& struct_obj,
-                 const FieldImmediate<validate>& field,
-                 const Value& field_value) {
+                 const FieldImmediate& field, const Value& field_value) {
     const StructType* struct_type = field.struct_imm.struct_type;
     ValueKind field_kind = struct_type->field(field.field_imm.index).kind();
     int offset = StructFieldOffset(struct_type, field.field_imm.index);
@@ -5596,7 +5585,7 @@
     StoreObjectField(obj.gp(), no_reg, offset, value, pinned, field_kind);
   }
 
-  void ArrayNew(FullDecoder* decoder, const ArrayIndexImmediate<validate>& imm,
+  void ArrayNew(FullDecoder* decoder, const ArrayIndexImmediate& imm,
                 ValueKind rtt_kind, bool initial_value_on_stack) {
     // Max length check.
     {
@@ -5673,21 +5662,20 @@
     __ PushRegister(kRef, obj);
   }
 
-  void ArrayNew(FullDecoder* decoder, const ArrayIndexImmediate<validate>& imm,
+  void ArrayNew(FullDecoder* decoder, const ArrayIndexImmediate& imm,
                 const Value& length_value, const Value& initial_value,
                 const Value& rtt, Value* result) {
     ArrayNew(decoder, imm, rtt.type.kind(), true);
   }
 
-  void ArrayNewDefault(FullDecoder* decoder,
-                       const ArrayIndexImmediate<validate>& imm,
+  void ArrayNewDefault(FullDecoder* decoder, const ArrayIndexImmediate& imm,
                        const Value& length, const Value& rtt, Value* result) {
     ArrayNew(decoder, imm, rtt.type.kind(), false);
   }
 
   void ArrayGet(FullDecoder* decoder, const Value& array_obj,
-                const ArrayIndexImmediate<validate>& imm,
-                const Value& index_val, bool is_signed, Value* result) {
+                const ArrayIndexImmediate& imm, const Value& index_val,
+                bool is_signed, Value* result) {
     LiftoffRegList pinned;
     LiftoffRegister index = pinned.set(__ PopToModifiableRegister(pinned));
     LiftoffRegister array = pinned.set(__ PopToRegister(pinned));
@@ -5708,8 +5696,8 @@
   }
 
   void ArraySet(FullDecoder* decoder, const Value& array_obj,
-                const ArrayIndexImmediate<validate>& imm,
-                const Value& index_val, const Value& value_val) {
+                const ArrayIndexImmediate& imm, const Value& index_val,
+                const Value& value_val) {
     LiftoffRegList pinned;
     LiftoffRegister value = pinned.set(__ PopToRegister(pinned));
     DCHECK_EQ(reg_class_for(imm.array_type->element_type().kind()),
@@ -5758,8 +5746,7 @@
     __ cache_state()->stack_state.pop_back(5);
   }
 
-  void ArrayNewFixed(FullDecoder* decoder,
-                     const ArrayIndexImmediate<validate>& imm,
+  void ArrayNewFixed(FullDecoder* decoder, const ArrayIndexImmediate& imm,
                      const base::Vector<Value>& elements, const Value& rtt,
                      Value* result) {
     ValueKind rtt_kind = rtt.type.kind();
@@ -5810,8 +5797,8 @@
   }
 
   void ArrayNewSegment(FullDecoder* decoder,
-                       const ArrayIndexImmediate<validate>& array_imm,
-                       const IndexImmediate<validate>& data_segment,
+                       const ArrayIndexImmediate& array_imm,
+                       const IndexImmediate& data_segment,
                        const Value& /* offset */, const Value& /* length */,
                        const Value& /* rtt */, Value* /* result */) {
     LiftoffRegList pinned;
@@ -5906,8 +5893,12 @@
                     NullSucceeds null_succeeds,
                     const FreezeCacheState& frozen) {
     Label match;
+    bool is_cast_from_any = obj_type.is_reference_to(HeapType::kAny);
 
-    if (obj_type.is_nullable()) {
+    // Skip the null check if casting from any and not {null_succeeds}.
+    // In that case the instance type check will identify null as not being a
+    // wasm object and fail.
+    if (obj_type.is_nullable() && (!is_cast_from_any || null_succeeds)) {
       __ emit_cond_jump(kEqual, null_succeeds ? &match : no_match,
                         obj_type.kind(), obj_reg, scratch_null, frozen);
     }
@@ -5934,6 +5925,17 @@
     // rtt.
     __ emit_cond_jump(kEqual, &match, rtt_type.kind(), tmp1, rtt_reg, frozen);
 
+    if (is_cast_from_any) {
+      // Check for map being a map for a wasm object (struct, array, func).
+      __ Load(LiftoffRegister(scratch2), tmp1, no_reg,
+              wasm::ObjectAccess::ToTagged(Map::kInstanceTypeOffset),
+              LoadType::kI32Load16U);
+      __ emit_i32_subi(scratch2, scratch2, FIRST_WASM_OBJECT_TYPE);
+      __ emit_i32_cond_jumpi(kUnsignedGreaterThan, no_match, scratch2,
+                             LAST_WASM_OBJECT_TYPE - FIRST_WASM_OBJECT_TYPE,
+                             frozen);
+    }
+
     // Constant-time subtyping check: load exactly one candidate RTT from the
     // supertypes list.
     // Step 1: load the WasmTypeInfo into {tmp1}.
@@ -5971,7 +5973,7 @@
     Register scratch_null =
         pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
     LiftoffRegister result = pinned.set(__ GetUnusedRegister(kGpReg, pinned));
-    if (obj.type.is_nullable()) LoadNullValue(scratch_null, pinned);
+    if (obj.type.is_nullable()) LoadNullValueForCompare(scratch_null, pinned);
 
     {
       FREEZE_STATE(frozen);
@@ -5997,8 +5999,8 @@
         return RefIsEq(decoder, obj, result_val, null_succeeds);
       case HeapType::kI31:
         return RefIsI31(decoder, obj, result_val, null_succeeds);
-      case HeapType::kData:
-        return RefIsData(decoder, obj, result_val, null_succeeds);
+      case HeapType::kStruct:
+        return RefIsStruct(decoder, obj, result_val, null_succeeds);
       case HeapType::kArray:
         return RefIsArray(decoder, obj, result_val, null_succeeds);
       case HeapType::kAny:
@@ -6010,7 +6012,7 @@
   }
 
   void RefCast(FullDecoder* decoder, const Value& obj, const Value& rtt,
-               Value* result) {
+               Value* result, bool null_succeeds) {
     if (v8_flags.experimental_wasm_assume_ref_cast_succeeds) {
       // Just drop the rtt.
       __ DropValues(1);
@@ -6024,17 +6026,37 @@
     Register scratch_null =
         pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
     Register scratch2 = pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
-    if (obj.type.is_nullable()) LoadNullValue(scratch_null, pinned);
+    if (obj.type.is_nullable()) LoadNullValueForCompare(scratch_null, pinned);
 
     {
       FREEZE_STATE(frozen);
+      NullSucceeds on_null = null_succeeds ? kNullSucceeds : kNullFails;
       SubtypeCheck(decoder->module_, obj_reg.gp(), obj.type, rtt_reg.gp(),
-                   rtt.type, scratch_null, scratch2, trap_label, kNullSucceeds,
+                   rtt.type, scratch_null, scratch2, trap_label, on_null,
                    frozen);
     }
     __ PushRegister(obj.type.kind(), obj_reg);
   }
 
+  void RefCastAbstract(FullDecoder* decoder, const Value& obj, HeapType type,
+                       Value* result_val, bool null_succeeds) {
+    switch (type.representation()) {
+      case HeapType::kEq:
+        return RefAsEq(decoder, obj, result_val, null_succeeds);
+      case HeapType::kI31:
+        return RefAsI31(decoder, obj, result_val, null_succeeds);
+      case HeapType::kStruct:
+        return RefAsStruct(decoder, obj, result_val, null_succeeds);
+      case HeapType::kArray:
+        return RefAsArray(decoder, obj, result_val, null_succeeds);
+      case HeapType::kAny:
+        // Any may never need a cast as it is either implicitly convertible or
+        // never convertible for any given type.
+      default:
+        UNREACHABLE();
+    }
+  }
+
   void BrOnCast(FullDecoder* decoder, const Value& obj, const Value& rtt,
                 Value* /* result_on_branch */, uint32_t depth) {
     // Avoid having sequences of branches do duplicate work.
@@ -6139,15 +6161,19 @@
   }
 
   // Abstract type checkers. They all fall through on match.
-  void DataCheck(TypeCheck& check, const FreezeCacheState& frozen) {
+  void StructCheck(TypeCheck& check, const FreezeCacheState& frozen) {
     LoadInstanceType(check, frozen, check.no_match);
-    // We're going to test a range of WasmObject instance types with a single
-    // unsigned comparison.
-    Register tmp = check.instance_type();
-    __ emit_i32_subi(tmp, tmp, FIRST_WASM_OBJECT_TYPE);
-    __ emit_i32_cond_jumpi(kUnsignedGreaterThan, check.no_match, tmp,
-                           LAST_WASM_OBJECT_TYPE - FIRST_WASM_OBJECT_TYPE,
-                           frozen);
+    LiftoffRegister instance_type(check.instance_type());
+    if (!v8_flags.wasm_gc_structref_as_dataref) {
+      __ emit_i32_cond_jumpi(kUnequal, check.no_match, check.instance_type(),
+                             WASM_STRUCT_TYPE, frozen);
+    } else {
+      Register tmp = check.instance_type();
+      __ emit_i32_subi(tmp, tmp, FIRST_WASM_OBJECT_TYPE);
+      __ emit_i32_cond_jumpi(kUnsignedGreaterThan, check.no_match, tmp,
+                             LAST_WASM_OBJECT_TYPE - FIRST_WASM_OBJECT_TYPE,
+                             frozen);
+    }
   }
 
   void ArrayCheck(TypeCheck& check, const FreezeCacheState& frozen) {
@@ -6206,9 +6232,9 @@
     __ PushRegister(kI32, result);
   }
 
-  void RefIsData(FullDecoder* /* decoder */, const Value& object,
-                 Value* /* result_val */, bool null_succeeds = false) {
-    AbstractTypeCheck<&LiftoffCompiler::DataCheck>(object, null_succeeds);
+  void RefIsStruct(FullDecoder* /* decoder */, const Value& object,
+                   Value* /* result_val */, bool null_succeeds = false) {
+    AbstractTypeCheck<&LiftoffCompiler::StructCheck>(object, null_succeeds);
   }
 
   void RefIsEq(FullDecoder* /* decoder */, const Value& object,
@@ -6228,27 +6254,44 @@
 
   template <TypeChecker type_checker>
   void AbstractTypeCast(const Value& object, FullDecoder* decoder,
-                        ValueKind result_kind) {
-    bool null_succeeds = false;  // TODO(mliedtke): Use parameter.
+                        ValueKind result_kind, bool null_succeeds = false) {
+    Label match;
     Label* trap_label =
         AddOutOfLineTrap(decoder, WasmCode::kThrowWasmTrapIllegalCast);
     TypeCheck check(object.type, trap_label, null_succeeds);
     Initialize(check, kPeek);
     FREEZE_STATE(frozen);
+
+    if (null_succeeds && check.obj_type.is_nullable()) {
+      __ emit_cond_jump(kEqual, &match, kRefNull, check.obj_reg,
+                        check.null_reg(), frozen);
+    }
     (this->*type_checker)(check, frozen);
+    __ bind(&match);
+  }
+
+  void RefAsEq(FullDecoder* decoder, const Value& object, Value* result,
+               bool null_succeeds = false) {
+    AbstractTypeCast<&LiftoffCompiler::EqCheck>(object, decoder, kRef,
+                                                null_succeeds);
   }
 
-  void RefAsData(FullDecoder* decoder, const Value& object,
-                 Value* /* result */) {
-    AbstractTypeCast<&LiftoffCompiler::DataCheck>(object, decoder, kRef);
+  void RefAsStruct(FullDecoder* decoder, const Value& object,
+                   Value* /* result */, bool null_succeeds = false) {
+    AbstractTypeCast<&LiftoffCompiler::StructCheck>(object, decoder, kRef,
+                                                    null_succeeds);
   }
 
-  void RefAsI31(FullDecoder* decoder, const Value& object, Value* result) {
-    AbstractTypeCast<&LiftoffCompiler::I31Check>(object, decoder, kRef);
+  void RefAsI31(FullDecoder* decoder, const Value& object, Value* result,
+                bool null_succeeds = false) {
+    AbstractTypeCast<&LiftoffCompiler::I31Check>(object, decoder, kRef,
+                                                 null_succeeds);
   }
 
-  void RefAsArray(FullDecoder* decoder, const Value& object, Value* result) {
-    AbstractTypeCast<&LiftoffCompiler::ArrayCheck>(object, decoder, kRef);
+  void RefAsArray(FullDecoder* decoder, const Value& object, Value* result,
+                  bool null_succeeds = false) {
+    AbstractTypeCast<&LiftoffCompiler::ArrayCheck>(object, decoder, kRef,
+                                                   null_succeeds);
   }
 
   template <TypeChecker type_checker>
@@ -6294,9 +6337,9 @@
     __ bind(&end);
   }
 
-  void BrOnData(FullDecoder* decoder, const Value& object,
-                Value* /* value_on_branch */, uint32_t br_depth) {
-    BrOnAbstractType<&LiftoffCompiler::DataCheck>(object, decoder, br_depth);
+  void BrOnStruct(FullDecoder* decoder, const Value& object,
+                  Value* /* value_on_branch */, uint32_t br_depth) {
+    BrOnAbstractType<&LiftoffCompiler::StructCheck>(object, decoder, br_depth);
   }
 
   void BrOnI31(FullDecoder* decoder, const Value& object,
@@ -6309,9 +6352,10 @@
     BrOnAbstractType<&LiftoffCompiler::ArrayCheck>(object, decoder, br_depth);
   }
 
-  void BrOnNonData(FullDecoder* decoder, const Value& object,
-                   Value* /* value_on_branch */, uint32_t br_depth) {
-    BrOnNonAbstractType<&LiftoffCompiler::DataCheck>(object, decoder, br_depth);
+  void BrOnNonStruct(FullDecoder* decoder, const Value& object,
+                     Value* /* value_on_branch */, uint32_t br_depth) {
+    BrOnNonAbstractType<&LiftoffCompiler::StructCheck>(object, decoder,
+                                                       br_depth);
   }
 
   void BrOnNonI31(FullDecoder* decoder, const Value& object,
@@ -6325,8 +6369,7 @@
                                                       br_depth);
   }
 
-  void StringNewWtf8(FullDecoder* decoder,
-                     const MemoryIndexImmediate<validate>& imm,
+  void StringNewWtf8(FullDecoder* decoder, const MemoryIndexImmediate& imm,
                      const unibrow::Utf8Variant variant, const Value& offset,
                      const Value& size, Value* result) {
     LiftoffRegList pinned;
@@ -6390,8 +6433,7 @@
     __ PushRegister(kRef, result_reg);
   }
 
-  void StringNewWtf16(FullDecoder* decoder,
-                      const MemoryIndexImmediate<validate>& imm,
+  void StringNewWtf16(FullDecoder* decoder, const MemoryIndexImmediate& imm,
                       const Value& offset, const Value& size, Value* result) {
     LiftoffRegList pinned;
     LiftoffRegister memory_reg =
@@ -6439,8 +6481,8 @@
     __ PushRegister(kRef, result_reg);
   }
 
-  void StringConst(FullDecoder* decoder,
-                   const StringConstImmediate<validate>& imm, Value* result) {
+  void StringConst(FullDecoder* decoder, const StringConstImmediate& imm,
+                   Value* result) {
     LiftoffRegList pinned;
     LiftoffRegister index_reg =
         pinned.set(__ GetUnusedRegister(kGpReg, pinned));
@@ -6500,8 +6542,7 @@
     __ PushRegister(kI32, value);
   }
 
-  void StringEncodeWtf8(FullDecoder* decoder,
-                        const MemoryIndexImmediate<validate>& imm,
+  void StringEncodeWtf8(FullDecoder* decoder, const MemoryIndexImmediate& imm,
                         const unibrow::Utf8Variant variant, const Value& str,
                         const Value& offset, Value* result) {
     LiftoffRegList pinned;
@@ -6581,8 +6622,7 @@
     __ PushRegister(kI32, result_reg);
   }
 
-  void StringEncodeWtf16(FullDecoder* decoder,
-                         const MemoryIndexImmediate<validate>& imm,
+  void StringEncodeWtf16(FullDecoder* decoder, const MemoryIndexImmediate& imm,
                          const Value& str, const Value& offset, Value* result) {
     LiftoffRegList pinned;
 
@@ -6687,7 +6727,7 @@
       LiftoffRegister null = pinned.set(__ GetUnusedRegister(kGpReg, pinned));
       bool check_for_null = a.type.is_nullable() || b.type.is_nullable();
       if (check_for_null) {
-        LoadNullValue(null.gp(), pinned);
+        LoadNullValueForCompare(null.gp(), pinned);
       }
 
       FREEZE_STATE(frozen);
@@ -6803,7 +6843,7 @@
   }
 
   void StringViewWtf8Encode(FullDecoder* decoder,
-                            const MemoryIndexImmediate<validate>& imm,
+                            const MemoryIndexImmediate& imm,
                             const unibrow::Utf8Variant variant,
                             const Value& view, const Value& addr,
                             const Value& pos, const Value& bytes,
@@ -6910,10 +6950,9 @@
   }
 
   void StringViewWtf16Encode(FullDecoder* decoder,
-                             const MemoryIndexImmediate<validate>& imm,
-                             const Value& view, const Value& offset,
-                             const Value& pos, const Value& codeunits,
-                             Value* result) {
+                             const MemoryIndexImmediate& imm, const Value& view,
+                             const Value& offset, const Value& pos,
+                             const Value& codeunits, Value* result) {
     LiftoffRegList pinned;
 
     LiftoffAssembler::VarState& codeunits_var =
@@ -7096,8 +7135,7 @@
   }
 
  private:
-  void CallDirect(FullDecoder* decoder,
-                  const CallFunctionImmediate<validate>& imm,
+  void CallDirect(FullDecoder* decoder, const CallFunctionImmediate& imm,
                   const Value args[], Value returns[], TailCall tail_call) {
     MostlySmallValueKindSig sig(compilation_zone_, imm.sig);
     for (ValueKind ret : sig.returns()) {
@@ -7184,8 +7222,7 @@
   }
 
   void CallIndirect(FullDecoder* decoder, const Value& index_val,
-                    const CallIndirectImmediate<validate>& imm,
-                    TailCall tail_call) {
+                    const CallIndirectImmediate& imm, TailCall tail_call) {
     MostlySmallValueKindSig sig(compilation_zone_, imm.sig);
     for (ValueKind ret : sig.returns()) {
       if (!CheckSupportedType(decoder, ret, "return")) return;
@@ -7436,11 +7473,29 @@
   }
 
   void LoadNullValue(Register null, LiftoffRegList pinned) {
+    // TODO(13449): Use root register instead of isolate to retrieve null.
     LOAD_INSTANCE_FIELD(null, IsolateRoot, kSystemPointerSize, pinned);
     __ LoadFullPointer(null, null,
                        IsolateData::root_slot_offset(RootIndex::kNullValue));
   }
 
+  // Stores the null value representation in the passed register.
+  // If pointer compression is active, only the compressed tagged pointer
+  // will be stored. Any operations with this register therefore must
+  // not compare this against 64 bits using quadword instructions.
+  void LoadNullValueForCompare(Register null, LiftoffRegList pinned) {
+    Tagged_t static_null =
+        wasm::GetWasmEngine()->compressed_null_value_or_zero();
+    if (static_null != 0) {
+      // static_null is only set for builds with pointer compression.
+      DCHECK_LE(static_null, std::numeric_limits<uint32_t>::max());
+      __ LoadConstant(LiftoffRegister(null),
+                      WasmValue(static_cast<uint32_t>(static_null)));
+    } else {
+      LoadNullValue(null, pinned);
+    }
+  }
+
   void LoadExceptionSymbol(Register dst, LiftoffRegList pinned,
                            RootIndex root_index) {
     LOAD_INSTANCE_FIELD(dst, IsolateRoot, kSystemPointerSize, pinned);
@@ -7450,12 +7505,13 @@
 
   void MaybeEmitNullCheck(FullDecoder* decoder, Register object,
                           LiftoffRegList pinned, ValueType type) {
-    if (v8_flags.experimental_wasm_skip_null_checks || !type.is_nullable())
+    if (v8_flags.experimental_wasm_skip_null_checks || !type.is_nullable()) {
       return;
+    }
     Label* trap_label =
         AddOutOfLineTrap(decoder, WasmCode::kThrowWasmTrapNullDereference);
     LiftoffRegister null = __ GetUnusedRegister(kGpReg, pinned);
-    LoadNullValue(null.gp(), pinned);
+    LoadNullValueForCompare(null.gp(), pinned);
     FREEZE_STATE(trapping);
     __ emit_cond_jump(LiftoffCondition::kEqual, trap_label, kRefNull, object,
                       null.gp(), trapping);
@@ -7734,7 +7790,7 @@
                  compiler_options.for_debugging == kForDebugging);
   WasmFeatures unused_detected_features;
 
-  WasmFullDecoder<Decoder::kBooleanValidation, LiftoffCompiler> decoder(
+  WasmFullDecoder<Decoder::BooleanValidationTag, LiftoffCompiler> decoder(
       &zone, env->module, env->enabled_features,
       compiler_options.detected_features ? compiler_options.detected_features
                                          : &unused_detected_features,
@@ -7787,6 +7843,8 @@
   }
 
   DCHECK(result.succeeded());
+  env->module->set_function_validated(compiler_options.func_index);
+
   return result;
 }
 
@@ -7812,7 +7870,7 @@
       code->for_debugging() == kForStepping
           ? base::ArrayVector(kSteppingBreakpoints)
           : base::Vector<const int>{};
-  WasmFullDecoder<Decoder::kBooleanValidation, LiftoffCompiler> decoder(
+  WasmFullDecoder<Decoder::BooleanValidationTag, LiftoffCompiler> decoder(
       &zone, native_module->module(), env.enabled_features, &detected,
       func_body, call_descriptor, &env, &zone,
       NewAssemblerBuffer(AssemblerBase::kDefaultBufferSize),
diff -r -u --color up/v8/src/wasm/baseline/loong64/liftoff-assembler-loong64.h nw/v8/src/wasm/baseline/loong64/liftoff-assembler-loong64.h
--- up/v8/src/wasm/baseline/loong64/liftoff-assembler-loong64.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/baseline/loong64/liftoff-assembler-loong64.h	2023-01-19 16:46:36.417276181 -0500
@@ -58,10 +58,9 @@
 //  -1   | StackFrame::WASM   |
 //  -2   |     instance       |
 //  -3   |     feedback vector|
-//  -4   |     tiering budget |
 //  -----+--------------------+---------------------------
-//  -5   |     slot 0         |   ^
-//  -6   |     slot 1         |   |
+//  -4   |     slot 0         |   ^
+//  -5   |     slot 1         |   |
 //       |                    | Frame slots
 //       |                    |   |
 //       |                    |   v
@@ -193,6 +192,22 @@
   return offset;
 }
 
+void LiftoffAssembler::CallFrameSetupStub(int declared_function_index) {
+  // TODO(jkummerow): Enable this check when we have C++20.
+  // static_assert(std::find(std::begin(wasm::kGpParamRegisters),
+  //                         std::end(wasm::kGpParamRegisters),
+  //                         kLiftoffFrameSetupFunctionReg) ==
+  //                         std::end(wasm::kGpParamRegisters));
+
+  // On LOONG64, we must push at least {ra} before calling the stub, otherwise
+  // it would get clobbered with no possibility to recover it. So just set
+  // up the frame here.
+  EnterFrame(StackFrame::WASM);
+  LoadConstant(LiftoffRegister(kLiftoffFrameSetupFunctionReg),
+               WasmValue(declared_function_index));
+  CallRuntimeStub(WasmCode::kWasmLiftoffFrameSetup);
+}
+
 void LiftoffAssembler::PrepareTailCall(int num_callee_stack_params,
                                        int stack_param_delta) {
   UseScratchRegisterScope temps(this);
@@ -224,6 +239,10 @@
   // pushed as part of frame construction, so we don't need to allocate memory
   // for them anymore.
   int frame_size = GetTotalFrameSize() - 2 * kSystemPointerSize;
+  // The frame setup builtin also pushes the feedback vector.
+  if (v8_flags.wasm_speculative_inlining) {
+    frame_size -= kSystemPointerSize;
+  }
 
   // We can't run out of space, just pass anything big enough to not cause the
   // assembler to try to grow the buffer.
diff -r -u --color up/v8/src/wasm/baseline/mips64/liftoff-assembler-mips64.h nw/v8/src/wasm/baseline/mips64/liftoff-assembler-mips64.h
--- up/v8/src/wasm/baseline/mips64/liftoff-assembler-mips64.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/baseline/mips64/liftoff-assembler-mips64.h	2023-01-19 16:46:36.417276181 -0500
@@ -58,10 +58,9 @@
 //  -1   | StackFrame::WASM   |
 //  -2   |     instance       |
 //  -3   |     feedback vector|
-//  -4   |     tiering budget |
 //  -----+--------------------+---------------------------
-//  -5   |     slot 0         |   ^
-//  -6   |     slot 1         |   |
+//  -4   |     slot 0         |   ^
+//  -5   |     slot 1         |   |
 //       |                    | Frame slots
 //       |                    |   |
 //       |                    |   v
@@ -311,6 +310,22 @@
   return offset;
 }
 
+void LiftoffAssembler::CallFrameSetupStub(int declared_function_index) {
+  // TODO(jkummerow): Enable this check when we have C++20.
+  // static_assert(std::find(std::begin(wasm::kGpParamRegisters),
+  //                         std::end(wasm::kGpParamRegisters),
+  //                         kLiftoffFrameSetupFunctionReg) ==
+  //                         std::end(wasm::kGpParamRegisters));
+
+  // On MIPS64, we must push at least {ra} before calling the stub, otherwise
+  // it would get clobbered with no possibility to recover it. So just set
+  // up the frame here.
+  EnterFrame(StackFrame::WASM);
+  LoadConstant(LiftoffRegister(kLiftoffFrameSetupFunctionReg),
+               WasmValue(declared_function_index));
+  CallRuntimeStub(WasmCode::kWasmLiftoffFrameSetup);
+}
+
 void LiftoffAssembler::PrepareTailCall(int num_callee_stack_params,
                                        int stack_param_delta) {
   UseScratchRegisterScope temps(this);
@@ -342,6 +357,10 @@
   // pushed as part of frame construction, so we don't need to allocate memory
   // for them anymore.
   int frame_size = GetTotalFrameSize() - 2 * kSystemPointerSize;
+  // The frame setup builtin also pushes the feedback vector.
+  if (v8_flags.wasm_speculative_inlining) {
+    frame_size -= kSystemPointerSize;
+  }
 
   // We can't run out of space, just pass anything big enough to not cause the
   // assembler to try to grow the buffer.
diff -r -u --color up/v8/src/wasm/baseline/ppc/liftoff-assembler-ppc.h nw/v8/src/wasm/baseline/ppc/liftoff-assembler-ppc.h
--- up/v8/src/wasm/baseline/ppc/liftoff-assembler-ppc.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/baseline/ppc/liftoff-assembler-ppc.h	2023-01-19 16:46:36.417276181 -0500
@@ -111,6 +111,20 @@
   return offset;
 }
 
+void LiftoffAssembler::CallFrameSetupStub(int declared_function_index) {
+  // TODO(jkummerow): Enable this check when we have C++20.
+  // static_assert(std::find(std::begin(wasm::kGpParamRegisters),
+  //                         std::end(wasm::kGpParamRegisters),
+  //                         kLiftoffFrameSetupFunctionReg) ==
+  //                         std::end(wasm::kGpParamRegisters));
+  Register scratch = ip;
+  mov(scratch, Operand(StackFrame::TypeToMarker(StackFrame::WASM)));
+  PushCommonFrame(scratch);
+  LoadConstant(LiftoffRegister(kLiftoffFrameSetupFunctionReg),
+               WasmValue(declared_function_index));
+  CallRuntimeStub(WasmCode::kWasmLiftoffFrameSetup);
+}
+
 void LiftoffAssembler::PrepareTailCall(int num_callee_stack_params,
                                        int stack_param_delta) {
   Register scratch = ip;
@@ -142,6 +156,10 @@
   int frame_size =
       GetTotalFrameSize() -
       (V8_EMBEDDED_CONSTANT_POOL_BOOL ? 3 : 2) * kSystemPointerSize;
+  // The frame setup builtin also pushes the feedback vector.
+  if (v8_flags.wasm_speculative_inlining) {
+    frame_size -= kSystemPointerSize;
+  }
 
   Assembler patching_assembler(
       AssemblerOptions{},
@@ -1644,7 +1662,20 @@
       case kRefNull:
       case kRtt:
         DCHECK(liftoff_cond == kEqual || liftoff_cond == kUnequal);
-        V8_FALLTHROUGH;
+#if defined(V8_COMPRESS_POINTERS)
+        if (use_signed) {
+          CmpS32(lhs, rhs);
+        } else {
+          CmpU32(lhs, rhs);
+        }
+#else
+        if (use_signed) {
+          CmpS64(lhs, rhs);
+        } else {
+          CmpU64(lhs, rhs);
+        }
+#endif
+        break;
       case kI64:
         if (use_signed) {
           CmpS64(lhs, rhs);
@@ -1767,64 +1798,72 @@
   return false;
 }
 
-#define SIMD_BINOP_LIST(V)         \
-  V(f64x2_add, F64x2Add)           \
-  V(f64x2_sub, F64x2Sub)           \
-  V(f64x2_mul, F64x2Mul)           \
-  V(f64x2_div, F64x2Div)           \
-  V(f64x2_eq, F64x2Eq)             \
-  V(f64x2_lt, F64x2Lt)             \
-  V(f64x2_le, F64x2Le)             \
-  V(f32x4_add, F32x4Add)           \
-  V(f32x4_sub, F32x4Sub)           \
-  V(f32x4_mul, F32x4Mul)           \
-  V(f32x4_div, F32x4Div)           \
-  V(f32x4_min, F32x4Min)           \
-  V(f32x4_max, F32x4Max)           \
-  V(f32x4_eq, F32x4Eq)             \
-  V(f32x4_lt, F32x4Lt)             \
-  V(f32x4_le, F32x4Le)             \
-  V(i64x2_add, I64x2Add)           \
-  V(i64x2_sub, I64x2Sub)           \
-  V(i64x2_eq, I64x2Eq)             \
-  V(i64x2_gt_s, I64x2GtS)          \
-  V(i32x4_add, I32x4Add)           \
-  V(i32x4_sub, I32x4Sub)           \
-  V(i32x4_mul, I32x4Mul)           \
-  V(i32x4_min_s, I32x4MinS)        \
-  V(i32x4_min_u, I32x4MinU)        \
-  V(i32x4_max_s, I32x4MaxS)        \
-  V(i32x4_max_u, I32x4MaxU)        \
-  V(i32x4_eq, I32x4Eq)             \
-  V(i32x4_gt_s, I32x4GtS)          \
-  V(i32x4_gt_u, I32x4GtU)          \
-  V(i16x8_add, I16x8Add)           \
-  V(i16x8_sub, I16x8Sub)           \
-  V(i16x8_mul, I16x8Mul)           \
-  V(i16x8_min_s, I16x8MinS)        \
-  V(i16x8_min_u, I16x8MinU)        \
-  V(i16x8_max_s, I16x8MaxS)        \
-  V(i16x8_max_u, I16x8MaxU)        \
-  V(i16x8_eq, I16x8Eq)             \
-  V(i16x8_gt_s, I16x8GtS)          \
-  V(i16x8_gt_u, I16x8GtU)          \
-  V(i16x8_add_sat_s, I16x8AddSatS) \
-  V(i16x8_sub_sat_s, I16x8SubSatS) \
-  V(i16x8_add_sat_u, I16x8AddSatU) \
-  V(i16x8_sub_sat_u, I16x8SubSatU) \
-  V(i8x16_add, I8x16Add)           \
-  V(i8x16_sub, I8x16Sub)           \
-  V(i8x16_min_s, I8x16MinS)        \
-  V(i8x16_min_u, I8x16MinU)        \
-  V(i8x16_max_s, I8x16MaxS)        \
-  V(i8x16_max_u, I8x16MaxU)        \
-  V(i8x16_eq, I8x16Eq)             \
-  V(i8x16_gt_s, I8x16GtS)          \
-  V(i8x16_gt_u, I8x16GtU)          \
-  V(i8x16_add_sat_s, I8x16AddSatS) \
-  V(i8x16_sub_sat_s, I8x16SubSatS) \
-  V(i8x16_add_sat_u, I8x16AddSatU) \
-  V(i8x16_sub_sat_u, I8x16SubSatU)
+#define SIMD_BINOP_LIST(V)                    \
+  V(f64x2_add, F64x2Add)                      \
+  V(f64x2_sub, F64x2Sub)                      \
+  V(f64x2_mul, F64x2Mul)                      \
+  V(f64x2_div, F64x2Div)                      \
+  V(f64x2_eq, F64x2Eq)                        \
+  V(f64x2_lt, F64x2Lt)                        \
+  V(f64x2_le, F64x2Le)                        \
+  V(f32x4_add, F32x4Add)                      \
+  V(f32x4_sub, F32x4Sub)                      \
+  V(f32x4_mul, F32x4Mul)                      \
+  V(f32x4_div, F32x4Div)                      \
+  V(f32x4_min, F32x4Min)                      \
+  V(f32x4_max, F32x4Max)                      \
+  V(f32x4_eq, F32x4Eq)                        \
+  V(f32x4_lt, F32x4Lt)                        \
+  V(f32x4_le, F32x4Le)                        \
+  V(i64x2_add, I64x2Add)                      \
+  V(i64x2_sub, I64x2Sub)                      \
+  V(i64x2_eq, I64x2Eq)                        \
+  V(i64x2_gt_s, I64x2GtS)                     \
+  V(i32x4_add, I32x4Add)                      \
+  V(i32x4_sub, I32x4Sub)                      \
+  V(i32x4_mul, I32x4Mul)                      \
+  V(i32x4_min_s, I32x4MinS)                   \
+  V(i32x4_min_u, I32x4MinU)                   \
+  V(i32x4_max_s, I32x4MaxS)                   \
+  V(i32x4_max_u, I32x4MaxU)                   \
+  V(i32x4_eq, I32x4Eq)                        \
+  V(i32x4_gt_s, I32x4GtS)                     \
+  V(i32x4_gt_u, I32x4GtU)                     \
+  V(i16x8_add, I16x8Add)                      \
+  V(i16x8_sub, I16x8Sub)                      \
+  V(i16x8_mul, I16x8Mul)                      \
+  V(i16x8_min_s, I16x8MinS)                   \
+  V(i16x8_min_u, I16x8MinU)                   \
+  V(i16x8_max_s, I16x8MaxS)                   \
+  V(i16x8_max_u, I16x8MaxU)                   \
+  V(i16x8_eq, I16x8Eq)                        \
+  V(i16x8_gt_s, I16x8GtS)                     \
+  V(i16x8_gt_u, I16x8GtU)                     \
+  V(i16x8_add_sat_s, I16x8AddSatS)            \
+  V(i16x8_sub_sat_s, I16x8SubSatS)            \
+  V(i16x8_add_sat_u, I16x8AddSatU)            \
+  V(i16x8_sub_sat_u, I16x8SubSatU)            \
+  V(i16x8_sconvert_i32x4, I16x8SConvertI32x4) \
+  V(i16x8_uconvert_i32x4, I16x8UConvertI32x4) \
+  V(i8x16_add, I8x16Add)                      \
+  V(i8x16_sub, I8x16Sub)                      \
+  V(i8x16_min_s, I8x16MinS)                   \
+  V(i8x16_min_u, I8x16MinU)                   \
+  V(i8x16_max_s, I8x16MaxS)                   \
+  V(i8x16_max_u, I8x16MaxU)                   \
+  V(i8x16_eq, I8x16Eq)                        \
+  V(i8x16_gt_s, I8x16GtS)                     \
+  V(i8x16_gt_u, I8x16GtU)                     \
+  V(i8x16_add_sat_s, I8x16AddSatS)            \
+  V(i8x16_sub_sat_s, I8x16SubSatS)            \
+  V(i8x16_add_sat_u, I8x16AddSatU)            \
+  V(i8x16_sub_sat_u, I8x16SubSatU)            \
+  V(i8x16_sconvert_i16x8, I8x16SConvertI16x8) \
+  V(i8x16_uconvert_i16x8, I8x16UConvertI16x8) \
+  V(s128_and, S128And)                        \
+  V(s128_or, S128Or)                          \
+  V(s128_xor, S128Xor)                        \
+  V(s128_and_not, S128AndNot)
 
 #define EMIT_SIMD_BINOP(name, op)                                              \
   void LiftoffAssembler::emit_##name(LiftoffRegister dst, LiftoffRegister lhs, \
@@ -1835,19 +1874,35 @@
 #undef EMIT_SIMD_BINOP
 #undef SIMD_BINOP_LIST
 
-#define SIMD_BINOP_WITH_SCRATCH_LIST(V) \
-  V(f64x2_ne, F64x2Ne)                  \
-  V(f32x4_ne, F32x4Ne)                  \
-  V(i64x2_ne, I64x2Ne)                  \
-  V(i64x2_ge_s, I64x2GeS)               \
-  V(i32x4_ne, I32x4Ne)                  \
-  V(i32x4_ge_s, I32x4GeS)               \
-  V(i32x4_ge_u, I32x4GeU)               \
-  V(i16x8_ne, I16x8Ne)                  \
-  V(i16x8_ge_s, I16x8GeS)               \
-  V(i16x8_ge_u, I16x8GeU)               \
-  V(i8x16_ne, I8x16Ne)                  \
-  V(i8x16_ge_s, I8x16GeS)               \
+#define SIMD_BINOP_WITH_SCRATCH_LIST(V)               \
+  V(f64x2_ne, F64x2Ne)                                \
+  V(f64x2_pmin, F64x2Pmin)                            \
+  V(f64x2_pmax, F64x2Pmax)                            \
+  V(f32x4_ne, F32x4Ne)                                \
+  V(f32x4_pmin, F32x4Pmin)                            \
+  V(f32x4_pmax, F32x4Pmax)                            \
+  V(i64x2_ne, I64x2Ne)                                \
+  V(i64x2_ge_s, I64x2GeS)                             \
+  V(i64x2_extmul_low_i32x4_s, I64x2ExtMulLowI32x4S)   \
+  V(i64x2_extmul_low_i32x4_u, I64x2ExtMulLowI32x4U)   \
+  V(i64x2_extmul_high_i32x4_s, I64x2ExtMulHighI32x4S) \
+  V(i64x2_extmul_high_i32x4_u, I64x2ExtMulHighI32x4U) \
+  V(i32x4_ne, I32x4Ne)                                \
+  V(i32x4_ge_s, I32x4GeS)                             \
+  V(i32x4_ge_u, I32x4GeU)                             \
+  V(i32x4_extmul_low_i16x8_s, I32x4ExtMulLowI16x8S)   \
+  V(i32x4_extmul_low_i16x8_u, I32x4ExtMulLowI16x8U)   \
+  V(i32x4_extmul_high_i16x8_s, I32x4ExtMulHighI16x8S) \
+  V(i32x4_extmul_high_i16x8_u, I32x4ExtMulHighI16x8U) \
+  V(i16x8_ne, I16x8Ne)                                \
+  V(i16x8_ge_s, I16x8GeS)                             \
+  V(i16x8_ge_u, I16x8GeU)                             \
+  V(i16x8_extmul_low_i8x16_s, I16x8ExtMulLowI8x16S)   \
+  V(i16x8_extmul_low_i8x16_u, I16x8ExtMulLowI8x16U)   \
+  V(i16x8_extmul_high_i8x16_s, I16x8ExtMulHighI8x16S) \
+  V(i16x8_extmul_high_i8x16_u, I16x8ExtMulHighI8x16U) \
+  V(i8x16_ne, I8x16Ne)                                \
+  V(i8x16_ge_s, I8x16GeS)                             \
   V(i8x16_ge_u, I8x16GeU)
 
 #define EMIT_SIMD_BINOP_WITH_SCRATCH(name, op)                                 \
@@ -1907,22 +1962,30 @@
 #undef EMIT_SIMD_SHIFT_RI
 #undef SIMD_SHIFT_RI_LIST
 
-#define SIMD_UNOP_LIST(V)                \
-  V(f64x2_abs, F64x2Abs, , void)         \
-  V(f64x2_neg, F64x2Neg, , void)         \
-  V(f64x2_sqrt, F64x2Sqrt, , void)       \
-  V(f64x2_ceil, F64x2Ceil, true, bool)   \
-  V(f64x2_floor, F64x2Floor, true, bool) \
-  V(f64x2_trunc, F64x2Trunc, true, bool) \
-  V(f32x4_abs, F32x4Abs, , void)         \
-  V(f32x4_neg, F32x4Neg, , void)         \
-  V(i64x2_neg, I64x2Neg, , void)         \
-  V(i32x4_neg, I32x4Neg, , void)         \
-  V(f32x4_sqrt, F32x4Sqrt, , void)       \
-  V(f32x4_ceil, F32x4Ceil, true, bool)   \
-  V(f32x4_floor, F32x4Floor, true, bool) \
-  V(f32x4_trunc, F32x4Trunc, true, bool) \
-  V(i8x16_popcnt, I8x16Popcnt, , void)
+#define SIMD_UNOP_LIST(V)                                      \
+  V(f64x2_abs, F64x2Abs, , void)                               \
+  V(f64x2_neg, F64x2Neg, , void)                               \
+  V(f64x2_sqrt, F64x2Sqrt, , void)                             \
+  V(f64x2_ceil, F64x2Ceil, true, bool)                         \
+  V(f64x2_floor, F64x2Floor, true, bool)                       \
+  V(f64x2_trunc, F64x2Trunc, true, bool)                       \
+  V(f32x4_abs, F32x4Abs, , void)                               \
+  V(f32x4_neg, F32x4Neg, , void)                               \
+  V(f32x4_sqrt, F32x4Sqrt, , void)                             \
+  V(f32x4_ceil, F32x4Ceil, true, bool)                         \
+  V(f32x4_floor, F32x4Floor, true, bool)                       \
+  V(f32x4_trunc, F32x4Trunc, true, bool)                       \
+  V(i64x2_neg, I64x2Neg, , void)                               \
+  V(f64x2_convert_low_i32x4_s, F64x2ConvertLowI32x4S, , void)  \
+  V(i64x2_sconvert_i32x4_low, I64x2SConvertI32x4Low, , void)   \
+  V(i64x2_sconvert_i32x4_high, I64x2SConvertI32x4High, , void) \
+  V(i32x4_neg, I32x4Neg, , void)                               \
+  V(i32x4_sconvert_i16x8_low, I32x4SConvertI16x8Low, , void)   \
+  V(i32x4_sconvert_i16x8_high, I32x4SConvertI16x8High, , void) \
+  V(i16x8_sconvert_i8x16_low, I16x8SConvertI8x16Low, , void)   \
+  V(i16x8_sconvert_i8x16_high, I16x8SConvertI8x16High, , void) \
+  V(i8x16_popcnt, I8x16Popcnt, , void)                         \
+  V(s128_not, S128Not, , void)
 
 #define EMIT_SIMD_UNOP(name, op, return_val, return_type)          \
   return_type LiftoffAssembler::emit_##name(LiftoffRegister dst,   \
@@ -1934,6 +1997,38 @@
 #undef EMIT_SIMD_UNOP
 #undef SIMD_UNOP_LIST
 
+#define SIMD_UNOP_WITH_SCRATCH_LIST(V) \
+  V(i64x2_abs, I64x2Abs, , void)       \
+  V(i32x4_abs, I32x4Abs, , void)       \
+  V(i16x8_abs, I16x8Abs, , void)       \
+  V(i16x8_neg, I16x8Neg, , void)       \
+  V(i8x16_abs, I8x16Abs, , void)       \
+  V(i8x16_neg, I8x16Neg, , void)
+
+#define EMIT_SIMD_UNOP_WITH_SCRATCH(name, op, return_val, return_type) \
+  return_type LiftoffAssembler::emit_##name(LiftoffRegister dst,       \
+                                            LiftoffRegister src) {     \
+    op(dst.fp().toSimd(), src.fp().toSimd(), kScratchSimd128Reg);      \
+    return return_val;                                                 \
+  }
+SIMD_UNOP_WITH_SCRATCH_LIST(EMIT_SIMD_UNOP_WITH_SCRATCH)
+#undef EMIT_SIMD_UNOP_WITH_SCRATCH
+#undef SIMD_UNOP_WITH_SCRATCH_LIST
+
+#define SIMD_ALL_TRUE_LIST(V)    \
+  V(i64x2_alltrue, I64x2AllTrue) \
+  V(i32x4_alltrue, I32x4AllTrue) \
+  V(i16x8_alltrue, I16x8AllTrue) \
+  V(i8x16_alltrue, I8x16AllTrue)
+#define EMIT_SIMD_ALL_TRUE(name, op)                             \
+  void LiftoffAssembler::emit_##name(LiftoffRegister dst,        \
+                                     LiftoffRegister src) {      \
+    op(dst.gp(), src.fp().toSimd(), r0, ip, kScratchSimd128Reg); \
+  }
+SIMD_ALL_TRUE_LIST(EMIT_SIMD_ALL_TRUE)
+#undef EMIT_SIMD_ALL_TRUE
+#undef SIMD_ALL_TRUE_LIST
+
 void LiftoffAssembler::emit_f64x2_splat(LiftoffRegister dst,
                                         LiftoffRegister src) {
   F64x2Splat(dst.fp().toSimd(), src.fp(), r0);
@@ -2068,36 +2163,6 @@
                    imm_lane_idx, kScratchSimd128Reg);
 }
 
-void LiftoffAssembler::emit_i64x2_abs(LiftoffRegister dst,
-                                      LiftoffRegister src) {
-  I64x2Abs(dst.fp().toSimd(), src.fp().toSimd(), kScratchSimd128Reg);
-}
-
-void LiftoffAssembler::emit_i32x4_abs(LiftoffRegister dst,
-                                      LiftoffRegister src) {
-  I32x4Abs(dst.fp().toSimd(), src.fp().toSimd(), kScratchSimd128Reg);
-}
-
-void LiftoffAssembler::emit_i16x8_abs(LiftoffRegister dst,
-                                      LiftoffRegister src) {
-  I16x8Abs(dst.fp().toSimd(), src.fp().toSimd(), kScratchSimd128Reg);
-}
-
-void LiftoffAssembler::emit_i16x8_neg(LiftoffRegister dst,
-                                      LiftoffRegister src) {
-  I16x8Neg(dst.fp().toSimd(), src.fp().toSimd(), kScratchSimd128Reg);
-}
-
-void LiftoffAssembler::emit_i8x16_abs(LiftoffRegister dst,
-                                      LiftoffRegister src) {
-  I8x16Abs(dst.fp().toSimd(), src.fp().toSimd(), kScratchSimd128Reg);
-}
-
-void LiftoffAssembler::emit_i8x16_neg(LiftoffRegister dst,
-                                      LiftoffRegister src) {
-  I8x16Neg(dst.fp().toSimd(), src.fp().toSimd(), kScratchSimd128Reg);
-}
-
 void LiftoffAssembler::emit_i64x2_mul(LiftoffRegister dst, LiftoffRegister lhs,
                                       LiftoffRegister rhs) {
   // TODO(miladfarca): Make use of UseScratchRegisterScope.
@@ -2199,16 +2264,6 @@
   bailout(kRelaxedSimd, "emit_s128_relaxed_laneselect");
 }
 
-void LiftoffAssembler::emit_f64x2_pmin(LiftoffRegister dst, LiftoffRegister lhs,
-                                       LiftoffRegister rhs) {
-  bailout(kSimd, "pmin unimplemented");
-}
-
-void LiftoffAssembler::emit_f64x2_pmax(LiftoffRegister dst, LiftoffRegister lhs,
-                                       LiftoffRegister rhs) {
-  bailout(kSimd, "pmax unimplemented");
-}
-
 void LiftoffAssembler::emit_f64x2_relaxed_min(LiftoffRegister dst,
                                               LiftoffRegister lhs,
                                               LiftoffRegister rhs) {
@@ -2221,14 +2276,10 @@
   bailout(kRelaxedSimd, "emit_f64x2_relaxed_max");
 }
 
-void LiftoffAssembler::emit_f64x2_convert_low_i32x4_s(LiftoffRegister dst,
-                                                      LiftoffRegister src) {
-  bailout(kSimd, "f64x2.convert_low_i32x4_s");
-}
-
 void LiftoffAssembler::emit_f64x2_convert_low_i32x4_u(LiftoffRegister dst,
                                                       LiftoffRegister src) {
-  bailout(kSimd, "f64x2.convert_low_i32x4_u");
+  F64x2ConvertLowI32x4U(dst.fp().toSimd(), src.fp().toSimd(), r0,
+                        kScratchSimd128Reg);
 }
 
 void LiftoffAssembler::emit_f64x2_promote_low_f32x4(LiftoffRegister dst,
@@ -2248,73 +2299,21 @@
   bailout(kUnsupportedArchitecture, "emit_f32x4_relaxed_max");
 }
 
-void LiftoffAssembler::emit_f32x4_pmin(LiftoffRegister dst, LiftoffRegister lhs,
-                                       LiftoffRegister rhs) {
-  bailout(kSimd, "pmin unimplemented");
-}
-
-void LiftoffAssembler::emit_f32x4_pmax(LiftoffRegister dst, LiftoffRegister lhs,
-                                       LiftoffRegister rhs) {
-  bailout(kSimd, "pmax unimplemented");
-}
-
-void LiftoffAssembler::emit_i64x2_alltrue(LiftoffRegister dst,
-                                          LiftoffRegister src) {
-  bailout(kSimd, "i64x2_alltrue");
-}
-
-void LiftoffAssembler::emit_i64x2_extmul_low_i32x4_s(LiftoffRegister dst,
-                                                     LiftoffRegister src1,
-                                                     LiftoffRegister src2) {
-  bailout(kSimd, "i64x2_extmul_low_i32x4_s unsupported");
-}
-
-void LiftoffAssembler::emit_i64x2_extmul_low_i32x4_u(LiftoffRegister dst,
-                                                     LiftoffRegister src1,
-                                                     LiftoffRegister src2) {
-  bailout(kSimd, "i64x2_extmul_low_i32x4_u unsupported");
-}
-
-void LiftoffAssembler::emit_i64x2_extmul_high_i32x4_s(LiftoffRegister dst,
-                                                      LiftoffRegister src1,
-                                                      LiftoffRegister src2) {
-  bailout(kSimd, "i64x2_extmul_high_i32x4_s unsupported");
-}
-
 void LiftoffAssembler::emit_i64x2_bitmask(LiftoffRegister dst,
                                           LiftoffRegister src) {
   bailout(kSimd, "i64x2_bitmask");
 }
 
-void LiftoffAssembler::emit_i64x2_sconvert_i32x4_low(LiftoffRegister dst,
-                                                     LiftoffRegister src) {
-  bailout(kSimd, "i64x2_sconvert_i32x4_low");
-}
-
-void LiftoffAssembler::emit_i64x2_sconvert_i32x4_high(LiftoffRegister dst,
-                                                      LiftoffRegister src) {
-  bailout(kSimd, "i64x2_sconvert_i32x4_high");
-}
-
 void LiftoffAssembler::emit_i64x2_uconvert_i32x4_low(LiftoffRegister dst,
                                                      LiftoffRegister src) {
-  bailout(kSimd, "i64x2_uconvert_i32x4_low");
+  I64x2UConvertI32x4Low(dst.fp().toSimd(), src.fp().toSimd(), r0,
+                        kScratchSimd128Reg);
 }
 
 void LiftoffAssembler::emit_i64x2_uconvert_i32x4_high(LiftoffRegister dst,
                                                       LiftoffRegister src) {
-  bailout(kSimd, "i64x2_uconvert_i32x4_high");
-}
-
-void LiftoffAssembler::emit_i64x2_extmul_high_i32x4_u(LiftoffRegister dst,
-                                                      LiftoffRegister src1,
-                                                      LiftoffRegister src2) {
-  bailout(kSimd, "i64x2_extmul_high_i32x4_u unsupported");
-}
-
-void LiftoffAssembler::emit_i32x4_alltrue(LiftoffRegister dst,
-                                          LiftoffRegister src) {
-  bailout(kSimd, "i32x4_alltrue");
+  I64x2UConvertI32x4High(dst.fp().toSimd(), src.fp().toSimd(), r0,
+                         kScratchSimd128Reg);
 }
 
 void LiftoffAssembler::emit_i32x4_bitmask(LiftoffRegister dst,
@@ -2338,35 +2337,6 @@
   bailout(kSimd, "i32x4.extadd_pairwise_i16x8_u");
 }
 
-void LiftoffAssembler::emit_i32x4_extmul_low_i16x8_s(LiftoffRegister dst,
-                                                     LiftoffRegister src1,
-                                                     LiftoffRegister src2) {
-  bailout(kSimd, "i32x4_extmul_low_i16x8_s unsupported");
-}
-
-void LiftoffAssembler::emit_i32x4_extmul_low_i16x8_u(LiftoffRegister dst,
-                                                     LiftoffRegister src1,
-                                                     LiftoffRegister src2) {
-  bailout(kSimd, "i32x4_extmul_low_i16x8_u unsupported");
-}
-
-void LiftoffAssembler::emit_i32x4_extmul_high_i16x8_s(LiftoffRegister dst,
-                                                      LiftoffRegister src1,
-                                                      LiftoffRegister src2) {
-  bailout(kSimd, "i32x4_extmul_high_i16x8_s unsupported");
-}
-
-void LiftoffAssembler::emit_i32x4_extmul_high_i16x8_u(LiftoffRegister dst,
-                                                      LiftoffRegister src1,
-                                                      LiftoffRegister src2) {
-  bailout(kSimd, "i32x4_extmul_high_i16x8_u unsupported");
-}
-
-void LiftoffAssembler::emit_i16x8_alltrue(LiftoffRegister dst,
-                                          LiftoffRegister src) {
-  bailout(kSimd, "i16x8_alltrue");
-}
-
 void LiftoffAssembler::emit_i16x8_bitmask(LiftoffRegister dst,
                                           LiftoffRegister src) {
   bailout(kSimd, "i16x8_bitmask");
@@ -2382,36 +2352,12 @@
   bailout(kSimd, "i16x8.extadd_pairwise_i8x16_u");
 }
 
-void LiftoffAssembler::emit_i16x8_extmul_low_i8x16_s(LiftoffRegister dst,
-                                                     LiftoffRegister src1,
-                                                     LiftoffRegister src2) {
-  bailout(kSimd, "i16x8.extmul_low_i8x16_s unsupported");
-}
-
-void LiftoffAssembler::emit_i16x8_extmul_low_i8x16_u(LiftoffRegister dst,
-                                                     LiftoffRegister src1,
-                                                     LiftoffRegister src2) {
-  bailout(kSimd, "i16x8.extmul_low_i8x16_u unsupported");
-}
-
-void LiftoffAssembler::emit_i16x8_extmul_high_i8x16_s(LiftoffRegister dst,
-                                                      LiftoffRegister src1,
-                                                      LiftoffRegister src2) {
-  bailout(kSimd, "i16x8.extmul_high_i8x16_s unsupported");
-}
-
 void LiftoffAssembler::emit_i16x8_q15mulr_sat_s(LiftoffRegister dst,
                                                 LiftoffRegister src1,
                                                 LiftoffRegister src2) {
   bailout(kSimd, "i16x8_q15mulr_sat_s");
 }
 
-void LiftoffAssembler::emit_i16x8_extmul_high_i8x16_u(LiftoffRegister dst,
-                                                      LiftoffRegister src1,
-                                                      LiftoffRegister src2) {
-  bailout(kSimd, "i16x8_extmul_high_i8x16_u unsupported");
-}
-
 void LiftoffAssembler::emit_i16x8_relaxed_q15mulr_s(LiftoffRegister dst,
                                                     LiftoffRegister src1,
                                                     LiftoffRegister src2) {
@@ -2441,12 +2387,7 @@
 
 void LiftoffAssembler::emit_v128_anytrue(LiftoffRegister dst,
                                          LiftoffRegister src) {
-  bailout(kSimd, "v8x16_anytrue");
-}
-
-void LiftoffAssembler::emit_i8x16_alltrue(LiftoffRegister dst,
-                                          LiftoffRegister src) {
-  bailout(kSimd, "i8x16_alltrue");
+  V128AnyTrue(dst.gp(), src.fp().toSimd(), r0, ip, kScratchSimd128Reg);
 }
 
 void LiftoffAssembler::emit_i8x16_bitmask(LiftoffRegister dst,
@@ -2459,25 +2400,6 @@
   bailout(kUnsupportedArchitecture, "emit_s128_const");
 }
 
-void LiftoffAssembler::emit_s128_not(LiftoffRegister dst, LiftoffRegister src) {
-  bailout(kUnsupportedArchitecture, "emit_s128_not");
-}
-
-void LiftoffAssembler::emit_s128_and(LiftoffRegister dst, LiftoffRegister lhs,
-                                     LiftoffRegister rhs) {
-  bailout(kUnsupportedArchitecture, "emit_s128_and");
-}
-
-void LiftoffAssembler::emit_s128_or(LiftoffRegister dst, LiftoffRegister lhs,
-                                    LiftoffRegister rhs) {
-  bailout(kUnsupportedArchitecture, "emit_s128_or");
-}
-
-void LiftoffAssembler::emit_s128_xor(LiftoffRegister dst, LiftoffRegister lhs,
-                                     LiftoffRegister rhs) {
-  bailout(kUnsupportedArchitecture, "emit_s128_xor");
-}
-
 void LiftoffAssembler::emit_s128_select(LiftoffRegister dst,
                                         LiftoffRegister src1,
                                         LiftoffRegister src2,
@@ -2510,68 +2432,28 @@
   bailout(kSimd, "f32x4.demote_f64x2_zero");
 }
 
-void LiftoffAssembler::emit_i8x16_sconvert_i16x8(LiftoffRegister dst,
-                                                 LiftoffRegister lhs,
-                                                 LiftoffRegister rhs) {
-  bailout(kUnsupportedArchitecture, "emit_i8x16_sconvert_i16x8");
-}
-
-void LiftoffAssembler::emit_i8x16_uconvert_i16x8(LiftoffRegister dst,
-                                                 LiftoffRegister lhs,
-                                                 LiftoffRegister rhs) {
-  bailout(kUnsupportedArchitecture, "emit_i8x16_uconvert_i16x8");
-}
-
-void LiftoffAssembler::emit_i16x8_sconvert_i32x4(LiftoffRegister dst,
-                                                 LiftoffRegister lhs,
-                                                 LiftoffRegister rhs) {
-  bailout(kUnsupportedArchitecture, "emit_i16x8_sconvert_i32x4");
-}
-
-void LiftoffAssembler::emit_i16x8_uconvert_i32x4(LiftoffRegister dst,
-                                                 LiftoffRegister lhs,
-                                                 LiftoffRegister rhs) {
-  bailout(kUnsupportedArchitecture, "emit_i16x8_uconvert_i32x4");
-}
-
-void LiftoffAssembler::emit_i16x8_sconvert_i8x16_low(LiftoffRegister dst,
-                                                     LiftoffRegister src) {
-  bailout(kUnsupportedArchitecture, "emit_i16x8_sconvert_i8x16_low");
-}
-
-void LiftoffAssembler::emit_i16x8_sconvert_i8x16_high(LiftoffRegister dst,
-                                                      LiftoffRegister src) {
-  bailout(kUnsupportedArchitecture, "emit_i16x8_sconvert_i8x16_high");
-}
-
 void LiftoffAssembler::emit_i16x8_uconvert_i8x16_low(LiftoffRegister dst,
                                                      LiftoffRegister src) {
-  bailout(kUnsupportedArchitecture, "emit_i16x8_uconvert_i8x16_low");
+  I16x8UConvertI8x16Low(dst.fp().toSimd(), src.fp().toSimd(), r0,
+                        kScratchSimd128Reg);
 }
 
 void LiftoffAssembler::emit_i16x8_uconvert_i8x16_high(LiftoffRegister dst,
                                                       LiftoffRegister src) {
-  bailout(kUnsupportedArchitecture, "emit_i16x8_uconvert_i8x16_high");
-}
-
-void LiftoffAssembler::emit_i32x4_sconvert_i16x8_low(LiftoffRegister dst,
-                                                     LiftoffRegister src) {
-  bailout(kUnsupportedArchitecture, "emit_i32x4_sconvert_i16x8_low");
-}
-
-void LiftoffAssembler::emit_i32x4_sconvert_i16x8_high(LiftoffRegister dst,
-                                                      LiftoffRegister src) {
-  bailout(kUnsupportedArchitecture, "emit_i32x4_sconvert_i16x8_high");
+  I16x8UConvertI8x16High(dst.fp().toSimd(), src.fp().toSimd(), r0,
+                         kScratchSimd128Reg);
 }
 
 void LiftoffAssembler::emit_i32x4_uconvert_i16x8_low(LiftoffRegister dst,
                                                      LiftoffRegister src) {
-  bailout(kUnsupportedArchitecture, "emit_i32x4_uconvert_i16x8_low");
+  I32x4UConvertI16x8Low(dst.fp().toSimd(), src.fp().toSimd(), r0,
+                        kScratchSimd128Reg);
 }
 
 void LiftoffAssembler::emit_i32x4_uconvert_i16x8_high(LiftoffRegister dst,
                                                       LiftoffRegister src) {
-  bailout(kUnsupportedArchitecture, "emit_i32x4_uconvert_i16x8_high");
+  I32x4UConvertI16x8High(dst.fp().toSimd(), src.fp().toSimd(), r0,
+                         kScratchSimd128Reg);
 }
 
 void LiftoffAssembler::emit_i32x4_trunc_sat_f64x2_s_zero(LiftoffRegister dst,
@@ -2584,12 +2466,6 @@
   bailout(kSimd, "i32x4.trunc_sat_f64x2_u_zero");
 }
 
-void LiftoffAssembler::emit_s128_and_not(LiftoffRegister dst,
-                                         LiftoffRegister lhs,
-                                         LiftoffRegister rhs) {
-  bailout(kUnsupportedArchitecture, "emit_s128_and_not");
-}
-
 void LiftoffAssembler::emit_i8x16_rounding_average_u(LiftoffRegister dst,
                                                      LiftoffRegister lhs,
                                                      LiftoffRegister rhs) {
diff -r -u --color up/v8/src/wasm/baseline/riscv/liftoff-assembler-riscv.h nw/v8/src/wasm/baseline/riscv/liftoff-assembler-riscv.h
--- up/v8/src/wasm/baseline/riscv/liftoff-assembler-riscv.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/baseline/riscv/liftoff-assembler-riscv.h	2023-01-19 16:46:36.417276181 -0500
@@ -91,6 +91,10 @@
   // pushed as part of frame construction, so we don't need to allocate memory
   // for them anymore.
   int frame_size = GetTotalFrameSize() - 2 * kSystemPointerSize;
+  // The frame setup builtin also pushes the feedback vector.
+  if (v8_flags.wasm_speculative_inlining) {
+    frame_size -= kSystemPointerSize;
+  }
   // We can't run out of space, just pass anything big enough to not cause the
   // assembler to try to grow the buffer.
   constexpr int kAvailableSpace = 256;
@@ -2227,6 +2231,22 @@
   StoreWord(scratch, MemOperand(dst));
 }
 
+void LiftoffAssembler::CallFrameSetupStub(int declared_function_index) {
+  // TODO(jkummerow): Enable this check when we have C++20.
+  // static_assert(std::find(std::begin(wasm::kGpParamRegisters),
+  //                         std::end(wasm::kGpParamRegisters),
+  //                         kLiftoffFrameSetupFunctionReg) ==
+  //                         std::end(wasm::kGpParamRegisters));
+
+  // On MIPS64, we must push at least {ra} before calling the stub, otherwise
+  // it would get clobbered with no possibility to recover it. So just set
+  // up the frame here.
+  EnterFrame(StackFrame::WASM);
+  LoadConstant(LiftoffRegister(kLiftoffFrameSetupFunctionReg),
+               WasmValue(declared_function_index));
+  CallRuntimeStub(WasmCode::kWasmLiftoffFrameSetup);
+}
+
 }  // namespace wasm
 }  // namespace internal
 }  // namespace v8
diff -r -u --color up/v8/src/wasm/baseline/riscv/liftoff-assembler-riscv32.h nw/v8/src/wasm/baseline/riscv/liftoff-assembler-riscv32.h
--- up/v8/src/wasm/baseline/riscv/liftoff-assembler-riscv32.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/baseline/riscv/liftoff-assembler-riscv32.h	2023-01-19 16:46:36.417276181 -0500
@@ -33,10 +33,9 @@
 //  -1   | StackFrame::WASM   |
 //  -2   |     instance       |
 //  -3   |     feedback vector|
-//  -4   |     tiering budget |
 //  -----+--------------------+---------------------------
-//  -5   |     slot 0         |   ^
-//  -6   |     slot 1         |   |
+//  -4   |     slot 0         |   ^
+//  -5   |     slot 1         |   |
 //       |                    | Frame slots
 //       |                    |   |
 //       |                    |   v
diff -r -u --color up/v8/src/wasm/baseline/riscv/liftoff-assembler-riscv64.h nw/v8/src/wasm/baseline/riscv/liftoff-assembler-riscv64.h
--- up/v8/src/wasm/baseline/riscv/liftoff-assembler-riscv64.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/baseline/riscv/liftoff-assembler-riscv64.h	2023-01-19 16:46:36.428109511 -0500
@@ -33,10 +33,9 @@
 //  -1   | StackFrame::WASM   |
 //  -2   |     instance       |
 //  -3   |     feedback vector|
-//  -4   |     tiering budget |
 //  -----+--------------------+---------------------------
-//  -5   |     slot 0         |   ^
-//  -6   |     slot 1         |   |
+//  -4   |     slot 0         |   ^
+//  -5   |     slot 1         |   |
 //       |                    | Frame slots
 //       |                    |   |
 //       |                    |   v
diff -r -u --color up/v8/src/wasm/baseline/s390/liftoff-assembler-s390.h nw/v8/src/wasm/baseline/s390/liftoff-assembler-s390.h
--- up/v8/src/wasm/baseline/s390/liftoff-assembler-s390.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/baseline/s390/liftoff-assembler-s390.h	2023-01-19 16:46:36.428109511 -0500
@@ -100,6 +100,22 @@
   return offset;
 }
 
+void LiftoffAssembler::CallFrameSetupStub(int declared_function_index) {
+  // TODO(jkummerow): Enable this check when we have C++20.
+  // static_assert(std::find(std::begin(wasm::kGpParamRegisters),
+  //                         std::end(wasm::kGpParamRegisters),
+  //                         kLiftoffFrameSetupFunctionReg) ==
+  //                         std::end(wasm::kGpParamRegisters));
+  // On ARM, we must push at least {lr} before calling the stub, otherwise
+  // it would get clobbered with no possibility to recover it.
+  Register scratch = ip;
+  mov(scratch, Operand(StackFrame::TypeToMarker(StackFrame::WASM)));
+  PushCommonFrame(scratch);
+  LoadConstant(LiftoffRegister(kLiftoffFrameSetupFunctionReg),
+               WasmValue(declared_function_index));
+  CallRuntimeStub(WasmCode::kWasmLiftoffFrameSetup);
+}
+
 void LiftoffAssembler::PrepareTailCall(int num_callee_stack_params,
                                        int stack_param_delta) {
   Register scratch = r1;
@@ -128,6 +144,10 @@
 void LiftoffAssembler::PatchPrepareStackFrame(
     int offset, SafepointTableBuilder* safepoint_table_builder) {
   int frame_size = GetTotalFrameSize() - 2 * kSystemPointerSize;
+  // The frame setup builtin also pushes the feedback vector.
+  if (v8_flags.wasm_speculative_inlining) {
+    frame_size -= kSystemPointerSize;
+  }
 
   constexpr int LayInstrSize = 6;
 
@@ -2169,7 +2189,20 @@
       case kRefNull:
       case kRtt:
         DCHECK(liftoff_cond == kEqual || liftoff_cond == kUnequal);
-        V8_FALLTHROUGH;
+#if defined(V8_COMPRESS_POINTERS)
+        if (use_signed) {
+          CmpS32(lhs, rhs);
+        } else {
+          CmpU32(lhs, rhs);
+        }
+#else
+        if (use_signed) {
+          CmpS64(lhs, rhs);
+        } else {
+          CmpU64(lhs, rhs);
+        }
+#endif
+        break;
       case kI64:
         if (use_signed) {
           CmpS64(lhs, rhs);
diff -r -u --color up/v8/src/wasm/baseline/x64/liftoff-assembler-x64.h nw/v8/src/wasm/baseline/x64/liftoff-assembler-x64.h
--- up/v8/src/wasm/baseline/x64/liftoff-assembler-x64.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/baseline/x64/liftoff-assembler-x64.h	2023-01-19 16:46:36.428109511 -0500
@@ -191,6 +191,18 @@
   return offset;
 }
 
+void LiftoffAssembler::CallFrameSetupStub(int declared_function_index) {
+  // TODO(jkummerow): Enable this check when we have C++20.
+  // static_assert(std::find(std::begin(wasm::kGpParamRegisters),
+  //                         std::end(wasm::kGpParamRegisters),
+  //                         kLiftoffFrameSetupFunctionReg) ==
+  //                         std::end(wasm::kGpParamRegisters));
+
+  LoadConstant(LiftoffRegister(kLiftoffFrameSetupFunctionReg),
+               WasmValue(declared_function_index));
+  CallRuntimeStub(WasmCode::kWasmLiftoffFrameSetup);
+}
+
 void LiftoffAssembler::PrepareTailCall(int num_callee_stack_params,
                                        int stack_param_delta) {
   // Push the return address and frame pointer to complete the stack frame.
@@ -219,6 +231,10 @@
   // pushed as part of frame construction, so we don't need to allocate memory
   // for them anymore.
   int frame_size = GetTotalFrameSize() - 2 * kSystemPointerSize;
+  // The frame setup builtin also pushes the feedback vector.
+  if (v8_flags.wasm_speculative_inlining) {
+    frame_size -= kSystemPointerSize;
+  }
   DCHECK_EQ(0, frame_size % kSystemPointerSize);
 
   // We can't run out of space when patching, just pass anything big enough to
@@ -2165,7 +2181,15 @@
       case kRefNull:
       case kRtt:
         DCHECK(liftoff_cond == kEqual || liftoff_cond == kUnequal);
-        V8_FALLTHROUGH;
+#if defined(V8_COMPRESS_POINTERS)
+        // It's enough to do a 32-bit comparison. This is also necessary for
+        // null checks which only compare against a 32 bit value, not a full
+        // pointer.
+        cmpl(lhs, rhs);
+#else
+        cmpq(lhs, rhs);
+#endif
+        break;
       case kI64:
         cmpq(lhs, rhs);
         break;
diff -r -u --color up/v8/src/wasm/constant-expression-interface.cc nw/v8/src/wasm/constant-expression-interface.cc
--- up/v8/src/wasm/constant-expression-interface.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/constant-expression-interface.cc	2023-01-19 16:46:36.428109511 -0500
@@ -36,7 +36,7 @@
 }
 
 void ConstantExpressionInterface::S128Const(FullDecoder* decoder,
-                                            Simd128Immediate<validate>& imm,
+                                            Simd128Immediate& imm,
                                             Value* result) {
   if (!generate_value()) return;
   result->runtime_value = WasmValue(imm.value, kWasmS128);
@@ -97,9 +97,8 @@
   result->runtime_value = WasmValue(internal, type);
 }
 
-void ConstantExpressionInterface::GlobalGet(
-    FullDecoder* decoder, Value* result,
-    const GlobalIndexImmediate<validate>& imm) {
+void ConstantExpressionInterface::GlobalGet(FullDecoder* decoder, Value* result,
+                                            const GlobalIndexImmediate& imm) {
   if (!generate_value()) return;
   const WasmGlobal& global = module_->globals[imm.index];
   DCHECK(!global.mutability);
@@ -116,9 +115,10 @@
                 global.type);
 }
 
-void ConstantExpressionInterface::StructNew(
-    FullDecoder* decoder, const StructIndexImmediate<validate>& imm,
-    const Value& rtt, const Value args[], Value* result) {
+void ConstantExpressionInterface::StructNew(FullDecoder* decoder,
+                                            const StructIndexImmediate& imm,
+                                            const Value& rtt,
+                                            const Value args[], Value* result) {
   if (!generate_value()) return;
   std::vector<WasmValue> field_values(imm.struct_type->field_count());
   for (size_t i = 0; i < field_values.size(); i++) {
@@ -131,9 +131,9 @@
                 ValueType::Ref(HeapType(imm.index)));
 }
 
-void ConstantExpressionInterface::StringConst(
-    FullDecoder* decoder, const StringConstImmediate<validate>& imm,
-    Value* result) {
+void ConstantExpressionInterface::StringConst(FullDecoder* decoder,
+                                              const StringConstImmediate& imm,
+                                              Value* result) {
   if (!generate_value()) return;
   static_assert(base::IsInRange(kV8MaxWasmStringLiterals, 0, Smi::kMaxValue));
 
@@ -180,8 +180,8 @@
 }  // namespace
 
 void ConstantExpressionInterface::StructNewDefault(
-    FullDecoder* decoder, const StructIndexImmediate<validate>& imm,
-    const Value& rtt, Value* result) {
+    FullDecoder* decoder, const StructIndexImmediate& imm, const Value& rtt,
+    Value* result) {
   if (!generate_value()) return;
   std::vector<WasmValue> field_values(imm.struct_type->field_count());
   for (uint32_t i = 0; i < field_values.size(); i++) {
@@ -194,10 +194,11 @@
                 ValueType::Ref(imm.index));
 }
 
-void ConstantExpressionInterface::ArrayNew(
-    FullDecoder* decoder, const ArrayIndexImmediate<validate>& imm,
-    const Value& length, const Value& initial_value, const Value& rtt,
-    Value* result) {
+void ConstantExpressionInterface::ArrayNew(FullDecoder* decoder,
+                                           const ArrayIndexImmediate& imm,
+                                           const Value& length,
+                                           const Value& initial_value,
+                                           const Value& rtt, Value* result) {
   if (!generate_value()) return;
   if (length.runtime_value.to_u32() >
       static_cast<uint32_t>(WasmArray::MaxLength(imm.array_type))) {
@@ -213,8 +214,8 @@
 }
 
 void ConstantExpressionInterface::ArrayNewDefault(
-    FullDecoder* decoder, const ArrayIndexImmediate<validate>& imm,
-    const Value& length, const Value& rtt, Value* result) {
+    FullDecoder* decoder, const ArrayIndexImmediate& imm, const Value& length,
+    const Value& rtt, Value* result) {
   if (!generate_value()) return;
   Value initial_value(decoder->pc(), imm.array_type->element_type());
   initial_value.runtime_value =
@@ -223,7 +224,7 @@
 }
 
 void ConstantExpressionInterface::ArrayNewFixed(
-    FullDecoder* decoder, const ArrayIndexImmediate<validate>& imm,
+    FullDecoder* decoder, const ArrayIndexImmediate& imm,
     const base::Vector<Value>& elements, const Value& rtt, Value* result) {
   if (!generate_value()) return;
   std::vector<WasmValue> element_values;
@@ -236,8 +237,8 @@
 }
 
 void ConstantExpressionInterface::ArrayNewSegment(
-    FullDecoder* decoder, const ArrayIndexImmediate<validate>& array_imm,
-    const IndexImmediate<validate>& segment_imm, const Value& offset_value,
+    FullDecoder* decoder, const ArrayIndexImmediate& array_imm,
+    const IndexImmediate& segment_imm, const Value& offset_value,
     const Value& length_value, const Value& rtt, Value* result) {
   if (!generate_value()) return;
 
diff -r -u --color up/v8/src/wasm/constant-expression-interface.h nw/v8/src/wasm/constant-expression-interface.h
--- up/v8/src/wasm/constant-expression-interface.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/constant-expression-interface.h	2023-01-19 16:46:36.428109511 -0500
@@ -30,10 +30,10 @@
 // if {!has_error()}, or with {error()} otherwise.
 class V8_EXPORT_PRIVATE ConstantExpressionInterface {
  public:
-  static constexpr Decoder::ValidateFlag validate = Decoder::kFullValidation;
+  using ValidationTag = Decoder::FullValidationTag;
   static constexpr DecodingMode decoding_mode = kConstantExpression;
 
-  struct Value : public ValueBase<validate> {
+  struct Value : public ValueBase<ValidationTag> {
     WasmValue runtime_value;
 
     template <typename... Args>
@@ -41,9 +41,10 @@
         : ValueBase(std::forward<Args>(args)...) {}
   };
 
-  using Control = ControlBase<Value, validate>;
+  using Control = ControlBase<Value, ValidationTag>;
   using FullDecoder =
-      WasmFullDecoder<validate, ConstantExpressionInterface, decoding_mode>;
+      WasmFullDecoder<ValidationTag, ConstantExpressionInterface,
+                      decoding_mode>;
 
   ConstantExpressionInterface(const WasmModule* module, Isolate* isolate,
                               Handle<WasmInstanceObject> instance)
diff -r -u --color up/v8/src/wasm/constant-expression.cc nw/v8/src/wasm/constant-expression.cc
--- up/v8/src/wasm/constant-expression.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/constant-expression.cc	2023-01-19 16:46:36.428109511 -0500
@@ -56,10 +56,10 @@
       auto sig = FixedSizeSignature<ValueType>::Returns(expected);
       FunctionBody body(&sig, ref.offset(), start, end);
       WasmFeatures detected;
-      // We use kFullValidation so we do not have to create another template
+      // We use FullValidationTag so we do not have to create another template
       // instance of WasmFullDecoder, which would cost us >50Kb binary code
       // size.
-      WasmFullDecoder<Decoder::kFullValidation, ConstantExpressionInterface,
+      WasmFullDecoder<Decoder::FullValidationTag, ConstantExpressionInterface,
                       kConstantExpression>
           decoder(zone, instance->module(), WasmFeatures::All(), &detected,
                   body, instance->module(), isolate, instance);
diff -r -u --color up/v8/src/wasm/decoder.h nw/v8/src/wasm/decoder.h
--- up/v8/src/wasm/decoder.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/decoder.h	2023-01-19 16:46:36.428109511 -0500
@@ -42,12 +42,36 @@
 // a buffer of bytes.
 class Decoder {
  public:
-  // {ValidateFlag} can be used in a boolean manner ({if (!validate) ...}).
-  enum ValidateFlag : int8_t {
-    kNoValidation = 0,   // Don't run validation, assume valid input.
-    kBooleanValidation,  // Run validation but only store a generic error.
-    kFullValidation      // Run full validation with error message and location.
+  // Don't run validation, assume valid input.
+  static constexpr struct NoValidationTag {
+    static constexpr bool validate = false;
+    static constexpr bool full_validation = false;
+  } kNoValidation = {};
+  // Run validation but only store a generic error.
+  static constexpr struct BooleanValidationTag {
+    static constexpr bool validate = true;
+    static constexpr bool full_validation = false;
+  } kBooleanValidation = {};
+  // Run full validation with error message and location.
+  static constexpr struct FullValidationTag {
+    static constexpr bool validate = true;
+    static constexpr bool full_validation = true;
+  } kFullValidation = {};
+
+  struct NoName {
+    constexpr NoName(const char*) {}
+    operator const char*() const { UNREACHABLE(); }
   };
+  // Pass a {NoName} if we know statically that we do not use it anyway (we are
+  // not tracing (in release mode) and not running full validation).
+#ifdef DEBUG
+  template <typename ValidationTag>
+  using Name = const char*;
+#else
+  template <typename ValidationTag>
+  using Name =
+      std::conditional_t<ValidationTag::full_validation, const char*, NoName>;
+#endif
 
   enum TraceFlag : bool { kTrace = true, kNoTrace = false };
 
@@ -66,96 +90,90 @@
 
   virtual ~Decoder() = default;
 
-  // Ensures there are at least {length} bytes left to read, starting at {pc}.
-  bool validate_size(const byte* pc, uint32_t length, const char* msg) {
-    DCHECK_LE(start_, pc);
-    if (V8_UNLIKELY(pc > end_ || length > static_cast<uint32_t>(end_ - pc))) {
-      error(pc, msg);
-      return false;
-    }
-    return true;
-  }
-
   // Reads an 8-bit unsigned integer.
-  template <ValidateFlag validate>
-  uint8_t read_u8(const byte* pc, const char* msg = "expected 1 byte") {
-    return read_little_endian<uint8_t, validate>(pc, msg);
+  template <typename ValidationTag>
+  uint8_t read_u8(const byte* pc, Name<ValidationTag> msg = "expected 1 byte") {
+    return read_little_endian<uint8_t, ValidationTag>(pc, msg);
   }
 
   // Reads a 16-bit unsigned integer (little endian).
-  template <ValidateFlag validate>
-  uint16_t read_u16(const byte* pc, const char* msg = "expected 2 bytes") {
-    return read_little_endian<uint16_t, validate>(pc, msg);
+  template <typename ValidationTag>
+  uint16_t read_u16(const byte* pc,
+                    Name<ValidationTag> msg = "expected 2 bytes") {
+    return read_little_endian<uint16_t, ValidationTag>(pc, msg);
   }
 
   // Reads a 32-bit unsigned integer (little endian).
-  template <ValidateFlag validate>
-  uint32_t read_u32(const byte* pc, const char* msg = "expected 4 bytes") {
-    return read_little_endian<uint32_t, validate>(pc, msg);
+  template <typename ValidationTag>
+  uint32_t read_u32(const byte* pc,
+                    Name<ValidationTag> msg = "expected 4 bytes") {
+    return read_little_endian<uint32_t, ValidationTag>(pc, msg);
   }
 
   // Reads a 64-bit unsigned integer (little endian).
-  template <ValidateFlag validate>
-  uint64_t read_u64(const byte* pc, const char* msg = "expected 8 bytes") {
-    return read_little_endian<uint64_t, validate>(pc, msg);
+  template <typename ValidationTag>
+  uint64_t read_u64(const byte* pc,
+                    Name<ValidationTag> msg = "expected 8 bytes") {
+    return read_little_endian<uint64_t, ValidationTag>(pc, msg);
   }
 
   // Reads a variable-length unsigned integer (little endian).
-  template <ValidateFlag validate>
+  template <typename ValidationTag>
   uint32_t read_u32v(const byte* pc, uint32_t* length,
-                     const char* name = "LEB32") {
-    return read_leb<uint32_t, validate, kNoTrace>(pc, length, name);
+                     Name<ValidationTag> name = "LEB32") {
+    return read_leb<uint32_t, ValidationTag, kNoTrace>(pc, length, name);
   }
 
   // Reads a variable-length signed integer (little endian).
-  template <ValidateFlag validate>
+  template <typename ValidationTag>
   int32_t read_i32v(const byte* pc, uint32_t* length,
-                    const char* name = "signed LEB32") {
-    return read_leb<int32_t, validate, kNoTrace>(pc, length, name);
+                    Name<ValidationTag> name = "signed LEB32") {
+    return read_leb<int32_t, ValidationTag, kNoTrace>(pc, length, name);
   }
 
   // Reads a variable-length unsigned integer (little endian).
-  template <ValidateFlag validate>
+  template <typename ValidationTag>
   uint64_t read_u64v(const byte* pc, uint32_t* length,
-                     const char* name = "LEB64") {
-    return read_leb<uint64_t, validate, kNoTrace>(pc, length, name);
+                     Name<ValidationTag> name = "LEB64") {
+    return read_leb<uint64_t, ValidationTag, kNoTrace>(pc, length, name);
   }
 
   // Reads a variable-length signed integer (little endian).
-  template <ValidateFlag validate>
+  template <typename ValidationTag>
   int64_t read_i64v(const byte* pc, uint32_t* length,
-                    const char* name = "signed LEB64") {
-    return read_leb<int64_t, validate, kNoTrace>(pc, length, name);
+                    Name<ValidationTag> name = "signed LEB64") {
+    return read_leb<int64_t, ValidationTag, kNoTrace>(pc, length, name);
   }
 
   // Reads a variable-length 33-bit signed integer (little endian).
-  template <ValidateFlag validate>
+  template <typename ValidationTag>
   int64_t read_i33v(const byte* pc, uint32_t* length,
-                    const char* name = "signed LEB33") {
-    return read_leb<int64_t, validate, kNoTrace, 33>(pc, length, name);
+                    Name<ValidationTag> name = "signed LEB33") {
+    return read_leb<int64_t, ValidationTag, kNoTrace, 33>(pc, length, name);
   }
 
   // Convenient overload for callers who don't care about length.
-  template <ValidateFlag validate>
+  template <typename ValidationTag>
   WasmOpcode read_prefixed_opcode(const byte* pc) {
     uint32_t len;
-    return read_prefixed_opcode<validate>(pc, &len);
+    return read_prefixed_opcode<ValidationTag>(pc, &len);
   }
 
   // Reads a prefixed-opcode, possibly with variable-length index.
   // `length` is set to the number of bytes that make up this opcode,
   // *including* the prefix byte. For most opcodes, it will be 2.
-  template <ValidateFlag validate>
-  WasmOpcode read_prefixed_opcode(const byte* pc, uint32_t* length,
-                                  const char* name = "prefixed opcode") {
+  template <typename ValidationTag>
+  WasmOpcode read_prefixed_opcode(
+      const byte* pc, uint32_t* length,
+      Name<ValidationTag> name = "prefixed opcode") {
     uint32_t index;
 
     // Prefixed opcodes all use LEB128 encoding.
-    index = read_u32v<validate>(pc + 1, length, "prefixed opcode index");
+    index = read_u32v<ValidationTag>(pc + 1, length, "prefixed opcode index");
     *length += 1;  // Prefix byte.
     // Only support opcodes that go up to 0xFFF (when decoded). Anything
     // bigger will need more than 2 bytes, and the '<< 12' below will be wrong.
-    if (validate && V8_UNLIKELY(index > 0xfff)) {
+    if (ValidationTag::validate && V8_UNLIKELY(index > 0xfff)) {
       errorf(pc, "Invalid prefixed opcode %d", index);
       // If size validation fails.
       index = 0;
@@ -195,7 +213,7 @@
   uint32_t consume_u32v(const char* name = "var_uint32") {
     uint32_t length = 0;
     uint32_t result =
-        read_leb<uint32_t, kFullValidation, kTrace>(pc_, &length, name);
+        read_leb<uint32_t, FullValidationTag, kTrace>(pc_, &length, name);
     pc_ += length;
     return result;
   }
@@ -203,7 +221,7 @@
   uint32_t consume_u32v(const char* name, Tracer& tracer) {
     uint32_t length = 0;
     uint32_t result =
-        read_leb<uint32_t, kFullValidation, kNoTrace>(pc_, &length, name);
+        read_leb<uint32_t, FullValidationTag, kNoTrace>(pc_, &length, name);
     tracer.Bytes(pc_, length);
     tracer.Description(name);
     pc_ += length;
@@ -214,7 +232,7 @@
   int32_t consume_i32v(const char* name = "var_int32") {
     uint32_t length = 0;
     int32_t result =
-        read_leb<int32_t, kFullValidation, kTrace>(pc_, &length, name);
+        read_leb<int32_t, FullValidationTag, kTrace>(pc_, &length, name);
     pc_ += length;
     return result;
   }
@@ -224,7 +242,7 @@
   uint64_t consume_u64v(const char* name, Tracer& tracer) {
     uint32_t length = 0;
     uint64_t result =
-        read_leb<uint64_t, kFullValidation, kNoTrace>(pc_, &length, name);
+        read_leb<uint64_t, FullValidationTag, kNoTrace>(pc_, &length, name);
     tracer.Bytes(pc_, length);
     tracer.Description(name);
     pc_ += length;
@@ -235,7 +253,7 @@
   int64_t consume_i64v(const char* name = "var_int64") {
     uint32_t length = 0;
     int64_t result =
-        read_leb<int64_t, kFullValidation, kTrace>(pc_, &length, name);
+        read_leb<int64_t, FullValidationTag, kTrace>(pc_, &length, name);
     pc_ += length;
     return result;
   }
@@ -257,10 +275,15 @@
     consume_bytes(size, nullptr);
   }
 
+  uint32_t available_bytes() const {
+    DCHECK_LE(pc_, end_);
+    DCHECK_GE(kMaxUInt32, end_ - pc_);
+    return static_cast<uint32_t>(end_ - pc_);
+  }
+
   // Check that at least {size} bytes exist between {pc_} and {end_}.
   bool checkAvailable(uint32_t size) {
-    DCHECK_LE(pc_, end_);
-    if (V8_UNLIKELY(size > static_cast<uint32_t>(end_ - pc_))) {
+    if (V8_UNLIKELY(size > available_bytes())) {
       errorf(pc_, "expected %u bytes, fell off end", size);
       return false;
     }
@@ -401,12 +424,20 @@
     onFirstError();
   }
 
-  template <typename IntType, ValidateFlag validate>
-  IntType read_little_endian(const byte* pc, const char* msg) {
-    if (!validate) {
-      DCHECK(validate_size(pc, sizeof(IntType), msg));
-    } else if (!validate_size(pc, sizeof(IntType), msg)) {
-      return IntType{0};
+  template <typename IntType, typename ValidationTag>
+  IntType read_little_endian(const byte* pc, Name<ValidationTag> msg) {
+    DCHECK_LE(start_, pc);
+
+    if (!ValidationTag::validate) {
+      DCHECK_LE(pc, end_);
+      DCHECK_LE(sizeof(IntType), end_ - pc);
+    } else if (V8_UNLIKELY(ptrdiff_t{sizeof(IntType)} > end_ - pc)) {
+      if (ValidationTag::full_validation) {
+        error(pc, msg);
+      } else {
+        MarkError();
+      }
+      return 0;
     }
     return base::ReadLittleEndianValue<IntType>(reinterpret_cast<Address>(pc));
   }
@@ -419,22 +450,23 @@
       pc_ = end_;
       return IntType{0};
     }
-    IntType val = read_little_endian<IntType, kNoValidation>(pc_, name);
+    IntType val = read_little_endian<IntType, NoValidationTag>(pc_, name);
     traceByteRange(pc_, pc_ + sizeof(IntType));
     TRACE_IF(trace, "= %d\n", val);
     pc_ += sizeof(IntType);
     return val;
   }
 
-  template <typename IntType, ValidateFlag validate, TraceFlag trace,
+  template <typename IntType, typename ValidationTag, TraceFlag trace,
             size_t size_in_bits = 8 * sizeof(IntType)>
   V8_INLINE IntType read_leb(const byte* pc, uint32_t* length,
-                             const char* name = "varint") {
+                             Name<ValidationTag> name = "varint") {
     static_assert(size_in_bits <= 8 * sizeof(IntType),
                   "leb does not fit in type");
-    TRACE_IF(trace, "  +%u  %-20s: ", pc_offset(), name);
+    TRACE_IF(trace, "  +%u  %-20s: ", pc_offset(),
+             implicit_cast<const char*>(name));
     // Fast path for single-byte integers.
-    if ((!validate || V8_LIKELY(pc < end_)) && !(*pc & 0x80)) {
+    if ((!ValidationTag::validate || V8_LIKELY(pc < end_)) && !(*pc & 0x80)) {
       TRACE_IF(trace, "%02x ", *pc);
       *length = 1;
       IntType result = *pc;
@@ -448,29 +480,29 @@
       }
       return result;
     }
-    return read_leb_slowpath<IntType, validate, trace, size_in_bits>(pc, length,
-                                                                     name);
+    return read_leb_slowpath<IntType, ValidationTag, trace, size_in_bits>(
+        pc, length, name);
   }
 
-  template <typename IntType, ValidateFlag validate, TraceFlag trace,
+  template <typename IntType, typename ValidationTag, TraceFlag trace,
             size_t size_in_bits = 8 * sizeof(IntType)>
   V8_NOINLINE IntType read_leb_slowpath(const byte* pc, uint32_t* length,
-                                        const char* name) {
+                                        Name<ValidationTag> name) {
     // Create an unrolled LEB decoding function per integer type.
-    return read_leb_tail<IntType, validate, trace, size_in_bits, 0>(pc, length,
-                                                                    name, 0);
+    return read_leb_tail<IntType, ValidationTag, trace, size_in_bits, 0>(
+        pc, length, name, 0);
   }
 
-  template <typename IntType, ValidateFlag validate, TraceFlag trace,
+  template <typename IntType, typename ValidationTag, TraceFlag trace,
             size_t size_in_bits, int byte_index>
   V8_INLINE IntType read_leb_tail(const byte* pc, uint32_t* length,
-                                  const char* name, IntType result) {
+                                  Name<ValidationTag> name, IntType result) {
     constexpr bool is_signed = std::is_signed<IntType>::value;
     constexpr int kMaxLength = (size_in_bits + 6) / 7;
     static_assert(byte_index < kMaxLength, "invalid template instantiation");
     constexpr int shift = byte_index * 7;
     constexpr bool is_last_byte = byte_index == kMaxLength - 1;
-    const bool at_end = validate && pc >= end_;
+    const bool at_end = ValidationTag::validate && pc >= end_;
     byte b = 0;
     if (V8_LIKELY(!at_end)) {
       DCHECK_LT(pc, end_);
@@ -485,13 +517,13 @@
       // Compilers are not smart enough to figure out statically that the
       // following call is unreachable if is_last_byte is false.
       constexpr int next_byte_index = byte_index + (is_last_byte ? 0 : 1);
-      return read_leb_tail<IntType, validate, trace, size_in_bits,
+      return read_leb_tail<IntType, ValidationTag, trace, size_in_bits,
                            next_byte_index>(pc + 1, length, name, result);
     }
     *length = byte_index + (at_end ? 0 : 1);
-    if (validate && V8_UNLIKELY(at_end || (b & 0x80))) {
+    if (ValidationTag::validate && V8_UNLIKELY(at_end || (b & 0x80))) {
       TRACE_IF(trace, at_end ? "<end> " : "<length overflow> ");
-      if (validate == kFullValidation) {
+      if constexpr (ValidationTag::full_validation) {
         errorf(pc, "expected %s", name);
       } else {
         MarkError();
@@ -499,7 +531,7 @@
       result = 0;
       *length = 0;
     }
-    if (is_last_byte) {
+    if constexpr (is_last_byte) {
       // A signed-LEB128 must sign-extend the final byte, excluding its
       // most-significant bit; e.g. for a 32-bit LEB128:
       //   kExtraBits = 4  (== 32 - (5-1) * 7)
@@ -513,10 +545,10 @@
       const bool valid_extra_bits =
           checked_bits == 0 ||
           (is_signed && checked_bits == kSignExtendedExtraBits);
-      if (!validate) {
+      if (!ValidationTag::validate) {
         DCHECK(valid_extra_bits);
       } else if (V8_UNLIKELY(!valid_extra_bits)) {
-        if (validate == kFullValidation) {
+        if (ValidationTag::full_validation) {
           error(pc, "extra bits in varint");
         } else {
           MarkError();
diff -r -u --color up/v8/src/wasm/function-body-decoder-impl.h nw/v8/src/wasm/function-body-decoder-impl.h
--- up/v8/src/wasm/function-body-decoder-impl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/function-body-decoder-impl.h	2023-01-19 16:46:36.428109511 -0500
@@ -44,12 +44,12 @@
 
 #define TRACE_INST_FORMAT "  @%-8d #%-30s|"
 
-// Return the evaluation of `condition` if validate==true, DCHECK that it's
-// true and always return true otherwise.
-#define VALIDATE(condition)                \
-  (validate ? V8_LIKELY(condition) : [&] { \
-    DCHECK(condition);                     \
-    return true;                           \
+// Return the evaluation of {condition} if {ValidationTag::validate} is true,
+// DCHECK that it is true and always return true otherwise.
+#define VALIDATE(condition)                               \
+  (ValidationTag::validate ? V8_LIKELY(condition) : [&] { \
+    DCHECK(condition);                                    \
+    return true;                                          \
   }())
 
 #define CHECK_PROTOTYPE_OPCODE(feat)                                         \
@@ -167,53 +167,49 @@
   V(I64AtomicStore32U, Uint32)
 
 // Decoder error with explicit PC and format arguments.
-template <Decoder::ValidateFlag validate, typename... Args>
+template <typename ValidationTag, typename... Args>
 void DecodeError(Decoder* decoder, const byte* pc, const char* str,
                  Args&&... args) {
-  CHECK(validate == Decoder::kFullValidation ||
-        validate == Decoder::kBooleanValidation);
+  if constexpr (!ValidationTag::validate) UNREACHABLE();
   static_assert(sizeof...(Args) > 0);
-  if (validate == Decoder::kBooleanValidation) {
-    decoder->MarkError();
-  } else {
+  if constexpr (ValidationTag::full_validation) {
     decoder->errorf(pc, str, std::forward<Args>(args)...);
+  } else {
+    decoder->MarkError();
   }
 }
 
 // Decoder error with explicit PC and no format arguments.
-template <Decoder::ValidateFlag validate>
+template <typename ValidationTag>
 void DecodeError(Decoder* decoder, const byte* pc, const char* str) {
-  CHECK(validate == Decoder::kFullValidation ||
-        validate == Decoder::kBooleanValidation);
-  if (validate == Decoder::kBooleanValidation) {
-    decoder->MarkError();
-  } else {
+  if constexpr (!ValidationTag::validate) UNREACHABLE();
+  if constexpr (ValidationTag::full_validation) {
     decoder->error(pc, str);
+  } else {
+    decoder->MarkError();
   }
 }
 
 // Decoder error without explicit PC, but with format arguments.
-template <Decoder::ValidateFlag validate, typename... Args>
+template <typename ValidationTag, typename... Args>
 void DecodeError(Decoder* decoder, const char* str, Args&&... args) {
-  CHECK(validate == Decoder::kFullValidation ||
-        validate == Decoder::kBooleanValidation);
+  if constexpr (!ValidationTag::validate) UNREACHABLE();
   static_assert(sizeof...(Args) > 0);
-  if (validate == Decoder::kBooleanValidation) {
-    decoder->MarkError();
-  } else {
+  if constexpr (ValidationTag::full_validation) {
     decoder->errorf(str, std::forward<Args>(args)...);
+  } else {
+    decoder->MarkError();
   }
 }
 
 // Decoder error without explicit PC and without format arguments.
-template <Decoder::ValidateFlag validate>
+template <typename ValidationTag>
 void DecodeError(Decoder* decoder, const char* str) {
-  CHECK(validate == Decoder::kFullValidation ||
-        validate == Decoder::kBooleanValidation);
-  if (validate == Decoder::kBooleanValidation) {
-    decoder->MarkError();
-  } else {
+  if constexpr (!ValidationTag::validate) UNREACHABLE();
+  if constexpr (ValidationTag::full_validation) {
     decoder->error(str);
+  } else {
+    decoder->MarkError();
   }
 }
 
@@ -221,16 +217,16 @@
 
 // If {module} is not null, the read index will be checked against the module's
 // type capacity.
-template <Decoder::ValidateFlag validate>
+template <typename ValidationTag>
 HeapType read_heap_type(Decoder* decoder, const byte* pc,
-                        uint32_t* const length, const WasmModule* module,
-                        const WasmFeatures& enabled) {
-  int64_t heap_index = decoder->read_i33v<validate>(pc, length, "heap type");
+                        uint32_t* const length, const WasmFeatures& enabled) {
+  int64_t heap_index =
+      decoder->read_i33v<ValidationTag>(pc, length, "heap type");
   if (heap_index < 0) {
     int64_t min_1_byte_leb128 = -64;
     if (!VALIDATE(heap_index >= min_1_byte_leb128)) {
-      DecodeError<validate>(decoder, pc, "Unknown heap type %" PRId64,
-                            heap_index);
+      DecodeError<ValidationTag>(decoder, pc, "Unknown heap type %" PRId64,
+                                 heap_index);
       return HeapType(HeapType::kBottom);
     }
     uint8_t uint_7_mask = 0x7F;
@@ -238,14 +234,14 @@
     switch (code) {
       case kEqRefCode:
       case kI31RefCode:
-      case kDataRefCode:
+      case kStructRefCode:
       case kArrayRefCode:
       case kAnyRefCode:
       case kNoneCode:
       case kNoExternCode:
       case kNoFuncCode:
         if (!VALIDATE(enabled.has_gc())) {
-          DecodeError<validate>(
+          DecodeError<ValidationTag>(
               decoder, pc,
               "invalid heap type '%s', enable with --experimental-wasm-gc",
               HeapType::from_code(code).name().c_str());
@@ -259,37 +255,32 @@
       case kStringViewWtf16Code:
       case kStringViewIterCode:
         if (!VALIDATE(enabled.has_stringref())) {
-          DecodeError<validate>(decoder, pc,
-                                "invalid heap type '%s', enable with "
-                                "--experimental-wasm-stringref",
-                                HeapType::from_code(code).name().c_str());
+          DecodeError<ValidationTag>(decoder, pc,
+                                     "invalid heap type '%s', enable with "
+                                     "--experimental-wasm-stringref",
+                                     HeapType::from_code(code).name().c_str());
         }
         return HeapType::from_code(code);
       default:
-        DecodeError<validate>(decoder, pc, "Unknown heap type %" PRId64,
-                              heap_index);
+        DecodeError<ValidationTag>(decoder, pc, "Unknown heap type %" PRId64,
+                                   heap_index);
         return HeapType(HeapType::kBottom);
     }
   } else {
     if (!VALIDATE(enabled.has_typed_funcref())) {
-      DecodeError<validate>(decoder, pc,
-                            "Invalid indexed heap type, enable with "
-                            "--experimental-wasm-typed-funcref");
+      DecodeError<ValidationTag>(decoder, pc,
+                                 "Invalid indexed heap type, enable with "
+                                 "--experimental-wasm-typed-funcref");
     }
     uint32_t type_index = static_cast<uint32_t>(heap_index);
     if (!VALIDATE(type_index < kV8MaxWasmTypes)) {
-      DecodeError<validate>(
+      DecodeError<ValidationTag>(
           decoder, pc,
           "Type index %u is greater than the maximum number %zu "
           "of type definitions supported by V8",
           type_index, kV8MaxWasmTypes);
       return HeapType(HeapType::kBottom);
     }
-    // We use capacity over size so this works mid-DecodeTypeSection.
-    if (!VALIDATE(module == nullptr || type_index < module->types.capacity())) {
-      DecodeError<validate>(decoder, pc, "Type index %u is out of bounds",
-                            type_index);
-    }
     return HeapType(type_index);
   }
 }
@@ -297,15 +288,14 @@
 // Read a value type starting at address {pc} using {decoder}.
 // No bytes are consumed.
 // The length of the read value type is written in {length}.
-// Registers an error for an invalid type only if {validate} is not
-// kNoValidate.
-template <Decoder::ValidateFlag validate>
+// Registers an error for an invalid type only if {ValidationTag::validate} is
+// true.
+template <typename ValidationTag>
 ValueType read_value_type(Decoder* decoder, const byte* pc,
-                          uint32_t* const length, const WasmModule* module,
-                          const WasmFeatures& enabled) {
+                          uint32_t* const length, const WasmFeatures& enabled) {
   *length = 1;
-  byte val = decoder->read_u8<validate>(pc, "value type opcode");
-  if (decoder->failed()) {
+  byte val = decoder->read_u8<ValidationTag>(pc, "value type opcode");
+  if (!VALIDATE(decoder->ok())) {
     *length = 0;
     return kWasmBottom;
   }
@@ -313,14 +303,14 @@
   switch (code) {
     case kEqRefCode:
     case kI31RefCode:
-    case kDataRefCode:
+    case kStructRefCode:
     case kArrayRefCode:
     case kAnyRefCode:
     case kNoneCode:
     case kNoExternCode:
     case kNoFuncCode:
       if (!VALIDATE(enabled.has_gc())) {
-        DecodeError<validate>(
+        DecodeError<ValidationTag>(
             decoder, pc,
             "invalid value type '%sref', enable with --experimental-wasm-gc",
             HeapType::from_code(code).name().c_str());
@@ -335,10 +325,10 @@
     case kStringViewWtf16Code:
     case kStringViewIterCode: {
       if (!VALIDATE(enabled.has_stringref())) {
-        DecodeError<validate>(decoder, pc,
-                              "invalid value type '%sref', enable with "
-                              "--experimental-wasm-stringref",
-                              HeapType::from_code(code).name().c_str());
+        DecodeError<ValidationTag>(decoder, pc,
+                                   "invalid value type '%sref', enable with "
+                                   "--experimental-wasm-stringref",
+                                   HeapType::from_code(code).name().c_str());
         return kWasmBottom;
       }
       return ValueType::RefNull(HeapType::from_code(code));
@@ -355,28 +345,23 @@
     case kRefNullCode: {
       Nullability nullability = code == kRefNullCode ? kNullable : kNonNullable;
       if (!VALIDATE(enabled.has_typed_funcref())) {
-        DecodeError<validate>(decoder, pc,
-                              "Invalid type '(ref%s <heaptype>)', enable with "
-                              "--experimental-wasm-typed-funcref",
-                              nullability == kNullable ? " null" : "");
+        DecodeError<ValidationTag>(
+            decoder, pc,
+            "Invalid type '(ref%s <heaptype>)', enable with "
+            "--experimental-wasm-typed-funcref",
+            nullability == kNullable ? " null" : "");
         return kWasmBottom;
       }
       HeapType heap_type =
-          read_heap_type<validate>(decoder, pc + 1, length, module, enabled);
+          read_heap_type<ValidationTag>(decoder, pc + 1, length, enabled);
       *length += 1;
       return heap_type.is_bottom()
                  ? kWasmBottom
                  : ValueType::RefMaybeNull(heap_type, nullability);
     }
     case kS128Code: {
-      if (!VALIDATE(enabled.has_simd())) {
-        DecodeError<validate>(
-            decoder, pc,
-            "invalid value type 's128', enable with --experimental-wasm-simd");
-        return kWasmBottom;
-      }
       if (!VALIDATE(CheckHardwareSupportsSimd())) {
-        DecodeError<validate>(decoder, pc, "Wasm SIMD unsupported");
+        DecodeError<ValidationTag>(decoder, pc, "Wasm SIMD unsupported");
         return kWasmBottom;
       }
       return kWasmS128;
@@ -387,179 +372,216 @@
     case kVoidCode:
     case kI8Code:
     case kI16Code:
-      if (validate) {
-        DecodeError<validate>(decoder, pc, "invalid value type 0x%x", code);
+      if (ValidationTag::validate) {
+        DecodeError<ValidationTag>(decoder, pc, "invalid value type 0x%x",
+                                   code);
       }
       return kWasmBottom;
   }
   // Anything that doesn't match an enumeration value is an invalid type code.
-  if (validate) {
-    DecodeError<validate>(decoder, pc, "invalid value type 0x%x", code);
+  if (ValidationTag::validate) {
+    DecodeError<ValidationTag>(decoder, pc, "invalid value type 0x%x", code);
   }
   return kWasmBottom;
 }
+
+template <typename ValidationTag>
+bool ValidateHeapType(Decoder* decoder, const byte* pc,
+                      const WasmModule* module, HeapType type) {
+  if (!type.is_index()) return true;
+  // A {nullptr} module is accepted if we are not validating anyway (e.g. for
+  // opcode length computation).
+  if (!ValidationTag::validate && module == nullptr) return true;
+  // We use capacity over size so this works mid-DecodeTypeSection.
+  if (!VALIDATE(type.ref_index() < module->types.capacity())) {
+    DecodeError<ValidationTag>(decoder, pc, "Type index %u is out of bounds",
+                               type.ref_index());
+    return false;
+  }
+  return true;
+}
+
+template <typename ValidationTag>
+bool ValidateValueType(Decoder* decoder, const byte* pc,
+                       const WasmModule* module, ValueType type) {
+  if (V8_LIKELY(!type.is_object_reference())) return true;
+  return ValidateHeapType<ValidationTag>(decoder, pc, module, type.heap_type());
+}
+
 }  // namespace value_type_reader
 
 enum DecodingMode { kFunctionBody, kConstantExpression };
 
 // Helpers for decoding different kinds of immediates which follow bytecodes.
-template <Decoder::ValidateFlag validate>
 struct ImmI32Immediate {
   int32_t value;
   uint32_t length;
-  ImmI32Immediate(Decoder* decoder, const byte* pc) {
-    value = decoder->read_i32v<validate>(pc, &length, "immi32");
+
+  template <typename ValidationTag>
+  ImmI32Immediate(Decoder* decoder, const byte* pc, ValidationTag = {}) {
+    value = decoder->read_i32v<ValidationTag>(pc, &length, "immi32");
   }
 };
 
-template <Decoder::ValidateFlag validate>
 struct ImmI64Immediate {
   int64_t value;
   uint32_t length;
-  ImmI64Immediate(Decoder* decoder, const byte* pc) {
-    value = decoder->read_i64v<validate>(pc, &length, "immi64");
+
+  template <typename ValidationTag>
+  ImmI64Immediate(Decoder* decoder, const byte* pc, ValidationTag = {}) {
+    value = decoder->read_i64v<ValidationTag>(pc, &length, "immi64");
   }
 };
 
-template <Decoder::ValidateFlag validate>
 struct ImmF32Immediate {
   float value;
   uint32_t length = 4;
-  ImmF32Immediate(Decoder* decoder, const byte* pc) {
+
+  template <typename ValidationTag>
+  ImmF32Immediate(Decoder* decoder, const byte* pc, ValidationTag = {}) {
     // We can't use base::bit_cast here because calling any helper function
     // that returns a float would potentially flip NaN bits per C++ semantics,
     // so we have to inline the memcpy call directly.
-    uint32_t tmp = decoder->read_u32<validate>(pc, "immf32");
+    uint32_t tmp = decoder->read_u32<ValidationTag>(pc, "immf32");
     memcpy(&value, &tmp, sizeof(value));
   }
 };
 
-template <Decoder::ValidateFlag validate>
 struct ImmF64Immediate {
   double value;
   uint32_t length = 8;
-  ImmF64Immediate(Decoder* decoder, const byte* pc) {
+
+  template <typename ValidationTag>
+  ImmF64Immediate(Decoder* decoder, const byte* pc, ValidationTag = {}) {
     // Avoid base::bit_cast because it might not preserve the signalling bit
     // of a NaN.
-    uint64_t tmp = decoder->read_u64<validate>(pc, "immf64");
+    uint64_t tmp = decoder->read_u64<ValidationTag>(pc, "immf64");
     memcpy(&value, &tmp, sizeof(value));
   }
 };
 
-// This is different than IndexImmediate because {index} is a byte.
-template <Decoder::ValidateFlag validate>
 struct MemoryIndexImmediate {
   uint8_t index = 0;
   uint32_t length = 1;
-  MemoryIndexImmediate(Decoder* decoder, const byte* pc) {
-    index = decoder->read_u8<validate>(pc, "memory index");
+
+  template <typename ValidationTag>
+  MemoryIndexImmediate(Decoder* decoder, const byte* pc, ValidationTag = {}) {
+    index = decoder->read_u8<ValidationTag>(pc, "memory index");
   }
 };
 
 // Parent class for all Immediates which read a u32v index value in their
 // constructor.
-template <Decoder::ValidateFlag validate>
 struct IndexImmediate {
   uint32_t index;
   uint32_t length;
 
-  IndexImmediate(Decoder* decoder, const byte* pc, const char* name) {
-    index = decoder->read_u32v<validate>(pc, &length, name);
+  template <typename ValidationTag>
+  IndexImmediate(Decoder* decoder, const byte* pc, const char* name,
+                 ValidationTag = {}) {
+    index = decoder->read_u32v<ValidationTag>(pc, &length, name);
   }
 };
 
-template <Decoder::ValidateFlag validate>
-struct TagIndexImmediate : public IndexImmediate<validate> {
+struct TagIndexImmediate : public IndexImmediate {
   const WasmTag* tag = nullptr;
 
-  TagIndexImmediate(Decoder* decoder, const byte* pc)
-      : IndexImmediate<validate>(decoder, pc, "tag index") {}
+  template <typename ValidationTag>
+  TagIndexImmediate(Decoder* decoder, const byte* pc,
+                    ValidationTag validate = {})
+      : IndexImmediate(decoder, pc, "tag index", validate) {}
 };
 
-template <Decoder::ValidateFlag validate>
-struct GlobalIndexImmediate : public IndexImmediate<validate> {
+struct GlobalIndexImmediate : public IndexImmediate {
   const WasmGlobal* global = nullptr;
 
-  GlobalIndexImmediate(Decoder* decoder, const byte* pc)
-      : IndexImmediate<validate>(decoder, pc, "global index") {}
+  template <typename ValidationTag>
+  GlobalIndexImmediate(Decoder* decoder, const byte* pc,
+                       ValidationTag validate = {})
+      : IndexImmediate(decoder, pc, "global index", validate) {}
 };
 
-template <Decoder::ValidateFlag validate>
-struct SigIndexImmediate : public IndexImmediate<validate> {
+struct SigIndexImmediate : public IndexImmediate {
   const FunctionSig* sig = nullptr;
 
-  SigIndexImmediate(Decoder* decoder, const byte* pc)
-      : IndexImmediate<validate>(decoder, pc, "signature index") {}
+  template <typename ValidationTag>
+  SigIndexImmediate(Decoder* decoder, const byte* pc,
+                    ValidationTag validate = {})
+      : IndexImmediate(decoder, pc, "signature index", validate) {}
 };
 
-template <Decoder::ValidateFlag validate>
-struct StructIndexImmediate : public IndexImmediate<validate> {
+struct StructIndexImmediate : public IndexImmediate {
   const StructType* struct_type = nullptr;
 
-  StructIndexImmediate(Decoder* decoder, const byte* pc)
-      : IndexImmediate<validate>(decoder, pc, "struct index") {}
+  template <typename ValidationTag>
+  StructIndexImmediate(Decoder* decoder, const byte* pc,
+                       ValidationTag validate = {})
+      : IndexImmediate(decoder, pc, "struct index", validate) {}
 };
 
-template <Decoder::ValidateFlag validate>
-struct ArrayIndexImmediate : public IndexImmediate<validate> {
+struct ArrayIndexImmediate : public IndexImmediate {
   const ArrayType* array_type = nullptr;
 
-  ArrayIndexImmediate(Decoder* decoder, const byte* pc)
-      : IndexImmediate<validate>(decoder, pc, "array index") {}
+  template <typename ValidationTag>
+  ArrayIndexImmediate(Decoder* decoder, const byte* pc,
+                      ValidationTag validate = {})
+      : IndexImmediate(decoder, pc, "array index", validate) {}
 };
-template <Decoder::ValidateFlag validate>
-struct CallFunctionImmediate : public IndexImmediate<validate> {
+
+struct CallFunctionImmediate : public IndexImmediate {
   const FunctionSig* sig = nullptr;
 
-  CallFunctionImmediate(Decoder* decoder, const byte* pc)
-      : IndexImmediate<validate>(decoder, pc, "function index") {}
+  template <typename ValidationTag>
+  CallFunctionImmediate(Decoder* decoder, const byte* pc,
+                        ValidationTag validate = {})
+      : IndexImmediate(decoder, pc, "function index", validate) {}
 };
 
-template <Decoder::ValidateFlag validate>
 struct SelectTypeImmediate {
   uint32_t length;
   ValueType type;
 
+  template <typename ValidationTag>
   SelectTypeImmediate(const WasmFeatures& enabled, Decoder* decoder,
-                      const byte* pc, const WasmModule* module) {
-    uint8_t num_types =
-        decoder->read_u32v<validate>(pc, &length, "number of select types");
+                      const byte* pc, ValidationTag = {}) {
+    uint8_t num_types = decoder->read_u32v<ValidationTag>(
+        pc, &length, "number of select types");
     if (!VALIDATE(num_types == 1)) {
-      DecodeError<validate>(
+      DecodeError<ValidationTag>(
           decoder, pc,
           "Invalid number of types. Select accepts exactly one type");
       return;
     }
     uint32_t type_length;
-    type = value_type_reader::read_value_type<validate>(
-        decoder, pc + length, &type_length, module, enabled);
+    type = value_type_reader::read_value_type<ValidationTag>(
+        decoder, pc + length, &type_length, enabled);
     length += type_length;
   }
 };
 
-template <Decoder::ValidateFlag validate>
 struct BlockTypeImmediate {
   uint32_t length = 1;
   ValueType type = kWasmVoid;
   uint32_t sig_index = 0;
   const FunctionSig* sig = nullptr;
 
+  template <typename ValidationTag>
   BlockTypeImmediate(const WasmFeatures& enabled, Decoder* decoder,
-                     const byte* pc, const WasmModule* module) {
+                     const byte* pc, ValidationTag = {}) {
     int64_t block_type =
-        decoder->read_i33v<validate>(pc, &length, "block type");
+        decoder->read_i33v<ValidationTag>(pc, &length, "block type");
     if (block_type < 0) {
       // All valid negative types are 1 byte in length, so we check against the
       // minimum 1-byte LEB128 value.
       constexpr int64_t min_1_byte_leb128 = -64;
       if (!VALIDATE(block_type >= min_1_byte_leb128)) {
-        DecodeError<validate>(decoder, pc, "invalid block type %" PRId64,
-                              block_type);
+        DecodeError<ValidationTag>(decoder, pc, "invalid block type %" PRId64,
+                                   block_type);
         return;
       }
       if (static_cast<ValueTypeCode>(block_type & 0x7F) == kVoidCode) return;
-      type = value_type_reader::read_value_type<validate>(decoder, pc, &length,
-                                                          module, enabled);
+      type = value_type_reader::read_value_type<ValidationTag>(
+          decoder, pc, &length, enabled);
     } else {
       type = kWasmBottom;
       sig_index = static_cast<uint32_t>(block_type);
@@ -587,53 +609,58 @@
   }
 };
 
-template <Decoder::ValidateFlag validate>
 struct BranchDepthImmediate {
   uint32_t depth;
   uint32_t length;
-  BranchDepthImmediate(Decoder* decoder, const byte* pc) {
-    depth = decoder->read_u32v<validate>(pc, &length, "branch depth");
+
+  template <typename ValidationTag>
+  BranchDepthImmediate(Decoder* decoder, const byte* pc, ValidationTag = {}) {
+    depth = decoder->read_u32v<ValidationTag>(pc, &length, "branch depth");
   }
 };
 
-template <Decoder::ValidateFlag validate>
 struct FieldImmediate {
-  StructIndexImmediate<validate> struct_imm;
-  IndexImmediate<validate> field_imm;
+  StructIndexImmediate struct_imm;
+  IndexImmediate field_imm;
   uint32_t length;
-  FieldImmediate(Decoder* decoder, const byte* pc)
-      : struct_imm(decoder, pc),
-        field_imm(decoder, pc + struct_imm.length, "field index"),
+
+  template <typename ValidationTag>
+  FieldImmediate(Decoder* decoder, const byte* pc, ValidationTag validate = {})
+      : struct_imm(decoder, pc, validate),
+        field_imm(decoder, pc + struct_imm.length, "field index", validate),
         length(struct_imm.length + field_imm.length) {}
 };
 
-template <Decoder::ValidateFlag validate>
 struct CallIndirectImmediate {
-  IndexImmediate<validate> sig_imm;
-  IndexImmediate<validate> table_imm;
+  IndexImmediate sig_imm;
+  IndexImmediate table_imm;
   uint32_t length;
   const FunctionSig* sig = nullptr;
-  CallIndirectImmediate(Decoder* decoder, const byte* pc)
-      : sig_imm(decoder, pc, "singature index"),
-        table_imm(decoder, pc + sig_imm.length, "table index"),
+
+  template <typename ValidationTag>
+  CallIndirectImmediate(Decoder* decoder, const byte* pc,
+                        ValidationTag validate = {})
+      : sig_imm(decoder, pc, "singature index", validate),
+        table_imm(decoder, pc + sig_imm.length, "table index", validate),
         length(sig_imm.length + table_imm.length) {}
 };
 
-template <Decoder::ValidateFlag validate>
 struct BranchTableImmediate {
   uint32_t table_count;
   const byte* start;
   const byte* table;
-  BranchTableImmediate(Decoder* decoder, const byte* pc) {
+
+  template <typename ValidationTag>
+  BranchTableImmediate(Decoder* decoder, const byte* pc, ValidationTag = {}) {
     start = pc;
     uint32_t len = 0;
-    table_count = decoder->read_u32v<validate>(pc, &len, "table count");
+    table_count = decoder->read_u32v<ValidationTag>(pc, &len, "table count");
     table = pc + len;
   }
 };
 
 // A helper to iterate over a branch table.
-template <Decoder::ValidateFlag validate>
+template <typename ValidationTag>
 class BranchTableIterator {
  public:
   uint32_t cur_index() { return index_; }
@@ -643,7 +670,7 @@
     index_++;
     uint32_t length;
     uint32_t result =
-        decoder_->read_u32v<validate>(pc_, &length, "branch table entry");
+        decoder_->read_u32v<ValidationTag>(pc_, &length, "branch table entry");
     pc_ += length;
     return result;
   }
@@ -655,8 +682,7 @@
   }
   const byte* pc() { return pc_; }
 
-  BranchTableIterator(Decoder* decoder,
-                      const BranchTableImmediate<validate>& imm)
+  BranchTableIterator(Decoder* decoder, const BranchTableImmediate& imm)
       : decoder_(decoder),
         start_(imm.start),
         pc_(imm.table),
@@ -670,136 +696,141 @@
   const uint32_t table_count_;  // the count of entries, not including default.
 };
 
-template <Decoder::ValidateFlag validate,
-          DecodingMode decoding_mode = kFunctionBody>
-class WasmDecoder;
-
-template <Decoder::ValidateFlag validate>
 struct MemoryAccessImmediate {
   uint32_t alignment;
   uint64_t offset;
   uint32_t length = 0;
+
+  template <typename ValidationTag>
   MemoryAccessImmediate(Decoder* decoder, const byte* pc,
-                        uint32_t max_alignment, bool is_memory64) {
+                        uint32_t max_alignment, bool is_memory64,
+                        ValidationTag = {}) {
     uint32_t alignment_length;
     alignment =
-        decoder->read_u32v<validate>(pc, &alignment_length, "alignment");
+        decoder->read_u32v<ValidationTag>(pc, &alignment_length, "alignment");
     if (!VALIDATE(alignment <= max_alignment)) {
-      DecodeError<validate>(
+      DecodeError<ValidationTag>(
           decoder, pc,
           "invalid alignment; expected maximum alignment is %u, "
           "actual alignment is %u",
           max_alignment, alignment);
     }
     uint32_t offset_length;
-    offset = is_memory64 ? decoder->read_u64v<validate>(
+    offset = is_memory64 ? decoder->read_u64v<ValidationTag>(
                                pc + alignment_length, &offset_length, "offset")
-                         : decoder->read_u32v<validate>(
+                         : decoder->read_u32v<ValidationTag>(
                                pc + alignment_length, &offset_length, "offset");
     length = alignment_length + offset_length;
   }
 };
 
 // Immediate for SIMD lane operations.
-template <Decoder::ValidateFlag validate>
 struct SimdLaneImmediate {
   uint8_t lane;
   uint32_t length = 1;
 
-  SimdLaneImmediate(Decoder* decoder, const byte* pc) {
-    lane = decoder->read_u8<validate>(pc, "lane");
+  template <typename ValidationTag>
+  SimdLaneImmediate(Decoder* decoder, const byte* pc, ValidationTag = {}) {
+    lane = decoder->read_u8<ValidationTag>(pc, "lane");
   }
 };
 
 // Immediate for SIMD S8x16 shuffle operations.
-template <Decoder::ValidateFlag validate>
 struct Simd128Immediate {
   uint8_t value[kSimd128Size] = {0};
 
-  Simd128Immediate(Decoder* decoder, const byte* pc) {
+  template <typename ValidationTag>
+  Simd128Immediate(Decoder* decoder, const byte* pc, ValidationTag = {}) {
     for (uint32_t i = 0; i < kSimd128Size; ++i) {
-      value[i] = decoder->read_u8<validate>(pc + i, "value");
+      value[i] = decoder->read_u8<ValidationTag>(pc + i, "value");
     }
   }
 };
 
-template <Decoder::ValidateFlag validate>
 struct MemoryInitImmediate {
-  IndexImmediate<validate> data_segment;
-  MemoryIndexImmediate<validate> memory;
+  IndexImmediate data_segment;
+  MemoryIndexImmediate memory;
   uint32_t length;
 
-  MemoryInitImmediate(Decoder* decoder, const byte* pc)
-      : data_segment(decoder, pc, "data segment index"),
-        memory(decoder, pc + data_segment.length),
+  template <typename ValidationTag>
+  MemoryInitImmediate(Decoder* decoder, const byte* pc,
+                      ValidationTag validate = {})
+      : data_segment(decoder, pc, "data segment index", validate),
+        memory(decoder, pc + data_segment.length, validate),
         length(data_segment.length + memory.length) {}
 };
 
-template <Decoder::ValidateFlag validate>
 struct MemoryCopyImmediate {
-  MemoryIndexImmediate<validate> memory_src;
-  MemoryIndexImmediate<validate> memory_dst;
+  MemoryIndexImmediate memory_src;
+  MemoryIndexImmediate memory_dst;
   uint32_t length;
 
-  MemoryCopyImmediate(Decoder* decoder, const byte* pc)
-      : memory_src(decoder, pc),
-        memory_dst(decoder, pc + memory_src.length),
+  template <typename ValidationTag>
+  MemoryCopyImmediate(Decoder* decoder, const byte* pc,
+                      ValidationTag validate = {})
+      : memory_src(decoder, pc, validate),
+        memory_dst(decoder, pc + memory_src.length, validate),
         length(memory_src.length + memory_dst.length) {}
 };
 
-template <Decoder::ValidateFlag validate>
 struct TableInitImmediate {
-  IndexImmediate<validate> element_segment;
-  IndexImmediate<validate> table;
+  IndexImmediate element_segment;
+  IndexImmediate table;
   uint32_t length;
 
-  TableInitImmediate(Decoder* decoder, const byte* pc)
-      : element_segment(decoder, pc, "element segment index"),
-        table(decoder, pc + element_segment.length, "table index"),
+  template <typename ValidationTag>
+  TableInitImmediate(Decoder* decoder, const byte* pc,
+                     ValidationTag validate = {})
+      : element_segment(decoder, pc, "element segment index", validate),
+        table(decoder, pc + element_segment.length, "table index", validate),
         length(element_segment.length + table.length) {}
 };
 
-template <Decoder::ValidateFlag validate>
 struct TableCopyImmediate {
-  IndexImmediate<validate> table_dst;
-  IndexImmediate<validate> table_src;
+  IndexImmediate table_dst;
+  IndexImmediate table_src;
   uint32_t length;
 
-  TableCopyImmediate(Decoder* decoder, const byte* pc)
-      : table_dst(decoder, pc, "table index"),
-        table_src(decoder, pc + table_dst.length, "table index"),
+  template <typename ValidationTag>
+  TableCopyImmediate(Decoder* decoder, const byte* pc,
+                     ValidationTag validate = {})
+      : table_dst(decoder, pc, "table index", validate),
+        table_src(decoder, pc + table_dst.length, "table index", validate),
         length(table_src.length + table_dst.length) {}
 };
 
-template <Decoder::ValidateFlag validate>
 struct HeapTypeImmediate {
   uint32_t length = 1;
   HeapType type;
+
+  template <typename ValidationTag>
   HeapTypeImmediate(const WasmFeatures& enabled, Decoder* decoder,
-                    const byte* pc, const WasmModule* module)
-      : type(value_type_reader::read_heap_type<validate>(decoder, pc, &length,
-                                                         module, enabled)) {}
+                    const byte* pc, ValidationTag = {})
+      : type(value_type_reader::read_heap_type<ValidationTag>(
+            decoder, pc, &length, enabled)) {}
 };
 
-template <Decoder::ValidateFlag validate>
 struct StringConstImmediate {
   uint32_t index;
   uint32_t length;
-  StringConstImmediate(Decoder* decoder, const byte* pc) {
-    index =
-        decoder->read_u32v<validate>(pc, &length, "stringref literal index");
+
+  template <typename ValidationTag>
+  StringConstImmediate(Decoder* decoder, const byte* pc, ValidationTag = {}) {
+    index = decoder->read_u32v<ValidationTag>(pc, &length,
+                                              "stringref literal index");
   }
 };
 
-template <Decoder::ValidateFlag validate>
+template <bool full_validation>
 struct PcForErrors {
+  static_assert(full_validation == false);
   explicit PcForErrors(const byte* /* pc */) {}
 
   const byte* pc() const { return nullptr; }
 };
 
 template <>
-struct PcForErrors<Decoder::kFullValidation> {
+struct PcForErrors<true> {
   const byte* pc_for_errors = nullptr;
 
   explicit PcForErrors(const byte* pc) : pc_for_errors(pc) {}
@@ -808,12 +839,12 @@
 };
 
 // An entry on the value stack.
-template <Decoder::ValidateFlag validate>
-struct ValueBase : public PcForErrors<validate> {
+template <typename ValidationTag>
+struct ValueBase : public PcForErrors<ValidationTag::full_validation> {
   ValueType type = kWasmVoid;
 
   ValueBase(const byte* pc, ValueType type)
-      : PcForErrors<validate>(pc), type(type) {}
+      : PcForErrors<ValidationTag::full_validation>(pc), type(type) {}
 };
 
 template <typename Value>
@@ -856,8 +887,8 @@
 };
 
 // An entry on the control stack (i.e. if, block, loop, or try).
-template <typename Value, Decoder::ValidateFlag validate>
-struct ControlBase : public PcForErrors<validate> {
+template <typename Value, typename ValidationTag>
+struct ControlBase : public PcForErrors<ValidationTag::full_validation> {
   ControlKind kind = kControlBlock;
   Reachability reachability = kReachable;
   uint32_t stack_depth = 0;  // Stack height at the beginning of the construct.
@@ -874,7 +905,7 @@
 
   ControlBase(ControlKind kind, uint32_t stack_depth, uint32_t init_stack_depth,
               const uint8_t* pc, Reachability reachability)
-      : PcForErrors<validate>(pc),
+      : PcForErrors<ValidationTag::full_validation>(pc),
         kind(kind),
         reachability(reachability),
         stack_depth(stack_depth),
@@ -933,29 +964,29 @@
   F(I64Const, Value* result, int64_t value)                                    \
   F(F32Const, Value* result, float value)                                      \
   F(F64Const, Value* result, double value)                                     \
-  F(S128Const, Simd128Immediate<validate>& imm, Value* result)                 \
-  F(GlobalGet, Value* result, const GlobalIndexImmediate<validate>& imm)       \
+  F(S128Const, Simd128Immediate& imm, Value* result)                           \
+  F(GlobalGet, Value* result, const GlobalIndexImmediate& imm)                 \
   F(DoReturn, uint32_t drop_values)                                            \
   F(BinOp, WasmOpcode opcode, const Value& lhs, const Value& rhs,              \
     Value* result)                                                             \
   F(RefNull, ValueType type, Value* result)                                    \
   F(RefFunc, uint32_t function_index, Value* result)                           \
-  F(StructNew, const StructIndexImmediate<validate>& imm, const Value& rtt,    \
+  F(StructNew, const StructIndexImmediate& imm, const Value& rtt,              \
     const Value args[], Value* result)                                         \
-  F(StructNewDefault, const StructIndexImmediate<validate>& imm,               \
-    const Value& rtt, Value* result)                                           \
-  F(ArrayNew, const ArrayIndexImmediate<validate>& imm, const Value& length,   \
+  F(StructNewDefault, const StructIndexImmediate& imm, const Value& rtt,       \
+    Value* result)                                                             \
+  F(ArrayNew, const ArrayIndexImmediate& imm, const Value& length,             \
     const Value& initial_value, const Value& rtt, Value* result)               \
-  F(ArrayNewDefault, const ArrayIndexImmediate<validate>& imm,                 \
-    const Value& length, const Value& rtt, Value* result)                      \
-  F(ArrayNewFixed, const ArrayIndexImmediate<validate>& imm,                   \
+  F(ArrayNewDefault, const ArrayIndexImmediate& imm, const Value& length,      \
+    const Value& rtt, Value* result)                                           \
+  F(ArrayNewFixed, const ArrayIndexImmediate& imm,                             \
     const base::Vector<Value>& elements, const Value& rtt, Value* result)      \
-  F(ArrayNewSegment, const ArrayIndexImmediate<validate>& array_imm,           \
-    const IndexImmediate<validate>& data_segment, const Value& offset,         \
+  F(ArrayNewSegment, const ArrayIndexImmediate& array_imm,                     \
+    const IndexImmediate& data_segment, const Value& offset,                   \
     const Value& length, const Value& rtt, Value* result)                      \
   F(I31New, const Value& input, Value* result)                                 \
   F(RttCanon, uint32_t type_index, Value* result)                              \
-  F(StringConst, const StringConstImmediate<validate>& imm, Value* result)
+  F(StringConst, const StringConstImmediate& imm, Value* result)
 
 #define INTERFACE_NON_CONSTANT_FUNCTIONS(F) /*       force 80 columns       */ \
   /* Control: */                                                               \
@@ -969,15 +1000,13 @@
   F(UnOp, WasmOpcode opcode, const Value& value, Value* result)                \
   F(RefAsNonNull, const Value& arg, Value* result)                             \
   F(Drop)                                                                      \
-  F(LocalGet, Value* result, const IndexImmediate<validate>& imm)              \
-  F(LocalSet, const Value& value, const IndexImmediate<validate>& imm)         \
-  F(LocalTee, const Value& value, Value* result,                               \
-    const IndexImmediate<validate>& imm)                                       \
-  F(GlobalSet, const Value& value, const GlobalIndexImmediate<validate>& imm)  \
-  F(TableGet, const Value& index, Value* result,                               \
-    const IndexImmediate<validate>& imm)                                       \
+  F(LocalGet, Value* result, const IndexImmediate& imm)                        \
+  F(LocalSet, const Value& value, const IndexImmediate& imm)                   \
+  F(LocalTee, const Value& value, Value* result, const IndexImmediate& imm)    \
+  F(GlobalSet, const Value& value, const GlobalIndexImmediate& imm)            \
+  F(TableGet, const Value& index, Value* result, const IndexImmediate& imm)    \
   F(TableSet, const Value& index, const Value& value,                          \
-    const IndexImmediate<validate>& imm)                                       \
+    const IndexImmediate& imm)                                                 \
   F(Trap, TrapReason reason)                                                   \
   F(NopForTestingUnsupportedInLiftoff)                                         \
   F(Forward, const Value& from, Value* to)                                     \
@@ -985,82 +1014,73 @@
     Value* result)                                                             \
   F(BrOrRet, uint32_t depth, uint32_t drop_values)                             \
   F(BrIf, const Value& cond, uint32_t depth)                                   \
-  F(BrTable, const BranchTableImmediate<validate>& imm, const Value& key)      \
+  F(BrTable, const BranchTableImmediate& imm, const Value& key)                \
   F(Else, Control* if_block)                                                   \
-  F(LoadMem, LoadType type, const MemoryAccessImmediate<validate>& imm,        \
+  F(LoadMem, LoadType type, const MemoryAccessImmediate& imm,                  \
     const Value& index, Value* result)                                         \
   F(LoadTransform, LoadType type, LoadTransformationKind transform,            \
-    const MemoryAccessImmediate<validate>& imm, const Value& index,            \
-    Value* result)                                                             \
+    const MemoryAccessImmediate& imm, const Value& index, Value* result)       \
   F(LoadLane, LoadType type, const Value& value, const Value& index,           \
-    const MemoryAccessImmediate<validate>& imm, const uint8_t laneidx,         \
-    Value* result)                                                             \
-  F(StoreMem, StoreType type, const MemoryAccessImmediate<validate>& imm,      \
+    const MemoryAccessImmediate& imm, const uint8_t laneidx, Value* result)    \
+  F(StoreMem, StoreType type, const MemoryAccessImmediate& imm,                \
     const Value& index, const Value& value)                                    \
-  F(StoreLane, StoreType type, const MemoryAccessImmediate<validate>& imm,     \
+  F(StoreLane, StoreType type, const MemoryAccessImmediate& imm,               \
     const Value& index, const Value& value, const uint8_t laneidx)             \
   F(CurrentMemoryPages, Value* result)                                         \
   F(MemoryGrow, const Value& value, Value* result)                             \
-  F(CallDirect, const CallFunctionImmediate<validate>& imm,                    \
-    const Value args[], Value returns[])                                       \
-  F(CallIndirect, const Value& index,                                          \
-    const CallIndirectImmediate<validate>& imm, const Value args[],            \
+  F(CallDirect, const CallFunctionImmediate& imm, const Value args[],          \
     Value returns[])                                                           \
+  F(CallIndirect, const Value& index, const CallIndirectImmediate& imm,        \
+    const Value args[], Value returns[])                                       \
   F(CallRef, const Value& func_ref, const FunctionSig* sig,                    \
     uint32_t sig_index, const Value args[], const Value returns[])             \
   F(ReturnCallRef, const Value& func_ref, const FunctionSig* sig,              \
     uint32_t sig_index, const Value args[])                                    \
-  F(ReturnCall, const CallFunctionImmediate<validate>& imm,                    \
+  F(ReturnCall, const CallFunctionImmediate& imm, const Value args[])          \
+  F(ReturnCallIndirect, const Value& index, const CallIndirectImmediate& imm,  \
     const Value args[])                                                        \
-  F(ReturnCallIndirect, const Value& index,                                    \
-    const CallIndirectImmediate<validate>& imm, const Value args[])            \
   F(BrOnNull, const Value& ref_object, uint32_t depth,                         \
     bool pass_null_along_branch, Value* result_on_fallthrough)                 \
   F(BrOnNonNull, const Value& ref_object, Value* result, uint32_t depth,       \
     bool drop_null_on_fallthrough)                                             \
   F(SimdOp, WasmOpcode opcode, base::Vector<Value> args, Value* result)        \
-  F(SimdLaneOp, WasmOpcode opcode, const SimdLaneImmediate<validate>& imm,     \
+  F(SimdLaneOp, WasmOpcode opcode, const SimdLaneImmediate& imm,               \
     const base::Vector<Value> inputs, Value* result)                           \
-  F(S128Const, const Simd128Immediate<validate>& imm, Value* result)           \
-  F(Simd8x16ShuffleOp, const Simd128Immediate<validate>& imm,                  \
-    const Value& input0, const Value& input1, Value* result)                   \
-  F(Throw, const TagIndexImmediate<validate>& imm,                             \
-    const base::Vector<Value>& args)                                           \
+  F(S128Const, const Simd128Immediate& imm, Value* result)                     \
+  F(Simd8x16ShuffleOp, const Simd128Immediate& imm, const Value& input0,       \
+    const Value& input1, Value* result)                                        \
+  F(Throw, const TagIndexImmediate& imm, const base::Vector<Value>& args)      \
   F(Rethrow, Control* block)                                                   \
-  F(CatchException, const TagIndexImmediate<validate>& imm, Control* block,    \
+  F(CatchException, const TagIndexImmediate& imm, Control* block,              \
     base::Vector<Value> caught_values)                                         \
   F(Delegate, uint32_t depth, Control* block)                                  \
   F(CatchAll, Control* block)                                                  \
   F(AtomicOp, WasmOpcode opcode, base::Vector<Value> args,                     \
-    const MemoryAccessImmediate<validate>& imm, Value* result)                 \
+    const MemoryAccessImmediate& imm, Value* result)                           \
   F(AtomicFence)                                                               \
-  F(MemoryInit, const MemoryInitImmediate<validate>& imm, const Value& dst,    \
+  F(MemoryInit, const MemoryInitImmediate& imm, const Value& dst,              \
     const Value& src, const Value& size)                                       \
-  F(DataDrop, const IndexImmediate<validate>& imm)                             \
-  F(MemoryCopy, const MemoryCopyImmediate<validate>& imm, const Value& dst,    \
+  F(DataDrop, const IndexImmediate& imm)                                       \
+  F(MemoryCopy, const MemoryCopyImmediate& imm, const Value& dst,              \
     const Value& src, const Value& size)                                       \
-  F(MemoryFill, const MemoryIndexImmediate<validate>& imm, const Value& dst,   \
+  F(MemoryFill, const MemoryIndexImmediate& imm, const Value& dst,             \
     const Value& value, const Value& size)                                     \
-  F(TableInit, const TableInitImmediate<validate>& imm,                        \
-    base::Vector<Value> args)                                                  \
-  F(ElemDrop, const IndexImmediate<validate>& imm)                             \
-  F(TableCopy, const TableCopyImmediate<validate>& imm,                        \
-    base::Vector<Value> args)                                                  \
-  F(TableGrow, const IndexImmediate<validate>& imm, const Value& value,        \
+  F(TableInit, const TableInitImmediate& imm, base::Vector<Value> args)        \
+  F(ElemDrop, const IndexImmediate& imm)                                       \
+  F(TableCopy, const TableCopyImmediate& imm, base::Vector<Value> args)        \
+  F(TableGrow, const IndexImmediate& imm, const Value& value,                  \
     const Value& delta, Value* result)                                         \
-  F(TableSize, const IndexImmediate<validate>& imm, Value* result)             \
-  F(TableFill, const IndexImmediate<validate>& imm, const Value& start,        \
+  F(TableSize, const IndexImmediate& imm, Value* result)                       \
+  F(TableFill, const IndexImmediate& imm, const Value& start,                  \
     const Value& value, const Value& count)                                    \
-  F(StructGet, const Value& struct_object,                                     \
-    const FieldImmediate<validate>& field, bool is_signed, Value* result)      \
-  F(StructSet, const Value& struct_object,                                     \
-    const FieldImmediate<validate>& field, const Value& field_value)           \
-  F(ArrayGet, const Value& array_obj,                                          \
-    const ArrayIndexImmediate<validate>& imm, const Value& index,              \
+  F(StructGet, const Value& struct_object, const FieldImmediate& field,        \
     bool is_signed, Value* result)                                             \
-  F(ArraySet, const Value& array_obj,                                          \
-    const ArrayIndexImmediate<validate>& imm, const Value& index,              \
-    const Value& value)                                                        \
+  F(StructSet, const Value& struct_object, const FieldImmediate& field,        \
+    const Value& field_value)                                                  \
+  F(ArrayGet, const Value& array_obj, const ArrayIndexImmediate& imm,          \
+    const Value& index, bool is_signed, Value* result)                         \
+  F(ArraySet, const Value& array_obj, const ArrayIndexImmediate& imm,          \
+    const Value& index, const Value& value)                                    \
   F(ArrayLen, const Value& array_obj, Value* result)                           \
   F(ArrayCopy, const Value& src, const Value& src_index, const Value& dst,     \
     const Value& dst_index, const Value& length)                               \
@@ -1070,47 +1090,52 @@
     bool null_succeeds)                                                        \
   F(RefTestAbstract, const Value& obj, HeapType type, Value* result,           \
     bool null_succeeds)                                                        \
-  F(RefCast, const Value& obj, const Value& rtt, Value* result)                \
+  F(RefCast, const Value& obj, const Value& rtt, Value* result,                \
+    bool null_succeeds)                                                        \
+  F(RefCastAbstract, const Value& obj, HeapType type, Value* result,           \
+    bool null_succeeds)                                                        \
   F(AssertNull, const Value& obj, Value* result)                               \
+  F(AssertNotNull, const Value& obj, Value* result)                            \
   F(BrOnCast, const Value& obj, const Value& rtt, Value* result_on_branch,     \
     uint32_t depth)                                                            \
   F(BrOnCastFail, const Value& obj, const Value& rtt,                          \
     Value* result_on_fallthrough, uint32_t depth)                              \
-  F(RefIsData, const Value& object, Value* result)                             \
+  F(RefIsStruct, const Value& object, Value* result)                           \
   F(RefIsEq, const Value& object, Value* result)                               \
   F(RefIsI31, const Value& object, Value* result)                              \
   F(RefIsArray, const Value& object, Value* result)                            \
-  F(RefAsData, const Value& object, Value* result)                             \
+  F(RefAsStruct, const Value& object, Value* result)                           \
   F(RefAsI31, const Value& object, Value* result)                              \
   F(RefAsArray, const Value& object, Value* result)                            \
-  F(BrOnData, const Value& object, Value* value_on_branch, uint32_t br_depth)  \
+  F(BrOnStruct, const Value& object, Value* value_on_branch,                   \
+    uint32_t br_depth)                                                         \
   F(BrOnI31, const Value& object, Value* value_on_branch, uint32_t br_depth)   \
   F(BrOnArray, const Value& object, Value* value_on_branch, uint32_t br_depth) \
-  F(BrOnNonData, const Value& object, Value* value_on_fallthrough,             \
+  F(BrOnNonStruct, const Value& object, Value* value_on_fallthrough,           \
     uint32_t br_depth)                                                         \
   F(BrOnNonI31, const Value& object, Value* value_on_fallthrough,              \
     uint32_t br_depth)                                                         \
   F(BrOnNonArray, const Value& object, Value* value_on_fallthrough,            \
     uint32_t br_depth)                                                         \
-  F(StringNewWtf8, const MemoryIndexImmediate<validate>& memory,               \
+  F(StringNewWtf8, const MemoryIndexImmediate& memory,                         \
     const unibrow::Utf8Variant variant, const Value& offset,                   \
     const Value& size, Value* result)                                          \
   F(StringNewWtf8Array, const unibrow::Utf8Variant variant,                    \
     const Value& array, const Value& start, const Value& end, Value* result)   \
-  F(StringNewWtf16, const MemoryIndexImmediate<validate>& memory,              \
-    const Value& offset, const Value& size, Value* result)                     \
+  F(StringNewWtf16, const MemoryIndexImmediate& memory, const Value& offset,   \
+    const Value& size, Value* result)                                          \
   F(StringNewWtf16Array, const Value& array, const Value& start,               \
     const Value& end, Value* result)                                           \
   F(StringMeasureWtf8, const unibrow::Utf8Variant variant, const Value& str,   \
     Value* result)                                                             \
   F(StringMeasureWtf16, const Value& str, Value* result)                       \
-  F(StringEncodeWtf8, const MemoryIndexImmediate<validate>& memory,            \
+  F(StringEncodeWtf8, const MemoryIndexImmediate& memory,                      \
     const unibrow::Utf8Variant variant, const Value& str,                      \
     const Value& address, Value* result)                                       \
   F(StringEncodeWtf8Array, const unibrow::Utf8Variant variant,                 \
     const Value& str, const Value& array, const Value& start, Value* result)   \
-  F(StringEncodeWtf16, const MemoryIndexImmediate<validate>& memory,           \
-    const Value& str, const Value& address, Value* result)                     \
+  F(StringEncodeWtf16, const MemoryIndexImmediate& memory, const Value& str,   \
+    const Value& address, Value* result)                                       \
   F(StringEncodeWtf16Array, const Value& str, const Value& array,              \
     const Value& start, Value* result)                                         \
   F(StringConcat, const Value& head, const Value& tail, Value* result)         \
@@ -1119,7 +1144,7 @@
   F(StringAsWtf8, const Value& str, Value* result)                             \
   F(StringViewWtf8Advance, const Value& view, const Value& pos,                \
     const Value& bytes, Value* result)                                         \
-  F(StringViewWtf8Encode, const MemoryIndexImmediate<validate>& memory,        \
+  F(StringViewWtf8Encode, const MemoryIndexImmediate& memory,                  \
     const unibrow::Utf8Variant variant, const Value& view, const Value& addr,  \
     const Value& pos, const Value& bytes, Value* next_pos,                     \
     Value* bytes_written)                                                      \
@@ -1128,7 +1153,7 @@
   F(StringAsWtf16, const Value& str, Value* result)                            \
   F(StringViewWtf16GetCodeUnit, const Value& view, const Value& pos,           \
     Value* result)                                                             \
-  F(StringViewWtf16Encode, const MemoryIndexImmediate<validate>& memory,       \
+  F(StringViewWtf16Encode, const MemoryIndexImmediate& memory,                 \
     const Value& view, const Value& addr, const Value& pos,                    \
     const Value& codeunits, Value* result)                                     \
   F(StringViewWtf16Slice, const Value& view, const Value& start,               \
@@ -1146,12 +1171,119 @@
 // the current instruction trace pointer in the default case
 const std::pair<uint32_t, uint32_t> invalid_instruction_trace = {0, 0};
 
+// A fast vector implementation, without implicit bounds checks (see
+// https://crbug.com/1358853).
+template <typename T>
+class FastZoneVector {
+ public:
+#ifdef DEBUG
+  ~FastZoneVector() {
+    // Check that {Reset} was called on this vector.
+    DCHECK_NULL(begin_);
+  }
+#endif
+
+  void Reset(Zone* zone) {
+    if (begin_ == nullptr) return;
+    if constexpr (!std::is_trivially_destructible_v<T>) {
+      for (T* ptr = begin_; ptr != end_; ++ptr) {
+        ptr->~T();
+      }
+    }
+    zone->DeleteArray(begin_, capacity_end_ - begin_);
+    begin_ = nullptr;
+    end_ = nullptr;
+    capacity_end_ = nullptr;
+  }
+
+  T* begin() const { return begin_; }
+  T* end() const { return end_; }
+
+  T& front() {
+    DCHECK(!empty());
+    return begin_[0];
+  }
+
+  T& back() {
+    DCHECK(!empty());
+    return end_[-1];
+  }
+
+  uint32_t size() const { return static_cast<uint32_t>(end_ - begin_); }
+
+  bool empty() const { return begin_ == end_; }
+
+  T& operator[](uint32_t index) {
+    DCHECK_GE(size(), index);
+    return begin_[index];
+  }
+
+  void shrink_to(uint32_t new_size) {
+    DCHECK_GE(size(), new_size);
+    end_ = begin_ + new_size;
+  }
+
+  void pop(uint32_t num = 1) {
+    DCHECK_GE(size(), num);
+    for (T* new_end = end_ - num; end_ != new_end;) {
+      --end_;
+      end_->~T();
+    }
+  }
+
+  void push(T value) {
+    DCHECK_GT(capacity_end_, end_);
+    *end_ = std::move(value);
+    ++end_;
+  }
+
+  template <typename... Args>
+  void emplace_back(Args&&... args) {
+    DCHECK_GT(capacity_end_, end_);
+    new (end_) T{std::forward<Args>(args)...};
+    ++end_;
+  }
+
+  V8_INLINE void EnsureMoreCapacity(int slots_needed, Zone* zone) {
+    if (V8_LIKELY(capacity_end_ - end_ >= slots_needed)) return;
+    Grow(slots_needed, zone);
+  }
+
+ private:
+  V8_NOINLINE void Grow(int slots_needed, Zone* zone) {
+    size_t new_capacity = std::max(
+        size_t{8}, base::bits::RoundUpToPowerOfTwo(size() + slots_needed));
+    CHECK_GE(kMaxUInt32, new_capacity);
+    DCHECK_LT(capacity_end_ - begin_, new_capacity);
+    T* new_begin = zone->template NewArray<T>(new_capacity);
+    if (begin_) {
+      for (T *ptr = begin_, *new_ptr = new_begin; ptr != end_;
+           ++ptr, ++new_ptr) {
+        new (new_ptr) T{std::move(*ptr)};
+        ptr->~T();
+      }
+      zone->DeleteArray(begin_, capacity_end_ - begin_);
+    }
+    end_ = new_begin + (end_ - begin_);
+    begin_ = new_begin;
+    capacity_end_ = new_begin + new_capacity;
+  }
+
+  // The array is zone-allocated inside {EnsureMoreCapacity}.
+  T* begin_ = nullptr;
+  T* end_ = nullptr;
+  T* capacity_end_ = nullptr;
+};
+
 // Generic Wasm bytecode decoder with utilities for decoding immediates,
 // lengths, etc.
-template <Decoder::ValidateFlag validate, DecodingMode decoding_mode>
+template <typename ValidationTag, DecodingMode decoding_mode = kFunctionBody>
 class WasmDecoder : public Decoder {
+  // {full_validation} implies {validate}.
+  static_assert(!ValidationTag::full_validation || ValidationTag::validate);
+
  public:
-  WasmDecoder(Zone* zone, const WasmModule* module, const WasmFeatures& enabled,
+  WasmDecoder(Zone* zone, const WasmModule* module, WasmFeatures enabled,
               WasmFeatures* detected, const FunctionSig* sig, const byte* start,
               const byte* end, uint32_t buffer_offset = 0)
       : Decoder(start, end, buffer_offset),
@@ -1200,13 +1332,22 @@
     *total_length = 0;
 
     // Decode local declarations, if any.
-    uint32_t entries = read_u32v<validate>(pc, &length, "local decls count");
+    uint32_t entries =
+        read_u32v<ValidationTag>(pc, &length, "local decls count");
     if (!VALIDATE(ok())) {
       return DecodeError(pc + *total_length, "invalid local decls count");
     }
     *total_length += length;
     TRACE("local decls count: %u\n", entries);
 
+    // Do an early validity check, to avoid allocating too much memory below.
+    // Every entry needs at least two bytes (count plus type); if that many are
+    // not available any more, flag that as an error.
+    if (available_bytes() / 2 < entries) {
+      return DecodeError(
+          pc, "local decls count bigger than remaining function size");
+    }
+
     struct DecodedLocalEntry {
       uint32_t count;
       ValueType type;
@@ -1219,7 +1360,7 @@
       }
 
       uint32_t count =
-          read_u32v<validate>(pc + *total_length, &length, "local count");
+          read_u32v<ValidationTag>(pc + *total_length, &length, "local count");
       if (!VALIDATE(ok())) {
         return DecodeError(pc + *total_length, "invalid local count");
       }
@@ -1229,8 +1370,9 @@
       }
       *total_length += length;
 
-      ValueType type = value_type_reader::read_value_type<validate>(
-          this, pc + *total_length, &length, this->module_, enabled_);
+      ValueType type = value_type_reader::read_value_type<ValidationTag>(
+          this, pc + *total_length, &length, enabled_);
+      ValidateValueType(pc + *total_length, type);
       if (!VALIDATE(ok())) return;
       *total_length += length;
 
@@ -1259,10 +1401,10 @@
   }
 
   // Shorthand that forwards to the {DecodeError} functions above, passing our
-  // {validate} flag.
+  // {ValidationTag}.
   template <typename... Args>
   void DecodeError(Args... args) {
-    wasm::DecodeError<validate>(this, std::forward<Args>(args)...);
+    wasm::DecodeError<ValidationTag>(this, std::forward<Args>(args)...);
   }
 
   // Returns a BitVector of length {locals_count + 1} representing the set of
@@ -1290,7 +1432,7 @@
           break;
         case kExprLocalSet:
         case kExprLocalTee: {
-          IndexImmediate<validate> imm(decoder, pc + 1, "local index");
+          IndexImmediate imm(decoder, pc + 1, "local index", validate);
           // Unverified code might have an out-of-bounds index.
           if (imm.index < locals_count) assigned->Add(imm.index);
           break;
@@ -1315,7 +1457,7 @@
     return VALIDATE(decoder->ok()) ? assigned : nullptr;
   }
 
-  bool Validate(const byte* pc, TagIndexImmediate<validate>& imm) {
+  bool Validate(const byte* pc, TagIndexImmediate& imm) {
     if (!VALIDATE(imm.index < module_->tags.size())) {
       DecodeError(pc, "Invalid tag index: %u", imm.index);
       return false;
@@ -1324,7 +1466,7 @@
     return true;
   }
 
-  bool Validate(const byte* pc, GlobalIndexImmediate<validate>& imm) {
+  bool Validate(const byte* pc, GlobalIndexImmediate& imm) {
     // We compare with the current size of the globals vector. This is important
     // if we are decoding a constant expression in the global section.
     if (!VALIDATE(imm.index < module_->globals.size())) {
@@ -1350,7 +1492,7 @@
     return true;
   }
 
-  bool Validate(const byte* pc, SigIndexImmediate<validate>& imm) {
+  bool Validate(const byte* pc, SigIndexImmediate& imm) {
     if (!VALIDATE(module_->has_signature(imm.index))) {
       DecodeError(pc, "invalid signature index: %u", imm.index);
       return false;
@@ -1359,7 +1501,7 @@
     return true;
   }
 
-  bool Validate(const byte* pc, StructIndexImmediate<validate>& imm) {
+  bool Validate(const byte* pc, StructIndexImmediate& imm) {
     if (!VALIDATE(module_->has_struct(imm.index))) {
       DecodeError(pc, "invalid struct index: %u", imm.index);
       return false;
@@ -1368,7 +1510,7 @@
     return true;
   }
 
-  bool Validate(const byte* pc, FieldImmediate<validate>& imm) {
+  bool Validate(const byte* pc, FieldImmediate& imm) {
     if (!Validate(pc, imm.struct_imm)) return false;
     if (!VALIDATE(imm.field_imm.index <
                   imm.struct_imm.struct_type->field_count())) {
@@ -1379,7 +1521,7 @@
     return true;
   }
 
-  bool Validate(const byte* pc, ArrayIndexImmediate<validate>& imm) {
+  bool Validate(const byte* pc, ArrayIndexImmediate& imm) {
     if (!VALIDATE(module_->has_array(imm.index))) {
       DecodeError(pc, "invalid array index: %u", imm.index);
       return false;
@@ -1397,7 +1539,7 @@
     return true;
   }
 
-  bool Validate(const byte* pc, CallFunctionImmediate<validate>& imm) {
+  bool Validate(const byte* pc, CallFunctionImmediate& imm) {
     if (!VALIDATE(imm.index < module_->functions.size())) {
       DecodeError(pc, "function index #%u is out of bounds", imm.index);
       return false;
@@ -1406,7 +1548,7 @@
     return true;
   }
 
-  bool Validate(const byte* pc, CallIndirectImmediate<validate>& imm) {
+  bool Validate(const byte* pc, CallIndirectImmediate& imm) {
     if (!ValidateSignature(pc, imm.sig_imm)) return false;
     if (!ValidateTable(pc + imm.sig_imm.length, imm.table_imm)) {
       return false;
@@ -1434,7 +1576,7 @@
     return true;
   }
 
-  bool Validate(const byte* pc, BranchDepthImmediate<validate>& imm,
+  bool Validate(const byte* pc, BranchDepthImmediate& imm,
                 size_t control_depth) {
     if (!VALIDATE(imm.depth < control_depth)) {
       DecodeError(pc, "invalid branch depth: %u", imm.depth);
@@ -1443,8 +1585,7 @@
     return true;
   }
 
-  bool Validate(const byte* pc, BranchTableImmediate<validate>& imm,
-                size_t block_depth) {
+  bool Validate(const byte* pc, BranchTableImmediate& imm, size_t block_depth) {
     if (!VALIDATE(imm.table_count <= kV8MaxWasmFunctionBrTableSize)) {
       DecodeError(pc, "invalid table count (> max br_table size): %u",
                   imm.table_count);
@@ -1453,8 +1594,7 @@
     return checkAvailable(imm.table_count);
   }
 
-  bool Validate(const byte* pc, WasmOpcode opcode,
-                SimdLaneImmediate<validate>& imm) {
+  bool Validate(const byte* pc, WasmOpcode opcode, SimdLaneImmediate& imm) {
     uint8_t num_lanes = 0;
     switch (opcode) {
       case kExprF64x2ExtractLane:
@@ -1499,7 +1639,7 @@
     }
   }
 
-  bool Validate(const byte* pc, Simd128Immediate<validate>& imm) {
+  bool Validate(const byte* pc, Simd128Immediate& imm) {
     uint8_t max_lane = 0;
     for (uint32_t i = 0; i < kSimd128Size; ++i) {
       max_lane = std::max(max_lane, imm.value[i]);
@@ -1512,18 +1652,20 @@
     return true;
   }
 
-  bool Validate(const byte* pc, BlockTypeImmediate<validate>& imm) {
-    if (imm.type != kWasmBottom) return true;
-    if (!VALIDATE(module_->has_signature(imm.sig_index))) {
-      DecodeError(pc, "block type index %u is not a signature definition",
-                  imm.sig_index);
-      return false;
+  bool Validate(const byte* pc, BlockTypeImmediate& imm) {
+    if (!ValidateValueType(pc, imm.type)) return false;
+    if (imm.type == kWasmBottom) {
+      if (!VALIDATE(module_->has_signature(imm.sig_index))) {
+        DecodeError(pc, "block type index %u is not a signature definition",
+                    imm.sig_index);
+        return false;
+      }
+      imm.sig = module_->signature(imm.sig_index);
     }
-    imm.sig = module_->signature(imm.sig_index);
     return true;
   }
 
-  bool Validate(const byte* pc, MemoryIndexImmediate<validate>& imm) {
+  bool Validate(const byte* pc, MemoryIndexImmediate& imm) {
     if (!VALIDATE(this->module_->has_memory)) {
       this->DecodeError(pc, "memory instruction with no memory");
       return false;
@@ -1535,7 +1677,7 @@
     return true;
   }
 
-  bool Validate(const byte* pc, MemoryAccessImmediate<validate>& imm) {
+  bool Validate(const byte* pc, MemoryAccessImmediate& imm) {
     if (!VALIDATE(this->module_->has_memory)) {
       this->DecodeError(pc, "memory instruction with no memory");
       return false;
@@ -1543,17 +1685,17 @@
     return true;
   }
 
-  bool Validate(const byte* pc, MemoryInitImmediate<validate>& imm) {
+  bool Validate(const byte* pc, MemoryInitImmediate& imm) {
     return ValidateDataSegment(pc, imm.data_segment) &&
            Validate(pc + imm.data_segment.length, imm.memory);
   }
 
-  bool Validate(const byte* pc, MemoryCopyImmediate<validate>& imm) {
+  bool Validate(const byte* pc, MemoryCopyImmediate& imm) {
     return Validate(pc, imm.memory_src) &&
            Validate(pc + imm.memory_src.length, imm.memory_dst);
   }
 
-  bool Validate(const byte* pc, TableInitImmediate<validate>& imm) {
+  bool Validate(const byte* pc, TableInitImmediate& imm) {
     if (!ValidateElementSegment(pc, imm.element_segment)) return false;
     if (!ValidateTable(pc + imm.element_segment.length, imm.table)) {
       return false;
@@ -1569,7 +1711,7 @@
     return true;
   }
 
-  bool Validate(const byte* pc, TableCopyImmediate<validate>& imm) {
+  bool Validate(const byte* pc, TableCopyImmediate& imm) {
     if (!ValidateTable(pc, imm.table_src)) return false;
     if (!ValidateTable(pc + imm.table_src.length, imm.table_dst)) return false;
     ValueType src_type = module_->tables[imm.table_src.index].type;
@@ -1582,7 +1724,7 @@
     return true;
   }
 
-  bool Validate(const byte* pc, StringConstImmediate<validate>& imm) {
+  bool Validate(const byte* pc, StringConstImmediate& imm) {
     if (!VALIDATE(imm.index < module_->stringref_literals.size())) {
       DecodeError(pc, "Invalid string literal index: %u", imm.index);
       return false;
@@ -1592,7 +1734,7 @@
 
   // The following Validate* functions all validate an IndexImmediate, albeit
   // differently according to context.
-  bool ValidateTable(const byte* pc, IndexImmediate<validate>& imm) {
+  bool ValidateTable(const byte* pc, IndexImmediate& imm) {
     if (imm.index > 0 || imm.length > 1) {
       this->detected_->Add(kFeature_reftypes);
     }
@@ -1603,7 +1745,7 @@
     return true;
   }
 
-  bool ValidateElementSegment(const byte* pc, IndexImmediate<validate>& imm) {
+  bool ValidateElementSegment(const byte* pc, IndexImmediate& imm) {
     if (!VALIDATE(imm.index < module_->elem_segments.size())) {
       DecodeError(pc, "invalid element segment index: %u", imm.index);
       return false;
@@ -1611,7 +1753,7 @@
     return true;
   }
 
-  bool ValidateLocal(const byte* pc, IndexImmediate<validate>& imm) {
+  bool ValidateLocal(const byte* pc, IndexImmediate& imm) {
     if (!VALIDATE(imm.index < num_locals())) {
       DecodeError(pc, "invalid local index: %u", imm.index);
       return false;
@@ -1619,7 +1761,7 @@
     return true;
   }
 
-  bool ValidateType(const byte* pc, IndexImmediate<validate>& imm) {
+  bool ValidateType(const byte* pc, IndexImmediate& imm) {
     if (!VALIDATE(module_->has_type(imm.index))) {
       DecodeError(pc, "invalid type index: %u", imm.index);
       return false;
@@ -1627,7 +1769,7 @@
     return true;
   }
 
-  bool ValidateSignature(const byte* pc, IndexImmediate<validate>& imm) {
+  bool ValidateSignature(const byte* pc, IndexImmediate& imm) {
     if (!VALIDATE(module_->has_signature(imm.index))) {
       DecodeError(pc, "invalid signature index: %u", imm.index);
       return false;
@@ -1635,7 +1777,7 @@
     return true;
   }
 
-  bool ValidateFunction(const byte* pc, IndexImmediate<validate>& imm) {
+  bool ValidateFunction(const byte* pc, IndexImmediate& imm) {
     if (!VALIDATE(imm.index < module_->functions.size())) {
       DecodeError(pc, "function index #%u is out of bounds", imm.index);
       return false;
@@ -1648,7 +1790,7 @@
     return true;
   }
 
-  bool ValidateDataSegment(const byte* pc, IndexImmediate<validate>& imm) {
+  bool ValidateDataSegment(const byte* pc, IndexImmediate& imm) {
     if (!VALIDATE(imm.index < module_->num_declared_data_segments)) {
       DecodeError(pc, "invalid data segment index: %u", imm.index);
       return false;
@@ -1656,48 +1798,28 @@
     return true;
   }
 
-  class EmptyImmediateObserver {
-   public:
-    void BlockType(BlockTypeImmediate<validate>& imm) {}
-    void HeapType(HeapTypeImmediate<validate>& imm) {}
-    void BranchDepth(BranchDepthImmediate<validate>& imm) {}
-    void BranchTable(BranchTableImmediate<validate>& imm) {}
-    void CallIndirect(CallIndirectImmediate<validate>& imm) {}
-    void SelectType(SelectTypeImmediate<validate>& imm) {}
-    void MemoryAccess(MemoryAccessImmediate<validate>& imm) {}
-    void SimdLane(SimdLaneImmediate<validate>& imm) {}
-    void Field(FieldImmediate<validate>& imm) {}
-    void Length(IndexImmediate<validate>& imm) {}
-
-    void TagIndex(TagIndexImmediate<validate>& imm) {}
-    void FunctionIndex(IndexImmediate<validate>& imm) {}
-    void TypeIndex(IndexImmediate<validate>& imm) {}
-    void LocalIndex(IndexImmediate<validate>& imm) {}
-    void GlobalIndex(IndexImmediate<validate>& imm) {}
-    void TableIndex(IndexImmediate<validate>& imm) {}
-    void MemoryIndex(MemoryIndexImmediate<validate>& imm) {}
-    void DataSegmentIndex(IndexImmediate<validate>& imm) {}
-    void ElemSegmentIndex(IndexImmediate<validate>& imm) {}
-
-    void I32Const(ImmI32Immediate<validate>& imm) {}
-    void I64Const(ImmI64Immediate<validate>& imm) {}
-    void F32Const(ImmF32Immediate<validate>& imm) {}
-    void F64Const(ImmF64Immediate<validate>& imm) {}
-    void S128Const(Simd128Immediate<validate>& imm) {}
-    void StringConst(StringConstImmediate<validate>& imm) {}
-
-    void MemoryInit(MemoryInitImmediate<validate>& imm) {}
-    void MemoryCopy(MemoryCopyImmediate<validate>& imm) {}
-    void TableInit(TableInitImmediate<validate>& imm) {}
-    void TableCopy(TableCopyImmediate<validate>& imm) {}
-    void ArrayCopy(IndexImmediate<validate>& dst,
-                   IndexImmediate<validate>& src) {}
-  };
+  bool Validate(const byte* pc, SelectTypeImmediate& imm) {
+    return ValidateValueType(pc, imm.type);
+  }
+
+  bool Validate(const byte* pc, HeapTypeImmediate& imm) {
+    return ValidateHeapType(pc, imm.type);
+  }
+
+  bool ValidateValueType(const byte* pc, ValueType type) {
+    return value_type_reader::ValidateValueType<ValidationTag>(this, pc,
+                                                               module_, type);
+  }
+
+  bool ValidateHeapType(const byte* pc, HeapType type) {
+    return value_type_reader::ValidateHeapType<ValidationTag>(this, pc, module_,
+                                                              type);
+  }
 
   // Returns the length of the opcode under {pc}.
-  template <class ImmediateObserver = EmptyImmediateObserver>
+  template <typename... ImmediateObservers>
   static uint32_t OpcodeLength(WasmDecoder* decoder, const byte* pc,
-                               ImmediateObserver* io = nullptr) {
+                               ImmediateObservers&... ios) {
     WasmOpcode opcode = static_cast<WasmOpcode>(*pc);
     // We don't have information about the module here, so we just assume that
     // memory64 is enabled when parsing memory access immediates. This is
@@ -1717,9 +1839,8 @@
       case kExprIf:
       case kExprLoop:
       case kExprBlock: {
-        BlockTypeImmediate<validate> imm(WasmFeatures::All(), decoder, pc + 1,
-                                         nullptr);
-        if (io) io->BlockType(imm);
+        BlockTypeImmediate imm(WasmFeatures::All(), decoder, pc + 1, validate);
+        (ios.BlockType(imm), ...);
         return 1 + imm.length;
       }
       case kExprRethrow:
@@ -1728,107 +1849,105 @@
       case kExprBrOnNull:
       case kExprBrOnNonNull:
       case kExprDelegate: {
-        BranchDepthImmediate<validate> imm(decoder, pc + 1);
-        if (io) io->BranchDepth(imm);
+        BranchDepthImmediate imm(decoder, pc + 1, validate);
+        (ios.BranchDepth(imm), ...);
         return 1 + imm.length;
       }
       case kExprBrTable: {
-        BranchTableImmediate<validate> imm(decoder, pc + 1);
-        if (io) io->BranchTable(imm);
-        BranchTableIterator<validate> iterator(decoder, imm);
+        BranchTableImmediate imm(decoder, pc + 1, validate);
+        (ios.BranchTable(imm), ...);
+        BranchTableIterator<ValidationTag> iterator(decoder, imm);
         return 1 + iterator.length();
       }
       case kExprThrow:
       case kExprCatch: {
-        TagIndexImmediate<validate> imm(decoder, pc + 1);
-        if (io) io->TagIndex(imm);
+        TagIndexImmediate imm(decoder, pc + 1, validate);
+        (ios.TagIndex(imm), ...);
         return 1 + imm.length;
       }
 
       /********** Misc opcodes **********/
       case kExprCallFunction:
       case kExprReturnCall: {
-        CallFunctionImmediate<validate> imm(decoder, pc + 1);
-        if (io) io->FunctionIndex(imm);
+        CallFunctionImmediate imm(decoder, pc + 1, validate);
+        (ios.FunctionIndex(imm), ...);
         return 1 + imm.length;
       }
       case kExprCallIndirect:
       case kExprReturnCallIndirect: {
-        CallIndirectImmediate<validate> imm(decoder, pc + 1);
-        if (io) io->CallIndirect(imm);
+        CallIndirectImmediate imm(decoder, pc + 1, validate);
+        (ios.CallIndirect(imm), ...);
         return 1 + imm.length;
       }
+      case kExprCallRefDeprecated:  // TODO(7748): Drop after grace period.
       case kExprCallRef:
       case kExprReturnCallRef: {
-        SigIndexImmediate<validate> imm(decoder, pc + 1);
-        if (io) io->TypeIndex(imm);
+        SigIndexImmediate imm(decoder, pc + 1, validate);
+        (ios.TypeIndex(imm), ...);
         return 1 + imm.length;
       }
-      case kExprCallRefDeprecated:  // TODO(7748): Drop after grace period.
       case kExprDrop:
       case kExprSelect:
       case kExprCatchAll:
         return 1;
       case kExprSelectWithType: {
-        SelectTypeImmediate<validate> imm(WasmFeatures::All(), decoder, pc + 1,
-                                          nullptr);
-        if (io) io->SelectType(imm);
+        SelectTypeImmediate imm(WasmFeatures::All(), decoder, pc + 1, validate);
+        (ios.SelectType(imm), ...);
         return 1 + imm.length;
       }
 
       case kExprLocalGet:
       case kExprLocalSet:
       case kExprLocalTee: {
-        IndexImmediate<validate> imm(decoder, pc + 1, "local index");
-        if (io) io->LocalIndex(imm);
+        IndexImmediate imm(decoder, pc + 1, "local index", validate);
+        (ios.LocalIndex(imm), ...);
         return 1 + imm.length;
       }
       case kExprGlobalGet:
       case kExprGlobalSet: {
-        GlobalIndexImmediate<validate> imm(decoder, pc + 1);
-        if (io) io->GlobalIndex(imm);
+        GlobalIndexImmediate imm(decoder, pc + 1, validate);
+        (ios.GlobalIndex(imm), ...);
         return 1 + imm.length;
       }
       case kExprTableGet:
       case kExprTableSet: {
-        IndexImmediate<validate> imm(decoder, pc + 1, "table index");
-        if (io) io->TableIndex(imm);
+        IndexImmediate imm(decoder, pc + 1, "table index", validate);
+        (ios.TableIndex(imm), ...);
         return 1 + imm.length;
       }
       case kExprI32Const: {
-        ImmI32Immediate<validate> imm(decoder, pc + 1);
-        if (io) io->I32Const(imm);
+        ImmI32Immediate imm(decoder, pc + 1, validate);
+        (ios.I32Const(imm), ...);
         return 1 + imm.length;
       }
       case kExprI64Const: {
-        ImmI64Immediate<validate> imm(decoder, pc + 1);
-        if (io) io->I64Const(imm);
+        ImmI64Immediate imm(decoder, pc + 1, validate);
+        (ios.I64Const(imm), ...);
         return 1 + imm.length;
       }
       case kExprF32Const:
-        if (io) {
-          ImmF32Immediate<validate> imm(decoder, pc + 1);
-          io->F32Const(imm);
+        if (sizeof...(ios) > 0) {
+          ImmF32Immediate imm(decoder, pc + 1, validate);
+          (ios.F32Const(imm), ...);
         }
         return 5;
       case kExprF64Const:
-        if (io) {
-          ImmF64Immediate<validate> imm(decoder, pc + 1);
-          io->F64Const(imm);
+        if (sizeof...(ios) > 0) {
+          ImmF64Immediate imm(decoder, pc + 1, validate);
+          (ios.F64Const(imm), ...);
         }
         return 9;
       case kExprRefNull: {
-        HeapTypeImmediate<validate> imm(WasmFeatures::All(), decoder, pc + 1,
-                                        nullptr);
-        if (io) io->HeapType(imm);
+        HeapTypeImmediate imm(WasmFeatures::All(), decoder, pc + 1, validate);
+        (ios.HeapType(imm), ...);
         return 1 + imm.length;
       }
       case kExprRefIsNull:
       case kExprRefAsNonNull:
         return 1;
       case kExprRefFunc: {
-        IndexImmediate<validate> imm(decoder, pc + 1, "function index");
-        if (io) io->FunctionIndex(imm);
+        IndexImmediate imm(decoder, pc + 1, "function index", validate);
+        (ios.FunctionIndex(imm), ...);
         return 1 + imm.length;
       }
 
@@ -1840,23 +1959,23 @@
         return 1;
       FOREACH_LOAD_MEM_OPCODE(DECLARE_OPCODE_CASE)
       FOREACH_STORE_MEM_OPCODE(DECLARE_OPCODE_CASE) {
-        MemoryAccessImmediate<validate> imm(decoder, pc + 1, UINT32_MAX,
-                                            kConservativelyAssumeMemory64);
-        if (io) io->MemoryAccess(imm);
+        MemoryAccessImmediate imm(decoder, pc + 1, UINT32_MAX,
+                                  kConservativelyAssumeMemory64, validate);
+        (ios.MemoryAccess(imm), ...);
         return 1 + imm.length;
       }
       // clang-format on
       case kExprMemoryGrow:
       case kExprMemorySize: {
-        MemoryIndexImmediate<validate> imm(decoder, pc + 1);
-        if (io) io->MemoryIndex(imm);
+        MemoryIndexImmediate imm(decoder, pc + 1, validate);
+        (ios.MemoryIndex(imm), ...);
         return 1 + imm.length;
       }
 
       /********** Prefixed opcodes **********/
       case kNumericPrefix: {
         uint32_t length = 0;
-        opcode = decoder->read_prefixed_opcode<validate>(pc, &length);
+        opcode = decoder->read_prefixed_opcode<ValidationTag>(pc, &length);
         switch (opcode) {
           case kExprI32SConvertSatF32:
           case kExprI32UConvertSatF32:
@@ -1868,51 +1987,51 @@
           case kExprI64UConvertSatF64:
             return length;
           case kExprMemoryInit: {
-            MemoryInitImmediate<validate> imm(decoder, pc + length);
-            if (io) io->MemoryInit(imm);
+            MemoryInitImmediate imm(decoder, pc + length, validate);
+            (ios.MemoryInit(imm), ...);
             return length + imm.length;
           }
           case kExprDataDrop: {
-            IndexImmediate<validate> imm(decoder, pc + length,
-                                         "data segment index");
-            if (io) io->DataSegmentIndex(imm);
+            IndexImmediate imm(decoder, pc + length, "data segment index",
+                               validate);
+            (ios.DataSegmentIndex(imm), ...);
             return length + imm.length;
           }
           case kExprMemoryCopy: {
-            MemoryCopyImmediate<validate> imm(decoder, pc + length);
-            if (io) io->MemoryCopy(imm);
+            MemoryCopyImmediate imm(decoder, pc + length, validate);
+            (ios.MemoryCopy(imm), ...);
             return length + imm.length;
           }
           case kExprMemoryFill: {
-            MemoryIndexImmediate<validate> imm(decoder, pc + length);
-            if (io) io->MemoryIndex(imm);
+            MemoryIndexImmediate imm(decoder, pc + length, validate);
+            (ios.MemoryIndex(imm), ...);
             return length + imm.length;
           }
           case kExprTableInit: {
-            TableInitImmediate<validate> imm(decoder, pc + length);
-            if (io) io->TableInit(imm);
+            TableInitImmediate imm(decoder, pc + length, validate);
+            (ios.TableInit(imm), ...);
             return length + imm.length;
           }
           case kExprElemDrop: {
-            IndexImmediate<validate> imm(decoder, pc + length,
-                                         "element segment index");
-            if (io) io->ElemSegmentIndex(imm);
+            IndexImmediate imm(decoder, pc + length, "element segment index",
+                               validate);
+            (ios.ElemSegmentIndex(imm), ...);
             return length + imm.length;
           }
           case kExprTableCopy: {
-            TableCopyImmediate<validate> imm(decoder, pc + length);
-            if (io) io->TableCopy(imm);
+            TableCopyImmediate imm(decoder, pc + length, validate);
+            (ios.TableCopy(imm), ...);
             return length + imm.length;
           }
           case kExprTableGrow:
           case kExprTableSize:
           case kExprTableFill: {
-            IndexImmediate<validate> imm(decoder, pc + length, "table index");
-            if (io) io->TableIndex(imm);
+            IndexImmediate imm(decoder, pc + length, "table index", validate);
+            (ios.TableIndex(imm), ...);
             return length + imm.length;
           }
           default:
-            if (validate) {
+            if (ValidationTag::validate) {
               decoder->DecodeError(pc, "invalid numeric opcode");
             }
             return length;
@@ -1920,33 +2039,32 @@
       }
       case kSimdPrefix: {
         uint32_t length = 0;
-        opcode = decoder->read_prefixed_opcode<validate>(pc, &length);
+        opcode = decoder->read_prefixed_opcode<ValidationTag>(pc, &length);
         switch (opcode) {
           // clang-format off
           FOREACH_SIMD_0_OPERAND_OPCODE(DECLARE_OPCODE_CASE)
             return length;
           FOREACH_SIMD_1_OPERAND_OPCODE(DECLARE_OPCODE_CASE)
-            if (io) {
-              SimdLaneImmediate<validate> lane_imm(decoder, pc + length);
-              io->SimdLane(lane_imm);
+        if (sizeof...(ios) > 0) {
+              SimdLaneImmediate lane_imm(decoder, pc + length, validate);
+             (ios.SimdLane(lane_imm), ...);
             }
             return length + 1;
           FOREACH_SIMD_MEM_OPCODE(DECLARE_OPCODE_CASE) {
-            MemoryAccessImmediate<validate> imm(decoder, pc + length,
-                                                UINT32_MAX,
-                                                kConservativelyAssumeMemory64);
-            if (io) io->MemoryAccess(imm);
+            MemoryAccessImmediate imm(decoder, pc + length, UINT32_MAX,
+                                      kConservativelyAssumeMemory64, validate);
+            (ios.MemoryAccess(imm), ...);
             return length + imm.length;
           }
           FOREACH_SIMD_MEM_1_OPERAND_OPCODE(DECLARE_OPCODE_CASE) {
-            MemoryAccessImmediate<validate> imm(
+            MemoryAccessImmediate imm(
                 decoder, pc + length, UINT32_MAX,
-                kConservativelyAssumeMemory64);
-            if (io) {
-              SimdLaneImmediate<validate> lane_imm(decoder,
-                                                   pc + length + imm.length);
-              io->MemoryAccess(imm);
-              io->SimdLane(lane_imm);
+                kConservativelyAssumeMemory64, validate);
+        if (sizeof...(ios) > 0) {
+              SimdLaneImmediate lane_imm(decoder,
+                                         pc + length + imm.length, validate);
+             (ios.MemoryAccess(imm), ...);
+             (ios.SimdLane(lane_imm), ...);
             }
             // 1 more byte for lane index immediate.
             return length + imm.length + 1;
@@ -1955,13 +2073,13 @@
           // Shuffles require a byte per lane, or 16 immediate bytes.
           case kExprS128Const:
           case kExprI8x16Shuffle:
-            if (io) {
-              Simd128Immediate<validate> imm(decoder, pc + length);
-              io->S128Const(imm);
+            if (sizeof...(ios) > 0) {
+              Simd128Immediate imm(decoder, pc + length, validate);
+              (ios.S128Const(imm), ...);
             }
             return length + kSimd128Size;
           default:
-            if (validate) {
+            if (ValidationTag::validate) {
               decoder->DecodeError(pc, "invalid SIMD opcode");
             }
             return length;
@@ -1969,14 +2087,13 @@
       }
       case kAtomicPrefix: {
         uint32_t length = 0;
-        opcode = decoder->read_prefixed_opcode<validate>(pc, &length,
-                                                         "atomic_index");
+        opcode = decoder->read_prefixed_opcode<ValidationTag>(pc, &length,
+                                                              "atomic_index");
         switch (opcode) {
           FOREACH_ATOMIC_OPCODE(DECLARE_OPCODE_CASE) {
-            MemoryAccessImmediate<validate> imm(decoder, pc + length,
-                                                UINT32_MAX,
-                                                kConservativelyAssumeMemory64);
-            if (io) io->MemoryAccess(imm);
+            MemoryAccessImmediate imm(decoder, pc + length, UINT32_MAX,
+                                      kConservativelyAssumeMemory64, validate);
+            (ios.MemoryAccess(imm), ...);
             return length + imm.length;
           }
           FOREACH_ATOMIC_0_OPERAND_OPCODE(DECLARE_OPCODE_CASE) {
@@ -1984,7 +2101,7 @@
             return length + 1;
           }
           default:
-            if (validate) {
+            if (ValidationTag::validate) {
               decoder->DecodeError(pc, "invalid Atomics opcode");
             }
             return length;
@@ -1992,21 +2109,21 @@
       }
       case kGCPrefix: {
         uint32_t length = 0;
-        opcode =
-            decoder->read_prefixed_opcode<validate>(pc, &length, "gc_index");
+        opcode = decoder->read_prefixed_opcode<ValidationTag>(pc, &length,
+                                                              "gc_index");
         switch (opcode) {
           case kExprStructNew:
           case kExprStructNewDefault: {
-            StructIndexImmediate<validate> imm(decoder, pc + length);
-            if (io) io->TypeIndex(imm);
+            StructIndexImmediate imm(decoder, pc + length, validate);
+            (ios.TypeIndex(imm), ...);
             return length + imm.length;
           }
           case kExprStructGet:
           case kExprStructGetS:
           case kExprStructGetU:
           case kExprStructSet: {
-            FieldImmediate<validate> imm(decoder, pc + length);
-            if (io) io->Field(imm);
+            FieldImmediate imm(decoder, pc + length, validate);
+            (ios.Field(imm), ...);
             return length + imm.length;
           }
           case kExprArrayNew:
@@ -2016,75 +2133,77 @@
           case kExprArrayGetU:
           case kExprArraySet:
           case kExprArrayLenDeprecated: {
-            ArrayIndexImmediate<validate> imm(decoder, pc + length);
-            if (io) io->TypeIndex(imm);
+            ArrayIndexImmediate imm(decoder, pc + length, validate);
+            (ios.TypeIndex(imm), ...);
             return length + imm.length;
           }
           case kExprArrayNewFixed: {
-            ArrayIndexImmediate<validate> array_imm(decoder, pc + length);
-            IndexImmediate<validate> length_imm(
-                decoder, pc + length + array_imm.length, "array length");
-            if (io) io->TypeIndex(array_imm);
-            if (io) io->Length(length_imm);
+            ArrayIndexImmediate array_imm(decoder, pc + length, validate);
+            IndexImmediate length_imm(decoder, pc + length + array_imm.length,
+                                      "array length", validate);
+            (ios.TypeIndex(array_imm), ...);
+            (ios.Length(length_imm), ...);
             return length + array_imm.length + length_imm.length;
           }
           case kExprArrayCopy: {
-            ArrayIndexImmediate<validate> dst_imm(decoder, pc + length);
-            ArrayIndexImmediate<validate> src_imm(decoder,
-                                                  pc + length + dst_imm.length);
-            if (io) io->ArrayCopy(dst_imm, src_imm);
+            ArrayIndexImmediate dst_imm(decoder, pc + length, validate);
+            ArrayIndexImmediate src_imm(decoder, pc + length + dst_imm.length,
+                                        validate);
+            (ios.ArrayCopy(dst_imm, src_imm), ...);
             return length + dst_imm.length + src_imm.length;
           }
           case kExprArrayNewData:
           case kExprArrayNewElem: {
-            ArrayIndexImmediate<validate> array_imm(decoder, pc + length);
-            IndexImmediate<validate> data_imm(
-                decoder, pc + length + array_imm.length, "segment index");
-            if (io) io->TypeIndex(array_imm);
-            if (io) io->DataSegmentIndex(data_imm);
+            ArrayIndexImmediate array_imm(decoder, pc + length, validate);
+            IndexImmediate data_imm(decoder, pc + length + array_imm.length,
+                                    "segment index", validate);
+            (ios.TypeIndex(array_imm), ...);
+            (ios.DataSegmentIndex(data_imm), ...);
             return length + array_imm.length + data_imm.length;
           }
           case kExprBrOnArray:
-          case kExprBrOnData:
+          case kExprBrOnStruct:
           case kExprBrOnI31:
           case kExprBrOnNonArray:
-          case kExprBrOnNonData:
+          case kExprBrOnNonStruct:
           case kExprBrOnNonI31: {
-            BranchDepthImmediate<validate> imm(decoder, pc + length);
-            if (io) io->BranchDepth(imm);
+            BranchDepthImmediate imm(decoder, pc + length, validate);
+            (ios.BranchDepth(imm), ...);
             return length + imm.length;
           }
+          case kExprRefCast:
+          case kExprRefCastNull:
           case kExprRefTest:
           case kExprRefTestNull: {
-            HeapTypeImmediate<validate> imm(WasmFeatures::All(), decoder,
-                                            pc + length, nullptr);
-            if (io) io->HeapType(imm);
+            HeapTypeImmediate imm(WasmFeatures::All(), decoder, pc + length,
+                                  validate);
+            (ios.HeapType(imm), ...);
             return length + imm.length;
           }
           case kExprRefTestDeprecated:
-          case kExprRefCast:
+          case kExprRefCastDeprecated:
           case kExprRefCastNop: {
-            IndexImmediate<validate> imm(decoder, pc + length, "type index");
-            if (io) io->TypeIndex(imm);
+            IndexImmediate imm(decoder, pc + length, "type index", validate);
+            (ios.TypeIndex(imm), ...);
             return length + imm.length;
           }
           case kExprBrOnCast:
           case kExprBrOnCastFail: {
-            BranchDepthImmediate<validate> branch(decoder, pc + length);
-            IndexImmediate<validate> index(decoder, pc + length + branch.length,
-                                           "type index");
-            if (io) io->BranchDepth(branch);
-            if (io) io->TypeIndex(index);
+            BranchDepthImmediate branch(decoder, pc + length, validate);
+            IndexImmediate index(decoder, pc + length + branch.length,
+                                 "type index", validate);
+            (ios.BranchDepth(branch), ...);
+            (ios.TypeIndex(index), ...);
             return length + branch.length + index.length;
           }
           case kExprI31New:
           case kExprI31GetS:
           case kExprI31GetU:
           case kExprRefAsArray:
-          case kExprRefAsData:
+          case kExprRefAsStruct:
           case kExprRefAsI31:
           case kExprRefIsArray:
-          case kExprRefIsData:
+          case kExprRefIsStruct:
           case kExprRefIsI31:
           case kExprExternInternalize:
           case kExprExternExternalize:
@@ -2102,13 +2221,13 @@
           case kExprStringNewWtf16:
           case kExprStringEncodeWtf16:
           case kExprStringViewWtf16Encode: {
-            MemoryIndexImmediate<validate> imm(decoder, pc + length);
-            if (io) io->MemoryIndex(imm);
+            MemoryIndexImmediate imm(decoder, pc + length, validate);
+            (ios.MemoryIndex(imm), ...);
             return length + imm.length;
           }
           case kExprStringConst: {
-            StringConstImmediate<validate> imm(decoder, pc + length);
-            if (io) io->StringConst(imm);
+            StringConstImmediate imm(decoder, pc + length, validate);
+            (ios.StringConst(imm), ...);
             return length + imm.length;
           }
           case kExprStringMeasureUtf8:
@@ -2140,7 +2259,7 @@
             return length;
           default:
             // This is unreachable except for malformed modules.
-            if (validate) {
+            if (ValidationTag::validate) {
               decoder->DecodeError(pc, "invalid gc opcode");
             }
             return length;
@@ -2164,7 +2283,7 @@
 #undef DECLARE_OPCODE_CASE
     }
     // Invalid modules will reach this point.
-    if (validate) {
+    if (ValidationTag::validate) {
       decoder->DecodeError(pc, "invalid opcode");
     }
     return 1;
@@ -2214,19 +2333,19 @@
       case kExprMemorySize:
         return {0, 1};
       case kExprCallFunction: {
-        CallFunctionImmediate<validate> imm(this, pc + 1);
+        CallFunctionImmediate imm(this, pc + 1, validate);
         CHECK(Validate(pc + 1, imm));
         return {imm.sig->parameter_count(), imm.sig->return_count()};
       }
       case kExprCallIndirect: {
-        CallIndirectImmediate<validate> imm(this, pc + 1);
+        CallIndirectImmediate imm(this, pc + 1, validate);
         CHECK(Validate(pc + 1, imm));
         // Indirect calls pop an additional argument for the table index.
         return {imm.sig->parameter_count() + 1,
                 imm.sig->return_count()};
       }
       case kExprThrow: {
-        TagIndexImmediate<validate> imm(this, pc + 1);
+        TagIndexImmediate imm(this, pc + 1, validate);
         CHECK(Validate(pc + 1, imm));
         DCHECK_EQ(0, imm.tag->sig->return_count());
         return {imm.tag->sig->parameter_count(), 0};
@@ -2251,7 +2370,7 @@
       case kNumericPrefix:
       case kAtomicPrefix:
       case kSimdPrefix: {
-        opcode = this->read_prefixed_opcode<validate>(pc);
+        opcode = this->read_prefixed_opcode<ValidationTag>(pc);
         switch (opcode) {
           FOREACH_SIMD_1_OPERAND_1_PARAM_OPCODE(DECLARE_OPCODE_CASE)
             return {1, 1};
@@ -2278,7 +2397,7 @@
       }
       case kGCPrefix: {
         uint32_t unused_length;
-        opcode = this->read_prefixed_opcode<validate>(pc, &unused_length);
+        opcode = this->read_prefixed_opcode<ValidationTag>(pc, &unused_length);
         switch (opcode) {
           case kExprStructGet:
           case kExprStructGetS:
@@ -2293,6 +2412,8 @@
           case kExprRefTestNull:
           case kExprRefTestDeprecated:
           case kExprRefCast:
+          case kExprRefCastNull:
+          case kExprRefCastDeprecated:
           case kExprRefCastNop:
           case kExprBrOnCast:
           case kExprBrOnCastFail:
@@ -2313,14 +2434,14 @@
           case kExprStructNewDefault:
             return {0, 1};
           case kExprStructNew: {
-            StructIndexImmediate<validate> imm(this, pc + 2);
+            StructIndexImmediate imm(this, pc + 2, validate);
             CHECK(Validate(pc + 2, imm));
             return {imm.struct_type->field_count(), 1};
           }
           case kExprArrayNewFixed: {
-            ArrayIndexImmediate<validate> array_imm(this, pc + 2);
-            IndexImmediate<validate> length_imm(this, pc + 2 + array_imm.length,
-                                                "array length");
+            ArrayIndexImmediate array_imm(this, pc + 2, validate);
+            IndexImmediate length_imm(this, pc + 2 + array_imm.length,
+                                                "array length", validate);
             return {length_imm.index, 1};
           }
           case kExprStringConst:
@@ -2381,6 +2502,8 @@
     // clang-format on
   }
 
+  static constexpr ValidationTag validate = {};
+
   Zone* const compilation_zone_;
 
   ValueType* local_types_ = nullptr;
@@ -2421,9 +2544,9 @@
     }                                                           \
   } while (false)
 
-template <Decoder::ValidateFlag validate, typename Interface,
+template <typename ValidationTag, typename Interface,
           DecodingMode decoding_mode = kFunctionBody>
-class WasmFullDecoder : public WasmDecoder<validate, decoding_mode> {
+class WasmFullDecoder : public WasmDecoder<ValidationTag, decoding_mode> {
   using Value = typename Interface::Value;
   using Control = typename Interface::Control;
   using ArgVector = base::Vector<Value>;
@@ -2438,18 +2561,21 @@
   WasmFullDecoder(Zone* zone, const WasmModule* module,
                   const WasmFeatures& enabled, WasmFeatures* detected,
                   const FunctionBody& body, InterfaceArgs&&... interface_args)
-      : WasmDecoder<validate, decoding_mode>(zone, module, enabled, detected,
-                                             body.sig, body.start, body.end,
-                                             body.offset),
-        interface_(std::forward<InterfaceArgs>(interface_args)...),
-        initialized_locals_(zone),
-        locals_initializers_stack_(zone),
-        control_(zone) {}
+      : WasmDecoder<ValidationTag, decoding_mode>(
+            zone, module, enabled, detected, body.sig, body.start, body.end,
+            body.offset),
+        interface_(std::forward<InterfaceArgs>(interface_args)...) {}
+
+  ~WasmFullDecoder() {
+    control_.Reset(this->compilation_zone_);
+    stack_.Reset(this->compilation_zone_);
+    locals_initializers_stack_.Reset(this->compilation_zone_);
+  }
 
   Interface& interface() { return interface_; }
 
   bool Decode() {
-    DCHECK_EQ(stack_end_, stack_);
+    DCHECK(stack_.empty());
     DCHECK(control_.empty());
     DCHECK_LE(this->pc_, this->end_);
     DCHECK_EQ(this->num_locals(), 0);
@@ -2457,7 +2583,7 @@
     locals_offset_ = this->pc_offset();
     uint32_t locals_length;
     this->DecodeLocals(this->pc(), &locals_length);
-    if (this->failed()) return TraceFailed();
+    if (!VALIDATE(this->ok())) return TraceFailed();
     this->consume_bytes(locals_length);
     int non_defaultable = 0;
     uint32_t params_count =
@@ -2470,7 +2596,7 @@
     // Cannot use CALL_INTERFACE_* macros because control is empty.
     interface().StartFunction(this);
     DecodeFunctionBody();
-    if (this->failed()) return TraceFailed();
+    if (!VALIDATE(this->ok())) return TraceFailed();
 
     if (!VALIDATE(control_.empty())) {
       if (control_.size() > 1) {
@@ -2483,13 +2609,14 @@
     }
     // Cannot use CALL_INTERFACE_* macros because control is empty.
     interface().FinishFunction(this);
-    if (this->failed()) return TraceFailed();
+    if (!VALIDATE(this->ok())) return TraceFailed();
 
     TRACE("wasm-decode ok\n\n");
     return true;
   }
 
   bool TraceFailed() {
+    if constexpr (!ValidationTag::validate) UNREACHABLE();
     if (this->error_.offset()) {
       TRACE("wasm-error module+%-6d func+%d: %s\n\n", this->error_.offset(),
             this->GetBufferRelativeOffset(this->error_.offset()),
@@ -2507,7 +2634,8 @@
     if (!WasmOpcodes::IsPrefixOpcode(opcode)) {
       return WasmOpcodes::OpcodeName(static_cast<WasmOpcode>(opcode));
     }
-    opcode = this->template read_prefixed_opcode<Decoder::kFullValidation>(pc);
+    opcode =
+        this->template read_prefixed_opcode<Decoder::FullValidationTag>(pc);
     return WasmOpcodes::OpcodeName(opcode);
   }
 
@@ -2523,19 +2651,15 @@
 
   Control* control_at(uint32_t depth) {
     DCHECK_GT(control_.size(), depth);
-    return &control_.back() - depth;
+    return control_.end() - 1 - depth;
   }
 
-  uint32_t stack_size() const {
-    DCHECK_GE(stack_end_, stack_);
-    DCHECK_GE(kMaxUInt32, stack_end_ - stack_);
-    return static_cast<uint32_t>(stack_end_ - stack_);
-  }
+  uint32_t stack_size() const { return stack_.size(); }
 
   Value* stack_value(uint32_t depth) const {
     DCHECK_LT(0, depth);
-    DCHECK_GE(stack_size(), depth);
-    return stack_end_ - depth;
+    DCHECK_GE(stack_.size(), depth);
+    return stack_.end() - depth;
   }
 
   int32_t current_catch() const { return current_catch_; }
@@ -2557,17 +2681,19 @@
   }
 
   bool is_local_initialized(uint32_t local_index) {
+    DCHECK_GT(this->num_locals_, local_index);
     if (!has_nondefaultable_locals_) return true;
     return initialized_locals_[local_index];
   }
 
   void set_local_initialized(uint32_t local_index) {
+    DCHECK_GT(this->num_locals_, local_index);
     if (!has_nondefaultable_locals_) return;
     // This implicitly covers defaultable locals too (which are always
     // initialized).
     if (is_local_initialized(local_index)) return;
     initialized_locals_[local_index] = true;
-    locals_initializers_stack_.push_back(local_index);
+    locals_initializers_stack_.push(local_index);
   }
 
   uint32_t locals_initialization_stack_depth() const {
@@ -2579,7 +2705,7 @@
     uint32_t previous_stack_height = c->init_stack_depth;
     while (locals_initializers_stack_.size() > previous_stack_height) {
       uint32_t local_index = locals_initializers_stack_.back();
-      locals_initializers_stack_.pop_back();
+      locals_initializers_stack_.pop();
       initialized_locals_[local_index] = false;
     }
   }
@@ -2587,18 +2713,18 @@
   void InitializeInitializedLocalsTracking(int non_defaultable_locals) {
     has_nondefaultable_locals_ = non_defaultable_locals > 0;
     if (!has_nondefaultable_locals_) return;
-    initialized_locals_.assign(this->num_locals_, false);
-    // Parameters count as initialized...
+    initialized_locals_ =
+        this->compilation_zone_->template NewArray<bool>(this->num_locals_);
+    // Parameters are always initialized.
     const size_t num_params = this->sig_->parameter_count();
-    for (size_t i = 0; i < num_params; i++) {
-      initialized_locals_[i] = true;
-    }
-    // ...and so do defaultable locals.
+    std::fill_n(initialized_locals_, num_params, true);
+    // Locals are initialized if they are defaultable.
     for (size_t i = num_params; i < this->num_locals_; i++) {
-      if (this->local_types_[i].is_defaultable()) initialized_locals_[i] = true;
+      initialized_locals_[i] = this->local_types_[i].is_defaultable();
     }
-    if (non_defaultable_locals == 0) return;
-    locals_initializers_stack_.reserve(non_defaultable_locals);
+    DCHECK(locals_initializers_stack_.empty());
+    locals_initializers_stack_.EnsureMoreCapacity(non_defaultable_locals,
+                                                  this->compilation_zone_);
   }
 
   void DecodeFunctionBody() {
@@ -2611,6 +2737,7 @@
       DCHECK(control_.empty());
       constexpr uint32_t kStackDepth = 0;
       constexpr uint32_t kInitStackDepth = 0;
+      control_.EnsureMoreCapacity(1, this->compilation_zone_);
       control_.emplace_back(kControlBlock, kStackDepth, kInitStackDepth,
                             this->pc_, kReachable);
       Control* c = &control_.back();
@@ -2638,7 +2765,7 @@
         // and binary operations, local.get, constants, ...). Thus check that
         // there is enough space for those operations centrally, and avoid any
         // bounds checks in those operations.
-        EnsureStackSpace(1);
+        stack_.EnsureMoreCapacity(1, this->compilation_zone_);
         uint8_t first_byte = *this->pc_;
         WasmOpcode opcode = static_cast<WasmOpcode>(first_byte);
         CALL_INTERFACE_IF_OK_AND_REACHABLE(NextInstruction, opcode);
@@ -2676,7 +2803,7 @@
         // and binary operations, local.get, constants, ...). Thus check that
         // there is enough space for those operations centrally, and avoid any
         // bounds checks in those operations.
-        EnsureStackSpace(1);
+        stack_.EnsureMoreCapacity(1, this->compilation_zone_);
         uint8_t first_byte = *this->pc_;
         WasmOpcode opcode = static_cast<WasmOpcode>(first_byte);
         CALL_INTERFACE_IF_OK_AND_REACHABLE(NextInstruction, opcode);
@@ -2696,24 +2823,21 @@
   Interface interface_;
 
   // The value stack, stored as individual pointers for maximum performance.
-  Value* stack_ = nullptr;
-  Value* stack_end_ = nullptr;
-  Value* stack_capacity_end_ = nullptr;
-  ASSERT_TRIVIALLY_COPYABLE(Value);
+  FastZoneVector<Value> stack_;
 
   // Indicates whether the local with the given index is currently initialized.
-  // Entries for defaultable locals are meaningless; we have a bit for each
+  // Entries for defaultable locals are meaningless; we have a byte for each
   // local because we expect that the effort required to densify this bit
   // vector would more than offset the memory savings.
-  ZoneVector<bool> initialized_locals_;
+  bool* initialized_locals_;
   // Keeps track of initializing assignments to non-defaultable locals that
   // happened, so they can be discarded at the end of the current block.
   // Contains no duplicates, so the size of this stack is bounded (and pre-
   // allocated) to the number of non-defaultable locals in the function.
-  ZoneVector<uint32_t> locals_initializers_stack_;
+  FastZoneVector<uint32_t> locals_initializers_stack_;
 
-  // stack of blocks, loops, and ifs.
-  ZoneVector<Control> control_;
+  // Control stack (blocks, loops, ifs, ...).
+  FastZoneVector<Control> control_;
 
   // Controls whether code should be generated for the current block (basically
   // a cache for {ok() && control_.back().reachable()}).
@@ -2741,10 +2865,10 @@
     return true;
   }
 
-  MemoryAccessImmediate<validate> MakeMemoryAccessImmediate(
-      uint32_t pc_offset, uint32_t max_alignment) {
-    return MemoryAccessImmediate<validate>(
-        this, this->pc_ + pc_offset, max_alignment, this->module_->is_memory64);
+  MemoryAccessImmediate MakeMemoryAccessImmediate(uint32_t pc_offset,
+                                                  uint32_t max_alignment) {
+    return MemoryAccessImmediate(this, this->pc_ + pc_offset, max_alignment,
+                                 this->module_->is_memory64, validate);
   }
 
 #ifdef DEBUG
@@ -2813,7 +2937,7 @@
         if (!c.reachable()) Append("%c", c.unreachable() ? '*' : '#');
       }
       Append(" | ");
-      for (size_t i = 0; i < decoder_->stack_size(); ++i) {
+      for (uint32_t i = 0; i < decoder_->stack_.size(); ++i) {
         Value& val = decoder_->stack_[i];
         Append(" %c", val.type.short_name());
       }
@@ -2874,8 +2998,7 @@
 #undef BUILD_SIMPLE_OPCODE
 
   DECODE(Block) {
-    BlockTypeImmediate<validate> imm(this->enabled_, this, this->pc_ + 1,
-                                     this->module_);
+    BlockTypeImmediate imm(this->enabled_, this, this->pc_ + 1, validate);
     if (!this->Validate(this->pc_ + 1, imm)) return 0;
     ArgVector args = PeekArgs(imm.sig);
     Control* block = PushControl(kControlBlock, args.length());
@@ -2887,8 +3010,8 @@
   }
 
   DECODE(Rethrow) {
-    CHECK_PROTOTYPE_OPCODE(eh);
-    BranchDepthImmediate<validate> imm(this, this->pc_ + 1);
+    this->detected_->Add(kFeature_eh);
+    BranchDepthImmediate imm(this, this->pc_ + 1, validate);
     if (!this->Validate(this->pc_ + 1, imm, control_.size())) return 0;
     Control* c = control_at(imm.depth);
     if (!VALIDATE(c->is_try_catchall() || c->is_try_catch())) {
@@ -2901,8 +3024,8 @@
   }
 
   DECODE(Throw) {
-    CHECK_PROTOTYPE_OPCODE(eh);
-    TagIndexImmediate<validate> imm(this, this->pc_ + 1);
+    this->detected_->Add(kFeature_eh);
+    TagIndexImmediate imm(this, this->pc_ + 1, validate);
     if (!this->Validate(this->pc_ + 1, imm)) return 0;
     ArgVector args = PeekArgs(imm.tag->ToFunctionSig());
     CALL_INTERFACE_IF_OK_AND_REACHABLE(Throw, imm, base::VectorOf(args));
@@ -2912,9 +3035,8 @@
   }
 
   DECODE(Try) {
-    CHECK_PROTOTYPE_OPCODE(eh);
-    BlockTypeImmediate<validate> imm(this->enabled_, this, this->pc_ + 1,
-                                     this->module_);
+    this->detected_->Add(kFeature_eh);
+    BlockTypeImmediate imm(this->enabled_, this, this->pc_ + 1, validate);
     if (!this->Validate(this->pc_ + 1, imm)) return 0;
     ArgVector args = PeekArgs(imm.sig);
     Control* try_block = PushControl(kControlTry, args.length());
@@ -2928,8 +3050,8 @@
   }
 
   DECODE(Catch) {
-    CHECK_PROTOTYPE_OPCODE(eh);
-    TagIndexImmediate<validate> imm(this, this->pc_ + 1);
+    this->detected_->Add(kFeature_eh);
+    TagIndexImmediate imm(this, this->pc_ + 1, validate);
     if (!this->Validate(this->pc_ + 1, imm)) return 0;
     DCHECK(!control_.empty());
     Control* c = &control_.back();
@@ -2945,23 +3067,24 @@
     c->kind = kControlTryCatch;
     // TODO(jkummerow): Consider moving the stack manipulation after the
     // INTERFACE call for consistency.
-    DCHECK_LE(stack_ + c->stack_depth, stack_end_);
-    stack_end_ = stack_ + c->stack_depth;
+    stack_.shrink_to(c->stack_depth);
     c->reachability = control_at(1)->innerReachability();
     RollbackLocalsInitialization(c);
     const WasmTagSig* sig = imm.tag->sig;
-    EnsureStackSpace(static_cast<int>(sig->parameter_count()));
+    stack_.EnsureMoreCapacity(static_cast<int>(sig->parameter_count()),
+                              this->compilation_zone_);
     for (ValueType type : sig->parameters()) Push(CreateValue(type));
-    base::Vector<Value> values(stack_ + c->stack_depth, sig->parameter_count());
+    base::Vector<Value> values(stack_.begin() + c->stack_depth,
+                               sig->parameter_count());
     current_catch_ = c->previous_catch;  // Pop try scope.
     CALL_INTERFACE_IF_OK_AND_PARENT_REACHABLE(CatchException, imm, c, values);
-    current_code_reachable_and_ok_ = this->ok() && c->reachable();
+    current_code_reachable_and_ok_ = VALIDATE(this->ok()) && c->reachable();
     return 1 + imm.length;
   }
 
   DECODE(Delegate) {
-    CHECK_PROTOTYPE_OPCODE(eh);
-    BranchDepthImmediate<validate> imm(this, this->pc_ + 1);
+    this->detected_->Add(kFeature_eh);
+    BranchDepthImmediate imm(this, this->pc_ + 1, validate);
     // -1 because the current try block is not included in the count.
     if (!this->Validate(this->pc_ + 1, imm, control_depth() - 1)) return 0;
     Control* c = &control_.back();
@@ -2986,7 +3109,7 @@
   }
 
   DECODE(CatchAll) {
-    CHECK_PROTOTYPE_OPCODE(eh);
+    this->detected_->Add(kFeature_eh);
     DCHECK(!control_.empty());
     Control* c = &control_.back();
     if (!VALIDATE(c->is_try())) {
@@ -3003,14 +3126,14 @@
     RollbackLocalsInitialization(c);
     current_catch_ = c->previous_catch;  // Pop try scope.
     CALL_INTERFACE_IF_OK_AND_PARENT_REACHABLE(CatchAll, c);
-    stack_end_ = stack_ + c->stack_depth;
-    current_code_reachable_and_ok_ = this->ok() && c->reachable();
+    stack_.shrink_to(c->stack_depth);
+    current_code_reachable_and_ok_ = VALIDATE(this->ok()) && c->reachable();
     return 1;
   }
 
   DECODE(BrOnNull) {
     CHECK_PROTOTYPE_OPCODE(typed_funcref);
-    BranchDepthImmediate<validate> imm(this, this->pc_ + 1);
+    BranchDepthImmediate imm(this, this->pc_ + 1, validate);
     if (!this->Validate(this->pc_ + 1, imm, control_.size())) return 0;
     Value ref_object = Peek(0);
     Control* c = control_at(imm.depth);
@@ -3047,7 +3170,7 @@
 
   DECODE(BrOnNonNull) {
     CHECK_PROTOTYPE_OPCODE(gc);
-    BranchDepthImmediate<validate> imm(this, this->pc_ + 1);
+    BranchDepthImmediate imm(this, this->pc_ + 1, validate);
     if (!this->Validate(this->pc_ + 1, imm, control_.size())) return 0;
     Value ref_object = Peek(0);
     if (!VALIDATE(ref_object.type.is_object_reference() ||
@@ -3100,8 +3223,7 @@
   }
 
   DECODE(Loop) {
-    BlockTypeImmediate<validate> imm(this->enabled_, this, this->pc_ + 1,
-                                     this->module_);
+    BlockTypeImmediate imm(this->enabled_, this, this->pc_ + 1, validate);
     if (!this->Validate(this->pc_ + 1, imm)) return 0;
     ArgVector args = PeekArgs(imm.sig);
     Control* block = PushControl(kControlLoop, args.length());
@@ -3113,8 +3235,7 @@
   }
 
   DECODE(If) {
-    BlockTypeImmediate<validate> imm(this->enabled_, this, this->pc_ + 1,
-                                     this->module_);
+    BlockTypeImmediate imm(this->enabled_, this, this->pc_ + 1, validate);
     if (!this->Validate(this->pc_ + 1, imm)) return 0;
     Value cond = Peek(0, 0, kWasmI32);
     ArgVector args = PeekArgs(imm.sig, 1);
@@ -3146,7 +3267,7 @@
     RollbackLocalsInitialization(c);
     PushMergeValues(c, &c->start_merge);
     c->reachability = control_at(1)->innerReachability();
-    current_code_reachable_and_ok_ = this->ok() && c->reachable();
+    current_code_reachable_and_ok_ = VALIDATE(this->ok()) && c->reachable();
     return 1;
   }
 
@@ -3165,7 +3286,7 @@
         c->reachability = control_at(1)->innerReachability();
         CALL_INTERFACE_IF_OK_AND_PARENT_REACHABLE(CatchAll, c);
         current_code_reachable_and_ok_ =
-            this->ok() && control_.back().reachable();
+            VALIDATE(this->ok()) && control_.back().reachable();
         CALL_INTERFACE_IF_OK_AND_REACHABLE(Rethrow, c);
         EndControl();
         PopControl();
@@ -3190,7 +3311,7 @@
       // The result of the block is the return value.
       trace_msg->Append("\n" TRACE_INST_FORMAT, startrel(this->pc_),
                         "(implicit) return");
-      control_.clear();
+      control_.pop();
       return 1;
     }
 
@@ -3218,9 +3339,9 @@
 
   DECODE(SelectWithType) {
     this->detected_->Add(kFeature_reftypes);
-    SelectTypeImmediate<validate> imm(this->enabled_, this, this->pc_ + 1,
-                                      this->module_);
-    if (this->failed()) return 0;
+    SelectTypeImmediate imm(this->enabled_, this, this->pc_ + 1, validate);
+    this->Validate(this->pc_ + 1, imm);
+    if (!VALIDATE(this->ok())) return 0;
     Value cond = Peek(0, 2, kWasmI32);
     Value fval = Peek(1, 1, imm.type);
     Value tval = Peek(2, 0, imm.type);
@@ -3232,7 +3353,7 @@
   }
 
   DECODE(Br) {
-    BranchDepthImmediate<validate> imm(this, this->pc_ + 1);
+    BranchDepthImmediate imm(this, this->pc_ + 1, validate);
     if (!this->Validate(this->pc_ + 1, imm, control_.size())) return 0;
     Control* c = control_at(imm.depth);
     if (!VALIDATE(TypeCheckBranch<false>(c, 0))) return 0;
@@ -3245,7 +3366,7 @@
   }
 
   DECODE(BrIf) {
-    BranchDepthImmediate<validate> imm(this, this->pc_ + 1);
+    BranchDepthImmediate imm(this, this->pc_ + 1, validate);
     if (!this->Validate(this->pc_ + 1, imm, control_.size())) return 0;
     Value cond = Peek(0, 0, kWasmI32);
     Control* c = control_at(imm.depth);
@@ -3259,10 +3380,10 @@
   }
 
   DECODE(BrTable) {
-    BranchTableImmediate<validate> imm(this, this->pc_ + 1);
-    BranchTableIterator<validate> iterator(this, imm);
+    BranchTableImmediate imm(this, this->pc_ + 1, validate);
+    BranchTableIterator<ValidationTag> iterator(this, imm);
     Value key = Peek(0, 0, kWasmI32);
-    if (this->failed()) return 0;
+    if (!VALIDATE(this->ok())) return 0;
     if (!this->Validate(this->pc_ + 1, imm, control_.size())) return 0;
 
     // Cache the branch targets during the iteration, so that we can set
@@ -3283,7 +3404,7 @@
       if (br_targets[target]) continue;
       br_targets[target] = true;
 
-      if (validate) {
+      if (ValidationTag::validate) {
         if (index == 0) {
           arity = control_at(target)->br_merge()->arity;
         } else if (!VALIDATE(control_at(target)->br_merge()->arity == arity)) {
@@ -3319,7 +3440,7 @@
   }
 
   DECODE(I32Const) {
-    ImmI32Immediate<validate> imm(this, this->pc_ + 1);
+    ImmI32Immediate imm(this, this->pc_ + 1, validate);
     Value value = CreateValue(kWasmI32);
     CALL_INTERFACE_IF_OK_AND_REACHABLE(I32Const, &value, imm.value);
     Push(value);
@@ -3327,7 +3448,7 @@
   }
 
   DECODE(I64Const) {
-    ImmI64Immediate<validate> imm(this, this->pc_ + 1);
+    ImmI64Immediate imm(this, this->pc_ + 1, validate);
     Value value = CreateValue(kWasmI64);
     CALL_INTERFACE_IF_OK_AND_REACHABLE(I64Const, &value, imm.value);
     Push(value);
@@ -3335,7 +3456,7 @@
   }
 
   DECODE(F32Const) {
-    ImmF32Immediate<validate> imm(this, this->pc_ + 1);
+    ImmF32Immediate imm(this, this->pc_ + 1, validate);
     Value value = CreateValue(kWasmF32);
     CALL_INTERFACE_IF_OK_AND_REACHABLE(F32Const, &value, imm.value);
     Push(value);
@@ -3343,7 +3464,7 @@
   }
 
   DECODE(F64Const) {
-    ImmF64Immediate<validate> imm(this, this->pc_ + 1);
+    ImmF64Immediate imm(this, this->pc_ + 1, validate);
     Value value = CreateValue(kWasmF64);
     CALL_INTERFACE_IF_OK_AND_REACHABLE(F64Const, &value, imm.value);
     Push(value);
@@ -3352,8 +3473,8 @@
 
   DECODE(RefNull) {
     this->detected_->Add(kFeature_reftypes);
-    HeapTypeImmediate<validate> imm(this->enabled_, this, this->pc_ + 1,
-                                    this->module_);
+    HeapTypeImmediate imm(this->enabled_, this, this->pc_ + 1, validate);
+    this->Validate(this->pc_ + 1, imm);
     if (!VALIDATE(this->ok())) return 0;
     ValueType type = ValueType::RefNull(imm.type);
     Value value = CreateValue(type);
@@ -3383,7 +3504,7 @@
         Push(result);
         return 1;
       default:
-        if (validate) {
+        if (ValidationTag::validate) {
           PopTypeError(0, value, "reference type");
           return 0;
         }
@@ -3393,7 +3514,7 @@
 
   DECODE(RefFunc) {
     this->detected_->Add(kFeature_reftypes);
-    IndexImmediate<validate> imm(this, this->pc_ + 1, "function index");
+    IndexImmediate imm(this, this->pc_ + 1, "function index", validate);
     if (!this->ValidateFunction(this->pc_ + 1, imm)) return 0;
     HeapType heap_type(this->enabled_.has_typed_funcref()
                            ? this->module_->functions[imm.index].sig_index
@@ -3421,7 +3542,7 @@
         return 1;
       }
       default:
-        if (validate) {
+        if (ValidationTag::validate) {
           PopTypeError(0, value, "reference type");
         }
         return 0;
@@ -3429,7 +3550,7 @@
   }
 
   V8_INLINE DECODE(LocalGet) {
-    IndexImmediate<validate> imm(this, this->pc_ + 1, "local index");
+    IndexImmediate imm(this, this->pc_ + 1, "local index", validate);
     if (!this->ValidateLocal(this->pc_ + 1, imm)) return 0;
     if (!VALIDATE(this->is_local_initialized(imm.index))) {
       this->DecodeError(this->pc_, "uninitialized non-defaultable local: %u",
@@ -3443,7 +3564,7 @@
   }
 
   DECODE(LocalSet) {
-    IndexImmediate<validate> imm(this, this->pc_ + 1, "local index");
+    IndexImmediate imm(this, this->pc_ + 1, "local index", validate);
     if (!this->ValidateLocal(this->pc_ + 1, imm)) return 0;
     Value value = Peek(0, 0, this->local_type(imm.index));
     CALL_INTERFACE_IF_OK_AND_REACHABLE(LocalSet, value, imm);
@@ -3453,7 +3574,7 @@
   }
 
   DECODE(LocalTee) {
-    IndexImmediate<validate> imm(this, this->pc_ + 1, "local index");
+    IndexImmediate imm(this, this->pc_ + 1, "local index", validate);
     if (!this->ValidateLocal(this->pc_ + 1, imm)) return 0;
     ValueType local_type = this->local_type(imm.index);
     Value value = Peek(0, 0, local_type);
@@ -3473,7 +3594,7 @@
   }
 
   DECODE(GlobalGet) {
-    GlobalIndexImmediate<validate> imm(this, this->pc_ + 1);
+    GlobalIndexImmediate imm(this, this->pc_ + 1, validate);
     if (!this->Validate(this->pc_ + 1, imm)) return 0;
     Value result = CreateValue(imm.global->type);
     CALL_INTERFACE_IF_OK_AND_REACHABLE(GlobalGet, &result, imm);
@@ -3482,7 +3603,7 @@
   }
 
   DECODE(GlobalSet) {
-    GlobalIndexImmediate<validate> imm(this, this->pc_ + 1);
+    GlobalIndexImmediate imm(this, this->pc_ + 1, validate);
     if (!this->Validate(this->pc_ + 1, imm)) return 0;
     if (!VALIDATE(imm.global->mutability)) {
       this->DecodeError("immutable global #%u cannot be assigned", imm.index);
@@ -3496,7 +3617,7 @@
 
   DECODE(TableGet) {
     this->detected_->Add(kFeature_reftypes);
-    IndexImmediate<validate> imm(this, this->pc_ + 1, "table index");
+    IndexImmediate imm(this, this->pc_ + 1, "table index", validate);
     if (!this->ValidateTable(this->pc_ + 1, imm)) return 0;
     Value index = Peek(0, 0, kWasmI32);
     Value result = CreateValue(this->module_->tables[imm.index].type);
@@ -3508,7 +3629,7 @@
 
   DECODE(TableSet) {
     this->detected_->Add(kFeature_reftypes);
-    IndexImmediate<validate> imm(this, this->pc_ + 1, "table index");
+    IndexImmediate imm(this, this->pc_ + 1, "table index", validate);
     if (!this->ValidateTable(this->pc_ + 1, imm)) return 0;
     Value value = Peek(0, 1, this->module_->tables[imm.index].type);
     Value index = Peek(1, 0, kWasmI32);
@@ -3522,7 +3643,7 @@
   DECODE(StoreMem) { return DecodeStoreMem(GetStoreType(opcode)); }
 
   DECODE(MemoryGrow) {
-    MemoryIndexImmediate<validate> imm(this, this->pc_ + 1);
+    MemoryIndexImmediate imm(this, this->pc_ + 1, validate);
     if (!this->Validate(this->pc_ + 1, imm)) return 0;
     // This opcode will not be emitted by the asm translator.
     DCHECK_EQ(kWasmOrigin, this->module_->origin);
@@ -3536,7 +3657,7 @@
   }
 
   DECODE(MemorySize) {
-    MemoryIndexImmediate<validate> imm(this, this->pc_ + 1);
+    MemoryIndexImmediate imm(this, this->pc_ + 1, validate);
     if (!this->Validate(this->pc_ + 1, imm)) return 0;
     ValueType result_type = this->module_->is_memory64 ? kWasmI64 : kWasmI32;
     Value result = CreateValue(result_type);
@@ -3546,7 +3667,7 @@
   }
 
   DECODE(CallFunction) {
-    CallFunctionImmediate<validate> imm(this, this->pc_ + 1);
+    CallFunctionImmediate imm(this, this->pc_ + 1, validate);
     if (!this->Validate(this->pc_ + 1, imm)) return 0;
     ArgVector args = PeekArgs(imm.sig);
     ReturnVector returns = CreateReturnValues(imm.sig);
@@ -3558,7 +3679,7 @@
   }
 
   DECODE(CallIndirect) {
-    CallIndirectImmediate<validate> imm(this, this->pc_ + 1);
+    CallIndirectImmediate imm(this, this->pc_ + 1, validate);
     if (!this->Validate(this->pc_ + 1, imm)) return 0;
     Value index =
         Peek(0, static_cast<int>(imm.sig->parameter_count()), kWasmI32);
@@ -3574,7 +3695,7 @@
 
   DECODE(ReturnCall) {
     CHECK_PROTOTYPE_OPCODE(return_call);
-    CallFunctionImmediate<validate> imm(this, this->pc_ + 1);
+    CallFunctionImmediate imm(this, this->pc_ + 1, validate);
     if (!this->Validate(this->pc_ + 1, imm)) return 0;
     if (!VALIDATE(this->CanReturnCall(imm.sig))) {
       this->DecodeError("%s: %s", WasmOpcodes::OpcodeName(kExprReturnCall),
@@ -3590,7 +3711,7 @@
 
   DECODE(ReturnCallIndirect) {
     CHECK_PROTOTYPE_OPCODE(return_call);
-    CallIndirectImmediate<validate> imm(this, this->pc_ + 1);
+    CallIndirectImmediate imm(this, this->pc_ + 1, validate);
     if (!this->Validate(this->pc_ + 1, imm)) return 0;
     if (!VALIDATE(this->CanReturnCall(imm.sig))) {
       this->DecodeError("%s: %s",
@@ -3611,32 +3732,22 @@
   // TODO(7748): After a certain grace period, drop this in favor of "CallRef".
   DECODE(CallRefDeprecated) {
     CHECK_PROTOTYPE_OPCODE(typed_funcref);
-    Value func_ref = Peek(0);
-    ValueType func_type = func_ref.type;
-    if (func_type == kWasmBottom) {
-      // We are in unreachable code, maintain the polymorphic stack.
-      return 1;
-    }
-    if (!VALIDATE(func_type.is_object_reference() && func_type.has_index() &&
-                  this->module_->has_signature(func_type.ref_index()))) {
-      PopTypeError(0, func_ref, "function reference");
-      return 0;
-    }
-    const FunctionSig* sig = this->module_->signature(func_type.ref_index());
-    ArgVector args = PeekArgs(sig, 1);
-    ReturnVector returns = CreateReturnValues(sig);
-    CALL_INTERFACE_IF_OK_AND_REACHABLE(CallRef, func_ref, sig,
-                                       func_type.ref_index(), args.begin(),
-                                       returns.begin());
+    SigIndexImmediate imm(this, this->pc_ + 1, validate);
+    if (!this->Validate(this->pc_ + 1, imm)) return 0;
+    Value func_ref = Peek(0, 0, ValueType::RefNull(imm.index));
+    ArgVector args = PeekArgs(imm.sig, 1);
+    ReturnVector returns = CreateReturnValues(imm.sig);
+    CALL_INTERFACE_IF_OK_AND_REACHABLE(CallRef, func_ref, imm.sig, imm.index,
+                                       args.begin(), returns.begin());
     Drop(func_ref);
-    DropArgs(sig);
+    DropArgs(imm.sig);
     PushReturns(returns);
-    return 1;
+    return 1 + imm.length;
   }
 
   DECODE(CallRef) {
     CHECK_PROTOTYPE_OPCODE(typed_funcref);
-    SigIndexImmediate<validate> imm(this, this->pc_ + 1);
+    SigIndexImmediate imm(this, this->pc_ + 1, validate);
     if (!this->Validate(this->pc_ + 1, imm)) return 0;
     Value func_ref = Peek(0, 0, ValueType::RefNull(imm.index));
     ArgVector args = PeekArgs(imm.sig, 1);
@@ -3652,7 +3763,7 @@
   DECODE(ReturnCallRef) {
     CHECK_PROTOTYPE_OPCODE(typed_funcref);
     CHECK_PROTOTYPE_OPCODE(return_call);
-    SigIndexImmediate<validate> imm(this, this->pc_ + 1);
+    SigIndexImmediate imm(this, this->pc_ + 1, validate);
     if (!this->Validate(this->pc_ + 1, imm)) return 0;
     Value func_ref = Peek(0, 0, ValueType::RefNull(imm.index));
     ArgVector args = PeekArgs(imm.sig, 1);
@@ -3666,7 +3777,7 @@
 
   DECODE(Numeric) {
     uint32_t opcode_length = 0;
-    WasmOpcode full_opcode = this->template read_prefixed_opcode<validate>(
+    WasmOpcode full_opcode = this->template read_prefixed_opcode<ValidationTag>(
         this->pc_, &opcode_length, "numeric index");
     if (full_opcode == kExprTableGrow || full_opcode == kExprTableSize ||
         full_opcode == kExprTableFill) {
@@ -3677,7 +3788,7 @@
   }
 
   DECODE(Simd) {
-    CHECK_PROTOTYPE_OPCODE(simd);
+    this->detected_->Add(kFeature_simd);
     if (!CheckHardwareSupportsSimd()) {
       if (v8_flags.correctness_fuzzer_suppressions) {
         FATAL("Aborting on missing Wasm SIMD support");
@@ -3686,7 +3797,7 @@
       return 0;
     }
     uint32_t opcode_length = 0;
-    WasmOpcode full_opcode = this->template read_prefixed_opcode<validate>(
+    WasmOpcode full_opcode = this->template read_prefixed_opcode<ValidationTag>(
         this->pc_, &opcode_length);
     if (!VALIDATE(this->ok())) return 0;
     trace_msg->AppendOpcode(full_opcode);
@@ -3697,9 +3808,9 @@
   }
 
   DECODE(Atomic) {
-    CHECK_PROTOTYPE_OPCODE(threads);
+    this->detected_->Add(kFeature_threads);
     uint32_t opcode_length = 0;
-    WasmOpcode full_opcode = this->template read_prefixed_opcode<validate>(
+    WasmOpcode full_opcode = this->template read_prefixed_opcode<ValidationTag>(
         this->pc_, &opcode_length, "atomic index");
     trace_msg->AppendOpcode(full_opcode);
     return DecodeAtomicOpcode(full_opcode, opcode_length);
@@ -3707,7 +3818,7 @@
 
   DECODE(GC) {
     uint32_t opcode_length = 0;
-    WasmOpcode full_opcode = this->template read_prefixed_opcode<validate>(
+    WasmOpcode full_opcode = this->template read_prefixed_opcode<ValidationTag>(
         this->pc_, &opcode_length, "gc index");
     trace_msg->AppendOpcode(full_opcode);
     if (full_opcode >= kExprStringNewUtf8) {
@@ -3846,8 +3957,7 @@
   void EndControl() {
     DCHECK(!control_.empty());
     Control* current = &control_.back();
-    DCHECK_LE(stack_ + current->stack_depth, stack_end_);
-    stack_end_ = stack_ + current->stack_depth;
+    stack_.shrink_to(current->stack_depth);
     current->reachability = kUnreachable;
     current_code_reachable_and_ok_ = false;
   }
@@ -3867,8 +3977,7 @@
 
   // Initializes start- and end-merges of {c} with values according to the
   // in- and out-types of {c} respectively.
-  void SetBlockType(Control* c, BlockTypeImmediate<validate>& imm,
-                    Value* args) {
+  void SetBlockType(Control* c, BlockTypeImmediate& imm, Value* args) {
     const byte* pc = this->pc_;
     InitMerge(&c->end_merge, imm.out_arity(), [pc, &imm](uint32_t i) {
       return Value{pc, imm.out_type(i)};
@@ -3890,29 +3999,34 @@
   // to the difference, and return that number.
   V8_INLINE int EnsureStackArguments(int count) {
     uint32_t limit = control_.back().stack_depth;
-    if (V8_LIKELY(stack_size() >= count + limit)) return 0;
+    if (V8_LIKELY(stack_.size() >= count + limit)) return 0;
     return EnsureStackArguments_Slow(count, limit);
   }
 
   V8_NOINLINE int EnsureStackArguments_Slow(int count, uint32_t limit) {
     if (!VALIDATE(control_.back().unreachable())) {
-      NotEnoughArgumentsError(count, stack_size() - limit);
+      NotEnoughArgumentsError(count, stack_.size() - limit);
     }
     // Silently create unreachable values out of thin air underneath the
     // existing stack values. To do so, we have to move existing stack values
     // upwards in the stack, then instantiate the new Values as
     // {UnreachableValue}.
-    int current_values = stack_size() - limit;
+    int current_values = stack_.size() - limit;
     int additional_values = count - current_values;
     DCHECK_GT(additional_values, 0);
-    EnsureStackSpace(additional_values);
-    stack_end_ += additional_values;
-    Value* stack_base = stack_value(current_values + additional_values);
-    for (int i = current_values - 1; i >= 0; i--) {
-      stack_base[additional_values + i] = stack_base[i];
-    }
-    for (int i = 0; i < additional_values; i++) {
-      stack_base[i] = UnreachableValue(this->pc_);
+    stack_.EnsureMoreCapacity(additional_values, this->compilation_zone_);
+    Value unreachable_value = UnreachableValue(this->pc_);
+    for (int i = 0; i < additional_values; ++i) stack_.push(unreachable_value);
+    if (current_values > 0) {
+      // Move the current values up to the end of the stack, and create
+      // unreachable values below.
+      Value* stack_base = stack_value(current_values + additional_values);
+      for (int i = current_values - 1; i >= 0; i--) {
+        stack_base[additional_values + i] = stack_base[i];
+      }
+      for (int i = 0; i < additional_values; i++) {
+        stack_base[i] = UnreachableValue(this->pc_);
+      }
     }
     return additional_values;
   }
@@ -3961,11 +4075,6 @@
     return args;
   }
 
-  ValueType GetReturnType(const FunctionSig* sig) {
-    DCHECK_GE(1, sig->return_count());
-    return sig->return_count() == 0 ? kWasmVoid : sig->GetReturn();
-  }
-
   // TODO(jkummerow): Consider refactoring control stack management so
   // that {drop_values} is never needed. That would require decoupling
   // creation of the Control object from setting of its stack depth.
@@ -3974,12 +4083,14 @@
     Reachability reachability = control_.back().innerReachability();
     // In unreachable code, we may run out of stack.
     uint32_t stack_depth =
-        stack_size() >= drop_values ? stack_size() - drop_values : 0;
+        stack_.size() >= drop_values ? stack_.size() - drop_values : 0;
     stack_depth = std::max(stack_depth, control_.back().stack_depth);
     uint32_t init_stack_depth = this->locals_initialization_stack_depth();
+    control_.EnsureMoreCapacity(1, this->compilation_zone_);
     control_.emplace_back(kind, stack_depth, init_stack_depth, this->pc_,
                           reachability);
-    current_code_reachable_and_ok_ = this->ok() && reachability == kReachable;
+    current_code_reachable_and_ok_ =
+        VALIDATE(this->ok()) && reachability == kReachable;
     return &control_.back();
   }
 
@@ -3987,7 +4098,7 @@
     // This cannot be the outermost control block.
     DCHECK_LT(1, control_.size());
     Control* c = &control_.back();
-    DCHECK_LE(stack_ + c->stack_depth, stack_end_);
+    DCHECK_LE(c->stack_depth, stack_.size());
 
     CALL_INTERFACE_IF_OK_AND_PARENT_REACHABLE(PopControl, c);
 
@@ -4003,15 +4114,16 @@
 
     bool parent_reached =
         c->reachable() || c->end_merge.reached || c->is_onearmed_if();
-    control_.pop_back();
+    control_.pop();
     // If the parent block was reachable before, but the popped control does not
     // return to here, this block becomes "spec only reachable".
     if (!parent_reached) SetSucceedingCodeDynamicallyUnreachable();
-    current_code_reachable_and_ok_ = this->ok() && control_.back().reachable();
+    current_code_reachable_and_ok_ =
+        VALIDATE(this->ok()) && control_.back().reachable();
   }
 
   int DecodeLoadMem(LoadType type, int prefix_len = 1) {
-    MemoryAccessImmediate<validate> imm =
+    MemoryAccessImmediate imm =
         MakeMemoryAccessImmediate(prefix_len, type.size_log_2());
     if (!this->Validate(this->pc_ + prefix_len, imm)) return 0;
     ValueType index_type = this->module_->is_memory64 ? kWasmI64 : kWasmI32;
@@ -4028,7 +4140,7 @@
     // Load extends always load 64-bits.
     uint32_t max_alignment =
         transform == LoadTransformationKind::kExtend ? 3 : type.size_log_2();
-    MemoryAccessImmediate<validate> imm =
+    MemoryAccessImmediate imm =
         MakeMemoryAccessImmediate(opcode_length, max_alignment);
     if (!this->Validate(this->pc_ + opcode_length, imm)) return 0;
     ValueType index_type = this->module_->is_memory64 ? kWasmI64 : kWasmI32;
@@ -4042,11 +4154,11 @@
   }
 
   int DecodeLoadLane(WasmOpcode opcode, LoadType type, uint32_t opcode_length) {
-    MemoryAccessImmediate<validate> mem_imm =
+    MemoryAccessImmediate mem_imm =
         MakeMemoryAccessImmediate(opcode_length, type.size_log_2());
     if (!this->Validate(this->pc_ + opcode_length, mem_imm)) return 0;
-    SimdLaneImmediate<validate> lane_imm(
-        this, this->pc_ + opcode_length + mem_imm.length);
+    SimdLaneImmediate lane_imm(this, this->pc_ + opcode_length + mem_imm.length,
+                               validate);
     if (!this->Validate(this->pc_ + opcode_length, opcode, lane_imm)) return 0;
     Value v128 = Peek(0, 1, kWasmS128);
     Value index = Peek(1, 0, kWasmI32);
@@ -4061,11 +4173,11 @@
 
   int DecodeStoreLane(WasmOpcode opcode, StoreType type,
                       uint32_t opcode_length) {
-    MemoryAccessImmediate<validate> mem_imm =
+    MemoryAccessImmediate mem_imm =
         MakeMemoryAccessImmediate(opcode_length, type.size_log_2());
     if (!this->Validate(this->pc_ + opcode_length, mem_imm)) return 0;
-    SimdLaneImmediate<validate> lane_imm(
-        this, this->pc_ + opcode_length + mem_imm.length);
+    SimdLaneImmediate lane_imm(this, this->pc_ + opcode_length + mem_imm.length,
+                               validate);
     if (!this->Validate(this->pc_ + opcode_length, opcode, lane_imm)) return 0;
     Value v128 = Peek(0, 1, kWasmS128);
     Value index = Peek(1, 0, kWasmI32);
@@ -4077,7 +4189,7 @@
   }
 
   int DecodeStoreMem(StoreType store, int prefix_len = 1) {
-    MemoryAccessImmediate<validate> imm =
+    MemoryAccessImmediate imm =
         MakeMemoryAccessImmediate(prefix_len, store.size_log_2());
     if (!this->Validate(this->pc_ + prefix_len, imm)) return 0;
     Value value = Peek(0, 1, store.value_type());
@@ -4089,7 +4201,7 @@
   }
 
   uint32_t SimdConstOp(uint32_t opcode_length) {
-    Simd128Immediate<validate> imm(this, this->pc_ + opcode_length);
+    Simd128Immediate imm(this, this->pc_ + opcode_length, validate);
     Value result = CreateValue(kWasmS128);
     CALL_INTERFACE_IF_OK_AND_REACHABLE(S128Const, imm, &result);
     Push(result);
@@ -4098,7 +4210,7 @@
 
   uint32_t SimdExtractLane(WasmOpcode opcode, ValueType type,
                            uint32_t opcode_length) {
-    SimdLaneImmediate<validate> imm(this, this->pc_ + opcode_length);
+    SimdLaneImmediate imm(this, this->pc_ + opcode_length, validate);
     if (this->Validate(this->pc_ + opcode_length, opcode, imm)) {
       Value inputs[] = {Peek(0, 0, kWasmS128)};
       Value result = CreateValue(type);
@@ -4112,7 +4224,7 @@
 
   uint32_t SimdReplaceLane(WasmOpcode opcode, ValueType type,
                            uint32_t opcode_length) {
-    SimdLaneImmediate<validate> imm(this, this->pc_ + opcode_length);
+    SimdLaneImmediate imm(this, this->pc_ + opcode_length, validate);
     if (this->Validate(this->pc_ + opcode_length, opcode, imm)) {
       Value inputs[2] = {Peek(1, 0, kWasmS128), Peek(0, 1, type)};
       Value result = CreateValue(kWasmS128);
@@ -4125,7 +4237,7 @@
   }
 
   uint32_t Simd8x16ShuffleOp(uint32_t opcode_length) {
-    Simd128Immediate<validate> imm(this, this->pc_ + opcode_length);
+    Simd128Immediate imm(this, this->pc_ + opcode_length, validate);
     if (this->Validate(this->pc_ + opcode_length, imm)) {
       Value input1 = Peek(0, 1, kWasmS128);
       Value input0 = Peek(1, 0, kWasmS128);
@@ -4315,9 +4427,11 @@
   }
 
   int DecodeGCOpcode(WasmOpcode opcode, uint32_t opcode_length) {
+    // This assumption might help the big switch below.
+    V8_ASSUME(opcode >> 8 == kGCPrefix);
     switch (opcode) {
       case kExprStructNew: {
-        StructIndexImmediate<validate> imm(this, this->pc_ + opcode_length);
+        StructIndexImmediate imm(this, this->pc_ + opcode_length, validate);
         if (!this->Validate(this->pc_ + opcode_length, imm)) return 0;
         ValueType rtt_type = ValueType::Rtt(imm.index);
         Value rtt = CreateValue(rtt_type);
@@ -4333,15 +4447,10 @@
         return opcode_length + imm.length;
       }
       case kExprStructNewDefault: {
-        StructIndexImmediate<validate> imm(this, this->pc_ + opcode_length);
+        StructIndexImmediate imm(this, this->pc_ + opcode_length, validate);
         if (!this->Validate(this->pc_ + opcode_length, imm)) return 0;
-        if (validate) {
+        if (ValidationTag::validate) {
           for (uint32_t i = 0; i < imm.struct_type->field_count(); i++) {
-            if (!VALIDATE(imm.struct_type->mutability(i))) {
-              this->DecodeError("%s: struct_type %d has immutable field %d",
-                                WasmOpcodes::OpcodeName(opcode), imm.index, i);
-              return 0;
-            }
             ValueType ftype = imm.struct_type->field(i);
             if (!VALIDATE(ftype.is_defaultable())) {
               this->DecodeError(
@@ -4364,7 +4473,7 @@
       }
       case kExprStructGet: {
         NON_CONST_ONLY
-        FieldImmediate<validate> field(this, this->pc_ + opcode_length);
+        FieldImmediate field(this, this->pc_ + opcode_length, validate);
         if (!this->Validate(this->pc_ + opcode_length, field)) return 0;
         ValueType field_type =
             field.struct_imm.struct_type->field(field.field_imm.index);
@@ -4388,7 +4497,7 @@
       case kExprStructGetU:
       case kExprStructGetS: {
         NON_CONST_ONLY
-        FieldImmediate<validate> field(this, this->pc_ + opcode_length);
+        FieldImmediate field(this, this->pc_ + opcode_length, validate);
         if (!this->Validate(this->pc_ + opcode_length, field)) return 0;
         ValueType field_type =
             field.struct_imm.struct_type->field(field.field_imm.index);
@@ -4411,7 +4520,7 @@
       }
       case kExprStructSet: {
         NON_CONST_ONLY
-        FieldImmediate<validate> field(this, this->pc_ + opcode_length);
+        FieldImmediate field(this, this->pc_ + opcode_length, validate);
         if (!this->Validate(this->pc_ + opcode_length, field)) return 0;
         const StructType* struct_type = field.struct_imm.struct_type;
         if (!VALIDATE(struct_type->mutability(field.field_imm.index))) {
@@ -4429,7 +4538,7 @@
         return opcode_length + field.length;
       }
       case kExprArrayNew: {
-        ArrayIndexImmediate<validate> imm(this, this->pc_ + opcode_length);
+        ArrayIndexImmediate imm(this, this->pc_ + opcode_length, validate);
         if (!this->Validate(this->pc_ + opcode_length, imm)) return 0;
         ValueType rtt_type = ValueType::Rtt(imm.index);
         Value rtt = CreateValue(rtt_type);
@@ -4446,13 +4555,8 @@
         return opcode_length + imm.length;
       }
       case kExprArrayNewDefault: {
-        ArrayIndexImmediate<validate> imm(this, this->pc_ + opcode_length);
+        ArrayIndexImmediate imm(this, this->pc_ + opcode_length, validate);
         if (!this->Validate(this->pc_ + opcode_length, imm)) return 0;
-        if (!VALIDATE(imm.array_type->mutability())) {
-          this->DecodeError("%s: array type %d is immutable",
-                            WasmOpcodes::OpcodeName(opcode), imm.index);
-          return 0;
-        }
         if (!VALIDATE(imm.array_type->element_type().is_defaultable())) {
           this->DecodeError(
               "%s: array type %d has non-defaultable element type %s",
@@ -4473,8 +4577,8 @@
         return opcode_length + imm.length;
       }
       case kExprArrayNewData: {
-        ArrayIndexImmediate<validate> array_imm(this,
-                                                this->pc_ + opcode_length);
+        ArrayIndexImmediate array_imm(this, this->pc_ + opcode_length,
+                                      validate);
         if (!this->Validate(this->pc_ + opcode_length, array_imm)) return 0;
         ValueType element_type = array_imm.array_type->element_type();
         if (element_type.is_reference()) {
@@ -4494,8 +4598,8 @@
 #endif
         const byte* data_index_pc =
             this->pc_ + opcode_length + array_imm.length;
-        IndexImmediate<validate> data_segment(this, data_index_pc,
-                                              "data segment");
+        IndexImmediate data_segment(this, data_index_pc, "data segment",
+                                    validate);
         if (!this->ValidateDataSegment(data_index_pc, data_segment)) return 0;
 
         ValueType rtt_type = ValueType::Rtt(array_imm.index);
@@ -4515,8 +4619,8 @@
         return opcode_length + array_imm.length + data_segment.length;
       }
       case kExprArrayNewElem: {
-        ArrayIndexImmediate<validate> array_imm(this,
-                                                this->pc_ + opcode_length);
+        ArrayIndexImmediate array_imm(this, this->pc_ + opcode_length,
+                                      validate);
         if (!this->Validate(this->pc_ + opcode_length, array_imm)) return 0;
         ValueType element_type = array_imm.array_type->element_type();
         if (element_type.is_numeric()) {
@@ -4528,8 +4632,8 @@
         }
         const byte* elem_index_pc =
             this->pc_ + opcode_length + array_imm.length;
-        IndexImmediate<validate> elem_segment(this, elem_index_pc,
-                                              "data segment");
+        IndexImmediate elem_segment(this, elem_index_pc, "data segment",
+                                    validate);
         if (!this->ValidateElementSegment(elem_index_pc, elem_segment)) {
           return 0;
         }
@@ -4563,7 +4667,7 @@
       case kExprArrayGetS:
       case kExprArrayGetU: {
         NON_CONST_ONLY
-        ArrayIndexImmediate<validate> imm(this, this->pc_ + opcode_length);
+        ArrayIndexImmediate imm(this, this->pc_ + opcode_length, validate);
         if (!this->Validate(this->pc_ + opcode_length, imm)) return 0;
         if (!VALIDATE(imm.array_type->element_type().is_packed())) {
           this->DecodeError(
@@ -4584,7 +4688,7 @@
       }
       case kExprArrayGet: {
         NON_CONST_ONLY
-        ArrayIndexImmediate<validate> imm(this, this->pc_ + opcode_length);
+        ArrayIndexImmediate imm(this, this->pc_ + opcode_length, validate);
         if (!this->Validate(this->pc_ + opcode_length, imm)) return 0;
         if (!VALIDATE(!imm.array_type->element_type().is_packed())) {
           this->DecodeError(
@@ -4604,7 +4708,7 @@
       }
       case kExprArraySet: {
         NON_CONST_ONLY
-        ArrayIndexImmediate<validate> imm(this, this->pc_ + opcode_length);
+        ArrayIndexImmediate imm(this, this->pc_ + opcode_length, validate);
         if (!this->Validate(this->pc_ + opcode_length, imm)) return 0;
         if (!VALIDATE(imm.array_type->mutability())) {
           this->DecodeError("array.set: immediate array type %d is immutable",
@@ -4632,7 +4736,7 @@
         NON_CONST_ONLY
         // Read but ignore an immediate array type index.
         // TODO(7748): Remove this once we are ready to make breaking changes.
-        ArrayIndexImmediate<validate> imm(this, this->pc_ + opcode_length);
+        ArrayIndexImmediate imm(this, this->pc_ + opcode_length, validate);
         Value array_obj = Peek(0, 0, kWasmArrayRef);
         Value value = CreateValue(kWasmI32);
         CALL_INTERFACE_IF_OK_AND_REACHABLE(ArrayLen, array_obj, &value);
@@ -4642,7 +4746,7 @@
       }
       case kExprArrayCopy: {
         NON_CONST_ONLY
-        ArrayIndexImmediate<validate> dst_imm(this, this->pc_ + opcode_length);
+        ArrayIndexImmediate dst_imm(this, this->pc_ + opcode_length, validate);
         if (!this->Validate(this->pc_ + opcode_length, dst_imm)) return 0;
         if (!VALIDATE(dst_imm.array_type->mutability())) {
           this->DecodeError(
@@ -4650,8 +4754,8 @@
               dst_imm.index);
           return 0;
         }
-        ArrayIndexImmediate<validate> src_imm(
-            this, this->pc_ + opcode_length + dst_imm.length);
+        ArrayIndexImmediate src_imm(
+            this, this->pc_ + opcode_length + dst_imm.length, validate);
         if (!this->Validate(this->pc_ + opcode_length + dst_imm.length,
                             src_imm)) {
           return 0;
@@ -4676,12 +4780,12 @@
         return opcode_length + dst_imm.length + src_imm.length;
       }
       case kExprArrayNewFixed: {
-        ArrayIndexImmediate<validate> array_imm(this,
-                                                this->pc_ + opcode_length);
+        ArrayIndexImmediate array_imm(this, this->pc_ + opcode_length,
+                                      validate);
         if (!this->Validate(this->pc_ + opcode_length, array_imm)) return 0;
-        IndexImmediate<validate> length_imm(
-            this, this->pc_ + opcode_length + array_imm.length,
-            "array.new_fixed length");
+        IndexImmediate length_imm(this,
+                                  this->pc_ + opcode_length + array_imm.length,
+                                  "array.new_fixed length", validate);
         uint32_t elem_count = length_imm.index;
         if (!VALIDATE(elem_count <= kV8MaxWasmArrayNewFixedLength)) {
           this->DecodeError(
@@ -4731,11 +4835,92 @@
         Push(value);
         return opcode_length;
       }
+      case kExprRefCast:
+      case kExprRefCastNull: {
+        NON_CONST_ONLY
+        HeapTypeImmediate imm(this->enabled_, this, this->pc_ + opcode_length,
+                              validate);
+        this->Validate(this->pc_ + opcode_length, imm);
+        if (!VALIDATE(this->ok())) return 0;
+        opcode_length += imm.length;
+
+        std::optional<Value> rtt;
+        HeapType target_type = imm.type;
+        if (imm.type.is_index()) {
+          rtt = CreateValue(ValueType::Rtt(imm.type.ref_index()));
+          CALL_INTERFACE_IF_OK_AND_REACHABLE(RttCanon, imm.type.ref_index(),
+                                             &rtt.value());
+          Push(rtt.value());
+        }
+
+        Value obj = Peek(rtt.has_value() ? 1 : 0);
+        if (!VALIDATE((obj.type.is_object_reference() &&
+                       IsSameTypeHierarchy(obj.type.heap_type(), target_type,
+                                           this->module_)) ||
+                      obj.type.is_bottom())) {
+          this->DecodeError(
+              obj.pc(),
+              "Invalid types for ref.cast: %s of type %s has to "
+              "be in the same reference type hierarchy as (ref %s)",
+              SafeOpcodeNameAt(obj.pc()), obj.type.name().c_str(),
+              target_type.name().c_str());
+          return 0;
+        }
+
+        bool null_succeeds = opcode == kExprRefCastNull;
+        Value value = CreateValue(ValueType::RefMaybeNull(
+            imm.type, (obj.type.is_bottom() || !null_succeeds)
+                          ? kNonNullable
+                          : obj.type.nullability()));
+        if (current_code_reachable_and_ok_) {
+          // This logic ensures that code generation can assume that functions
+          // can only be cast to function types, and data objects to data types.
+          if (V8_UNLIKELY(TypeCheckAlwaysSucceeds(obj, target_type))) {
+            // Drop the rtt from the stack, then forward the object value to the
+            // result.
+            if (rtt.has_value()) {
+              CALL_INTERFACE(Drop);
+            }
+            if (obj.type.is_nullable() && !null_succeeds) {
+              CALL_INTERFACE(AssertNotNull, obj, &value);
+            } else {
+              CALL_INTERFACE(Forward, obj, &value);
+            }
+          } else if (V8_UNLIKELY(TypeCheckAlwaysFails(obj, target_type,
+                                                      null_succeeds))) {
+            if (rtt.has_value()) {
+              CALL_INTERFACE(Drop);
+            }
+            // Unrelated types. The only way this will not trap is if the object
+            // is null.
+            if (obj.type.is_nullable() && null_succeeds) {
+              // Drop rtt from the stack, then assert that obj is null.
+              CALL_INTERFACE(AssertNull, obj, &value);
+            } else {
+              CALL_INTERFACE(Trap, TrapReason::kTrapIllegalCast);
+              // We know that the following code is not reachable, but according
+              // to the spec it technically is. Set it to spec-only reachable.
+              SetSucceedingCodeDynamicallyUnreachable();
+            }
+          } else {
+            if (rtt.has_value()) {
+              CALL_INTERFACE(RefCast, obj, rtt.value(), &value, null_succeeds);
+            } else {
+              CALL_INTERFACE(RefCastAbstract, obj, target_type, &value,
+                             null_succeeds);
+            }
+          }
+        }
+        Drop(1 + rtt.has_value());
+        Push(value);
+        return opcode_length;
+      }
       case kExprRefTestNull:
       case kExprRefTest: {
         NON_CONST_ONLY
-        HeapTypeImmediate<validate> imm(
-            this->enabled_, this, this->pc_ + opcode_length, this->module_);
+        HeapTypeImmediate imm(this->enabled_, this, this->pc_ + opcode_length,
+                              validate);
+        this->Validate(this->pc_ + opcode_length, imm);
         if (!VALIDATE(this->ok())) return 0;
         opcode_length += imm.length;
 
@@ -4806,8 +4991,8 @@
       }
       case kExprRefTestDeprecated: {
         NON_CONST_ONLY
-        IndexImmediate<validate> imm(this, this->pc_ + opcode_length,
-                                     "type index");
+        IndexImmediate imm(this, this->pc_ + opcode_length, "type index",
+                           validate);
         if (!this->ValidateType(this->pc_ + opcode_length, imm)) return 0;
         opcode_length += imm.length;
         Value rtt = CreateValue(ValueType::Rtt(imm.index));
@@ -4816,9 +5001,12 @@
         Value obj = Peek(1);
         Value value = CreateValue(kWasmI32);
         if (!VALIDATE(IsSubtypeOf(obj.type, kWasmFuncRef, this->module_) ||
-                      IsSubtypeOf(obj.type, kWasmDataRef, this->module_) ||
+                      IsSubtypeOf(obj.type, kWasmStructRef, this->module_) ||
+                      IsSubtypeOf(obj.type, kWasmArrayRef, this->module_) ||
                       obj.type.is_bottom())) {
-          PopTypeError(0, obj, "subtype of (ref null func) or (ref null data)");
+          PopTypeError(0, obj,
+                       "subtype of (ref null func), (ref null struct) or (ref "
+                       "null array)");
           return 0;
         }
         if (current_code_reachable_and_ok_) {
@@ -4856,15 +5044,18 @@
               "--experimental-wasm-ref-cast-nop)");
           return 0;
         }
-        IndexImmediate<validate> imm(this, this->pc_ + opcode_length,
-                                     "type index");
+        IndexImmediate imm(this, this->pc_ + opcode_length, "type index",
+                           validate);
         if (!this->ValidateType(this->pc_ + opcode_length, imm)) return 0;
         opcode_length += imm.length;
         Value obj = Peek(0);
         if (!VALIDATE(IsSubtypeOf(obj.type, kWasmFuncRef, this->module_) ||
-                      IsSubtypeOf(obj.type, kWasmDataRef, this->module_) ||
+                      IsSubtypeOf(obj.type, kWasmStructRef, this->module_) ||
+                      IsSubtypeOf(obj.type, kWasmArrayRef, this->module_) ||
                       obj.type.is_bottom())) {
-          PopTypeError(0, obj, "subtype of (ref null func) or (ref null data)");
+          PopTypeError(0, obj,
+                       "subtype of (ref null func), (ref null struct) or (ref "
+                       "null array)");
           return 0;
         }
         Value value = CreateValue(ValueType::RefMaybeNull(
@@ -4875,10 +5066,10 @@
         Push(value);
         return opcode_length;
       }
-      case kExprRefCast: {
+      case kExprRefCastDeprecated: {
         NON_CONST_ONLY
-        IndexImmediate<validate> imm(this, this->pc_ + opcode_length,
-                                     "type index");
+        IndexImmediate imm(this, this->pc_ + opcode_length, "type index",
+                           validate);
         if (!this->ValidateType(this->pc_ + opcode_length, imm)) return 0;
         opcode_length += imm.length;
         Value rtt = CreateValue(ValueType::Rtt(imm.index));
@@ -4886,9 +5077,12 @@
         Push(rtt);
         Value obj = Peek(1);
         if (!VALIDATE(IsSubtypeOf(obj.type, kWasmFuncRef, this->module_) ||
-                      IsSubtypeOf(obj.type, kWasmDataRef, this->module_) ||
+                      IsSubtypeOf(obj.type, kWasmStructRef, this->module_) ||
+                      IsSubtypeOf(obj.type, kWasmArrayRef, this->module_) ||
                       obj.type.is_bottom())) {
-          PopTypeError(0, obj, "subtype of (ref null func) or (ref null data)");
+          PopTypeError(0, obj,
+                       "subtype of (ref null func), (ref null struct) or (ref "
+                       "null array)");
           return 0;
         }
         // If either value is bottom, we emit the most specific type possible.
@@ -4918,7 +5112,8 @@
               SetSucceedingCodeDynamicallyUnreachable();
             }
           } else {
-            CALL_INTERFACE(RefCast, obj, rtt, &value);
+            bool null_succeeds = true;
+            CALL_INTERFACE(RefCast, obj, rtt, &value, null_succeeds);
           }
         }
         Drop(2);
@@ -4927,14 +5122,14 @@
       }
       case kExprBrOnCast: {
         NON_CONST_ONLY
-        BranchDepthImmediate<validate> branch_depth(this,
-                                                    this->pc_ + opcode_length);
+        BranchDepthImmediate branch_depth(this, this->pc_ + opcode_length,
+                                          validate);
         if (!this->Validate(this->pc_ + opcode_length, branch_depth,
                             control_.size())) {
           return 0;
         }
         uint32_t pc_offset = opcode_length + branch_depth.length;
-        IndexImmediate<validate> imm(this, this->pc_ + pc_offset, "type index");
+        IndexImmediate imm(this, this->pc_ + pc_offset, "type index", validate);
         if (!this->ValidateType(this->pc_ + opcode_length, imm)) return 0;
         pc_offset += imm.length;
         Value rtt = CreateValue(ValueType::Rtt(imm.index));
@@ -4943,9 +5138,12 @@
         // anyway.
         Value obj = Peek(0);
         if (!VALIDATE(IsSubtypeOf(obj.type, kWasmFuncRef, this->module_) ||
-                      IsSubtypeOf(obj.type, kWasmDataRef, this->module_) ||
+                      IsSubtypeOf(obj.type, kWasmStructRef, this->module_) ||
+                      IsSubtypeOf(obj.type, kWasmArrayRef, this->module_) ||
                       obj.type.is_bottom())) {
-          PopTypeError(0, obj, "subtype of (ref null func) or (ref null data)");
+          PopTypeError(0, obj,
+                       "subtype of (ref null func), (ref null struct) or (ref "
+                       "null array)");
           return 0;
         }
         Control* c = control_at(branch_depth.depth);
@@ -4996,23 +5194,26 @@
       }
       case kExprBrOnCastFail: {
         NON_CONST_ONLY
-        BranchDepthImmediate<validate> branch_depth(this,
-                                                    this->pc_ + opcode_length);
+        BranchDepthImmediate branch_depth(this, this->pc_ + opcode_length,
+                                          validate);
         if (!this->Validate(this->pc_ + opcode_length, branch_depth,
                             control_.size())) {
           return 0;
         }
         uint32_t pc_offset = opcode_length + branch_depth.length;
-        IndexImmediate<validate> imm(this, this->pc_ + pc_offset, "type index");
+        IndexImmediate imm(this, this->pc_ + pc_offset, "type index", validate);
         if (!this->ValidateType(this->pc_ + opcode_length, imm)) return 0;
         pc_offset += imm.length;
         Value rtt = CreateValue(ValueType::Rtt(imm.index));
         CALL_INTERFACE_IF_OK_AND_REACHABLE(RttCanon, imm.index, &rtt);
         Value obj = Peek(0);
         if (!VALIDATE(IsSubtypeOf(obj.type, kWasmFuncRef, this->module_) ||
-                      IsSubtypeOf(obj.type, kWasmDataRef, this->module_) ||
+                      IsSubtypeOf(obj.type, kWasmStructRef, this->module_) ||
+                      IsSubtypeOf(obj.type, kWasmArrayRef, this->module_) ||
                       obj.type.is_bottom())) {
-          PopTypeError(0, obj, "subtype of (ref null func) or (ref null data)");
+          PopTypeError(0, obj,
+                       "subtype of (ref null func), (ref null struct) or (ref "
+                       "null array)");
           return 0;
         }
         Control* c = control_at(branch_depth.depth);
@@ -5067,7 +5268,7 @@
   case kExprRefIs##h_type: {                                                   \
     NON_CONST_ONLY                                                             \
     Value arg = Peek(0, 0, kWasmAnyRef);                                       \
-    if (this->failed()) return 0;                                              \
+    if (!VALIDATE(this->ok())) return 0;                                       \
     Value result = CreateValue(kWasmI32);                                      \
     if (V8_LIKELY(current_code_reachable_and_ok_)) {                           \
       if (IsHeapSubtypeOf(arg.type.heap_type(), HeapType(HeapType::k##h_type), \
@@ -5092,7 +5293,7 @@
     Push(result);                                                              \
     return opcode_length;                                                      \
   }
-        ABSTRACT_TYPE_CHECK(Data)
+        ABSTRACT_TYPE_CHECK(Struct)
         ABSTRACT_TYPE_CHECK(I31)
         ABSTRACT_TYPE_CHECK(Array)
 #undef ABSTRACT_TYPE_CHECK
@@ -5126,17 +5327,17 @@
     Push(result);                                                              \
     return opcode_length;                                                      \
   }
-        ABSTRACT_TYPE_CAST(Data)
+        ABSTRACT_TYPE_CAST(Struct)
         ABSTRACT_TYPE_CAST(I31)
         ABSTRACT_TYPE_CAST(Array)
 #undef ABSTRACT_TYPE_CAST
 
-      case kExprBrOnData:
+      case kExprBrOnStruct:
       case kExprBrOnArray:
       case kExprBrOnI31: {
         NON_CONST_ONLY
-        BranchDepthImmediate<validate> branch_depth(this,
-                                                    this->pc_ + opcode_length);
+        BranchDepthImmediate branch_depth(this, this->pc_ + opcode_length,
+                                          validate);
         if (!this->Validate(this->pc_ + opcode_length, branch_depth,
                             control_.size())) {
           return 0;
@@ -5157,7 +5358,7 @@
         Value obj = Peek(0, 0, kWasmAnyRef);
         Drop(obj);
         HeapType::Representation heap_type =
-            opcode == kExprBrOnData    ? HeapType::kData
+            opcode == kExprBrOnStruct  ? HeapType::kStruct
             : opcode == kExprBrOnArray ? HeapType::kArray
                                        : HeapType::kI31;
         Value result_on_branch = CreateValue(ValueType::Ref(heap_type));
@@ -5168,8 +5369,9 @@
         // {result_on_branch} which was passed-by-value to {Push}.
         Value* value_on_branch = stack_value(1);
         if (V8_LIKELY(current_code_reachable_and_ok_)) {
-          if (opcode == kExprBrOnData) {
-            CALL_INTERFACE(BrOnData, obj, value_on_branch, branch_depth.depth);
+          if (opcode == kExprBrOnStruct) {
+            CALL_INTERFACE(BrOnStruct, obj, value_on_branch,
+                           branch_depth.depth);
           } else if (opcode == kExprBrOnArray) {
             CALL_INTERFACE(BrOnArray, obj, value_on_branch, branch_depth.depth);
           } else {
@@ -5181,12 +5383,12 @@
         Push(obj);  // Restore stack state on fallthrough.
         return opcode_length + branch_depth.length;
       }
-      case kExprBrOnNonData:
+      case kExprBrOnNonStruct:
       case kExprBrOnNonArray:
       case kExprBrOnNonI31: {
         NON_CONST_ONLY
-        BranchDepthImmediate<validate> branch_depth(this,
-                                                    this->pc_ + opcode_length);
+        BranchDepthImmediate branch_depth(this, this->pc_ + opcode_length,
+                                          validate);
         if (!this->Validate(this->pc_ + opcode_length, branch_depth,
                             control_.size())) {
           return 0;
@@ -5202,14 +5404,14 @@
 
         Value obj = Peek(0, 0, kWasmAnyRef);
         HeapType::Representation heap_type =
-            opcode == kExprBrOnNonData    ? HeapType::kData
+            opcode == kExprBrOnNonStruct  ? HeapType::kStruct
             : opcode == kExprBrOnNonArray ? HeapType::kArray
                                           : HeapType::kI31;
         Value value_on_fallthrough = CreateValue(ValueType::Ref(heap_type));
 
         if (V8_LIKELY(current_code_reachable_and_ok_)) {
-          if (opcode == kExprBrOnNonData) {
-            CALL_INTERFACE(BrOnNonData, obj, &value_on_fallthrough,
+          if (opcode == kExprBrOnNonStruct) {
+            CALL_INTERFACE(BrOnNonStruct, obj, &value_on_fallthrough,
                            branch_depth.depth);
           } else if (opcode == kExprBrOnNonArray) {
             CALL_INTERFACE(BrOnNonArray, obj, &value_on_fallthrough,
@@ -5257,7 +5459,7 @@
   int DecodeStringNewWtf8(unibrow::Utf8Variant variant,
                           uint32_t opcode_length) {
     NON_CONST_ONLY
-    MemoryIndexImmediate<validate> memory(this, this->pc_ + opcode_length);
+    MemoryIndexImmediate memory(this, this->pc_ + opcode_length, validate);
     if (!this->Validate(this->pc_ + opcode_length, memory)) return 0;
     ValueType addr_type = this->module_->is_memory64 ? kWasmI64 : kWasmI32;
     Value offset = Peek(1, 0, addr_type);
@@ -5285,7 +5487,7 @@
   int DecodeStringEncodeWtf8(unibrow::Utf8Variant variant,
                              uint32_t opcode_length) {
     NON_CONST_ONLY
-    MemoryIndexImmediate<validate> memory(this, this->pc_ + opcode_length);
+    MemoryIndexImmediate memory(this, this->pc_ + opcode_length, validate);
     if (!this->Validate(this->pc_ + opcode_length, memory)) return 0;
     ValueType addr_type = this->module_->is_memory64 ? kWasmI64 : kWasmI32;
     Value str = Peek(1, 0, kWasmStringRef);
@@ -5301,7 +5503,7 @@
   int DecodeStringViewWtf8Encode(unibrow::Utf8Variant variant,
                                  uint32_t opcode_length) {
     NON_CONST_ONLY
-    MemoryIndexImmediate<validate> memory(this, this->pc_ + opcode_length);
+    MemoryIndexImmediate memory(this, this->pc_ + opcode_length, validate);
     if (!this->Validate(this->pc_ + opcode_length, memory)) return 0;
     ValueType addr_type = this->module_->is_memory64 ? kWasmI64 : kWasmI32;
     Value view = Peek(3, 0, kWasmStringViewWtf8);
@@ -5348,6 +5550,8 @@
   }
 
   int DecodeStringRefOpcode(WasmOpcode opcode, uint32_t opcode_length) {
+    // This assumption might help the big switch below.
+    V8_ASSUME(opcode >> 8 == kGCPrefix);
     switch (opcode) {
       case kExprStringNewUtf8:
         return DecodeStringNewWtf8(unibrow::Utf8Variant::kUtf8, opcode_length);
@@ -5358,7 +5562,7 @@
         return DecodeStringNewWtf8(unibrow::Utf8Variant::kWtf8, opcode_length);
       case kExprStringNewWtf16: {
         NON_CONST_ONLY
-        MemoryIndexImmediate<validate> imm(this, this->pc_ + opcode_length);
+        MemoryIndexImmediate imm(this, this->pc_ + opcode_length, validate);
         if (!this->Validate(this->pc_ + opcode_length, imm)) return 0;
         ValueType addr_type = this->module_->is_memory64 ? kWasmI64 : kWasmI32;
         Value offset = Peek(1, 0, addr_type);
@@ -5371,7 +5575,7 @@
         return opcode_length + imm.length;
       }
       case kExprStringConst: {
-        StringConstImmediate<validate> imm(this, this->pc_ + opcode_length);
+        StringConstImmediate imm(this, this->pc_ + opcode_length, validate);
         if (!this->Validate(this->pc_ + opcode_length, imm)) return 0;
         Value result = CreateValue(ValueType::Ref(HeapType::kString));
         CALL_INTERFACE_IF_OK_AND_REACHABLE(StringConst, imm, &result);
@@ -5404,7 +5608,7 @@
                                       opcode_length);
       case kExprStringEncodeWtf16: {
         NON_CONST_ONLY
-        MemoryIndexImmediate<validate> imm(this, this->pc_ + opcode_length);
+        MemoryIndexImmediate imm(this, this->pc_ + opcode_length, validate);
         if (!this->Validate(this->pc_ + opcode_length, imm)) return 0;
         ValueType addr_type = this->module_->is_memory64 ? kWasmI64 : kWasmI32;
         Value str = Peek(1, 0, kWasmStringRef);
@@ -5518,7 +5722,7 @@
       }
       case kExprStringViewWtf16Encode: {
         NON_CONST_ONLY
-        MemoryIndexImmediate<validate> imm(this, this->pc_ + opcode_length);
+        MemoryIndexImmediate imm(this, this->pc_ + opcode_length, validate);
         if (!this->Validate(this->pc_ + opcode_length, imm)) return 0;
         ValueType addr_type = this->module_->is_memory64 ? kWasmI64 : kWasmI32;
         Value view = Peek(3, 0, kWasmStringViewWtf16);
@@ -5653,18 +5857,17 @@
 #undef NON_CONST_ONLY
 
   uint32_t DecodeAtomicOpcode(WasmOpcode opcode, uint32_t opcode_length) {
-    ValueType ret_type;
-    const FunctionSig* sig = WasmOpcodes::Signature(opcode);
-    if (!VALIDATE(sig != nullptr)) {
-      this->DecodeError("invalid atomic opcode");
+    // Fast check for out-of-range opcodes (only allow 0xfeXX).
+    if (!VALIDATE((opcode >> 8) == kAtomicPrefix)) {
+      this->DecodeError("invalid atomic opcode: 0x%x", opcode);
       return 0;
     }
+
     MachineType memtype;
     switch (opcode) {
 #define CASE_ATOMIC_STORE_OP(Name, Type)          \
   case kExpr##Name: {                             \
     memtype = MachineType::Type();                \
-    ret_type = kWasmVoid;                         \
     break; /* to generic mem access code below */ \
   }
       ATOMIC_STORE_OP_LIST(CASE_ATOMIC_STORE_OP)
@@ -5672,14 +5875,13 @@
 #define CASE_ATOMIC_OP(Name, Type)                \
   case kExpr##Name: {                             \
     memtype = MachineType::Type();                \
-    ret_type = GetReturnType(sig);                \
     break; /* to generic mem access code below */ \
   }
       ATOMIC_OP_LIST(CASE_ATOMIC_OP)
 #undef CASE_ATOMIC_OP
       case kExprAtomicFence: {
-        byte zero =
-            this->template read_u8<validate>(this->pc_ + opcode_length, "zero");
+        byte zero = this->template read_u8<ValidationTag>(
+            this->pc_ + opcode_length, "zero");
         if (!VALIDATE(zero == 0)) {
           this->DecodeError(this->pc_ + opcode_length,
                             "invalid atomic operand");
@@ -5689,11 +5891,14 @@
         return 1 + opcode_length;
       }
       default:
-        this->DecodeError("invalid atomic opcode");
+        this->DecodeError("invalid atomic opcode: 0x%x", opcode);
         return 0;
     }
 
-    MemoryAccessImmediate<validate> imm = MakeMemoryAccessImmediate(
+    const FunctionSig* sig = WasmOpcodes::Signature(opcode);
+    V8_ASSUME(sig != nullptr);
+
+    MemoryAccessImmediate imm = MakeMemoryAccessImmediate(
         opcode_length, ElementSizeLog2Of(memtype.representation()));
     if (!this->Validate(this->pc_ + opcode_length, imm)) return false;
 
@@ -5701,12 +5906,13 @@
     // then).
     CHECK(!this->module_->is_memory64);
     ArgVector args = PeekArgs(sig);
-    if (ret_type == kWasmVoid) {
+    if (sig->return_count() == 0) {
       CALL_INTERFACE_IF_OK_AND_REACHABLE(AtomicOp, opcode, base::VectorOf(args),
                                          imm, nullptr);
       DropArgs(sig);
     } else {
-      Value result = CreateValue(GetReturnType(sig));
+      DCHECK_EQ(1, sig->return_count());
+      Value result = CreateValue(sig->GetReturn());
       CALL_INTERFACE_IF_OK_AND_REACHABLE(AtomicOp, opcode, base::VectorOf(args),
                                          imm, &result);
       DropArgs(sig);
@@ -5716,6 +5922,14 @@
   }
 
   unsigned DecodeNumericOpcode(WasmOpcode opcode, uint32_t opcode_length) {
+    // Fast check for out-of-range opcodes (only allow 0xfcXX).
+    // This avoids a dynamic check in signature lookup, and might also help the
+    // big switch below.
+    if (!VALIDATE((opcode >> 8) == kNumericPrefix)) {
+      this->DecodeError("invalid numeric opcode: 0x%x", opcode);
+      return 0;
+    }
+
     const FunctionSig* sig = WasmOpcodes::Signature(opcode);
     switch (opcode) {
       case kExprI32SConvertSatF32:
@@ -5730,7 +5944,7 @@
         return opcode_length;
       }
       case kExprMemoryInit: {
-        MemoryInitImmediate<validate> imm(this, this->pc_ + opcode_length);
+        MemoryInitImmediate imm(this, this->pc_ + opcode_length, validate);
         if (!this->Validate(this->pc_ + opcode_length, imm)) return 0;
         ValueType mem_type = this->module_->is_memory64 ? kWasmI64 : kWasmI32;
         Value size = Peek(0, 2, kWasmI32);
@@ -5741,8 +5955,8 @@
         return opcode_length + imm.length;
       }
       case kExprDataDrop: {
-        IndexImmediate<validate> imm(this, this->pc_ + opcode_length,
-                                     "data segment index");
+        IndexImmediate imm(this, this->pc_ + opcode_length,
+                           "data segment index", validate);
         if (!this->ValidateDataSegment(this->pc_ + opcode_length, imm)) {
           return 0;
         }
@@ -5750,7 +5964,7 @@
         return opcode_length + imm.length;
       }
       case kExprMemoryCopy: {
-        MemoryCopyImmediate<validate> imm(this, this->pc_ + opcode_length);
+        MemoryCopyImmediate imm(this, this->pc_ + opcode_length, validate);
         if (!this->Validate(this->pc_ + opcode_length, imm)) return 0;
         ValueType mem_type = this->module_->is_memory64 ? kWasmI64 : kWasmI32;
         Value size = Peek(0, 2, mem_type);
@@ -5761,7 +5975,7 @@
         return opcode_length + imm.length;
       }
       case kExprMemoryFill: {
-        MemoryIndexImmediate<validate> imm(this, this->pc_ + opcode_length);
+        MemoryIndexImmediate imm(this, this->pc_ + opcode_length, validate);
         if (!this->Validate(this->pc_ + opcode_length, imm)) return 0;
         ValueType mem_type = this->module_->is_memory64 ? kWasmI64 : kWasmI32;
         Value size = Peek(0, 2, mem_type);
@@ -5772,7 +5986,7 @@
         return opcode_length + imm.length;
       }
       case kExprTableInit: {
-        TableInitImmediate<validate> imm(this, this->pc_ + opcode_length);
+        TableInitImmediate imm(this, this->pc_ + opcode_length, validate);
         if (!this->Validate(this->pc_ + opcode_length, imm)) return 0;
         ArgVector args = PeekArgs(sig);
         CALL_INTERFACE_IF_OK_AND_REACHABLE(TableInit, imm,
@@ -5781,8 +5995,8 @@
         return opcode_length + imm.length;
       }
       case kExprElemDrop: {
-        IndexImmediate<validate> imm(this, this->pc_ + opcode_length,
-                                     "element segment index");
+        IndexImmediate imm(this, this->pc_ + opcode_length,
+                           "element segment index", validate);
         if (!this->ValidateElementSegment(this->pc_ + opcode_length, imm)) {
           return 0;
         }
@@ -5790,7 +6004,7 @@
         return opcode_length + imm.length;
       }
       case kExprTableCopy: {
-        TableCopyImmediate<validate> imm(this, this->pc_ + opcode_length);
+        TableCopyImmediate imm(this, this->pc_ + opcode_length, validate);
         if (!this->Validate(this->pc_ + opcode_length, imm)) return 0;
         ArgVector args = PeekArgs(sig);
         CALL_INTERFACE_IF_OK_AND_REACHABLE(TableCopy, imm,
@@ -5799,8 +6013,8 @@
         return opcode_length + imm.length;
       }
       case kExprTableGrow: {
-        IndexImmediate<validate> imm(this, this->pc_ + opcode_length,
-                                     "table index");
+        IndexImmediate imm(this, this->pc_ + opcode_length, "table index",
+                           validate);
         if (!this->ValidateTable(this->pc_ + opcode_length, imm)) return 0;
         Value delta = Peek(0, 1, kWasmI32);
         Value value = Peek(1, 0, this->module_->tables[imm.index].type);
@@ -5812,8 +6026,8 @@
         return opcode_length + imm.length;
       }
       case kExprTableSize: {
-        IndexImmediate<validate> imm(this, this->pc_ + opcode_length,
-                                     "table index");
+        IndexImmediate imm(this, this->pc_ + opcode_length, "table index",
+                           validate);
         if (!this->ValidateTable(this->pc_ + opcode_length, imm)) return 0;
         Value result = CreateValue(kWasmI32);
         CALL_INTERFACE_IF_OK_AND_REACHABLE(TableSize, imm, &result);
@@ -5821,8 +6035,8 @@
         return opcode_length + imm.length;
       }
       case kExprTableFill: {
-        IndexImmediate<validate> imm(this, this->pc_ + opcode_length,
-                                     "table index");
+        IndexImmediate imm(this, this->pc_ + opcode_length, "table index",
+                           validate);
         if (!this->ValidateTable(this->pc_ + opcode_length, imm)) return 0;
         Value count = Peek(0, 2, kWasmI32);
         Value value = Peek(1, 1, this->module_->tables[imm.index].type);
@@ -5832,59 +6046,36 @@
         return opcode_length + imm.length;
       }
       default:
-        this->DecodeError("invalid numeric opcode");
+        this->DecodeError("invalid numeric opcode: 0x%x", opcode);
         return 0;
     }
   }
 
-  V8_INLINE void EnsureStackSpace(int slots_needed) {
-    if (V8_LIKELY(stack_capacity_end_ - stack_end_ >= slots_needed)) return;
-    GrowStackSpace(slots_needed);
-  }
-
-  V8_NOINLINE void GrowStackSpace(int slots_needed) {
-    size_t new_stack_capacity =
-        std::max(size_t{8},
-                 base::bits::RoundUpToPowerOfTwo(stack_size() + slots_needed));
-    Value* new_stack =
-        this->zone()->template NewArray<Value>(new_stack_capacity);
-    if (stack_) {
-      std::copy(stack_, stack_end_, new_stack);
-      this->zone()->DeleteArray(stack_, stack_capacity_end_ - stack_);
-    }
-    stack_end_ = new_stack + (stack_end_ - stack_);
-    stack_ = new_stack;
-    stack_capacity_end_ = new_stack + new_stack_capacity;
-  }
-
   V8_INLINE Value CreateValue(ValueType type) { return Value{this->pc_, type}; }
   V8_INLINE void Push(Value value) {
     DCHECK_NE(kWasmVoid, value.type);
-    // {EnsureStackSpace} should have been called before, either in the central
-    // decoding loop, or individually if more than one element is pushed.
-    DCHECK_GT(stack_capacity_end_, stack_end_);
-    *stack_end_ = value;
-    ++stack_end_;
+    // {stack_.EnsureMoreCapacity} should have been called before, either in the
+    // central decoding loop, or individually if more than one element is
+    // pushed.
+    stack_.push(value);
   }
 
   void PushMergeValues(Control* c, Merge<Value>* merge) {
     if (decoding_mode == kConstantExpression) return;
     DCHECK_EQ(c, &control_.back());
     DCHECK(merge == &c->start_merge || merge == &c->end_merge);
-    DCHECK_LE(stack_ + c->stack_depth, stack_end_);
-    stack_end_ = stack_ + c->stack_depth;
+    stack_.shrink_to(c->stack_depth);
     if (merge->arity == 1) {
-      // {EnsureStackSpace} should have been called before in the central
-      // decoding loop.
-      DCHECK_GT(stack_capacity_end_, stack_end_);
-      *stack_end_++ = merge->vals.first;
+      // {stack_.EnsureMoreCapacity} should have been called before in the
+      // central decoding loop.
+      stack_.push(merge->vals.first);
     } else {
-      EnsureStackSpace(merge->arity);
+      stack_.EnsureMoreCapacity(merge->arity, this->compilation_zone_);
       for (uint32_t i = 0; i < merge->arity; i++) {
-        *stack_end_++ = merge->vals.array[i];
+        stack_.push(merge->vals.array[i]);
       }
     }
-    DCHECK_EQ(c->stack_depth + merge->arity, stack_size());
+    DCHECK_EQ(c->stack_depth + merge->arity, stack_.size());
   }
 
   V8_INLINE ReturnVector CreateReturnValues(const FunctionSig* sig) {
@@ -5895,7 +6086,8 @@
     return values;
   }
   V8_INLINE void PushReturns(ReturnVector values) {
-    EnsureStackSpace(static_cast<int>(values.size()));
+    stack_.EnsureMoreCapacity(static_cast<int>(values.size()),
+                              this->compilation_zone_);
     for (Value& value : values) Push(value);
   }
 
@@ -5937,16 +6129,16 @@
   V8_INLINE Value Peek(int depth) {
     DCHECK(!control_.empty());
     uint32_t limit = control_.back().stack_depth;
-    if (V8_UNLIKELY(stack_size() <= limit + depth)) {
+    if (V8_UNLIKELY(stack_.size() <= limit + depth)) {
       // Peeking past the current control start in reachable code.
       if (!VALIDATE(decoding_mode == kFunctionBody &&
                     control_.back().unreachable())) {
-        NotEnoughArgumentsError(depth + 1, stack_size() - limit);
+        NotEnoughArgumentsError(depth + 1, stack_.size() - limit);
       }
       return UnreachableValue(this->pc_);
     }
-    DCHECK_LE(stack_, stack_end_ - depth - 1);
-    return *(stack_end_ - depth - 1);
+    DCHECK_LT(depth, stack_.size());
+    return *(stack_.end() - depth - 1);
   }
 
   Value PeekPackedArray(uint32_t stack_depth, uint32_t operand_index,
@@ -5991,12 +6183,11 @@
   V8_INLINE void Drop(int count = 1) {
     DCHECK(!control_.empty());
     uint32_t limit = control_.back().stack_depth;
-    if (V8_UNLIKELY(stack_size() < limit + count)) {
+    if (V8_UNLIKELY(stack_.size() < limit + count)) {
       // Pop what we can.
-      count = std::min(count, static_cast<int>(stack_size() - limit));
+      count = std::min(count, static_cast<int>(stack_.size() - limit));
     }
-    DCHECK_LE(stack_, stack_end_ - count);
-    stack_end_ -= count;
+    stack_.pop(count);
   }
   // Drop the top stack element if present. Takes a Value input for more
   // descriptive call sites.
@@ -6030,14 +6221,13 @@
   template <StackElementsCountMode strict_count, bool push_branch_values,
             MergeType merge_type>
   bool TypeCheckStackAgainstMerge(uint32_t drop_values, Merge<Value>* merge) {
-    static_assert(validate, "Call this function only within VALIDATE");
     constexpr const char* merge_description =
         merge_type == kBranchMerge     ? "branch"
         : merge_type == kReturnMerge   ? "return"
         : merge_type == kInitExprMerge ? "constant expression"
                                        : "fallthru";
     uint32_t arity = merge->arity;
-    uint32_t actual = stack_size() - control_.back().stack_depth;
+    uint32_t actual = stack_.size() - control_.back().stack_depth;
     // Here we have to check for !unreachable(), because we need to typecheck as
     // if the current code is reachable even if it is spec-only reachable.
     if (V8_LIKELY(decoding_mode == kConstantExpression ||
@@ -6050,7 +6240,7 @@
         return false;
       }
       // Typecheck the topmost {merge->arity} values on the stack.
-      Value* stack_values = stack_end_ - (arity + drop_values);
+      Value* stack_values = stack_.end() - (arity + drop_values);
       for (uint32_t i = 0; i < arity; ++i) {
         Value& val = stack_values[i];
         Value& old = (*merge)[i];
@@ -6078,9 +6268,10 @@
       uint32_t inserted_value_count =
           static_cast<uint32_t>(EnsureStackArguments(drop_values + arity));
       if (inserted_value_count > 0) {
-        // EnsureStackSpace may have inserted unreachable values into the bottom
-        // of the stack. If so, mark them with the correct type. If drop values
-        // were also inserted, disregard them, as they will be dropped anyway.
+        // stack_.EnsureMoreCapacity() may have inserted unreachable values into
+        // the bottom of the stack. If so, mark them with the correct type. If
+        // drop values were also inserted, disregard them, as they will be
+        // dropped anyway.
         Value* stack_base = stack_value(drop_values + arity);
         for (uint32_t i = 0; i < std::min(arity, inserted_value_count); i++) {
           if (stack_base[i].type == kWasmBottom) {
@@ -6089,7 +6280,7 @@
         }
       }
     }
-    return this->ok();
+    return VALIDATE(this->ok());
   }
 
   template <StackElementsCountMode strict_count, MergeType merge_type>
@@ -6099,7 +6290,7 @@
       return false;
     }
     DCHECK_IMPLIES(current_code_reachable_and_ok_,
-                   stack_size() >= this->sig_->return_count());
+                   stack_.size() >= this->sig_->return_count());
     CALL_INTERFACE_IF_OK_AND_REACHABLE(DoReturn, 0);
     EndControl();
     return true;
@@ -6116,7 +6307,6 @@
   }
 
   bool TypeCheckOneArmedIf(Control* c) {
-    static_assert(validate, "Call this function only within VALIDATE");
     DCHECK(c->is_onearmed_if());
     if (c->end_merge.arity != c->start_merge.arity) {
       this->DecodeError(c->pc(),
@@ -6136,7 +6326,6 @@
   }
 
   bool TypeCheckFallThru() {
-    static_assert(validate, "Call this function only within VALIDATE");
     return TypeCheckStackAgainstMerge<kStrictCounting, true, kFallthroughMerge>(
         0, &control_.back().end_merge);
   }
@@ -6153,7 +6342,6 @@
   // (index) and br_on_null (reference), and 0 for all other branches.
   template <bool push_branch_values>
   bool TypeCheckBranch(Control* c, uint32_t drop_values) {
-    static_assert(validate, "Call this function only within VALIDATE");
     return TypeCheckStackAgainstMerge<kNonStrictCounting, push_branch_values,
                                       kBranchMerge>(drop_values, c->br_merge());
   }
@@ -6221,15 +6409,17 @@
   }
   FOREACH_SIGNATURE(DEFINE_SIMPLE_SIG_OPERATOR)
 #undef DEFINE_SIMPLE_SIG_OPERATOR
+
+  static constexpr ValidationTag validate = {};
 };
 
 class EmptyInterface {
  public:
-  static constexpr Decoder::ValidateFlag validate = Decoder::kFullValidation;
+  using ValidationTag = Decoder::FullValidationTag;
   static constexpr DecodingMode decoding_mode = kFunctionBody;
-  using Value = ValueBase<validate>;
-  using Control = ControlBase<Value, validate>;
-  using FullDecoder = WasmFullDecoder<validate, EmptyInterface>;
+  using Value = ValueBase<ValidationTag>;
+  using Control = ControlBase<Value, ValidationTag>;
+  using FullDecoder = WasmFullDecoder<ValidationTag, EmptyInterface>;
 
 #define DEFINE_EMPTY_CALLBACK(name, ...) \
   void name(FullDecoder* decoder, ##__VA_ARGS__) {}
diff -r -u --color up/v8/src/wasm/function-body-decoder.cc nw/v8/src/wasm/function-body-decoder.cc
--- up/v8/src/wasm/function-body-decoder.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/function-body-decoder.cc	2023-01-19 16:46:36.428109511 -0500
@@ -16,16 +16,18 @@
 namespace internal {
 namespace wasm {
 
-bool DecodeLocalDecls(const WasmFeatures& enabled, BodyLocalDecls* decls,
+template <typename ValidationTag>
+bool DecodeLocalDecls(WasmFeatures enabled, BodyLocalDecls* decls,
                       const WasmModule* module, const byte* start,
                       const byte* end, Zone* zone) {
+  if constexpr (ValidationTag::validate) DCHECK_NOT_NULL(module);
   WasmFeatures no_features = WasmFeatures::None();
   constexpr FixedSizeSignature<ValueType, 0, 0> kNoSig;
-  WasmDecoder<Decoder::kFullValidation> decoder(
-      zone, module, enabled, &no_features, &kNoSig, start, end, 0);
+  WasmDecoder<ValidationTag> decoder(zone, module, enabled, &no_features,
+                                     &kNoSig, start, end);
   uint32_t length;
   decoder.DecodeLocals(decoder.pc(), &length);
-  if (decoder.failed()) {
+  if (ValidationTag::validate && decoder.failed()) {
     decls->encoded_size = 0;
     return false;
   }
@@ -38,6 +40,22 @@
   return true;
 }
 
+void DecodeLocalDecls(WasmFeatures enabled, BodyLocalDecls* decls,
+                      const byte* start, const byte* end, Zone* zone) {
+  constexpr WasmModule* kNoModule = nullptr;
+  DecodeLocalDecls<Decoder::NoValidationTag>(enabled, decls, kNoModule, start,
+                                             end, zone);
+}
+
+bool ValidateAndDecodeLocalDeclsForTesting(WasmFeatures enabled,
+                                           BodyLocalDecls* decls,
+                                           const WasmModule* module,
+                                           const byte* start, const byte* end,
+                                           Zone* zone) {
+  return DecodeLocalDecls<Decoder::BooleanValidationTag>(enabled, decls, module,
+                                                         start, end, zone);
+}
+
 BytecodeIterator::BytecodeIterator(const byte* start, const byte* end)
     : Decoder(start, end) {}
 
@@ -46,10 +64,9 @@
     : Decoder(start, end) {
   DCHECK_NOT_NULL(decls);
   DCHECK_NOT_NULL(zone);
-  if (DecodeLocalDecls(WasmFeatures::All(), decls, nullptr, start, end, zone)) {
-    pc_ += decls->encoded_size;
-    if (pc_ > end_) pc_ = end_;
-  }
+  DecodeLocalDecls(WasmFeatures::All(), decls, start, end, zone);
+  pc_ += decls->encoded_size;
+  if (pc_ > end_) pc_ = end_;
 }
 
 DecodeResult ValidateFunctionBody(AccountingAllocator* allocator,
@@ -58,7 +75,7 @@
                                   WasmFeatures* detected,
                                   const FunctionBody& body) {
   Zone zone(allocator, ZONE_NAME);
-  WasmFullDecoder<Decoder::kFullValidation, EmptyInterface> decoder(
+  WasmFullDecoder<Decoder::FullValidationTag, EmptyInterface> decoder(
       &zone, module, enabled, detected, body);
   decoder.Decode();
   return decoder.toResult(nullptr);
@@ -69,10 +86,10 @@
   Zone* no_zone = nullptr;
   WasmModule* no_module = nullptr;
   FunctionSig* no_sig = nullptr;
-  WasmDecoder<Decoder::kNoValidation> decoder(
+  WasmDecoder<Decoder::NoValidationTag> decoder(
       no_zone, no_module, WasmFeatures::All(), &unused_detected_features,
       no_sig, pc, end, 0);
-  return WasmDecoder<Decoder::kNoValidation>::OpcodeLength(&decoder, pc);
+  return WasmDecoder<Decoder::NoValidationTag>::OpcodeLength(&decoder, pc);
 }
 
 bool CheckHardwareSupportsSimd() { return CpuFeatures::SupportsWasmSimd128(); }
@@ -82,7 +99,7 @@
                                           const byte* pc, const byte* end) {
   WasmFeatures unused_detected_features = WasmFeatures::None();
   Zone* no_zone = nullptr;
-  WasmDecoder<Decoder::kNoValidation> decoder(
+  WasmDecoder<Decoder::NoValidationTag> decoder(
       no_zone, module, WasmFeatures::All(), &unused_detected_features, sig, pc,
       end);
   return decoder.StackEffect(pc);
@@ -131,7 +148,7 @@
                       std::ostream& os, std::vector<int>* line_numbers) {
   Zone zone(allocator, ZONE_NAME);
   WasmFeatures unused_detected_features = WasmFeatures::None();
-  WasmDecoder<Decoder::kNoValidation> decoder(
+  WasmDecoder<Decoder::NoValidationTag> decoder(
       &zone, module, WasmFeatures::All(), &unused_detected_features, body.sig,
       body.start, body.end);
   int line_nr = 0;
@@ -181,7 +198,7 @@
   unsigned control_depth = 0;
   for (; i.has_next(); i.next()) {
     unsigned length =
-        WasmDecoder<Decoder::kNoValidation>::OpcodeLength(&decoder, i.pc());
+        WasmDecoder<Decoder::NoValidationTag>::OpcodeLength(&decoder, i.pc());
 
     unsigned offset = 1;
     WasmOpcode opcode = i.current();
@@ -216,9 +233,8 @@
       if (i.pc()[1] & 0x80) {
         uint32_t temp_length;
         ValueType type =
-            value_type_reader::read_value_type<Decoder::kNoValidation>(
-                &decoder, i.pc() + 1, &temp_length, module,
-                WasmFeatures::All());
+            value_type_reader::read_value_type<Decoder::NoValidationTag>(
+                &decoder, i.pc() + 1, &temp_length, WasmFeatures::All());
         if (temp_length == 1) {
           os << type.name() << ",";
         } else {
@@ -251,8 +267,8 @@
       case kExprIf:
       case kExprBlock:
       case kExprTry: {
-        BlockTypeImmediate<Decoder::kNoValidation> imm(WasmFeatures::All(), &i,
-                                                       i.pc() + 1, module);
+        BlockTypeImmediate imm(WasmFeatures::All(), &i, i.pc() + 1,
+                               Decoder::kNoValidation);
         os << " @" << i.pc_offset();
         CHECK(decoder.Validate(i.pc() + 1, imm));
         for (uint32_t j = 0; j < imm.out_arity(); j++) {
@@ -266,29 +282,29 @@
         control_depth--;
         break;
       case kExprBr: {
-        BranchDepthImmediate<Decoder::kNoValidation> imm(&i, i.pc() + 1);
+        BranchDepthImmediate imm(&i, i.pc() + 1, Decoder::kNoValidation);
         os << " depth=" << imm.depth;
         break;
       }
       case kExprBrIf: {
-        BranchDepthImmediate<Decoder::kNoValidation> imm(&i, i.pc() + 1);
+        BranchDepthImmediate imm(&i, i.pc() + 1, Decoder::kNoValidation);
         os << " depth=" << imm.depth;
         break;
       }
       case kExprBrTable: {
-        BranchTableImmediate<Decoder::kNoValidation> imm(&i, i.pc() + 1);
+        BranchTableImmediate imm(&i, i.pc() + 1, Decoder::kNoValidation);
         os << " entries=" << imm.table_count;
         break;
       }
       case kExprCallIndirect: {
-        CallIndirectImmediate<Decoder::kNoValidation> imm(&i, i.pc() + 1);
+        CallIndirectImmediate imm(&i, i.pc() + 1, Decoder::kNoValidation);
         os << " sig #" << imm.sig_imm.index;
         CHECK(decoder.Validate(i.pc() + 1, imm));
         os << ": " << *imm.sig;
         break;
       }
       case kExprCallFunction: {
-        CallFunctionImmediate<Decoder::kNoValidation> imm(&i, i.pc() + 1);
+        CallFunctionImmediate imm(&i, i.pc() + 1, Decoder::kNoValidation);
         os << " function #" << imm.index;
         CHECK(decoder.Validate(i.pc() + 1, imm));
         os << ": " << *imm.sig;
@@ -309,9 +325,9 @@
 BitVector* AnalyzeLoopAssignmentForTesting(Zone* zone, uint32_t num_locals,
                                            const byte* start, const byte* end) {
   WasmFeatures no_features = WasmFeatures::None();
-  WasmDecoder<Decoder::kFullValidation> decoder(
+  WasmDecoder<Decoder::FullValidationTag> decoder(
       zone, nullptr, no_features, &no_features, nullptr, start, end, 0);
-  return WasmDecoder<Decoder::kFullValidation>::AnalyzeLoopAssignment(
+  return WasmDecoder<Decoder::FullValidationTag>::AnalyzeLoopAssignment(
       &decoder, start, num_locals, zone);
 }
 
diff -r -u --color up/v8/src/wasm/function-body-decoder.h nw/v8/src/wasm/function-body-decoder.h
--- up/v8/src/wasm/function-body-decoder.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/function-body-decoder.h	2023-01-19 16:46:36.428109511 -0500
@@ -68,12 +68,17 @@
   ValueType* local_types = nullptr;
 };
 
-V8_EXPORT_PRIVATE bool DecodeLocalDecls(const WasmFeatures& enabled,
+// Decode locals; validation is not performed.
+V8_EXPORT_PRIVATE void DecodeLocalDecls(WasmFeatures enabled,
                                         BodyLocalDecls* decls,
-                                        const WasmModule* module,
                                         const byte* start, const byte* end,
                                         Zone* zone);
 
+// Decode locals, including validation.
+V8_EXPORT_PRIVATE bool ValidateAndDecodeLocalDeclsForTesting(
+    WasmFeatures enabled, BodyLocalDecls* decls, const WasmModule* module,
+    const byte* start, const byte* end, Zone* zone);
+
 V8_EXPORT_PRIVATE BitVector* AnalyzeLoopAssignmentForTesting(
     Zone* zone, uint32_t num_locals, const byte* start, const byte* end);
 
@@ -170,7 +175,7 @@
 
   WasmOpcode current() {
     return static_cast<WasmOpcode>(
-        read_u8<Decoder::kNoValidation>(pc_, "expected bytecode"));
+        read_u8<Decoder::NoValidationTag>(pc_, "expected bytecode"));
   }
 
   void next() {
@@ -183,7 +188,7 @@
   bool has_next() { return pc_ < end_; }
 
   WasmOpcode prefixed_opcode() {
-    return read_prefixed_opcode<Decoder::kNoValidation>(pc_);
+    return read_prefixed_opcode<Decoder::NoValidationTag>(pc_);
   }
 };
 
diff -r -u --color up/v8/src/wasm/function-compiler.cc nw/v8/src/wasm/function-compiler.cc
--- up/v8/src/wasm/function-compiler.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/function-compiler.cc	2023-01-19 16:46:36.428109511 -0500
@@ -130,6 +130,17 @@
       V8_FALLTHROUGH;
 
     case ExecutionTier::kTurbofan:
+      // Before executing TurboFan compilation, make sure that the function was
+      // validated (because TurboFan compilation assumes valid input).
+      if (V8_UNLIKELY(!env->module->function_was_validated(func_index_))) {
+        AccountingAllocator allocator;
+        if (ValidateFunctionBody(&allocator, env->enabled_features, env->module,
+                                 detected, func_body)
+                .failed()) {
+          return {};
+        }
+        env->module->set_function_validated(func_index_);
+      }
       result = compiler::ExecuteTurbofanWasmCompilation(
           env, wire_bytes_storage, func_body, func_index_, counters,
           buffer_cache, detected);
diff -r -u --color up/v8/src/wasm/graph-builder-interface.cc nw/v8/src/wasm/graph-builder-interface.cc
--- up/v8/src/wasm/graph-builder-interface.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/graph-builder-interface.cc	2023-01-19 16:46:36.428109511 -0500
@@ -71,11 +71,12 @@
 
 class WasmGraphBuildingInterface {
  public:
-  static constexpr Decoder::ValidateFlag validate = Decoder::kFullValidation;
-  using FullDecoder = WasmFullDecoder<validate, WasmGraphBuildingInterface>;
+  using ValidationTag = Decoder::NoValidationTag;
+  using FullDecoder =
+      WasmFullDecoder<ValidationTag, WasmGraphBuildingInterface>;
   using CheckForNull = compiler::WasmGraphBuilder::CheckForNull;
 
-  struct Value : public ValueBase<validate> {
+  struct Value : public ValueBase<ValidationTag> {
     TFNode* node = nullptr;
 
     template <typename... Args>
@@ -88,7 +89,6 @@
   struct TryInfo : public ZoneObject {
     SsaEnv* catch_env;
     TFNode* exception = nullptr;
-    bool first_catch = true;
 
     bool might_throw() const { return exception != nullptr; }
 
@@ -97,7 +97,7 @@
     explicit TryInfo(SsaEnv* c) : catch_env(c) {}
   };
 
-  struct Control : public ControlBase<Value, validate> {
+  struct Control : public ControlBase<Value, ValidationTag> {
     SsaEnv* merge_env = nullptr;  // merge environment for the construct.
     SsaEnv* false_env = nullptr;  // false environment (only for if).
     TryInfo* try_info = nullptr;  // information about try statements.
@@ -252,7 +252,7 @@
     builder_->TerminateLoop(effect(), control());
     // Doing a preprocessing pass to analyze loop assignments seems to pay off
     // compared to reallocating Nodes when rearranging Phis in Goto.
-    BitVector* assigned = WasmDecoder<validate>::AnalyzeLoopAssignment(
+    BitVector* assigned = WasmDecoder<ValidationTag>::AnalyzeLoopAssignment(
         decoder, decoder->pc(), decoder->num_locals(), decoder->zone());
     if (decoder->failed()) return;
     int instance_cache_index = decoder->num_locals();
@@ -408,7 +408,7 @@
     SetAndTypeNode(result, builder_->Float64Constant(value));
   }
 
-  void S128Const(FullDecoder* decoder, const Simd128Immediate<validate>& imm,
+  void S128Const(FullDecoder* decoder, const Simd128Immediate& imm,
                  Value* result) {
     SetAndTypeNode(result, builder_->Simd128Constant(imm.value));
   }
@@ -422,49 +422,46 @@
   }
 
   void RefAsNonNull(FullDecoder* decoder, const Value& arg, Value* result) {
-    TFNode* cast_node =
-        v8_flags.experimental_wasm_skip_null_checks
-            ? builder_->TypeGuard(arg.node, result->type)
-            : builder_->RefAsNonNull(arg.node, decoder->position());
+    TFNode* cast_node = builder_->AssertNotNull(arg.node, decoder->position());
     SetAndTypeNode(result, cast_node);
   }
 
   void Drop(FullDecoder* decoder) {}
 
   void LocalGet(FullDecoder* decoder, Value* result,
-                const IndexImmediate<validate>& imm) {
+                const IndexImmediate& imm) {
     result->node = ssa_env_->locals[imm.index];
   }
 
   void LocalSet(FullDecoder* decoder, const Value& value,
-                const IndexImmediate<validate>& imm) {
+                const IndexImmediate& imm) {
     ssa_env_->locals[imm.index] = value.node;
   }
 
   void LocalTee(FullDecoder* decoder, const Value& value, Value* result,
-                const IndexImmediate<validate>& imm) {
+                const IndexImmediate& imm) {
     result->node = value.node;
     ssa_env_->locals[imm.index] = value.node;
   }
 
   void GlobalGet(FullDecoder* decoder, Value* result,
-                 const GlobalIndexImmediate<validate>& imm) {
+                 const GlobalIndexImmediate& imm) {
     SetAndTypeNode(result, builder_->GlobalGet(imm.index));
   }
 
   void GlobalSet(FullDecoder* decoder, const Value& value,
-                 const GlobalIndexImmediate<validate>& imm) {
+                 const GlobalIndexImmediate& imm) {
     builder_->GlobalSet(imm.index, value.node);
   }
 
   void TableGet(FullDecoder* decoder, const Value& index, Value* result,
-                const IndexImmediate<validate>& imm) {
+                const IndexImmediate& imm) {
     SetAndTypeNode(
         result, builder_->TableGet(imm.index, index.node, decoder->position()));
   }
 
   void TableSet(FullDecoder* decoder, const Value& index, const Value& value,
-                const IndexImmediate<validate>& imm) {
+                const IndexImmediate& imm) {
     builder_->TableSet(imm.index, index.node, value.node, decoder->position());
   }
 
@@ -478,6 +475,12 @@
     Forward(decoder, obj, result);
   }
 
+  void AssertNotNull(FullDecoder* decoder, const Value& obj, Value* result) {
+    builder_->TrapIfTrue(wasm::TrapReason::kTrapIllegalCast,
+                         builder_->IsNull(obj.node), decoder->position());
+    Forward(decoder, obj, result);
+  }
+
   void NopForTestingUnsupportedInLiftoff(FullDecoder* decoder) {}
 
   void Select(FullDecoder* decoder, const Value& cond, const Value& fval,
@@ -567,11 +570,11 @@
     SetEnv(fenv);
   }
 
-  void BrTable(FullDecoder* decoder, const BranchTableImmediate<validate>& imm,
+  void BrTable(FullDecoder* decoder, const BranchTableImmediate& imm,
                const Value& key) {
     if (imm.table_count == 0) {
       // Only a default target. Do the equivalent of br.
-      uint32_t target = BranchTableIterator<validate>(decoder, imm).next();
+      uint32_t target = BranchTableIterator<ValidationTag>(decoder, imm).next();
       BrOrRet(decoder, target, 1);
       return;
     }
@@ -582,7 +585,7 @@
 
     SsaEnv* copy = Steal(decoder->zone(), branch_env);
     SetEnv(copy);
-    BranchTableIterator<validate> iterator(decoder, imm);
+    BranchTableIterator<ValidationTag> iterator(decoder, imm);
     while (iterator.has_next()) {
       uint32_t i = iterator.cur_index();
       uint32_t target = iterator.next();
@@ -604,7 +607,7 @@
   }
 
   void LoadMem(FullDecoder* decoder, LoadType type,
-               const MemoryAccessImmediate<validate>& imm, const Value& index,
+               const MemoryAccessImmediate& imm, const Value& index,
                Value* result) {
     SetAndTypeNode(result, builder_->LoadMem(
                                type.value_type(), type.mem_type(), index.node,
@@ -613,8 +616,8 @@
 
   void LoadTransform(FullDecoder* decoder, LoadType type,
                      LoadTransformationKind transform,
-                     const MemoryAccessImmediate<validate>& imm,
-                     const Value& index, Value* result) {
+                     const MemoryAccessImmediate& imm, const Value& index,
+                     Value* result) {
     SetAndTypeNode(result,
                    builder_->LoadTransform(type.value_type(), type.mem_type(),
                                            transform, index.node, imm.offset,
@@ -622,7 +625,7 @@
   }
 
   void LoadLane(FullDecoder* decoder, LoadType type, const Value& value,
-                const Value& index, const MemoryAccessImmediate<validate>& imm,
+                const Value& index, const MemoryAccessImmediate& imm,
                 const uint8_t laneidx, Value* result) {
     SetAndTypeNode(
         result, builder_->LoadLane(
@@ -631,14 +634,14 @@
   }
 
   void StoreMem(FullDecoder* decoder, StoreType type,
-                const MemoryAccessImmediate<validate>& imm, const Value& index,
+                const MemoryAccessImmediate& imm, const Value& index,
                 const Value& value) {
     builder_->StoreMem(type.mem_rep(), index.node, imm.offset, imm.alignment,
                        value.node, decoder->position(), type.value_type());
   }
 
   void StoreLane(FullDecoder* decoder, StoreType type,
-                 const MemoryAccessImmediate<validate>& imm, const Value& index,
+                 const MemoryAccessImmediate& imm, const Value& index,
                  const Value& value, const uint8_t laneidx) {
     builder_->StoreLane(type.mem_rep(), index.node, imm.offset, imm.alignment,
                         value.node, laneidx, decoder->position(),
@@ -655,8 +658,7 @@
     LoadContextIntoSsa(ssa_env_, decoder);
   }
 
-  void CallDirect(FullDecoder* decoder,
-                  const CallFunctionImmediate<validate>& imm,
+  void CallDirect(FullDecoder* decoder, const CallFunctionImmediate& imm,
                   const Value args[], Value returns[]) {
     int maybe_call_count = -1;
     if (v8_flags.wasm_speculative_inlining && type_feedback_.size() > 0) {
@@ -668,8 +670,7 @@
            args, returns);
   }
 
-  void ReturnCall(FullDecoder* decoder,
-                  const CallFunctionImmediate<validate>& imm,
+  void ReturnCall(FullDecoder* decoder, const CallFunctionImmediate& imm,
                   const Value args[]) {
     int maybe_call_count = -1;
     if (v8_flags.wasm_speculative_inlining && type_feedback_.size() > 0) {
@@ -682,8 +683,8 @@
   }
 
   void CallIndirect(FullDecoder* decoder, const Value& index,
-                    const CallIndirectImmediate<validate>& imm,
-                    const Value args[], Value returns[]) {
+                    const CallIndirectImmediate& imm, const Value args[],
+                    Value returns[]) {
     DoCall(
         decoder,
         CallInfo::CallIndirect(index, imm.table_imm.index, imm.sig_imm.index),
@@ -691,7 +692,7 @@
   }
 
   void ReturnCallIndirect(FullDecoder* decoder, const Value& index,
-                          const CallIndirectImmediate<validate>& imm,
+                          const CallIndirectImmediate& imm,
                           const Value args[]) {
     DoReturnCall(
         decoder,
@@ -886,23 +887,22 @@
   }
 
   void SimdLaneOp(FullDecoder* decoder, WasmOpcode opcode,
-                  const SimdLaneImmediate<validate>& imm,
-                  base::Vector<Value> inputs, Value* result) {
+                  const SimdLaneImmediate& imm, base::Vector<Value> inputs,
+                  Value* result) {
     NodeVector nodes(inputs.size());
     GetNodes(nodes.begin(), inputs);
     SetAndTypeNode(result,
                    builder_->SimdLaneOp(opcode, imm.lane, nodes.begin()));
   }
 
-  void Simd8x16ShuffleOp(FullDecoder* decoder,
-                         const Simd128Immediate<validate>& imm,
+  void Simd8x16ShuffleOp(FullDecoder* decoder, const Simd128Immediate& imm,
                          const Value& input0, const Value& input1,
                          Value* result) {
     TFNode* input_nodes[] = {input0.node, input1.node};
     SetAndTypeNode(result, builder_->Simd8x16ShuffleOp(imm.value, input_nodes));
   }
 
-  void Throw(FullDecoder* decoder, const TagIndexImmediate<validate>& imm,
+  void Throw(FullDecoder* decoder, const TagIndexImmediate& imm,
              const base::Vector<Value>& value_args) {
     int count = value_args.length();
     ZoneVector<TFNode*> args(count, decoder->zone());
@@ -911,7 +911,8 @@
     }
     CheckForException(decoder,
                       builder_->Throw(imm.index, imm.tag, base::VectorOf(args),
-                                      decoder->position()));
+                                      decoder->position()),
+                      kDontReloadContext);
     builder_->TerminateThrow(effect(), control());
   }
 
@@ -919,13 +920,13 @@
     DCHECK(block->is_try_catchall() || block->is_try_catch());
     TFNode* exception = block->try_info->exception;
     DCHECK_NOT_NULL(exception);
-    CheckForException(decoder, builder_->Rethrow(exception));
+    CheckForException(decoder, builder_->Rethrow(exception),
+                      kDontReloadContext);
     builder_->TerminateThrow(effect(), control());
   }
 
-  void CatchException(FullDecoder* decoder,
-                      const TagIndexImmediate<validate>& imm, Control* block,
-                      base::Vector<Value> values) {
+  void CatchException(FullDecoder* decoder, const TagIndexImmediate& imm,
+                      Control* block, base::Vector<Value> values) {
     DCHECK(block->is_try_catch());
     // The catch block is unreachable if no possible throws in the try block
     // exist. We only build a landing pad if some node in the try block can
@@ -937,10 +938,6 @@
 
     TFNode* exception = block->try_info->exception;
     SetEnv(block->try_info->catch_env);
-    if (block->try_info->first_catch) {
-      LoadContextIntoSsa(ssa_env_, decoder);
-      block->try_info->first_catch = false;
-    }
 
     TFNode* if_catch = nullptr;
     TFNode* if_no_catch = nullptr;
@@ -1018,14 +1015,11 @@
     }
 
     SetEnv(block->try_info->catch_env);
-    if (block->try_info->first_catch) {
-      LoadContextIntoSsa(ssa_env_, decoder);
-    }
   }
 
   void AtomicOp(FullDecoder* decoder, WasmOpcode opcode,
-                base::Vector<Value> args,
-                const MemoryAccessImmediate<validate>& imm, Value* result) {
+                base::Vector<Value> args, const MemoryAccessImmediate& imm,
+                Value* result) {
     NodeVector inputs(args.size());
     GetNodes(inputs.begin(), args);
     TFNode* node = builder_->AtomicOp(opcode, inputs.begin(), imm.alignment,
@@ -1035,65 +1029,61 @@
 
   void AtomicFence(FullDecoder* decoder) { builder_->AtomicFence(); }
 
-  void MemoryInit(FullDecoder* decoder,
-                  const MemoryInitImmediate<validate>& imm, const Value& dst,
-                  const Value& src, const Value& size) {
+  void MemoryInit(FullDecoder* decoder, const MemoryInitImmediate& imm,
+                  const Value& dst, const Value& src, const Value& size) {
     builder_->MemoryInit(imm.data_segment.index, dst.node, src.node, size.node,
                          decoder->position());
   }
 
-  void DataDrop(FullDecoder* decoder, const IndexImmediate<validate>& imm) {
+  void DataDrop(FullDecoder* decoder, const IndexImmediate& imm) {
     builder_->DataDrop(imm.index, decoder->position());
   }
 
-  void MemoryCopy(FullDecoder* decoder,
-                  const MemoryCopyImmediate<validate>& imm, const Value& dst,
-                  const Value& src, const Value& size) {
+  void MemoryCopy(FullDecoder* decoder, const MemoryCopyImmediate& imm,
+                  const Value& dst, const Value& src, const Value& size) {
     builder_->MemoryCopy(dst.node, src.node, size.node, decoder->position());
   }
 
-  void MemoryFill(FullDecoder* decoder,
-                  const MemoryIndexImmediate<validate>& imm, const Value& dst,
-                  const Value& value, const Value& size) {
+  void MemoryFill(FullDecoder* decoder, const MemoryIndexImmediate& imm,
+                  const Value& dst, const Value& value, const Value& size) {
     builder_->MemoryFill(dst.node, value.node, size.node, decoder->position());
   }
 
-  void TableInit(FullDecoder* decoder, const TableInitImmediate<validate>& imm,
+  void TableInit(FullDecoder* decoder, const TableInitImmediate& imm,
                  base::Vector<Value> args) {
     builder_->TableInit(imm.table.index, imm.element_segment.index,
                         args[0].node, args[1].node, args[2].node,
                         decoder->position());
   }
 
-  void ElemDrop(FullDecoder* decoder, const IndexImmediate<validate>& imm) {
+  void ElemDrop(FullDecoder* decoder, const IndexImmediate& imm) {
     builder_->ElemDrop(imm.index, decoder->position());
   }
 
-  void TableCopy(FullDecoder* decoder, const TableCopyImmediate<validate>& imm,
+  void TableCopy(FullDecoder* decoder, const TableCopyImmediate& imm,
                  base::Vector<Value> args) {
     builder_->TableCopy(imm.table_dst.index, imm.table_src.index, args[0].node,
                         args[1].node, args[2].node, decoder->position());
   }
 
-  void TableGrow(FullDecoder* decoder, const IndexImmediate<validate>& imm,
+  void TableGrow(FullDecoder* decoder, const IndexImmediate& imm,
                  const Value& value, const Value& delta, Value* result) {
     SetAndTypeNode(result,
                    builder_->TableGrow(imm.index, value.node, delta.node));
   }
 
-  void TableSize(FullDecoder* decoder, const IndexImmediate<validate>& imm,
+  void TableSize(FullDecoder* decoder, const IndexImmediate& imm,
                  Value* result) {
     SetAndTypeNode(result, builder_->TableSize(imm.index));
   }
 
-  void TableFill(FullDecoder* decoder, const IndexImmediate<validate>& imm,
+  void TableFill(FullDecoder* decoder, const IndexImmediate& imm,
                  const Value& start, const Value& value, const Value& count) {
     builder_->TableFill(imm.index, start.node, value.node, count.node);
   }
 
-  void StructNew(FullDecoder* decoder,
-                 const StructIndexImmediate<validate>& imm, const Value& rtt,
-                 const Value args[], Value* result) {
+  void StructNew(FullDecoder* decoder, const StructIndexImmediate& imm,
+                 const Value& rtt, const Value args[], Value* result) {
     uint32_t field_count = imm.struct_type->field_count();
     NodeVector arg_nodes(field_count);
     for (uint32_t i = 0; i < field_count; i++) {
@@ -1103,8 +1093,7 @@
                    builder_->StructNew(imm.index, imm.struct_type, rtt.node,
                                        base::VectorOf(arg_nodes)));
   }
-  void StructNewDefault(FullDecoder* decoder,
-                        const StructIndexImmediate<validate>& imm,
+  void StructNewDefault(FullDecoder* decoder, const StructIndexImmediate& imm,
                         const Value& rtt, Value* result) {
     uint32_t field_count = imm.struct_type->field_count();
     NodeVector arg_nodes(field_count);
@@ -1119,8 +1108,7 @@
   }
 
   void StructGet(FullDecoder* decoder, const Value& struct_object,
-                 const FieldImmediate<validate>& field, bool is_signed,
-                 Value* result) {
+                 const FieldImmediate& field, bool is_signed, Value* result) {
     SetAndTypeNode(result, builder_->StructGet(struct_object.node,
                                                field.struct_imm.struct_type,
                                                field.field_imm.index,
@@ -1129,14 +1117,13 @@
   }
 
   void StructSet(FullDecoder* decoder, const Value& struct_object,
-                 const FieldImmediate<validate>& field,
-                 const Value& field_value) {
+                 const FieldImmediate& field, const Value& field_value) {
     builder_->StructSet(struct_object.node, field.struct_imm.struct_type,
                         field.field_imm.index, field_value.node,
                         NullCheckFor(struct_object.type), decoder->position());
   }
 
-  void ArrayNew(FullDecoder* decoder, const ArrayIndexImmediate<validate>& imm,
+  void ArrayNew(FullDecoder* decoder, const ArrayIndexImmediate& imm,
                 const Value& length, const Value& initial_value,
                 const Value& rtt, Value* result) {
     SetAndTypeNode(result, builder_->ArrayNew(imm.index, imm.array_type,
@@ -1147,8 +1134,7 @@
     if (!loop_infos_.empty()) loop_infos_.back().can_be_innermost = false;
   }
 
-  void ArrayNewDefault(FullDecoder* decoder,
-                       const ArrayIndexImmediate<validate>& imm,
+  void ArrayNewDefault(FullDecoder* decoder, const ArrayIndexImmediate& imm,
                        const Value& length, const Value& rtt, Value* result) {
     // This will be set in {builder_}.
     TFNode* initial_value = nullptr;
@@ -1158,7 +1144,7 @@
   }
 
   void ArrayGet(FullDecoder* decoder, const Value& array_obj,
-                const ArrayIndexImmediate<validate>& imm, const Value& index,
+                const ArrayIndexImmediate& imm, const Value& index,
                 bool is_signed, Value* result) {
     SetAndTypeNode(
         result, builder_->ArrayGet(array_obj.node, imm.array_type, index.node,
@@ -1167,7 +1153,7 @@
   }
 
   void ArraySet(FullDecoder* decoder, const Value& array_obj,
-                const ArrayIndexImmediate<validate>& imm, const Value& index,
+                const ArrayIndexImmediate& imm, const Value& index,
                 const Value& value) {
     builder_->ArraySet(array_obj.node, imm.array_type, index.node, value.node,
                        NullCheckFor(array_obj.type), decoder->position());
@@ -1187,8 +1173,7 @@
                         length.node, decoder->position());
   }
 
-  void ArrayNewFixed(FullDecoder* decoder,
-                     const ArrayIndexImmediate<validate>& imm,
+  void ArrayNewFixed(FullDecoder* decoder, const ArrayIndexImmediate& imm,
                      const base::Vector<Value>& elements, const Value& rtt,
                      Value* result) {
     NodeVector element_nodes(elements.size());
@@ -1200,10 +1185,9 @@
   }
 
   void ArrayNewSegment(FullDecoder* decoder,
-                       const ArrayIndexImmediate<validate>& array_imm,
-                       const IndexImmediate<validate>& data_segment,
-                       const Value& offset, const Value& length,
-                       const Value& rtt, Value* result) {
+                       const ArrayIndexImmediate& array_imm,
+                       const IndexImmediate& data_segment, const Value& offset,
+                       const Value& length, const Value& rtt, Value* result) {
     SetAndTypeNode(result,
                    builder_->ArrayNewSegment(
                        array_imm.array_type, data_segment.index, offset.node,
@@ -1232,26 +1216,12 @@
 
   using WasmTypeCheckConfig = v8::internal::compiler::WasmTypeCheckConfig;
 
-  WasmTypeCheckConfig ComputeWasmTypeCheckConfig(ValueType object_type,
-                                                 ValueType rtt_type,
-                                                 const WasmModule* module,
-                                                 bool null_succeeds) {
-    WasmTypeCheckConfig result;
-    result.object_can_be_null = object_type.is_nullable();
-    DCHECK(object_type.is_object_reference());  // Checked by validation.
-    result.null_succeeds = null_succeeds;
-    // In the bottom case, the result is irrelevant.
-    result.rtt_depth = rtt_type.is_bottom()
-                           ? 0 /* unused */
-                           : static_cast<uint8_t>(GetSubtypingDepth(
-                                 module, rtt_type.ref_index()));
-    return result;
-  }
-
   void RefTest(FullDecoder* decoder, const Value& object, const Value& rtt,
                Value* result, bool null_succeeds) {
-    WasmTypeCheckConfig config = ComputeWasmTypeCheckConfig(
-        object.type, rtt.type, decoder->module_, null_succeeds);
+    WasmTypeCheckConfig config = {
+        object.type,
+        ValueType::RefMaybeNull(rtt.type.ref_index(),
+                                null_succeeds ? kNullable : kNonNullable)};
     SetAndTypeNode(result, builder_->RefTest(object.node, rtt.node, config));
   }
 
@@ -1262,11 +1232,11 @@
   }
 
   void RefCast(FullDecoder* decoder, const Value& object, const Value& rtt,
-               Value* result) {
-    // TODO(mliedtke): Should be a parameter for generic ref.cast instructions.
-    const bool null_succeeds = false;
-    WasmTypeCheckConfig config = ComputeWasmTypeCheckConfig(
-        object.type, rtt.type, decoder->module_, null_succeeds);
+               Value* result, bool null_succeeds) {
+    WasmTypeCheckConfig config = {
+        object.type,
+        ValueType::RefMaybeNull(rtt.type.ref_index(),
+                                null_succeeds ? kNullable : kNonNullable)};
     TFNode* cast_node = v8_flags.experimental_wasm_assume_ref_cast_succeeds
                             ? builder_->TypeGuard(object.node, result->type)
                             : builder_->RefCast(object.node, rtt.node, config,
@@ -1274,17 +1244,27 @@
     SetAndTypeNode(result, cast_node);
   }
 
+  void RefCastAbstract(FullDecoder* decoder, const Value& object,
+                       wasm::HeapType type, Value* result, bool null_succeeds) {
+    TFNode* node = object.node;
+    if (!v8_flags.experimental_wasm_assume_ref_cast_succeeds) {
+      node = builder_->RefCastAbstract(object.node, type, decoder->position(),
+                                       null_succeeds);
+    }
+    SetAndTypeNode(result, builder_->TypeGuard(node, result->type));
+  }
+
   template <void (compiler::WasmGraphBuilder::*branch_function)(
       TFNode*, TFNode*, WasmTypeCheckConfig, TFNode**, TFNode**, TFNode**,
       TFNode**)>
   void BrOnCastAbs(FullDecoder* decoder, const Value& object, const Value& rtt,
                    Value* forwarding_value, uint32_t br_depth,
                    bool branch_on_match) {
-    // TODO(mliedtke): Should be a parameter for generic br_on_cast
-    // instructions.
-    const bool null_succeeds = false;
-    WasmTypeCheckConfig config = ComputeWasmTypeCheckConfig(
-        object.type, rtt.type, decoder->module_, null_succeeds);
+    // TODO(mliedtke): Add generic br_on_cast instructions where null succeeds.
+    WasmTypeCheckConfig config = {object.type,
+                                  !rtt.type.is_bottom()
+                                      ? ValueType::Ref(rtt.type.ref_index())
+                                      : kWasmBottom};
     SsaEnv* branch_env = Split(decoder->zone(), ssa_env_);
     SsaEnv* no_branch_env = Steal(decoder->zone(), ssa_env_);
     no_branch_env->SetNotMerged();
@@ -1322,30 +1302,32 @@
                                      null_succeeds));
   }
 
-  void RefIsData(FullDecoder* decoder, const Value& object, Value* result) {
+  void RefIsStruct(FullDecoder* decoder, const Value& object, Value* result) {
     bool null_succeeds = false;
     SetAndTypeNode(result,
-                   builder_->RefIsData(object.node, object.type.is_nullable(),
-                                       null_succeeds));
+                   builder_->RefIsStruct(object.node, object.type.is_nullable(),
+                                         null_succeeds));
   }
 
-  void RefAsData(FullDecoder* decoder, const Value& object, Value* result) {
-    TFNode* cast_object = builder_->RefAsData(
-        object.node, object.type.is_nullable(), decoder->position());
+  void RefAsStruct(FullDecoder* decoder, const Value& object, Value* result) {
+    bool null_succeeds = false;
+    TFNode* cast_object =
+        builder_->RefAsStruct(object.node, object.type.is_nullable(),
+                              decoder->position(), null_succeeds);
     TFNode* rename = builder_->TypeGuard(cast_object, result->type);
     SetAndTypeNode(result, rename);
   }
 
-  void BrOnData(FullDecoder* decoder, const Value& object,
-                Value* value_on_branch, uint32_t br_depth) {
-    BrOnCastAbs<&compiler::WasmGraphBuilder::BrOnData>(
+  void BrOnStruct(FullDecoder* decoder, const Value& object,
+                  Value* value_on_branch, uint32_t br_depth) {
+    BrOnCastAbs<&compiler::WasmGraphBuilder::BrOnStruct>(
         decoder, object, Value{nullptr, kWasmBottom}, value_on_branch, br_depth,
         true);
   }
 
-  void BrOnNonData(FullDecoder* decoder, const Value& object,
-                   Value* value_on_fallthrough, uint32_t br_depth) {
-    BrOnCastAbs<&compiler::WasmGraphBuilder::BrOnData>(
+  void BrOnNonStruct(FullDecoder* decoder, const Value& object,
+                     Value* value_on_fallthrough, uint32_t br_depth) {
+    BrOnCastAbs<&compiler::WasmGraphBuilder::BrOnStruct>(
         decoder, object, Value{nullptr, kWasmBottom}, value_on_fallthrough,
         br_depth, false);
   }
@@ -1358,8 +1340,10 @@
   }
 
   void RefAsArray(FullDecoder* decoder, const Value& object, Value* result) {
-    TFNode* cast_object = builder_->RefAsArray(
-        object.node, object.type.is_nullable(), decoder->position());
+    bool null_succeeds = false;
+    TFNode* cast_object =
+        builder_->RefAsArray(object.node, object.type.is_nullable(),
+                             decoder->position(), null_succeeds);
     TFNode* rename = builder_->TypeGuard(cast_object, result->type);
     SetAndTypeNode(result, rename);
   }
@@ -1384,7 +1368,9 @@
   }
 
   void RefAsI31(FullDecoder* decoder, const Value& object, Value* result) {
-    TFNode* cast_object = builder_->RefAsI31(object.node, decoder->position());
+    bool null_succeeds = false;
+    TFNode* cast_object =
+        builder_->RefAsI31(object.node, decoder->position(), null_succeeds);
     TFNode* rename = builder_->TypeGuard(cast_object, result->type);
     SetAndTypeNode(result, rename);
   }
@@ -1403,8 +1389,7 @@
         br_depth, false);
   }
 
-  void StringNewWtf8(FullDecoder* decoder,
-                     const MemoryIndexImmediate<validate>& memory,
+  void StringNewWtf8(FullDecoder* decoder, const MemoryIndexImmediate& memory,
                      const unibrow::Utf8Variant variant, const Value& offset,
                      const Value& size, Value* result) {
     SetAndTypeNode(result, builder_->StringNewWtf8(memory.index, variant,
@@ -1419,8 +1404,7 @@
                                                         start.node, end.node));
   }
 
-  void StringNewWtf16(FullDecoder* decoder,
-                      const MemoryIndexImmediate<validate>& imm,
+  void StringNewWtf16(FullDecoder* decoder, const MemoryIndexImmediate& imm,
                       const Value& offset, const Value& size, Value* result) {
     SetAndTypeNode(result,
                    builder_->StringNewWtf16(imm.index, offset.node, size.node));
@@ -1433,8 +1417,8 @@
                                                          end.node));
   }
 
-  void StringConst(FullDecoder* decoder,
-                   const StringConstImmediate<validate>& imm, Value* result) {
+  void StringConst(FullDecoder* decoder, const StringConstImmediate& imm,
+                   Value* result) {
     SetAndTypeNode(result, builder_->StringConst(imm.index));
   }
 
@@ -1461,7 +1445,7 @@
   }
 
   void StringEncodeWtf8(FullDecoder* decoder,
-                        const MemoryIndexImmediate<validate>& memory,
+                        const MemoryIndexImmediate& memory,
                         const unibrow::Utf8Variant variant, const Value& str,
                         const Value& offset, Value* result) {
     result->node = builder_->StringEncodeWtf8(memory.index, variant, str.node,
@@ -1478,8 +1462,7 @@
         NullCheckFor(array.type), start.node, decoder->position());
   }
 
-  void StringEncodeWtf16(FullDecoder* decoder,
-                         const MemoryIndexImmediate<validate>& imm,
+  void StringEncodeWtf16(FullDecoder* decoder, const MemoryIndexImmediate& imm,
                          const Value& str, const Value& offset, Value* result) {
     result->node =
         builder_->StringEncodeWtf16(imm.index, str.node, NullCheckFor(str.type),
@@ -1529,7 +1512,7 @@
   }
 
   void StringViewWtf8Encode(FullDecoder* decoder,
-                            const MemoryIndexImmediate<validate>& memory,
+                            const MemoryIndexImmediate& memory,
                             const unibrow::Utf8Variant variant,
                             const Value& view, const Value& addr,
                             const Value& pos, const Value& bytes,
@@ -1552,9 +1535,9 @@
     // Since we implement stringview_wtf16 as string, that's the type we'll
     // use for the Node. (The decoder's Value type must be stringview_wtf16
     // because static type validation relies on it.)
-    result->node =
-        builder_->SetType(builder_->RefAsNonNull(str.node, decoder->position()),
-                          ValueType::Ref(HeapType::kString));
+    result->node = builder_->SetType(
+        builder_->AssertNotNull(str.node, decoder->position()),
+        ValueType::Ref(HeapType::kString));
   }
 
   void StringViewWtf16GetCodeUnit(FullDecoder* decoder, const Value& view,
@@ -1564,10 +1547,9 @@
   }
 
   void StringViewWtf16Encode(FullDecoder* decoder,
-                             const MemoryIndexImmediate<validate>& imm,
-                             const Value& view, const Value& offset,
-                             const Value& pos, const Value& codeunits,
-                             Value* result) {
+                             const MemoryIndexImmediate& imm, const Value& view,
+                             const Value& offset, const Value& pos,
+                             const Value& codeunits, Value* result) {
     result->node = builder_->StringViewWtf16Encode(
         imm.index, view.node, NullCheckFor(view.type), offset.node, pos.node,
         codeunits.node, decoder->position());
@@ -1704,7 +1686,10 @@
     builder_->set_instance_cache(&env->instance_cache);
   }
 
-  TFNode* CheckForException(FullDecoder* decoder, TFNode* node) {
+  enum ReloadContextAfterException { kDontReloadContext, kReloadContext };
+
+  TFNode* CheckForException(FullDecoder* decoder, TFNode* node,
+                            ReloadContextAfterException reload_mode) {
     DCHECK_NOT_NULL(node);
 
     // We need to emit IfSuccess/IfException nodes if this node throws and has
@@ -1730,6 +1715,13 @@
     exception_env->effect = if_exception;
     SetEnv(exception_env);
 
+    // If the exceptional operation could have modified memory size, we need to
+    // reload the memory context into the exceptional control path.
+    if (reload_mode == kReloadContext &&
+        decoder->module_->initial_pages != decoder->module_->maximum_pages) {
+      LoadContextIntoSsa(ssa_env_, decoder);
+    }
+
     if (emit_loop_exits()) {
       ValueVector values;
       BuildNestedLoopExits(decoder,
@@ -1972,28 +1964,29 @@
       arg_nodes[i + 1] = args[i].node;
     }
     switch (call_info.call_mode()) {
-      case CallInfo::kCallIndirect:
-        CheckForException(
-            decoder, builder_->CallIndirect(
-                         call_info.table_index(), call_info.sig_index(),
-                         base::VectorOf(arg_nodes),
-                         base::VectorOf(return_nodes), decoder->position()));
+      case CallInfo::kCallIndirect: {
+        TFNode* call = builder_->CallIndirect(
+            call_info.table_index(), call_info.sig_index(),
+            base::VectorOf(arg_nodes), base::VectorOf(return_nodes),
+            decoder->position());
+        CheckForException(decoder, call, kReloadContext);
         break;
+      }
       case CallInfo::kCallDirect: {
         TFNode* call = builder_->CallDirect(
             call_info.callee_index(), base::VectorOf(arg_nodes),
             base::VectorOf(return_nodes), decoder->position());
         builder_->StoreCallCount(call, call_info.call_count());
-        CheckForException(decoder, call);
+        CheckForException(decoder, call, kReloadContext);
         break;
       }
-      case CallInfo::kCallRef:
-        CheckForException(
-            decoder,
-            builder_->CallRef(sig, base::VectorOf(arg_nodes),
-                              base::VectorOf(return_nodes),
-                              call_info.null_check(), decoder->position()));
+      case CallInfo::kCallRef: {
+        TFNode* call = builder_->CallRef(
+            sig, base::VectorOf(arg_nodes), base::VectorOf(return_nodes),
+            call_info.null_check(), decoder->position());
+        CheckForException(decoder, call, kReloadContext);
         break;
+      }
     }
     for (size_t i = 0; i < return_count; ++i) {
       SetAndTypeNode(&returns[i], return_nodes[i]);
@@ -2106,7 +2099,7 @@
       }
       if (exception_value != nullptr) {
         *exception_value = builder_->LoopExitValue(
-            *exception_value, MachineRepresentation::kWord32);
+            *exception_value, MachineRepresentation::kTaggedPointer);
       }
       if (wrap_exit_values) {
         WrapLocalsAtLoopExit(decoder, control);
@@ -2116,9 +2109,8 @@
 
   CheckForNull NullCheckFor(ValueType type) {
     DCHECK(type.is_object_reference());
-    return (!v8_flags.experimental_wasm_skip_null_checks && type.is_nullable())
-               ? CheckForNull::kWithNullCheck
-               : CheckForNull::kWithoutNullCheck;
+    return type.is_nullable() ? CheckForNull::kWithNullCheck
+                              : CheckForNull::kWithoutNullCheck;
   }
 
   void SetAndTypeNode(Value* value, TFNode* node) {
@@ -2138,7 +2130,7 @@
                           compiler::NodeOriginTable* node_origins,
                           int func_index, InlinedStatus inlined_status) {
   Zone zone(allocator, ZONE_NAME);
-  WasmFullDecoder<Decoder::kFullValidation, WasmGraphBuildingInterface> decoder(
+  WasmFullDecoder<Decoder::NoValidationTag, WasmGraphBuildingInterface> decoder(
       &zone, module, enabled, detected, body, builder, func_index,
       inlined_status);
   if (node_origins) {
diff -r -u --color up/v8/src/wasm/module-compiler.cc nw/v8/src/wasm/module-compiler.cc
--- up/v8/src/wasm/module-compiler.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/module-compiler.cc	2023-01-19 16:46:36.428109511 -0500
@@ -5,9 +5,7 @@
 #include "src/wasm/module-compiler.h"
 
 #include <algorithm>
-#include <mutex>  // NOLINT(build/c++11)
 #include <queue>
-#include <shared_mutex>
 
 #include "src/api/api-inl.h"
 #include "src/base/enum-set.h"
@@ -25,6 +23,7 @@
 #include "src/wasm/assembler-buffer-cache.h"
 #include "src/wasm/code-space-access.h"
 #include "src/wasm/module-decoder.h"
+#include "src/wasm/pgo.h"
 #include "src/wasm/streaming-decoder.h"
 #include "src/wasm/wasm-code-manager.h"
 #include "src/wasm/wasm-engine.h"
@@ -138,14 +137,14 @@
   Queue* GetQueueForTask(int task_id) {
     int required_queues = task_id + 1;
     {
-      std::shared_lock<std::shared_mutex> queues_guard{queues_mutex_};
+      base::SharedMutexGuard<base::kShared> queues_guard{&queues_mutex_};
       if (V8_LIKELY(static_cast<int>(queues_.size()) >= required_queues)) {
         return queues_[task_id].get();
       }
     }
 
     // Otherwise increase the number of queues.
-    std::unique_lock<std::shared_mutex> queues_guard{queues_mutex_};
+    base::SharedMutexGuard<base::kExclusive> queues_guard{&queues_mutex_};
     int num_queues = static_cast<int>(queues_.size());
     while (num_queues < required_queues) {
       int steal_from = num_queues + 1;
@@ -200,7 +199,7 @@
     QueueImpl* queue;
     {
       int queue_to_add = next_queue_to_add.load(std::memory_order_relaxed);
-      std::shared_lock<std::shared_mutex> queues_guard{queues_mutex_};
+      base::SharedMutexGuard<base::kShared> queues_guard{&queues_mutex_};
       while (!next_queue_to_add.compare_exchange_weak(
           queue_to_add, next_task_id(queue_to_add, queues_.size()),
           std::memory_order_relaxed)) {
@@ -234,7 +233,7 @@
   }
 
   void AddTopTierPriorityUnit(WasmCompilationUnit unit, size_t priority) {
-    std::shared_lock<std::shared_mutex> queues_guard{queues_mutex_};
+    base::SharedMutexGuard<base::kShared> queues_guard{&queues_mutex_};
     // Add to the individual queues in a round-robin fashion. No special care is
     // taken to balance them; they will be balanced by work stealing.
     // Priorities should only be seen as a hint here; without balancing, we
@@ -382,7 +381,7 @@
     // Try to steal from all other queues. If this succeeds, return one of the
     // stolen units.
     {
-      std::shared_lock<std::shared_mutex> guard{queues_mutex_};
+      base::SharedMutexGuard<base::kShared> guard{&queues_mutex_};
       for (size_t steal_trials = 0; steal_trials < queues_.size();
            ++steal_trials, ++steal_task_id) {
         if (steal_task_id >= static_cast<int>(queues_.size())) {
@@ -439,7 +438,7 @@
     // Try to steal from all other queues. If this succeeds, return one of the
     // stolen units.
     {
-      std::shared_lock<std::shared_mutex> guard{queues_mutex_};
+      base::SharedMutexGuard<base::kShared> guard{&queues_mutex_};
       for (size_t steal_trials = 0; steal_trials < queues_.size();
            ++steal_trials, ++steal_task_id) {
         if (steal_task_id >= static_cast<int>(queues_.size())) {
@@ -514,7 +513,7 @@
   }
 
   // {queues_mutex_} protectes {queues_};
-  std::shared_mutex queues_mutex_;
+  base::SharedMutex queues_mutex_;
   std::vector<std::unique_ptr<QueueImpl>> queues_;
 
   const int num_declared_functions_;
@@ -565,11 +564,16 @@
   void ApplyCompilationHintToInitialProgress(const WasmCompilationHint& hint,
                                              size_t hint_idx);
 
+  // Use PGO information to choose a better initial compilation progress
+  // (tiering decisions).
+  void ApplyPgoInfoToInitialProgress(ProfileInformation* pgo_info);
+
   // Initialize compilation progress. Set compilation tiers to expect for
   // baseline and top tier compilation. Must be set before
   // {CommitCompilationUnits} is invoked which triggers background compilation.
   void InitializeCompilationProgress(int num_import_wrappers,
-                                     int num_export_wrappers);
+                                     int num_export_wrappers,
+                                     ProfileInformation* pgo_info);
 
   // Initialize the compilation progress after deserialization. This is needed
   // for recompilation (e.g. for tier down) to work later.
@@ -1109,12 +1113,13 @@
 
 void ValidateSequentially(
     const WasmModule* module, NativeModule* native_module, Counters* counters,
-    AccountingAllocator* allocator, ErrorThrower* thrower, bool lazy_module,
+    AccountingAllocator* allocator, ErrorThrower* thrower,
     OnlyLazyFunctions only_lazy_functions = kAllFunctions) {
   DCHECK(!thrower->error());
   uint32_t start = module->num_imported_functions;
   uint32_t end = start + module->num_declared_functions;
   auto enabled_features = native_module->enabled_features();
+  bool lazy_module = v8_flags.wasm_lazy_compilation;
   for (uint32_t func_index = start; func_index < end; func_index++) {
     // Skip non-lazy functions if requested.
     if (only_lazy_functions) {
@@ -1163,10 +1168,11 @@
 
 }  // namespace
 
-bool CompileLazy(Isolate* isolate, Handle<WasmInstanceObject> instance,
+bool CompileLazy(Isolate* isolate, WasmInstanceObject instance,
                  int func_index) {
-  Handle<WasmModuleObject> module_object(instance->module_object(), isolate);
-  NativeModule* native_module = module_object->native_module();
+  DisallowGarbageCollection no_gc;
+  WasmModuleObject module_object = instance.module_object();
+  NativeModule* native_module = module_object.native_module();
   Counters* counters = isolate->counters();
 
   // Put the timer scope around everything, including the {CodeSpaceWriteScope}
@@ -1225,12 +1231,11 @@
   DCHECK_EQ(func_index, code->index());
 
   if (WasmCode::ShouldBeLogged(isolate)) {
-    DisallowGarbageCollection no_gc;
-    Object url_obj = module_object->script().name();
+    Object url_obj = module_object.script().name();
     DCHECK(url_obj.IsString() || url_obj.IsUndefined());
     std::unique_ptr<char[]> url =
         url_obj.IsString() ? String::cast(url_obj).ToCString() : nullptr;
-    code->LogCode(isolate, url.get(), module_object->script().id());
+    code->LogCode(isolate, url.get(), module_object.script().id());
   }
 
   counters->wasm_lazily_compiled_functions()->Increment();
@@ -1243,17 +1248,6 @@
     WasmCompilationUnit tiering_unit{func_index, tiers.top_tier, kNoDebugging};
     compilation_state->CommitTopTierCompilationUnit(tiering_unit);
   }
-
-  // Allocate feedback vector if needed.
-  int feedback_vector_slots = NumFeedbackSlots(module, func_index);
-  if (feedback_vector_slots > 0) {
-    DCHECK(v8_flags.wasm_speculative_inlining);
-    Handle<FixedArray> vector =
-        isolate->factory()->NewFixedArrayWithZeroes(feedback_vector_slots);
-    instance->feedback_vectors().set(
-        declared_function_index(module, func_index), *vector);
-  }
-
   return true;
 }
 
@@ -1790,7 +1784,8 @@
 }
 
 std::unique_ptr<CompilationUnitBuilder> InitializeCompilation(
-    Isolate* isolate, NativeModule* native_module) {
+    Isolate* isolate, NativeModule* native_module,
+    ProfileInformation* pgo_info) {
   InitializeLazyCompilation(native_module);
   CompilationStateImpl* compilation_state =
       Impl(native_module->compilation_state());
@@ -1798,21 +1793,21 @@
   int num_import_wrappers = AddImportWrapperUnits(native_module, builder.get());
   int num_export_wrappers =
       AddExportWrapperUnits(isolate, native_module, builder.get());
-  compilation_state->InitializeCompilationProgress(num_import_wrappers,
-                                                   num_export_wrappers);
+  compilation_state->InitializeCompilationProgress(
+      num_import_wrappers, num_export_wrappers, pgo_info);
   return builder;
 }
 
 bool MayCompriseLazyFunctions(const WasmModule* module,
-                              const WasmFeatures& enabled_features,
-                              bool lazy_module) {
-  if (lazy_module || enabled_features.has_compilation_hints()) return true;
+                              const WasmFeatures& enabled_features) {
+  if (IsLazyModule(module)) return true;
+  if (enabled_features.has_compilation_hints()) return true;
 #ifdef ENABLE_SLOW_DCHECKS
   int start = module->num_imported_functions;
   int end = start + module->num_declared_functions;
   for (int func_index = start; func_index < end; func_index++) {
     SLOW_DCHECK(GetCompileStrategy(module, enabled_features, func_index,
-                                   lazy_module) != CompileStrategy::kLazy);
+                                   false) != CompileStrategy::kLazy);
   }
 #endif
   return false;
@@ -1893,19 +1888,18 @@
 void CompileNativeModule(Isolate* isolate,
                          v8::metrics::Recorder::ContextId context_id,
                          ErrorThrower* thrower, const WasmModule* wasm_module,
-                         std::shared_ptr<NativeModule> native_module) {
+                         std::shared_ptr<NativeModule> native_module,
+                         ProfileInformation* pgo_info) {
   CHECK(!v8_flags.jitless);
   ModuleWireBytes wire_bytes(native_module->wire_bytes());
-  const bool lazy_module = IsLazyModule(wasm_module);
   if (!v8_flags.wasm_lazy_validation && wasm_module->origin == kWasmOrigin &&
-      MayCompriseLazyFunctions(wasm_module, native_module->enabled_features(),
-                               lazy_module)) {
+      MayCompriseLazyFunctions(wasm_module,
+                               native_module->enabled_features())) {
     // Validate wasm modules for lazy compilation if requested. Never validate
     // asm.js modules as these are valid by construction (additionally a CHECK
     // will catch this during lazy compilation).
     ValidateSequentially(wasm_module, native_module.get(), isolate->counters(),
-                         isolate->allocator(), thrower, lazy_module,
-                         kOnlyLazyFunctions);
+                         isolate->allocator(), thrower, kOnlyLazyFunctions);
     // On error: Return and leave the module in an unexecutable state.
     if (thrower->error()) return;
   }
@@ -1922,21 +1916,21 @@
 
   // Initialize the compilation units and kick off background compile tasks.
   std::unique_ptr<CompilationUnitBuilder> builder =
-      InitializeCompilation(isolate, native_module.get());
+      InitializeCompilation(isolate, native_module.get(), pgo_info);
   compilation_state->InitializeCompilationUnits(std::move(builder));
 
   compilation_state->WaitForCompilationEvent(
       CompilationEvent::kFinishedExportWrappers);
 
   if (compilation_state->failed()) {
-    DCHECK_IMPLIES(lazy_module, !v8_flags.wasm_lazy_validation);
+    DCHECK_IMPLIES(IsLazyModule(wasm_module), !v8_flags.wasm_lazy_validation);
     ValidateSequentially(wasm_module, native_module.get(), isolate->counters(),
-                         isolate->allocator(), thrower, lazy_module);
+                         isolate->allocator(), thrower);
     CHECK(thrower->error());
     return;
   }
 
-  compilation_state->FinalizeJSToWasmWrappers(isolate, native_module->module());
+  compilation_state->FinalizeJSToWasmWrappers(isolate, wasm_module);
 
   compilation_state->WaitForCompilationEvent(
       CompilationEvent::kFinishedBaselineCompilation);
@@ -1944,9 +1938,9 @@
   compilation_state->PublishDetectedFeatures(isolate);
 
   if (compilation_state->failed()) {
-    DCHECK_IMPLIES(lazy_module, !v8_flags.wasm_lazy_validation);
+    DCHECK_IMPLIES(IsLazyModule(wasm_module), !v8_flags.wasm_lazy_validation);
     ValidateSequentially(wasm_module, native_module.get(), isolate->counters(),
-                         isolate->allocator(), thrower, lazy_module);
+                         isolate->allocator(), thrower);
     CHECK(thrower->error());
   }
 }
@@ -1988,7 +1982,8 @@
 std::shared_ptr<NativeModule> CompileToNativeModule(
     Isolate* isolate, const WasmFeatures& enabled, ErrorThrower* thrower,
     std::shared_ptr<const WasmModule> module, const ModuleWireBytes& wire_bytes,
-    int compilation_id, v8::metrics::Recorder::ContextId context_id) {
+    int compilation_id, v8::metrics::Recorder::ContextId context_id,
+    ProfileInformation* pgo_info) {
   const WasmModule* wasm_module = module.get();
   WasmEngine* engine = GetWasmEngine();
   base::OwnedVector<uint8_t> wire_bytes_copy =
@@ -2029,7 +2024,8 @@
   // Sync compilation is user blocking, so we increase the priority.
   native_module->compilation_state()->SetHighPriority();
 
-  CompileNativeModule(isolate, context_id, thrower, wasm_module, native_module);
+  CompileNativeModule(isolate, context_id, thrower, wasm_module, native_module,
+                      pgo_info);
   bool cache_hit = !engine->UpdateNativeModuleCache(thrower->error(),
                                                     &native_module, isolate);
   if (thrower->error()) return {};
@@ -2092,7 +2088,6 @@
       api_method_name_(api_method_name),
       enabled_features_(enabled),
       dynamic_tiering_(DynamicTiering{v8_flags.wasm_dynamic_tiering.value()}),
-      wasm_lazy_compilation_(v8_flags.wasm_lazy_compilation),
       start_time_(base::TimeTicks::Now()),
       bytes_copy_(std::move(bytes_copy)),
       wire_bytes_(bytes_copy_.get(), bytes_copy_.get() + length),
@@ -2282,7 +2277,7 @@
           true,                                     // streamed
           is_after_cache_hit,                       // cached
           is_after_deserialization,                 // deserialized
-          wasm_lazy_compilation_,                   // lazy
+          v8_flags.wasm_lazy_compilation,           // lazy
           !compilation_state->failed(),             // success
           native_module_->turbofan_code_size(),     // code_size_in_bytes
           native_module_->liftoff_bailout_count(),  // liftoff_bailout_count
@@ -2351,10 +2346,8 @@
 void AsyncCompileJob::AsyncCompileFailed() {
   ErrorThrower thrower(isolate_, api_method_name_);
   DCHECK_EQ(native_module_->module()->origin, kWasmOrigin);
-  const bool lazy_module = wasm_lazy_compilation_;
   ValidateSequentially(native_module_->module(), native_module_.get(),
-                       isolate_->counters(), isolate_->allocator(), &thrower,
-                       lazy_module);
+                       isolate_->counters(), isolate_->allocator(), &thrower);
   DCHECK(thrower.error());
   // {job} keeps the {this} pointer alive.
   std::shared_ptr<AsyncCompileJob> job =
@@ -2556,6 +2549,38 @@
   step_.reset(new Step(std::forward<Args>(args)...));
 }
 
+WasmError ValidateLazilyCompiledFunctions(const WasmModule* module,
+                                          ModuleWireBytes wire_bytes,
+                                          WasmFeatures enabled_features) {
+  if (v8_flags.wasm_lazy_validation) return {};
+  if (!MayCompriseLazyFunctions(module, enabled_features)) return {};
+
+  auto allocator = GetWasmEngine()->allocator();
+
+  // TODO(clemensb): Parallelize this.
+  const bool is_lazy_module = IsLazyModule(module);
+  for (const WasmFunction& function : module->declared_functions()) {
+    if (module->function_was_validated(function.func_index)) continue;
+    base::Vector<const uint8_t> code = wire_bytes.GetFunctionBytes(&function);
+
+    CompileStrategy strategy = GetCompileStrategy(
+        module, enabled_features, function.func_index, is_lazy_module);
+    if (strategy != CompileStrategy::kLazy &&
+        strategy != CompileStrategy::kLazyBaselineEagerTopTier) {
+      continue;
+    }
+    DecodeResult function_result = ValidateSingleFunction(
+        module, function.func_index, code, allocator, enabled_features);
+    if (function_result.failed()) {
+      WasmError error = std::move(function_result).error();
+      return GetWasmErrorWithName(wire_bytes, &function, module,
+                                  std::move(error));
+    }
+    module->set_function_validated(function.func_index);
+  }
+  return {};
+}
+
 //==========================================================================
 // Step 1: (async) Decode the module.
 //==========================================================================
@@ -2581,37 +2606,12 @@
           DecodingMethod::kAsync, GetWasmEngine()->allocator());
 
       // Validate lazy functions here if requested.
-      if (!v8_flags.wasm_lazy_validation && result.ok()) {
+      if (result.ok()) {
         const WasmModule* module = result.value().get();
-        DCHECK_EQ(module->origin, kWasmOrigin);
-        const bool lazy_module = job->wasm_lazy_compilation_;
-        if (MayCompriseLazyFunctions(module, enabled_features, lazy_module)) {
-          auto allocator = GetWasmEngine()->allocator();
-          int start = module->num_imported_functions;
-          int end = start + module->num_declared_functions;
-
-          for (int func_index = start; func_index < end; func_index++) {
-            const WasmFunction* func = &module->functions[func_index];
-            base::Vector<const uint8_t> code =
-                job->wire_bytes_.GetFunctionBytes(func);
-
-            CompileStrategy strategy = GetCompileStrategy(
-                module, enabled_features, func_index, lazy_module);
-            bool validate_lazily_compiled_function =
-                strategy == CompileStrategy::kLazy ||
-                strategy == CompileStrategy::kLazyBaselineEagerTopTier;
-            if (validate_lazily_compiled_function) {
-              DecodeResult function_result = ValidateSingleFunction(
-                  module, func_index, code, allocator, enabled_features);
-              if (function_result.failed()) {
-                WasmError error = function_result.error();
-                WasmError error_with_name = GetWasmErrorWithName(
-                    job->wire_bytes_, func, module, std::move(error));
-                result = ModuleResult(std::move(error_with_name));
-                break;
-              }
-            }
-          }
+        WasmError validation_error = ValidateLazilyCompiledFunctions(
+            module, job->wire_bytes_, job->enabled_features_);
+        if (validation_error.has_error()) {
+          result = ModuleResult{std::move(validation_error)};
         }
       }
     }
@@ -2675,6 +2675,25 @@
                                             code_size_estimate_)) {
       job->FinishCompile(true);
       return;
+    } else {
+      // If we are not streaming and did not get a cache hit, we might have hit
+      // the path where the streaming decoder got a prefix cache hit, but the
+      // module then turned out to be invalid, and we are running it through
+      // non-streaming decoding again. In this case, function bodies have not
+      // been validated yet (would have happened in the {DecodeModule} phase
+      // if we would not come via the non-streaming path). Thus do this now.
+      // Note that we only need to validate lazily compiled functions, others
+      // will be validated during eager compilation.
+      DCHECK(start_compilation_);
+      if (ValidateLazilyCompiledFunctions(
+              module_.get(), ModuleWireBytes{job->native_module_->wire_bytes()},
+              job->native_module_->enabled_features())
+              .has_error()) {
+        // TODO(clemensb): Use the error message instead of re-validation in
+        // {AsyncCompileFailed}.
+        job->AsyncCompileFailed();
+        return;
+      }
     }
 
     // Make sure all compilation tasks stopped running. Decoding (async step)
@@ -2695,12 +2714,14 @@
     }
 
     if (start_compilation_) {
-      std::unique_ptr<CompilationUnitBuilder> builder =
-          InitializeCompilation(job->isolate(), job->native_module_.get());
+      // TODO(13209): Use PGO for async compilation, if available.
+      constexpr ProfileInformation* kNoProfileInformation = nullptr;
+      std::unique_ptr<CompilationUnitBuilder> builder = InitializeCompilation(
+          job->isolate(), job->native_module_.get(), kNoProfileInformation);
       compilation_state->InitializeCompilationUnits(std::move(builder));
-      // We are in single-threaded mode, so there are no worker tasks that will
-      // do the compilation. We call {WaitForCompilationEvent} here so that the
-      // main thread paticipates and finishes the compilation.
+      // In single-threaded mode there are no worker tasks that will do the
+      // compilation. We call {WaitForCompilationEvent} here so that the main
+      // thread participates and finishes the compilation.
       if (v8_flags.wasm_num_compilation_tasks == 0) {
         compilation_state->WaitForCompilationEvent(
             CompilationEvent::kFinishedBaselineCompilation);
@@ -2915,8 +2936,10 @@
   // Set outstanding_finishers_ to 2, because both the AsyncCompileJob and the
   // AsyncStreamingProcessor have to finish.
   job_->outstanding_finishers_ = 2;
-  compilation_unit_builder_ =
-      InitializeCompilation(job_->isolate(), job_->native_module_.get());
+  // TODO(13209): Use PGO for streaming compilation, if available.
+  constexpr ProfileInformation* kNoProfileInformation = nullptr;
+  compilation_unit_builder_ = InitializeCompilation(
+      job_->isolate(), job_->native_module_.get(), kNoProfileInformation);
   return true;
 }
 
@@ -2930,7 +2953,7 @@
   // In case of {prefix_cache_hit} we still need the function body to be
   // decoded. Otherwise a later cache miss cannot be handled.
   decoder_.DecodeFunctionBody(func_index, static_cast<uint32_t>(bytes.length()),
-                              offset, false);
+                              offset);
 
   if (prefix_cache_hit_) {
     // Don't compile yet if we might have a cache hit.
@@ -2948,7 +2971,7 @@
   const WasmModule* module = decoder_.module();
   auto enabled_features = job_->enabled_features_;
   DCHECK_EQ(module->origin, kWasmOrigin);
-  const bool lazy_module = job_->wasm_lazy_compilation_;
+  const bool lazy_module = v8_flags.wasm_lazy_compilation;
   CompileStrategy strategy =
       GetCompileStrategy(module, enabled_features, func_index, lazy_module);
   bool validate_lazily_compiled_function =
@@ -3181,8 +3204,49 @@
                                  (old_baseline_tier != ExecutionTier::kNone);
 }
 
+void CompilationStateImpl::ApplyPgoInfoToInitialProgress(
+    ProfileInformation* pgo_info) {
+  // Functions that were executed in the profiling run are eagerly compiled to
+  // Liftoff.
+  const WasmModule* module = native_module_->module();
+  for (int func_index : pgo_info->executed_functions()) {
+    uint8_t& progress =
+        compilation_progress_[declared_function_index(module, func_index)];
+    ExecutionTier old_baseline_tier =
+        RequiredBaselineTierField::decode(progress);
+    // If the function is already marked for eager compilation, we are good.
+    if (old_baseline_tier != ExecutionTier::kNone) continue;
+
+    // Set the baseline tier to Liftoff, so we eagerly compile to Liftoff.
+    // TODO(13288): Compile Liftoff code in the background, if lazy compilation
+    // is enabled.
+    progress =
+        RequiredBaselineTierField::update(progress, ExecutionTier::kLiftoff);
+    ++outstanding_baseline_units_;
+  }
+
+  // Functions that were tiered up during PGO generation are eagerly compiled to
+  // TurboFan (in the background, not blocking instantiation).
+  for (int func_index : pgo_info->tiered_up_functions()) {
+    uint8_t& progress =
+        compilation_progress_[declared_function_index(module, func_index)];
+    ExecutionTier old_baseline_tier =
+        RequiredBaselineTierField::decode(progress);
+    ExecutionTier old_top_tier = RequiredTopTierField::decode(progress);
+    // If the function is already marked for eager or background compilation to
+    // TurboFan, we are good.
+    if (old_baseline_tier == ExecutionTier::kTurbofan) continue;
+    if (old_top_tier == ExecutionTier::kTurbofan) continue;
+
+    // Set top tier to TurboFan, so we eagerly trigger compilation in the
+    // background.
+    progress = RequiredTopTierField::update(progress, ExecutionTier::kTurbofan);
+  }
+}
+
 void CompilationStateImpl::InitializeCompilationProgress(
-    int num_import_wrappers, int num_export_wrappers) {
+    int num_import_wrappers, int num_export_wrappers,
+    ProfileInformation* pgo_info) {
   DCHECK(!failed());
   auto* module = native_module_->module();
 
@@ -3213,6 +3277,9 @@
     }
   }
 
+  // Apply PGO information, if available.
+  if (pgo_info) ApplyPgoInfoToInitialProgress(pgo_info);
+
   // Account for outstanding wrapper compilation.
   outstanding_baseline_units_ += num_import_wrappers;
   outstanding_export_wrappers_ = num_export_wrappers;
diff -r -u --color up/v8/src/wasm/module-compiler.h nw/v8/src/wasm/module-compiler.h
--- up/v8/src/wasm/module-compiler.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/module-compiler.h	2023-01-19 16:46:36.428109511 -0500
@@ -49,6 +49,7 @@
 class ErrorThrower;
 class ModuleCompiler;
 class NativeModule;
+class ProfileInformation;
 class StreamingDecoder;
 class WasmCode;
 struct WasmModule;
@@ -57,7 +58,8 @@
 std::shared_ptr<NativeModule> CompileToNativeModule(
     Isolate* isolate, const WasmFeatures& enabled, ErrorThrower* thrower,
     std::shared_ptr<const WasmModule> module, const ModuleWireBytes& wire_bytes,
-    int compilation_id, v8::metrics::Recorder::ContextId context_id);
+    int compilation_id, v8::metrics::Recorder::ContextId context_id,
+    ProfileInformation* pgo_info);
 
 void RecompileNativeModule(NativeModule* native_module,
                            TieringState new_tiering_state);
@@ -78,7 +80,7 @@
 // Triggered by the WasmCompileLazy builtin. The return value indicates whether
 // compilation was successful. Lazy compilation can fail only if validation is
 // also lazy.
-bool CompileLazy(Isolate*, Handle<WasmInstanceObject>, int func_index);
+bool CompileLazy(Isolate*, WasmInstanceObject, int func_index);
 
 // Throws the compilation error after failed lazy compilation.
 void ThrowLazyCompilationError(Isolate* isolate,
@@ -258,7 +260,6 @@
   const char* const api_method_name_;
   const WasmFeatures enabled_features_;
   const DynamicTiering dynamic_tiering_;
-  const bool wasm_lazy_compilation_;
   base::TimeTicks start_time_;
   // Copy of the module wire bytes, moved into the {native_module_} on its
   // creation.
diff -r -u --color up/v8/src/wasm/module-decoder-impl.h nw/v8/src/wasm/module-decoder-impl.h
--- up/v8/src/wasm/module-decoder-impl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/module-decoder-impl.h	2023-01-19 16:46:36.428109511 -0500
@@ -470,8 +470,7 @@
   }
 
   void DecodeSection(SectionCode section_code,
-                     base::Vector<const uint8_t> bytes, uint32_t offset,
-                     bool validate_functions = true) {
+                     base::Vector<const uint8_t> bytes, uint32_t offset) {
     if (failed()) return;
     Reset(bytes, offset);
     TRACE("Section: %s\n", SectionName(section_code));
@@ -507,7 +506,7 @@
         DecodeStartSection();
         break;
       case kCodeSectionCode:
-        DecodeCodeSection(validate_functions);
+        DecodeCodeSection();
         break;
       case kElementSectionCode:
         DecodeElementSection();
@@ -562,13 +561,7 @@
         DecodeDataCountSection();
         break;
       case kTagSectionCode:
-        if (enabled_features_.has_eh()) {
-          DecodeTagSection();
-        } else {
-          errorf(pc(),
-                 "unexpected section <%s> (enable with --experimental-wasm-eh)",
-                 SectionName(section_code));
-        }
+        DecodeTagSection();
         break;
       case kStringRefSectionCode:
         if (enabled_features_.has_stringref()) {
@@ -640,7 +633,7 @@
 
   TypeDefinition consume_subtype_definition() {
     DCHECK(enabled_features_.has_gc());
-    uint8_t kind = read_u8<Decoder::kFullValidation>(pc(), "type kind");
+    uint8_t kind = read_u8<Decoder::FullValidationTag>(pc(), "type kind");
     if (kind == kWasmSubtypeCode) {
       consume_bytes(1, " subtype, ", tracer_);
       constexpr uint32_t kMaximumSupertypes = 1;
@@ -672,7 +665,8 @@
       for (uint32_t i = 0; i < types_count; ++i) {
         TRACE("DecodeSignature[%d] module+%d\n", i,
               static_cast<int>(pc_ - start_));
-        uint8_t opcode = read_u8<kFullValidation>(pc(), "signature definition");
+        uint8_t opcode =
+            read_u8<FullValidationTag>(pc(), "signature definition");
         tracer_.Bytes(pc_, 1);
         tracer_.TypeOffset(pc_offset());
         tracer_.Description(" kind: ");
@@ -706,7 +700,7 @@
 
     for (uint32_t i = 0; ok() && i < types_count; ++i) {
       TRACE("DecodeType[%d] module+%d\n", i, static_cast<int>(pc_ - start_));
-      uint8_t kind = read_u8<Decoder::kFullValidation>(pc(), "type kind");
+      uint8_t kind = read_u8<Decoder::FullValidationTag>(pc(), "type kind");
       if (kind == kWasmRecursiveTypeGroupCode) {
         consume_bytes(1, "rec. group definition", tracer_);
         tracer_.NextLine();
@@ -809,30 +803,32 @@
             break;
           }
           table->type = type;
-          uint8_t flags = validate_table_flags("element count");
+          consume_table_flags("element count", &table->has_maximum_size);
           consume_resizable_limits(
               "element count", "elements", std::numeric_limits<uint32_t>::max(),
-              &table->initial_size, &table->has_maximum_size,
+              &table->initial_size, table->has_maximum_size,
               std::numeric_limits<uint32_t>::max(), &table->maximum_size,
-              flags);
+              k32BitLimits);
           break;
         }
         case kExternalMemory: {
           // ===== Imported memory =============================================
           if (!AddMemory(module_.get())) break;
-          uint8_t flags = validate_memory_flags(&module_->has_shared_memory,
-                                                &module_->is_memory64);
+          consume_memory_flags(&module_->has_shared_memory,
+                               &module_->is_memory64,
+                               &module_->has_maximum_pages);
           uint32_t max_pages = module_->is_memory64 ? kSpecMaxMemory64Pages
                                                     : kSpecMaxMemory32Pages;
-          consume_resizable_limits("memory", "pages", max_pages,
-                                   &module_->initial_pages,
-                                   &module_->has_maximum_pages, max_pages,
-                                   &module_->maximum_pages, flags);
+          consume_resizable_limits(
+              "memory", "pages", max_pages, &module_->initial_pages,
+              module_->has_maximum_pages, max_pages, &module_->maximum_pages,
+              module_->is_memory64 ? k64BitLimits : k32BitLimits);
           break;
         }
         case kExternalGlobal: {
           // ===== Imported global =============================================
           import->index = static_cast<uint32_t>(module_->globals.size());
+          module_->num_imported_globals++;
           module_->globals.push_back({kWasmVoid, false, {}, {0}, true, false});
           WasmGlobal* global = &module_->globals.back();
           global->type = consume_value_type();
@@ -845,11 +841,8 @@
         }
         case kExternalTag: {
           // ===== Imported tag ================================================
-          if (!enabled_features_.has_eh()) {
-            errorf(pos, "unknown import kind 0x%02x", import->kind);
-            break;
-          }
           import->index = static_cast<uint32_t>(module_->tags.size());
+          module_->num_imported_tags++;
           const WasmTagSig* tag_sig = nullptr;
           consume_exception_attribute();  // Attribute ignored for now.
           consume_tag_sig_index(module_.get(), &tag_sig);
@@ -875,23 +868,19 @@
     DCHECK_EQ(module_->functions.size(), module_->num_imported_functions);
     uint32_t total_function_count =
         module_->num_imported_functions + functions_count;
-    module_->functions.reserve(total_function_count);
+    module_->functions.resize(total_function_count);
     module_->num_declared_functions = functions_count;
-    for (uint32_t i = 0; i < functions_count; ++i) {
-      uint32_t func_index = static_cast<uint32_t>(module_->functions.size());
-      module_->functions.push_back({nullptr,     // sig
-                                    func_index,  // func_index
-                                    0,           // sig_index
-                                    {0, 0},      // code
-                                    false,       // imported
-                                    false,       // exported
-                                    false});     // declared
-      WasmFunction* function = &module_->functions.back();
-      tracer_.FunctionName(module_->num_imported_functions + i);
+    DCHECK_NULL(module_->validated_functions);
+    module_->validated_functions =
+        std::make_unique<std::atomic<uint8_t>[]>((functions_count + 7) / 8);
+    for (uint32_t func_index = module_->num_imported_functions;
+         func_index < total_function_count; ++func_index) {
+      WasmFunction* function = &module_->functions[func_index];
+      function->func_index = func_index;
+      tracer_.FunctionName(func_index);
       function->sig_index = consume_sig_index(module_.get(), &function->sig);
       if (!ok()) return;
     }
-    DCHECK_EQ(module_->functions.size(), total_function_count);
   }
 
   void DecodeTableSection() {
@@ -905,7 +894,7 @@
 
       bool has_initializer = false;
       if (enabled_features_.has_typed_funcref() &&
-          read_u8<Decoder::kFullValidation>(
+          read_u8<Decoder::FullValidationTag>(
               pc(), "table-with-initializer byte") == 0x40) {
         consume_bytes(1, "table-with-initializer byte");
         has_initializer = true;
@@ -924,11 +913,12 @@
       }
       table->type = table_type;
 
-      uint8_t flags = validate_table_flags("table elements");
-      consume_resizable_limits(
-          "table elements", "elements", std::numeric_limits<uint32_t>::max(),
-          &table->initial_size, &table->has_maximum_size,
-          std::numeric_limits<uint32_t>::max(), &table->maximum_size, flags);
+      consume_table_flags("table elements", &table->has_maximum_size);
+      consume_resizable_limits("table elements", "elements",
+                               std::numeric_limits<uint32_t>::max(),
+                               &table->initial_size, table->has_maximum_size,
+                               std::numeric_limits<uint32_t>::max(),
+                               &table->maximum_size, k32BitLimits);
 
       if (has_initializer) {
         table->initial_value = consume_init_expr(module_.get(), table_type);
@@ -942,14 +932,14 @@
     for (uint32_t i = 0; ok() && i < memory_count; i++) {
       tracer_.MemoryOffset(pc_offset());
       if (!AddMemory(module_.get())) break;
-      uint8_t flags = validate_memory_flags(&module_->has_shared_memory,
-                                            &module_->is_memory64);
+      consume_memory_flags(&module_->has_shared_memory, &module_->is_memory64,
+                           &module_->has_maximum_pages);
       uint32_t max_pages =
           module_->is_memory64 ? kSpecMaxMemory64Pages : kSpecMaxMemory32Pages;
-      consume_resizable_limits("memory", "pages", max_pages,
-                               &module_->initial_pages,
-                               &module_->has_maximum_pages, max_pages,
-                               &module_->maximum_pages, flags);
+      consume_resizable_limits(
+          "memory", "pages", max_pages, &module_->initial_pages,
+          module_->has_maximum_pages, max_pages, &module_->maximum_pages,
+          module_->is_memory64 ? k64BitLimits : k32BitLimits);
     }
   }
 
@@ -1034,10 +1024,6 @@
           break;
         }
         case kExternalTag: {
-          if (!enabled_features_.has_eh()) {
-            errorf(pos, "invalid export kind 0x%02x", exp->kind);
-            break;
-          }
           WasmTag* tag = nullptr;
           exp->index = consume_tag_index(module_.get(), &tag);
           break;
@@ -1118,7 +1104,7 @@
     }
   }
 
-  void DecodeCodeSection(bool validate_functions) {
+  void DecodeCodeSection() {
     // Make sure global offset were calculated before they get accessed during
     // function compilation.
     CalculateGlobalOffsets(module_.get());
@@ -1148,7 +1134,7 @@
       uint32_t offset = pc_offset();
       consume_bytes(size, "function body");
       if (failed()) break;
-      DecodeFunctionBody(function_index, size, offset, validate_functions);
+      DecodeFunctionBody(function_index, size, offset);
 
       // Now that the function has been decoded, we can compute module offsets.
       for (; inst_traces_it != this->inst_traces_.end() &&
@@ -1191,17 +1177,11 @@
     return true;
   }
 
-  void DecodeFunctionBody(uint32_t index, uint32_t length, uint32_t offset,
-                          bool validate_functions) {
-    WasmFunction* function = &module_->functions[index];
+  void DecodeFunctionBody(uint32_t func_index, uint32_t length,
+                          uint32_t offset) {
+    WasmFunction* function = &module_->functions[func_index];
     function->code = {offset, length};
     tracer_.FunctionBody(function, pc_ - (pc_offset() - offset));
-    if (validate_functions) {
-      ModuleWireBytes bytes(module_start_, module_end_);
-      ValidateFunctionBody(module_->signature_zone->allocator(),
-                           index + module_->num_imported_functions, bytes,
-                           module_.get(), function);
-    }
   }
 
   bool CheckDataSegmentsCount(uint32_t data_segments_count) {
@@ -1639,9 +1619,20 @@
     return toResult(std::move(module_));
   }
 
+  void ValidateAllFunctions() {
+    DCHECK(ok());
+
+    // Spawn a {ValidateFunctionsTask} and join it. The earliest error found
+    // will be set on this decoder.
+    std::unique_ptr<JobHandle> job_handle = V8::GetCurrentPlatform()->CreateJob(
+        TaskPriority::kUserVisible,
+        std::make_unique<ValidateFunctionsTask>(this));
+    job_handle->Join();
+  }
+
   // Decodes an entire module.
   ModuleResult DecodeModule(Counters* counters, AccountingAllocator* allocator,
-                            bool validate_functions = true) {
+                            bool validate_functions) {
     StartDecoding(counters, allocator);
     uint32_t offset = 0;
     base::Vector<const byte> orig_bytes(start(), end() - start());
@@ -1660,7 +1651,7 @@
       offset += section_iter.payload_start() - section_iter.section_start();
       if (section_iter.section_code() != SectionCode::kUnknownSectionCode) {
         DecodeSection(section_iter.section_code(), section_iter.payload(),
-                      offset, validate_functions);
+                      offset);
       }
       // Shift the offset by the remaining section payload
       offset += section_iter.payload_length();
@@ -1668,6 +1659,11 @@
       section_iter.advance(true);
     }
 
+    if (ok() && validate_functions) {
+      Reset(orig_bytes);
+      ValidateAllFunctions();
+    }
+
     if (v8_flags.dump_wasm_module) DumpModule(orig_bytes);
 
     if (decoder.failed()) {
@@ -1680,16 +1676,25 @@
   // Decodes a single anonymous function starting at {start_}.
   FunctionResult DecodeSingleFunctionForTesting(
       Zone* zone, const ModuleWireBytes& wire_bytes, const WasmModule* module) {
+    DCHECK(ok());
     pc_ = start_;
     expect_u8("type form", kWasmFunctionTypeCode);
     WasmFunction function;
     function.sig = consume_sig(zone);
     function.code = {off(pc_), static_cast<uint32_t>(end_ - pc_)};
-    if (!ok()) return FunctionResult{std::move(error_)};
 
-    ValidateFunctionBody(zone->allocator(), 0, wire_bytes, module, &function);
     if (!ok()) return FunctionResult{std::move(error_)};
 
+    AccountingAllocator* allocator = zone->allocator();
+
+    FunctionBody body{function.sig, off(pc_), pc_, end_};
+
+    WasmFeatures unused_detected_features;
+    DecodeResult result = ValidateFunctionBody(
+        allocator, enabled_features_, module, &unused_detected_features, body);
+
+    if (result.failed()) return FunctionResult{std::move(result).error()};
+
     return FunctionResult{std::make_unique<WasmFunction>(function)};
   }
 
@@ -1803,37 +1808,6 @@
     module->tagged_globals_buffer_size = tagged_offset;
   }
 
-  // Verifies the body (code) of a given function.
-  void ValidateFunctionBody(AccountingAllocator* allocator, uint32_t func_num,
-                            const ModuleWireBytes& wire_bytes,
-                            const WasmModule* module, WasmFunction* function) {
-    if (v8_flags.trace_wasm_decoder) {
-      WasmFunctionName func_name(function,
-                                 wire_bytes.GetNameOrNull(function, module));
-      StdoutStream{} << "Verifying wasm function " << func_name << std::endl;
-    }
-    FunctionBody body = {
-        function->sig, function->code.offset(),
-        start_ + GetBufferRelativeOffset(function->code.offset()),
-        start_ + GetBufferRelativeOffset(function->code.end_offset())};
-
-    WasmFeatures unused_detected_features = WasmFeatures::None();
-    DecodeResult result = wasm::ValidateFunctionBody(
-        allocator, enabled_features_, module, &unused_detected_features, body);
-
-    // If the decode failed and this is the first error, set error code and
-    // location.
-    if (result.failed() && error_.empty()) {
-      // Wrap the error message from the function decoder.
-      WasmFunctionName func_name(function,
-                                 wire_bytes.GetNameOrNull(function, module));
-      std::ostringstream error_msg;
-      error_msg << "in function " << func_name << ": "
-                << result.error().message();
-      error_ = WasmError{result.error().offset(), error_msg.str()};
-    }
-  }
-
   uint32_t consume_sig_index(WasmModule* module, const FunctionSig** sig) {
     const byte* pos = pc_;
     uint32_t sig_index = consume_u32v("signature index");
@@ -1909,74 +1883,65 @@
     return index;
   }
 
-  uint8_t validate_table_flags(const char* name) {
+  void consume_table_flags(const char* name, bool* has_maximum_out) {
     tracer_.Bytes(pc_, 1);
     uint8_t flags = consume_u8("table limits flags");
     tracer_.Description(flags == kNoMaximum ? " no maximum" : " with maximum");
     tracer_.NextLine();
-    static_assert(kNoMaximum < kWithMaximum);
+    static_assert(kNoMaximum == 0 && kWithMaximum == 1);
+    *has_maximum_out = flags == kWithMaximum;
     if (V8_UNLIKELY(flags > kWithMaximum)) {
       errorf(pc() - 1, "invalid %s limits flags", name);
     }
-    return flags;
   }
 
-  uint8_t validate_memory_flags(bool* has_shared_memory, bool* is_memory64) {
+  void consume_memory_flags(bool* is_shared_out, bool* is_memory64_out,
+                            bool* has_maximum_out) {
     tracer_.Bytes(pc_, 1);
     uint8_t flags = consume_u8("memory limits flags");
-    *has_shared_memory = false;
-    switch (flags) {
-      case kNoMaximum:
-      case kWithMaximum:
-        break;
-      case kSharedNoMaximum:
-      case kSharedWithMaximum:
-        if (!enabled_features_.has_threads()) {
-          errorf(pc() - 1,
-                 "invalid memory limits flags 0x%x (enable via "
-                 "--experimental-wasm-threads)",
-                 flags);
-        }
-        *has_shared_memory = true;
-        // V8 does not support shared memory without a maximum.
-        if (flags == kSharedNoMaximum) {
-          errorf(pc() - 1,
-                 "memory limits flags must have maximum defined if shared is "
-                 "true");
-        }
-        break;
-      case kMemory64NoMaximum:
-      case kMemory64WithMaximum:
-        if (!enabled_features_.has_memory64()) {
-          errorf(pc() - 1,
-                 "invalid memory limits flags 0x%x (enable via "
-                 "--experimental-wasm-memory64)",
-                 flags);
-        }
-        *is_memory64 = true;
-        break;
-      default:
-        errorf(pc() - 1, "invalid memory limits flags 0x%x", flags);
-        break;
-    }
-    if (*has_shared_memory) tracer_.Description(" shared");
-    if (*is_memory64) tracer_.Description(" mem64");
-    tracer_.Description((flags & 1) ? " with maximum" : " no maximum");
+    // Flags 0..7 are valid (3 bits).
+    if (flags & ~0x7) {
+      errorf(pc() - 1, "invalid memory limits flags 0x%x", flags);
+    }
+    // Decode the three bits.
+    bool has_maximum = flags & 0x1;
+    bool is_shared = flags & 0x2;
+    bool is_memory64 = flags & 0x4;
+    // Store into output parameters.
+    *has_maximum_out = has_maximum;
+    *is_shared_out = is_shared;
+    *is_memory64_out = is_memory64;
+
+    // V8 does not support shared memory without a maximum.
+    if (is_shared && !has_maximum) {
+      errorf(pc() - 1, "shared memory must have a maximum defined");
+    }
+
+    if (is_memory64 && !enabled_features_.has_memory64()) {
+      errorf(pc() - 1,
+             "invalid memory limits flags 0x%x (enable via "
+             "--experimental-wasm-memory64)",
+             flags);
+    }
+
+    // Tracing.
+    if (is_shared) tracer_.Description(" shared");
+    if (is_memory64) tracer_.Description(" mem64");
+    tracer_.Description(has_maximum ? " with maximum" : " no maximum");
     tracer_.NextLine();
-    return flags;
   }
 
+  enum ResizableLimitsType : bool { k32BitLimits, k64BitLimits };
   void consume_resizable_limits(const char* name, const char* units,
                                 uint32_t max_initial, uint32_t* initial,
-                                bool* has_max, uint32_t max_maximum,
-                                uint32_t* maximum, uint8_t flags) {
+                                bool has_maximum, uint32_t max_maximum,
+                                uint32_t* maximum, ResizableLimitsType type) {
     const byte* pos = pc();
-    // For memory64 we need to read the numbers as LEB-encoded 64-bit unsigned
-    // integer. All V8 limits are still within uint32_t range though.
-    const bool is_memory64 =
-        flags == kMemory64NoMaximum || flags == kMemory64WithMaximum;
-    uint64_t initial_64 = is_memory64 ? consume_u64v("initial size", tracer_)
-                                      : consume_u32v("initial size", tracer_);
+    // Note that even if we read the values as 64-bit value, all V8 limits are
+    // still within uint32_t range.
+    uint64_t initial_64 = type == k64BitLimits
+                              ? consume_u64v("initial size", tracer_)
+                              : consume_u32v("initial size", tracer_);
     if (initial_64 > max_initial) {
       errorf(pos,
              "initial %s size (%" PRIu64
@@ -1986,11 +1951,11 @@
     *initial = static_cast<uint32_t>(initial_64);
     tracer_.Description(*initial);
     tracer_.NextLine();
-    if (flags & 1) {
-      *has_max = true;
+    if (has_maximum) {
       pos = pc();
-      uint64_t maximum_64 = is_memory64 ? consume_u64v("maximum size", tracer_)
-                                        : consume_u32v("maximum size", tracer_);
+      uint64_t maximum_64 = type == k64BitLimits
+                                ? consume_u64v("maximum size", tracer_)
+                                : consume_u32v("maximum size", tracer_);
       if (maximum_64 > max_maximum) {
         errorf(pos,
                "maximum %s size (%" PRIu64
@@ -2006,7 +1971,6 @@
       tracer_.Description(*maximum);
       tracer_.NextLine();
     } else {
-      *has_max = false;
       *maximum = max_initial;
     }
   }
@@ -2048,7 +2012,7 @@
     switch (static_cast<WasmOpcode>(*pc())) {
       case kExprI32Const: {
         int32_t value =
-            read_i32v<kFullValidation>(pc() + 1, &length, "i32.const");
+            read_i32v<FullValidationTag>(pc() + 1, &length, "i32.const");
         if (V8_UNLIKELY(failed())) return {};
         if (V8_LIKELY(lookahead(1 + length, kExprEnd))) {
           TYPE_CHECK(kWasmI32)
@@ -2060,7 +2024,7 @@
       }
       case kExprRefFunc: {
         uint32_t index =
-            read_u32v<kFullValidation>(pc() + 1, &length, "ref.func");
+            read_u32v<FullValidationTag>(pc() + 1, &length, "ref.func");
         if (V8_UNLIKELY(failed())) return {};
         if (V8_LIKELY(lookahead(1 + length, kExprEnd))) {
           if (V8_UNLIKELY(index >= module_->functions.size())) {
@@ -2080,8 +2044,10 @@
         break;
       }
       case kExprRefNull: {
-        HeapType type = value_type_reader::read_heap_type<kFullValidation>(
-            this, pc() + 1, &length, module_.get(), enabled_features_);
+        HeapType type = value_type_reader::read_heap_type<FullValidationTag>(
+            this, pc() + 1, &length, enabled_features_);
+        value_type_reader::ValidateHeapType<FullValidationTag>(
+            this, pc_, module_.get(), type);
         if (V8_UNLIKELY(failed())) return {};
         if (V8_LIKELY(lookahead(1 + length, kExprEnd))) {
           TYPE_CHECK(ValueType::RefNull(type))
@@ -2100,7 +2066,7 @@
     auto sig = FixedSizeSignature<ValueType>::Returns(expected);
     FunctionBody body(&sig, buffer_offset_, pc_, end_);
     WasmFeatures detected;
-    WasmFullDecoder<Decoder::kFullValidation, ConstantExpressionInterface,
+    WasmFullDecoder<Decoder::FullValidationTag, ConstantExpressionInterface,
                     kConstantExpression>
         decoder(&init_expr_zone_, module, enabled_features_, &detected, body,
                 module);
@@ -2139,9 +2105,11 @@
 
   ValueType consume_value_type() {
     uint32_t type_length;
-    ValueType result = value_type_reader::read_value_type<kFullValidation>(
-        this, pc_, &type_length, module_.get(),
+    ValueType result = value_type_reader::read_value_type<FullValidationTag>(
+        this, pc_, &type_length,
         origin_ == kWasmOrigin ? enabled_features_ : WasmFeatures::None());
+    value_type_reader::ValidateValueType<FullValidationTag>(
+        this, pc_, module_.get(), result);
     tracer_.Bytes(pc_, type_length);
     tracer_.Description(result);
     consume_bytes(type_length, "value type");
@@ -2150,8 +2118,10 @@
 
   HeapType consume_super_type() {
     uint32_t type_length;
-    HeapType result = value_type_reader::read_heap_type<kFullValidation>(
-        this, pc_, &type_length, module_.get(), enabled_features_);
+    HeapType result = value_type_reader::read_heap_type<FullValidationTag>(
+        this, pc_, &type_length, enabled_features_);
+    value_type_reader::ValidateValueType<FullValidationTag>(
+        this, pc_, module_.get(), result);
     tracer_.Bytes(pc_, type_length);
     tracer_.Description(result);
     consume_bytes(type_length, "heap type");
@@ -2159,7 +2129,7 @@
   }
 
   ValueType consume_storage_type() {
-    uint8_t opcode = read_u8<kFullValidation>(this->pc());
+    uint8_t opcode = read_u8<FullValidationTag>(this->pc());
     switch (opcode) {
       case kI8Code:
         consume_bytes(1, " i8", tracer_);
@@ -2428,6 +2398,86 @@
     func->declared = true;
     return index;
   }
+
+  // A task that validates multiple functions in parallel, storing the earliest
+  // validation error in {this} decoder.
+  class ValidateFunctionsTask : public JobTask {
+   public:
+    ValidateFunctionsTask(ModuleDecoderTemplate* decoder)
+        : decoder_(decoder),
+          next_function_(decoder->module_->num_imported_functions),
+          after_last_function_(next_function_ +
+                               decoder->module_->num_declared_functions) {}
+
+    void Run(JobDelegate* delegate) override {
+      AccountingAllocator* allocator = decoder_->module_->allocator();
+      do {
+        // Get the index of the next function to validate.
+        // {fetch_add} might overrun {after_last_function_} by a bit. Since the
+        // number of functions is limited to a value much smaller than the
+        // integer range, this is highly unlikely.
+        static_assert(kV8MaxWasmFunctions < kMaxInt / 2);
+        int func_index = next_function_.fetch_add(1, std::memory_order_relaxed);
+        if (V8_UNLIKELY(func_index >= after_last_function_)) return;
+        DCHECK_LE(0, func_index);
+
+        if (!ValidateFunction(allocator, func_index)) {
+          // No need to validate any more functions.
+          next_function_.store(after_last_function_, std::memory_order_relaxed);
+          return;
+        }
+      } while (!delegate->ShouldYield());
+    }
+
+    size_t GetMaxConcurrency(size_t /* worker_count */) const override {
+      int next_func = next_function_.load(std::memory_order_relaxed);
+      return std::max(0, after_last_function_ - next_func);
+    }
+
+   private:
+    // Validate a single function; use {SetError} on errors.
+    bool ValidateFunction(AccountingAllocator* allocator, int func_index) {
+      DCHECK(!decoder_->module_->function_was_validated(func_index));
+      WasmFeatures unused_detected_features;
+      const WasmFunction& function = decoder_->module_->functions[func_index];
+      FunctionBody body{function.sig, function.code.offset(),
+                        decoder_->start_ + function.code.offset(),
+                        decoder_->start_ + function.code.end_offset()};
+      DecodeResult validation_result = ValidateFunctionBody(
+          allocator, decoder_->enabled_features_, decoder_->module_.get(),
+          &unused_detected_features, body);
+      if (V8_UNLIKELY(validation_result.failed())) {
+        SetError(func_index, std::move(validation_result).error());
+        return false;
+      }
+      decoder_->module_->set_function_validated(func_index);
+      return true;
+    }
+
+    // Set the error from the argument if it's earlier than the error we already
+    // have (or if we have none yet). Thread-safe.
+    void SetError(int func_index, WasmError error) {
+      base::MutexGuard mutex_guard{&set_error_mutex_};
+      if (decoder_->error_.empty() ||
+          decoder_->error_.offset() > error.offset()) {
+        // Wrap the error message from the function decoder.
+        const WasmFunction& function = decoder_->module_->functions[func_index];
+        WasmFunctionName func_name{
+            &function,
+            ModuleWireBytes{decoder_->start_, decoder_->end_}.GetNameOrNull(
+                &function, decoder_->module_.get())};
+        std::ostringstream error_msg;
+        error_msg << "in function " << func_name << ": " << error.message();
+        decoder_->error_ = WasmError{error.offset(), error_msg.str()};
+      }
+      DCHECK(!decoder_->ok());
+    }
+
+    ModuleDecoderTemplate* decoder_;
+    base::Mutex set_error_mutex_;
+    std::atomic<int> next_function_;
+    const int after_last_function_;
+  };
 };
 
 }  // namespace wasm
diff -r -u --color up/v8/src/wasm/module-decoder.cc nw/v8/src/wasm/module-decoder.cc
--- up/v8/src/wasm/module-decoder.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/module-decoder.cc	2023-01-19 16:46:36.428109511 -0500
@@ -177,9 +177,8 @@
 }
 
 void ModuleDecoder::DecodeFunctionBody(uint32_t index, uint32_t length,
-                                       uint32_t offset,
-                                       bool validate_functions) {
-  impl_->DecodeFunctionBody(index, length, offset, validate_functions);
+                                       uint32_t offset) {
+  impl_->DecodeFunctionBody(index, length, offset);
 }
 
 void ModuleDecoder::StartCodeSection(WireBytesRef section_bytes) {
diff -r -u --color up/v8/src/wasm/module-decoder.h nw/v8/src/wasm/module-decoder.h
--- up/v8/src/wasm/module-decoder.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/module-decoder.h	2023-01-19 16:46:36.428109511 -0500
@@ -155,8 +155,7 @@
 
   bool CheckFunctionsCount(uint32_t functions_count, uint32_t error_offset);
 
-  void DecodeFunctionBody(uint32_t index, uint32_t size, uint32_t offset,
-                          bool verify_functions = true);
+  void DecodeFunctionBody(uint32_t index, uint32_t size, uint32_t offset);
 
   ModuleResult FinishDecoding();
 
diff -r -u --color up/v8/src/wasm/module-instantiate.cc nw/v8/src/wasm/module-instantiate.cc
--- up/v8/src/wasm/module-instantiate.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/module-instantiate.cc	2023-01-19 16:46:36.428109511 -0500
@@ -732,25 +732,15 @@
   }
 
   //--------------------------------------------------------------------------
-  // Allocate type feedback vectors for functions.
+  // Allocate the array that will hold type feedback vectors.
   //--------------------------------------------------------------------------
   if (v8_flags.wasm_speculative_inlining) {
     int num_functions = static_cast<int>(module_->num_declared_functions);
-    Handle<FixedArray> vectors =
-        isolate_->factory()->NewFixedArray(num_functions, AllocationType::kOld);
+    // Zero-fill the array so we can do a quick Smi-check to test if a given
+    // slot was initialized.
+    Handle<FixedArray> vectors = isolate_->factory()->NewFixedArrayWithZeroes(
+        num_functions, AllocationType::kOld);
     instance->set_feedback_vectors(*vectors);
-    for (int i = 0; i < num_functions; i++) {
-      int func_index = module_->num_imported_functions + i;
-      int slots = NumFeedbackSlots(module_, func_index);
-      if (slots == 0) continue;
-      if (v8_flags.trace_wasm_speculative_inlining) {
-        PrintF("[Function %d (declared %d): allocating %d feedback slots]\n",
-               func_index, i, slots);
-      }
-      Handle<FixedArray> feedback =
-          isolate_->factory()->NewFixedArrayWithZeroes(slots);
-      vectors->set(i, *feedback);
-    }
   }
 
   //--------------------------------------------------------------------------
@@ -1128,7 +1118,7 @@
   // well as functions constructed via other means (e.g. WebAssembly.Function).
   if (WasmExternalFunction::IsWasmExternalFunction(*value)) {
     WasmInstanceObject::SetWasmInternalFunction(
-        isolate_, instance, func_index,
+        instance, func_index,
         WasmInternalFunction::FromExternal(
             Handle<WasmExternalFunction>::cast(value), isolate_)
             .ToHandleChecked());
@@ -1747,9 +1737,8 @@
   int maximum_pages = module_->has_maximum_pages
                           ? static_cast<int>(module_->maximum_pages)
                           : WasmMemoryObject::kNoMaximum;
-  auto shared = (module_->has_shared_memory && enabled_.has_threads())
-                    ? SharedFlag::kShared
-                    : SharedFlag::kNotShared;
+  auto shared =
+      module_->has_shared_memory ? SharedFlag::kShared : SharedFlag::kNotShared;
 
   auto mem_type = module_->is_memory64 ? WasmMemoryFlag::kWasmMemory64
                                        : WasmMemoryFlag::kWasmMemory32;
@@ -1781,7 +1770,7 @@
       Handle<Object> value = sanitized_imports_[index].value;
       if (WasmExternalFunction::IsWasmExternalFunction(*value)) {
         WasmInstanceObject::SetWasmInternalFunction(
-            isolate_, instance, import.index,
+            instance, import.index,
             WasmInternalFunction::FromExternal(
                 Handle<WasmExternalFunction>::cast(value), isolate_)
                 .ToHandleChecked());
diff -r -u --color up/v8/src/wasm/names-provider.cc nw/v8/src/wasm/names-provider.cc
--- up/v8/src/wasm/names-provider.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/names-provider.cc	2023-01-19 16:46:36.428109511 -0500
@@ -319,26 +319,30 @@
 }
 
 void NamesProvider::PrintElementSegmentName(StringBuilder& out,
-                                            uint32_t element_segment_index) {
+                                            uint32_t element_segment_index,
+                                            IndexAsComment index_as_comment) {
   DecodeNamesIfNotYetDone();
   WireBytesRef ref =
       Get(name_section_names_->element_segment_names_, element_segment_index);
   if (ref.is_set()) {
     out << '$';
     WriteRef(out, ref);
+    MaybeAddComment(out, element_segment_index, index_as_comment);
   } else {
     out << "$elem" << element_segment_index;
   }
 }
 
 void NamesProvider::PrintDataSegmentName(StringBuilder& out,
-                                         uint32_t data_segment_index) {
+                                         uint32_t data_segment_index,
+                                         IndexAsComment index_as_comment) {
   DecodeNamesIfNotYetDone();
   WireBytesRef ref =
       Get(name_section_names_->data_segment_names_, data_segment_index);
   if (ref.is_set()) {
     out << '$';
     WriteRef(out, ref);
+    MaybeAddComment(out, data_segment_index, index_as_comment);
   } else {
     out << "$data" << data_segment_index;
   }
diff -r -u --color up/v8/src/wasm/names-provider.h nw/v8/src/wasm/names-provider.h
--- up/v8/src/wasm/names-provider.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/names-provider.h	2023-01-19 16:46:36.428109511 -0500
@@ -56,9 +56,11 @@
                        IndexAsComment index_as_comment = kDontPrintIndex);
   void PrintGlobalName(StringBuilder& out, uint32_t global_index,
                        IndexAsComment index_as_comment = kDontPrintIndex);
-  void PrintElementSegmentName(StringBuilder& out,
-                               uint32_t element_segment_index);
-  void PrintDataSegmentName(StringBuilder& out, uint32_t data_segment_index);
+  void PrintElementSegmentName(
+      StringBuilder& out, uint32_t element_segment_index,
+      IndexAsComment index_as_comment = kDontPrintIndex);
+  void PrintDataSegmentName(StringBuilder& out, uint32_t data_segment_index,
+                            IndexAsComment index_as_comment = kDontPrintIndex);
   void PrintFieldName(StringBuilder& out, uint32_t struct_index,
                       uint32_t field_index,
                       IndexAsComment index_as_comment = kDontPrintIndex);
diff -r -u --color up/v8/src/wasm/pgo.cc nw/v8/src/wasm/pgo.cc
--- up/v8/src/wasm/pgo.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/pgo.cc	2023-01-19 16:46:36.428109511 -0500
@@ -9,17 +9,22 @@
 
 namespace v8::internal::wasm {
 
+constexpr uint8_t kFunctionExecutedBit = 1 << 0;
+constexpr uint8_t kFunctionTieredUpBit = 1 << 1;
+
 class ProfileGenerator {
  public:
-  ProfileGenerator(const WasmModule* module)
+  ProfileGenerator(const WasmModule* module,
+                   const uint32_t* tiering_budget_array)
       : module_(module),
-        type_feedback_mutex_guard_(&module->type_feedback.mutex) {}
+        type_feedback_mutex_guard_(&module->type_feedback.mutex),
+        tiering_budget_array_(tiering_budget_array) {}
 
   base::OwnedVector<uint8_t> GetProfileData() {
     ZoneBuffer buffer{&zone_};
 
     SerializeTypeFeedback(buffer);
-    // TODO(13209): Serialize tiering information.
+    SerializeTieringInfo(buffer);
 
     return base::OwnedVector<uint8_t>::Of(buffer);
   }
@@ -34,7 +39,7 @@
     std::vector<uint32_t> ordered_function_indexes;
     ordered_function_indexes.reserve(feedback_for_function.size());
     for (const auto& entry : feedback_for_function) {
-      // Skip functions for which we have to feedback.
+      // Skip functions for which we have no feedback.
       if (entry.second.feedback_vector.empty()) continue;
       ordered_function_indexes.push_back(entry.first);
     }
@@ -64,11 +69,36 @@
     }
   }
 
+  void SerializeTieringInfo(ZoneBuffer& buffer) {
+    std::unordered_map<uint32_t, FunctionTypeFeedback>& feedback_for_function =
+        module_->type_feedback.feedback_for_function;
+    const uint32_t initial_budget = v8_flags.wasm_tiering_budget;
+    for (uint32_t declared_index = 0;
+         declared_index < module_->num_declared_functions; ++declared_index) {
+      uint32_t func_index = declared_index + module_->num_imported_functions;
+      auto feedback_it = feedback_for_function.find(func_index);
+      int prio = feedback_it == feedback_for_function.end()
+                     ? 0
+                     : feedback_it->second.tierup_priority;
+      DCHECK_LE(0, prio);
+      uint32_t remaining_budget = tiering_budget_array_[declared_index];
+      DCHECK_GE(initial_budget, remaining_budget);
+
+      bool was_tiered_up = prio > 0;
+      bool was_executed = was_tiered_up || remaining_budget != initial_budget;
+
+      // TODO(13209): Make this less V8-specific for productionization.
+      buffer.write_u8((was_executed ? kFunctionExecutedBit : 0) |
+                      (was_tiered_up ? kFunctionTieredUpBit : 0));
+    }
+  }
+
  private:
   const WasmModule* module_;
   AccountingAllocator allocator_;
   Zone zone_{&allocator_, "wasm::ProfileGenerator"};
   base::MutexGuard type_feedback_mutex_guard_;
+  const uint32_t* const tiering_budget_array_;
 };
 
 void DeserializeTypeFeedback(Decoder& decoder, WasmModule* module) {
@@ -113,18 +143,42 @@
   }
 }
 
-void RestoreProfileData(WasmModule* module,
-                        base::Vector<uint8_t> profile_data) {
+std::unique_ptr<ProfileInformation> DeserializeTieringInformation(
+    Decoder& decoder, WasmModule* module) {
+  std::vector<uint32_t> executed_functions;
+  std::vector<uint32_t> tiered_up_functions;
+  uint32_t start = module->num_imported_functions;
+  uint32_t end = start + module->num_declared_functions;
+  for (uint32_t func_index = start; func_index < end; ++func_index) {
+    uint8_t tiering_info = decoder.consume_u8("tiering info");
+    CHECK_EQ(0, tiering_info & ~3);
+    bool was_executed = tiering_info & kFunctionExecutedBit;
+    bool was_tiered_up = tiering_info & kFunctionTieredUpBit;
+    if (was_tiered_up) tiered_up_functions.push_back(func_index);
+    if (was_executed) executed_functions.push_back(func_index);
+  }
+
+  return std::make_unique<ProfileInformation>(std::move(executed_functions),
+                                              std::move(tiered_up_functions));
+}
+
+std::unique_ptr<ProfileInformation> RestoreProfileData(
+    WasmModule* module, base::Vector<uint8_t> profile_data) {
   Decoder decoder{profile_data.begin(), profile_data.end()};
 
   DeserializeTypeFeedback(decoder, module);
+  std::unique_ptr<ProfileInformation> pgo_info =
+      DeserializeTieringInformation(decoder, module);
 
   CHECK(decoder.ok());
   CHECK_EQ(decoder.pc(), decoder.end());
+
+  return pgo_info;
 }
 
 void DumpProfileToFile(const WasmModule* module,
-                       base::Vector<const uint8_t> wire_bytes) {
+                       base::Vector<const uint8_t> wire_bytes,
+                       uint32_t* tiering_budget_array) {
   CHECK(!wire_bytes.empty());
   // File are named `profile-wasm-<hash>`.
   // We use the same hash as for reported scripts, to make it easier to
@@ -133,7 +187,7 @@
   base::EmbeddedVector<char, 32> filename;
   SNPrintF(filename, "profile-wasm-%08x", hash);
 
-  ProfileGenerator profile_generator{module};
+  ProfileGenerator profile_generator{module, tiering_budget_array};
   base::OwnedVector<uint8_t> profile_data = profile_generator.GetProfileData();
 
   PrintF("Dumping Wasm PGO data to file '%s' (%zu bytes)\n", filename.begin(),
@@ -145,8 +199,8 @@
   }
 }
 
-void LoadProfileFromFile(WasmModule* module,
-                         base::Vector<const uint8_t> wire_bytes) {
+std::unique_ptr<ProfileInformation> LoadProfileFromFile(
+    WasmModule* module, base::Vector<const uint8_t> wire_bytes) {
   CHECK(!wire_bytes.empty());
   // File are named `profile-wasm-<hash>`.
   // We use the same hash as for reported scripts, to make it easier to
@@ -158,7 +212,7 @@
   FILE* file = base::OS::FOpen(filename.begin(), "rb");
   if (!file) {
     PrintF("No Wasm PGO data found: Cannot open file '%s'\n", filename.begin());
-    return;
+    return {};
   }
 
   fseek(file, 0, SEEK_END);
@@ -176,11 +230,7 @@
 
   base::Fclose(file);
 
-  RestoreProfileData(module, profile_data.as_vector());
-
-  // Check that the generated profile is deterministic.
-  DCHECK_EQ(profile_data.as_vector(),
-            ProfileGenerator{module}.GetProfileData().as_vector());
+  return RestoreProfileData(module, profile_data.as_vector());
 }
 
 }  // namespace v8::internal::wasm
diff -r -u --color up/v8/src/wasm/pgo.h nw/v8/src/wasm/pgo.h
--- up/v8/src/wasm/pgo.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/pgo.h	2023-01-19 16:46:36.428109511 -0500
@@ -9,17 +9,43 @@
 #ifndef V8_WASM_PGO_H_
 #define V8_WASM_PGO_H_
 
+#include <vector>
+
 #include "src/base/vector.h"
 
 namespace v8::internal::wasm {
 
 struct WasmModule;
 
+class ProfileInformation {
+ public:
+  ProfileInformation(std::vector<uint32_t> executed_functions,
+                     std::vector<uint32_t> tiered_up_functions)
+      : executed_functions_(std::move(executed_functions)),
+        tiered_up_functions_(std::move(tiered_up_functions)) {}
+
+  // Disallow copying (not needed, so most probably a bug).
+  ProfileInformation(const ProfileInformation&) = delete;
+  ProfileInformation& operator=(const ProfileInformation&) = delete;
+
+  base::Vector<const uint32_t> executed_functions() const {
+    return base::VectorOf(executed_functions_);
+  }
+  base::Vector<const uint32_t> tiered_up_functions() const {
+    return base::VectorOf(tiered_up_functions_);
+  }
+
+ private:
+  const std::vector<uint32_t> executed_functions_;
+  const std::vector<uint32_t> tiered_up_functions_;
+};
+
 void DumpProfileToFile(const WasmModule* module,
-                       base::Vector<const uint8_t> wire_bytes);
+                       base::Vector<const uint8_t> wire_bytes,
+                       uint32_t* tiering_budget_array);
 
-void LoadProfileFromFile(WasmModule* module,
-                         base::Vector<const uint8_t> wire_bytes);
+V8_WARN_UNUSED_RESULT std::unique_ptr<ProfileInformation> LoadProfileFromFile(
+    WasmModule* module, base::Vector<const uint8_t> wire_bytes);
 
 }  // namespace v8::internal::wasm
 
diff -r -u --color up/v8/src/wasm/string-builder-multiline.h nw/v8/src/wasm/string-builder-multiline.h
--- up/v8/src/wasm/string-builder-multiline.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/string-builder-multiline.h	2023-01-19 16:46:36.428109511 -0500
@@ -92,7 +92,7 @@
       // Write the unfinished line into its new location.
       start_here();
       char* new_location = allocate(unfinished_length);
-      memcpy(new_location, unfinished_start, unfinished_length);
+      memmove(new_location, unfinished_start, unfinished_length);
       if (label_source >= unfinished_start &&
           label_source < unfinished_start + unfinished_length) {
         label_source = new_location + (label_source - unfinished_start);
@@ -137,6 +137,8 @@
     out.write(last_start, len);
   }
 
+  size_t ApproximateSizeMB() { return approximate_size_mb(); }
+
  private:
   struct Line {
     Line(const char* d, size_t length, uint32_t bytecode_offset)
diff -r -u --color up/v8/src/wasm/string-builder.h nw/v8/src/wasm/string-builder.h
--- up/v8/src/wasm/string-builder.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/string-builder.h	2023-01-19 16:46:36.428109511 -0500
@@ -70,6 +70,11 @@
   explicit StringBuilder(OnGrowth on_growth) : on_growth_(on_growth) {}
   void start_here() { start_ = cursor_; }
 
+  size_t approximate_size_mb() {
+    static_assert(kChunkSize == size_t{MB});
+    return chunks_.size();
+  }
+
  private:
   void Grow(size_t requested) {
     size_t used = length();
diff -r -u --color up/v8/src/wasm/value-type.h nw/v8/src/wasm/value-type.h
--- up/v8/src/wasm/value-type.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/value-type.h	2023-01-19 16:46:36.428109511 -0500
@@ -62,7 +62,7 @@
     kFunc = kV8MaxWasmTypes,  // shorthand: c
     kEq,                      // shorthand: q
     kI31,                     // shorthand: j
-    kData,                    // shorthand: o
+    kStruct,                  // shorthand: o
     kArray,                   // shorthand: g
     kAny,                     //
     kExtern,                  // shorthand: a.
@@ -90,8 +90,8 @@
         return HeapType(kAny);
       case ValueTypeCode::kExternRefCode:
         return HeapType(kExtern);
-      case ValueTypeCode::kDataRefCode:
-        return HeapType(kData);
+      case ValueTypeCode::kStructRefCode:
+        return HeapType(kStruct);
       case ValueTypeCode::kArrayRefCode:
         return HeapType(kArray);
       case ValueTypeCode::kStringRefCode:
@@ -156,8 +156,8 @@
         return std::string("eq");
       case kI31:
         return std::string("i31");
-      case kData:
-        return std::string("data");
+      case kStruct:
+        return std::string("struct");
       case kArray:
         return std::string("array");
       case kExtern:
@@ -195,8 +195,8 @@
         return mask | kEqRefCode;
       case kI31:
         return mask | kI31RefCode;
-      case kData:
-        return mask | kDataRefCode;
+      case kStruct:
+        return mask | kStructRefCode;
       case kArray:
         return mask | kArrayRefCode;
       case kExtern:
@@ -402,6 +402,7 @@
   }
 
   /******************************** Type checks *******************************/
+  // Includes s128.
   constexpr bool is_numeric() const { return wasm::is_numeric(kind()); }
 
   constexpr bool is_reference() const { return wasm::is_reference(kind()); }
@@ -428,6 +429,14 @@
 
   constexpr bool is_bottom() const { return kind() == kBottom; }
 
+  // These can occur as the result of type propagation, but never in
+  // reachable control flow.
+  constexpr bool is_uninhabited() const {
+    return is_non_nullable() && (is_reference_to(HeapType::kNone) ||
+                                 is_reference_to(HeapType::kNoExtern) ||
+                                 is_reference_to(HeapType::kNoFunc));
+  }
+
   constexpr bool is_packed() const { return wasm::is_packed(kind()); }
 
   constexpr ValueType Unpacked() const {
@@ -436,12 +445,6 @@
 
   // If {this} is (ref null $t), returns (ref $t). Otherwise, returns {this}.
   constexpr ValueType AsNonNull() const {
-    if (is_reference_to(HeapType::kNone) ||
-        is_reference_to(HeapType::kNoExtern) ||
-        is_reference_to(HeapType::kNoFunc)) {
-      // Non-null none type is not a valid type.
-      return ValueType::Primitive(kBottom);
-    }
     return is_nullable() ? Ref(heap_type()) : *this;
   }
 
@@ -550,8 +553,8 @@
             return kAnyRefCode;
           case HeapType::kI31:
             return kI31RefCode;
-          case HeapType::kData:
-            return kDataRefCode;
+          case HeapType::kStruct:
+            return kStructRefCode;
           case HeapType::kArray:
             return kArrayRefCode;
           case HeapType::kString:
@@ -702,7 +705,7 @@
 constexpr ValueType kWasmExternRef = ValueType::RefNull(HeapType::kExtern);
 constexpr ValueType kWasmEqRef = ValueType::RefNull(HeapType::kEq);
 constexpr ValueType kWasmI31Ref = ValueType::RefNull(HeapType::kI31);
-constexpr ValueType kWasmDataRef = ValueType::RefNull(HeapType::kData);
+constexpr ValueType kWasmStructRef = ValueType::RefNull(HeapType::kStruct);
 constexpr ValueType kWasmArrayRef = ValueType::RefNull(HeapType::kArray);
 constexpr ValueType kWasmStringRef = ValueType::RefNull(HeapType::kString);
 constexpr ValueType kWasmStringViewWtf8 =
diff -r -u --color up/v8/src/wasm/wasm-arguments.h nw/v8/src/wasm/wasm-arguments.h
--- up/v8/src/wasm/wasm-arguments.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/wasm-arguments.h	2023-01-19 16:46:36.428109511 -0500
@@ -52,11 +52,11 @@
   static int TotalSize(const FunctionSig* sig) {
     int return_size = 0;
     for (ValueType t : sig->returns()) {
-      return_size += t.value_kind_size();
+      return_size += t.value_kind_full_size();
     }
     int param_size = 0;
     for (ValueType t : sig->parameters()) {
-      param_size += t.value_kind_size();
+      param_size += t.value_kind_full_size();
     }
     return std::max(return_size, param_size);
   }
diff -r -u --color up/v8/src/wasm/wasm-code-manager.cc nw/v8/src/wasm/wasm-code-manager.cc
--- up/v8/src/wasm/wasm-code-manager.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/wasm-code-manager.cc	2023-01-19 16:46:36.428109511 -0500
@@ -785,7 +785,6 @@
     }
   }
   DCHECK(IsAligned(code_space.begin(), kCodeAlignment));
-  allocated_code_space_.Merge(code_space);
   generated_code_size_.fetch_add(code_space.size(), std::memory_order_relaxed);
 
   TRACE_HEAP("Code alloc for %p: 0x%" PRIxPTR ",+%zu\n", this,
@@ -1887,8 +1886,8 @@
   import_wrapper_cache_.reset();
 
   // If experimental PGO support is enabled, serialize the PGO data now.
-  if (V8_UNLIKELY(FLAG_experimental_wasm_pgo_to_file)) {
-    DumpProfileToFile(module_.get(), wire_bytes());
+  if (V8_UNLIKELY(v8_flags.experimental_wasm_pgo_to_file)) {
+    DumpProfileToFile(module_.get(), wire_bytes(), tiering_budgets_.get());
   }
 }
 
diff -r -u --color up/v8/src/wasm/wasm-code-manager.h nw/v8/src/wasm/wasm-code-manager.h
--- up/v8/src/wasm/wasm-code-manager.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/wasm-code-manager.h	2023-01-19 16:46:36.428109511 -0500
@@ -54,6 +54,7 @@
   FOREACH_WASM_TRAPREASON(VTRAP)         \
   V(WasmCompileLazy)                     \
   V(WasmTriggerTierUp)                   \
+  V(WasmLiftoffFrameSetup)               \
   V(WasmDebugBreak)                      \
   V(WasmInt32ToHeapNumber)               \
   V(WasmTaggedNonSmiToInt32)             \
@@ -62,10 +63,8 @@
   V(WasmTaggedToFloat64)                 \
   V(WasmAllocateJSArray)                 \
   V(WasmAtomicNotify)                    \
-  V(WasmI32AtomicWait32)                 \
-  V(WasmI32AtomicWait64)                 \
-  V(WasmI64AtomicWait32)                 \
-  V(WasmI64AtomicWait64)                 \
+  V(WasmI32AtomicWait)                   \
+  V(WasmI64AtomicWait)                   \
   V(WasmGetOwnProperty)                  \
   V(WasmRefFunc)                         \
   V(WasmMemoryGrow)                      \
@@ -596,8 +595,6 @@
   // Code space that was reserved and is available for allocations (subset of
   // {owned_code_space_}).
   DisjointAllocationPool free_code_space_;
-  // Code space that was allocated for code (subset of {owned_code_space_}).
-  DisjointAllocationPool allocated_code_space_;
   // Code space that was allocated before but is dead now. Full pages within
   // this region are discarded. It's still a subset of {owned_code_space_}.
   DisjointAllocationPool freed_code_space_;
diff -r -u --color up/v8/src/wasm/wasm-constants.h nw/v8/src/wasm/wasm-constants.h
--- up/v8/src/wasm/wasm-constants.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/wasm-constants.h	2023-01-19 16:46:36.428109511 -0500
@@ -48,7 +48,7 @@
   kRefNullCode = 0x6c,
   kRefCode = 0x6b,
   kI31RefCode = 0x6a,
-  kDataRefCode = 0x67,
+  kStructRefCode = 0x67,
   kArrayRefCode = 0x66,
   kNoneCode = 0x65,
   kStringRefCode = 0x64,
@@ -74,12 +74,14 @@
 };
 
 enum LimitsFlags : uint8_t {
-  kNoMaximum = 0x00,           // Also valid for table limits.
-  kWithMaximum = 0x01,         // Also valid for table limits.
-  kSharedNoMaximum = 0x02,     // Only valid for memory limits.
-  kSharedWithMaximum = 0x03,   // Only valid for memory limits.
-  kMemory64NoMaximum = 0x04,   // Only valid for memory limits.
-  kMemory64WithMaximum = 0x05  // Only valid for memory limits.
+  kNoMaximum = 0x00,                 // Also valid for table limits.
+  kWithMaximum = 0x01,               // Also valid for table limits.
+  kSharedNoMaximum = 0x02,           // Only valid for memory limits.
+  kSharedWithMaximum = 0x03,         // Only valid for memory limits.
+  kMemory64NoMaximum = 0x04,         // Only valid for memory limits.
+  kMemory64WithMaximum = 0x05,       // Only valid for memory limits.
+  kMemory64SharedNoMaximum = 0x06,   // Only valid for memory limits.
+  kMemory64SharedWithMaximum = 0x07  // Only valid for memory limits.
 };
 
 // Flags for data and element segments.
diff -r -u --color up/v8/src/wasm/wasm-disassembler-impl.h nw/v8/src/wasm/wasm-disassembler-impl.h
--- up/v8/src/wasm/wasm-disassembler-impl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/wasm-disassembler-impl.h	2023-01-19 16:46:36.428109511 -0500
@@ -21,7 +21,7 @@
 namespace internal {
 namespace wasm {
 
-template <Decoder::ValidateFlag validate>
+template <typename ValidationTag>
 class ImmediatesPrinter;
 
 using IndexAsComment = NamesProvider::IndexAsComment;
@@ -74,9 +74,9 @@
 // FunctionBodyDisassembler.
 
 class V8_EXPORT_PRIVATE FunctionBodyDisassembler
-    : public WasmDecoder<Decoder::kFullValidation> {
+    : public WasmDecoder<Decoder::FullValidationTag> {
  public:
-  static constexpr Decoder::ValidateFlag validate = Decoder::kFullValidation;
+  using ValidationTag = Decoder::FullValidationTag;
   enum FunctionHeader : bool { kSkipHeader = false, kPrintHeader = true };
 
   FunctionBodyDisassembler(Zone* zone, const WasmModule* module,
@@ -84,8 +84,8 @@
                            const FunctionSig* sig, const byte* start,
                            const byte* end, uint32_t offset,
                            NamesProvider* names)
-      : WasmDecoder<validate>(zone, module, WasmFeatures::All(), detected, sig,
-                              start, end, offset),
+      : WasmDecoder<ValidationTag>(zone, module, WasmFeatures::All(), detected,
+                                   sig, start, end, offset),
         func_index_(func_index),
         names_(names) {}
 
@@ -108,7 +108,7 @@
     return label_stack_[label_stack_.size() - 1 - depth];
   }
 
-  friend class ImmediatesPrinter<validate>;
+  friend class ImmediatesPrinter<ValidationTag>;
   uint32_t func_index_;
   WasmOpcode current_opcode_ = kExprUnreachable;
   NamesProvider* names_;
@@ -141,7 +141,7 @@
   V8_EXPORT_PRIVATE void PrintTypeDefinition(uint32_t type_index,
                                              Indentation indendation,
                                              IndexAsComment index_as_comment);
-  V8_EXPORT_PRIVATE void PrintModule(Indentation indentation);
+  V8_EXPORT_PRIVATE void PrintModule(Indentation indentation, size_t max_mb);
 
  private:
   void PrintImportName(const WasmImport& import);
diff -r -u --color up/v8/src/wasm/wasm-disassembler.cc nw/v8/src/wasm/wasm-disassembler.cc
--- up/v8/src/wasm/wasm-disassembler.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/wasm-disassembler.cc	2023-01-19 16:46:36.428109511 -0500
@@ -26,7 +26,7 @@
   AccountingAllocator allocator;
   ModuleDisassembler md(out, module, names, wire_bytes, &allocator,
                         function_body_offsets);
-  md.PrintModule({0, 2});
+  md.PrintModule({0, 2}, v8_flags.wasm_disassembly_max_mb);
   out.ToDisassemblyCollector(collector);
 }
 
@@ -255,7 +255,7 @@
   WasmOpcode opcode = static_cast<WasmOpcode>(*pc_);
   if (!WasmOpcodes::IsPrefixOpcode(opcode)) return opcode;
   uint32_t opcode_length;
-  return read_prefixed_opcode<validate>(pc_, &opcode_length);
+  return read_prefixed_opcode<ValidationTag>(pc_, &opcode_length);
 }
 
 void FunctionBodyDisassembler::PrintHexNumber(StringBuilder& out,
@@ -278,7 +278,7 @@
 ////////////////////////////////////////////////////////////////////////////////
 // ImmediatesPrinter.
 
-template <Decoder::ValidateFlag validate>
+template <typename ValidationTag>
 class ImmediatesPrinter {
  public:
   ImmediatesPrinter(StringBuilder& out, FunctionBodyDisassembler* owner)
@@ -309,7 +309,7 @@
     owner_->out_->PatchLabel(label_info, out_.start() + label_start_position);
   }
 
-  void BlockType(BlockTypeImmediate<validate>& imm) {
+  void BlockType(BlockTypeImmediate& imm) {
     if (imm.type == kWasmBottom) {
       const FunctionSig* sig = owner_->module_->signature(imm.sig_index);
       PrintSignatureOneLine(out_, sig, 0 /* ignored */, names(), false);
@@ -322,95 +322,91 @@
     }
   }
 
-  void HeapType(HeapTypeImmediate<validate>& imm) {
+  void HeapType(HeapTypeImmediate& imm) {
     out_ << " ";
     names()->PrintHeapType(out_, imm.type);
     if (imm.type.is_index()) use_type(imm.type.ref_index());
   }
 
-  void BranchDepth(BranchDepthImmediate<validate>& imm) {
-    PrintDepthAsLabel(imm.depth);
-  }
+  void BranchDepth(BranchDepthImmediate& imm) { PrintDepthAsLabel(imm.depth); }
 
-  void BranchTable(BranchTableImmediate<validate>& imm) {
+  void BranchTable(BranchTableImmediate& imm) {
     const byte* pc = imm.table;
     for (uint32_t i = 0; i <= imm.table_count; i++) {
       uint32_t length;
-      uint32_t target = owner_->read_u32v<validate>(pc, &length);
+      uint32_t target = owner_->read_u32v<ValidationTag>(pc, &length);
       PrintDepthAsLabel(target);
       pc += length;
     }
   }
 
-  void CallIndirect(CallIndirectImmediate<validate>& imm) {
+  void CallIndirect(CallIndirectImmediate& imm) {
     const FunctionSig* sig = owner_->module_->signature(imm.sig_imm.index);
     PrintSignatureOneLine(out_, sig, 0 /* ignored */, names(), false);
     if (imm.table_imm.index != 0) TableIndex(imm.table_imm);
   }
 
-  void SelectType(SelectTypeImmediate<validate>& imm) {
+  void SelectType(SelectTypeImmediate& imm) {
     out_ << " ";
     names()->PrintValueType(out_, imm.type);
   }
 
-  void MemoryAccess(MemoryAccessImmediate<validate>& imm) {
+  void MemoryAccess(MemoryAccessImmediate& imm) {
     if (imm.offset != 0) out_ << " offset=" << imm.offset;
     if (imm.alignment != GetDefaultAlignment(owner_->current_opcode_)) {
       out_ << " align=" << (1u << imm.alignment);
     }
   }
 
-  void SimdLane(SimdLaneImmediate<validate>& imm) {
-    out_ << " " << uint32_t{imm.lane};
-  }
+  void SimdLane(SimdLaneImmediate& imm) { out_ << " " << uint32_t{imm.lane}; }
 
-  void Field(FieldImmediate<validate>& imm) {
+  void Field(FieldImmediate& imm) {
     TypeIndex(imm.struct_imm);
     out_ << " ";
     names()->PrintFieldName(out_, imm.struct_imm.index, imm.field_imm.index);
   }
 
-  void Length(IndexImmediate<validate>& imm) {
+  void Length(IndexImmediate& imm) {
     out_ << " " << imm.index;  // --
   }
 
-  void TagIndex(TagIndexImmediate<validate>& imm) {
+  void TagIndex(TagIndexImmediate& imm) {
     out_ << " ";
     names()->PrintTagName(out_, imm.index);
   }
 
-  void FunctionIndex(IndexImmediate<validate>& imm) {
+  void FunctionIndex(IndexImmediate& imm) {
     out_ << " ";
     names()->PrintFunctionName(out_, imm.index, NamesProvider::kDevTools);
   }
 
-  void TypeIndex(IndexImmediate<validate>& imm) {
+  void TypeIndex(IndexImmediate& imm) {
     out_ << " ";
     names()->PrintTypeName(out_, imm.index);
     use_type(imm.index);
   }
 
-  void LocalIndex(IndexImmediate<validate>& imm) {
+  void LocalIndex(IndexImmediate& imm) {
     out_ << " ";
     names()->PrintLocalName(out_, func_index(), imm.index);
   }
 
-  void GlobalIndex(IndexImmediate<validate>& imm) {
+  void GlobalIndex(IndexImmediate& imm) {
     out_ << " ";
     names()->PrintGlobalName(out_, imm.index);
   }
 
-  void TableIndex(IndexImmediate<validate>& imm) {
+  void TableIndex(IndexImmediate& imm) {
     out_ << " ";
     names()->PrintTableName(out_, imm.index);
   }
 
-  void MemoryIndex(MemoryIndexImmediate<validate>& imm) {
+  void MemoryIndex(MemoryIndexImmediate& imm) {
     if (imm.index == 0) return;
     out_ << " " << imm.index;
   }
 
-  void DataSegmentIndex(IndexImmediate<validate>& imm) {
+  void DataSegmentIndex(IndexImmediate& imm) {
     if (kSkipDataSegmentNames) {
       out_ << " " << imm.index;
     } else {
@@ -419,16 +415,16 @@
     }
   }
 
-  void ElemSegmentIndex(IndexImmediate<validate>& imm) {
+  void ElemSegmentIndex(IndexImmediate& imm) {
     out_ << " ";
     names()->PrintElementSegmentName(out_, imm.index);
   }
 
-  void I32Const(ImmI32Immediate<validate>& imm) {
+  void I32Const(ImmI32Immediate& imm) {
     out_ << " " << imm.value;  // --
   }
 
-  void I64Const(ImmI64Immediate<validate>& imm) {
+  void I64Const(ImmI64Immediate& imm) {
     if (imm.value >= 0) {
       out_ << " " << static_cast<uint64_t>(imm.value);
     } else {
@@ -436,10 +432,12 @@
     }
   }
 
-  void F32Const(ImmF32Immediate<validate>& imm) {
+  void F32Const(ImmF32Immediate& imm) {
     float f = imm.value;
     if (f == 0) {
       out_ << (1 / f < 0 ? " -0.0" : " 0.0");
+    } else if (std::isinf(f)) {
+      out_ << (f > 0 ? " inf" : " -inf");
     } else if (std::isnan(f)) {
       uint32_t bits = base::bit_cast<uint32_t>(f);
       uint32_t payload = bits & 0x7F'FFFFu;
@@ -452,12 +450,15 @@
       }
     } else {
       std::ostringstream o;
-      o << std::setprecision(std::numeric_limits<float>::digits10 + 1) << f;
+      // TODO(dlehmann): Change to `std::format` (C++20) or to `std::to_chars`
+      // (C++17) once available, so that `0.1` isn't printed as `0.100000001`
+      // any more.
+      o << std::setprecision(std::numeric_limits<float>::max_digits10) << f;
       out_ << " " << o.str();
     }
   }
 
-  void F64Const(ImmF64Immediate<validate>& imm) {
+  void F64Const(ImmF64Immediate& imm) {
     double d = imm.value;
     if (d == 0) {
       out_ << (1 / d < 0 ? " -0.0" : " 0.0");
@@ -480,7 +481,7 @@
     }
   }
 
-  void S128Const(Simd128Immediate<validate>& imm) {
+  void S128Const(Simd128Immediate& imm) {
     if (owner_->current_opcode_ == kExprI8x16Shuffle) {
       for (int i = 0; i < 16; i++) {
         out_ << " " << uint32_t{imm.value[i]};
@@ -499,28 +500,28 @@
     }
   }
 
-  void StringConst(StringConstImmediate<validate>& imm) {
+  void StringConst(StringConstImmediate& imm) {
     // TODO(jkummerow): Print (a prefix of) the string?
     out_ << " " << imm.index;
   }
 
-  void MemoryInit(MemoryInitImmediate<validate>& imm) {
+  void MemoryInit(MemoryInitImmediate& imm) {
     DataSegmentIndex(imm.data_segment);
     if (imm.memory.index != 0) out_ << " " << uint32_t{imm.memory.index};
   }
 
-  void MemoryCopy(MemoryCopyImmediate<validate>& imm) {
+  void MemoryCopy(MemoryCopyImmediate& imm) {
     if (imm.memory_dst.index == 0 && imm.memory_src.index == 0) return;
     out_ << " " << uint32_t{imm.memory_dst.index};
     out_ << " " << uint32_t{imm.memory_src.index};
   }
 
-  void TableInit(TableInitImmediate<validate>& imm) {
+  void TableInit(TableInitImmediate& imm) {
     if (imm.table.index != 0) TableIndex(imm.table);
     ElemSegmentIndex(imm.element_segment);
   }
 
-  void TableCopy(TableCopyImmediate<validate>& imm) {
+  void TableCopy(TableCopyImmediate& imm) {
     if (imm.table_dst.index == 0 && imm.table_src.index == 0) return;
     out_ << " ";
     names()->PrintTableName(out_, imm.table_dst.index);
@@ -528,7 +529,7 @@
     names()->PrintTableName(out_, imm.table_src.index);
   }
 
-  void ArrayCopy(IndexImmediate<validate>& dst, IndexImmediate<validate>& src) {
+  void ArrayCopy(IndexImmediate& dst, IndexImmediate& src) {
     out_ << " ";
     names()->PrintTypeName(out_, dst.index);
     out_ << " ";
@@ -550,9 +551,9 @@
 
 uint32_t FunctionBodyDisassembler::PrintImmediatesAndGetLength(
     StringBuilder& out) {
-  using Printer = ImmediatesPrinter<validate>;
+  using Printer = ImmediatesPrinter<ValidationTag>;
   Printer imm_printer(out, this);
-  return WasmDecoder::OpcodeLength<Printer>(this, this->pc_, &imm_printer);
+  return WasmDecoder::OpcodeLength<Printer>(this, this->pc_, imm_printer);
 }
 
 ////////////////////////////////////////////////////////////////////////////////
@@ -564,11 +565,14 @@
 
   void CollectOffsets(const WasmModule* module, const byte* start,
                       const byte* end, AccountingAllocator* allocator) {
+    num_imported_tables_ = module->num_imported_tables;
+    num_imported_globals_ = module->num_imported_globals;
+    num_imported_tags_ = module->num_imported_tags;
     type_offsets_.reserve(module->types.size());
     import_offsets_.reserve(module->import_table.size());
-    table_offsets_.reserve(module->tables.size());
-    tag_offsets_.reserve(module->tags.size());
-    global_offsets_.reserve(module->globals.size());
+    table_offsets_.reserve(module->tables.size() - num_imported_tables_);
+    tag_offsets_.reserve(module->tags.size() - num_imported_tags_);
+    global_offsets_.reserve(module->globals.size() - num_imported_globals_);
     element_offsets_.reserve(module->elem_segments.size());
     data_offsets_.reserve(module->data_segments.size());
 
@@ -624,19 +628,31 @@
   }
   GETTER(type)
   GETTER(import)
-  GETTER(table)
-  GETTER(tag)
-  GETTER(global)
   GETTER(element)
   GETTER(data)
 #undef GETTER
 
+#define IMPORT_ADJUSTED_GETTER(name)                                  \
+  uint32_t name##_offset(uint32_t index) {                            \
+    if (!enabled_) return 0;                                          \
+    DCHECK(index >= num_imported_##name##s_ &&                        \
+           index - num_imported_##name##s_ < name##_offsets_.size()); \
+    return name##_offsets_[index - num_imported_##name##s_];          \
+  }
+  IMPORT_ADJUSTED_GETTER(table)
+  IMPORT_ADJUSTED_GETTER(tag)
+  IMPORT_ADJUSTED_GETTER(global)
+#undef IMPORT_ADJUSTED_GETTER
+
   uint32_t memory_offset() { return memory_offset_; }
 
   uint32_t start_offset() { return start_offset_; }
 
  private:
   bool enabled_{false};
+  uint32_t num_imported_tables_{0};
+  uint32_t num_imported_globals_{0};
+  uint32_t num_imported_tags_{0};
   std::vector<uint32_t> type_offsets_;
   std::vector<uint32_t> import_offsets_;
   std::vector<uint32_t> table_offsets_;
@@ -736,12 +752,11 @@
   }
 }
 
-void ModuleDisassembler::PrintModule(Indentation indentation) {
+void ModuleDisassembler::PrintModule(Indentation indentation, size_t max_mb) {
   // 0. General infrastructure.
   // We don't store import/export information on {WasmTag} currently.
   size_t num_tags = module_->tags.size();
   std::vector<bool> exported_tags(num_tags, false);
-  std::vector<bool> imported_tags(num_tags, false);
   for (const WasmExport& ex : module_->export_table) {
     if (ex.kind == kExternalTag) exported_tags[ex.index] = true;
   }
@@ -816,16 +831,16 @@
           PrintExportName(kExternalTag, import.index);
         }
         PrintTagSignature(module_->tags[import.index].sig);
-        imported_tags[import.index] = true;
         break;
     }
     out_ << ")";
   }
 
   // IV. Tables
-  for (uint32_t i = 0; i < module_->tables.size(); i++) {
+  for (uint32_t i = module_->num_imported_tables; i < module_->tables.size();
+       i++) {
     const WasmTable& table = module_->tables[i];
-    if (table.imported) continue;
+    DCHECK(!table.imported);
     out_.NextLine(offsets_->table_offset(i));
     out_ << indentation << "(table ";
     names_->PrintTableName(out_, i, kIndicesAsComments);
@@ -849,8 +864,7 @@
   }
 
   // VI.Tags
-  for (uint32_t i = 0; i < module_->tags.size(); i++) {
-    if (imported_tags[i]) continue;
+  for (uint32_t i = module_->num_imported_tags; i < module_->tags.size(); i++) {
     const WasmTag& tag = module_->tags[i];
     out_.NextLine(offsets_->tag_offset(i));
     out_ << indentation << "(tag ";
@@ -864,9 +878,10 @@
   // TODO(jkummerow/12868): Implement.
 
   // VIII. Globals
-  for (uint32_t i = 0; i < module_->globals.size(); i++) {
+  for (uint32_t i = module_->num_imported_globals; i < module_->globals.size();
+       i++) {
     const WasmGlobal& global = module_->globals[i];
-    if (global.imported) continue;
+    DCHECK(!global.imported);
     out_.NextLine(offsets_->global_offset(i));
     out_ << indentation << "(global ";
     names_->PrintGlobalName(out_, i, kIndicesAsComments);
@@ -890,7 +905,7 @@
     const WasmElemSegment& elem = module_->elem_segments[i];
     out_.NextLine(offsets_->element_offset(i));
     out_ << indentation << "(elem ";
-    names_->PrintElementSegmentName(out_, i);
+    names_->PrintElementSegmentName(out_, i, kIndicesAsComments);
     if (elem.status == WasmElemSegment::kStatusDeclarative) {
       out_ << " declare";
     } else if (elem.status == WasmElemSegment::kStatusActive) {
@@ -941,6 +956,10 @@
       function_body_offsets_->push_back(first_instruction_offset);
       function_body_offsets_->push_back(d.pc_offset());
     }
+    if (out_.ApproximateSizeMB() > max_mb) {
+      out_ << "<truncated...>";
+      return;
+    }
   }
 
   // XII. Data
@@ -950,7 +969,7 @@
     out_ << indentation << "(data";
     if (!kSkipDataSegmentNames) {
       out_ << " ";
-      names_->PrintDataSegmentName(out_, i);
+      names_->PrintDataSegmentName(out_, i, kIndicesAsComments);
     }
     if (data.active) {
       ValueType type = module_->is_memory64 ? kWasmI64 : kWasmI32;
@@ -960,6 +979,11 @@
     PrintString(data.source);
     out_ << "\")";
     out_.NextLine(0);
+
+    if (out_.ApproximateSizeMB() > max_mb) {
+      out_ << "<truncated...>";
+      return;
+    }
   }
 
   indentation.decrease();
diff -r -u --color up/v8/src/wasm/wasm-engine.cc nw/v8/src/wasm/wasm-engine.cc
--- up/v8/src/wasm/wasm-engine.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/wasm-engine.cc	2023-01-19 16:46:36.428109511 -0500
@@ -511,9 +511,10 @@
 
   // Transfer ownership of the WasmModule to the {Managed<WasmModule>} generated
   // in {CompileToNativeModule}.
+  constexpr ProfileInformation* kNoProfileInformation = nullptr;
   std::shared_ptr<NativeModule> native_module = CompileToNativeModule(
       isolate, WasmFeatures::ForAsmjs(), thrower, std::move(result).value(),
-      bytes, compilation_id, context_id);
+      bytes, compilation_id, context_id, kNoProfileInformation);
   if (!native_module) return {};
 
   return AsmWasmData::New(isolate, std::move(native_module), uses_bitset);
@@ -550,15 +551,16 @@
   }
 
   // If experimental PGO via files is enabled, load profile information now.
-  if (V8_UNLIKELY(FLAG_experimental_wasm_pgo_from_file)) {
-    LoadProfileFromFile(module.get(), bytes.module_bytes());
+  std::unique_ptr<ProfileInformation> pgo_info;
+  if (V8_UNLIKELY(v8_flags.experimental_wasm_pgo_from_file)) {
+    pgo_info = LoadProfileFromFile(module.get(), bytes.module_bytes());
   }
 
   // Transfer ownership of the WasmModule to the {Managed<WasmModule>} generated
   // in {CompileToNativeModule}.
   std::shared_ptr<NativeModule> native_module =
       CompileToNativeModule(isolate, enabled, thrower, std::move(module), bytes,
-                            compilation_id, context_id);
+                            compilation_id, context_id, pgo_info.get());
   if (!native_module) return {};
 
 #ifdef DEBUG
@@ -1018,6 +1020,14 @@
   DCHECK_EQ(0, isolates_.count(isolate));
   isolates_.emplace(isolate, std::make_unique<IsolateInfo>(isolate));
 
+#if defined(V8_COMPRESS_POINTERS)
+  // The null value is not accessible on mksnapshot runs.
+  if (isolate->snapshot_available()) {
+    null_tagged_compressed_ = V8HeapCompressionScheme::CompressTagged(
+        isolate->factory()->null_value()->ptr());
+  }
+#endif
+
   // Install sampling GC callback.
   // TODO(v8:7424): For now we sample module sizes in a GC callback. This will
   // bias samples towards apps with high memory pressure. We should switch to
@@ -1083,37 +1093,55 @@
 
 void WasmEngine::LogCode(base::Vector<WasmCode*> code_vec) {
   if (code_vec.empty()) return;
-  base::MutexGuard guard(&mutex_);
-  NativeModule* native_module = code_vec[0]->native_module();
-  DCHECK_EQ(1, native_modules_.count(native_module));
-  for (Isolate* isolate : native_modules_[native_module]->isolates) {
-    DCHECK_EQ(1, isolates_.count(isolate));
-    IsolateInfo* info = isolates_[isolate].get();
-    if (info->log_codes == false) continue;
-    if (info->log_codes_task == nullptr) {
-      auto new_task = std::make_unique<LogCodesTask>(
-          &mutex_, &info->log_codes_task, isolate, this);
-      info->log_codes_task = new_task.get();
-      info->foreground_task_runner->PostTask(std::move(new_task));
-    }
-    if (info->code_to_log.empty()) {
-      isolate->stack_guard()->RequestLogWasmCode();
-    }
-    for (WasmCode* code : code_vec) {
-      DCHECK_EQ(native_module, code->native_module());
-      code->IncRef();
-    }
-
-    auto script_it = info->scripts.find(native_module);
-    // If the script does not yet exist, logging will happen later. If the weak
-    // handle is cleared already, we also don't need to log any more.
-    if (script_it == info->scripts.end()) continue;
-    auto& log_entry = info->code_to_log[script_it->second.script_id()];
-    if (!log_entry.source_url) {
-      log_entry.source_url = script_it->second.source_url();
+  using TaskToSchedule =
+      std::pair<std::shared_ptr<v8::TaskRunner>, std::unique_ptr<LogCodesTask>>;
+  std::vector<TaskToSchedule> to_schedule;
+  {
+    base::MutexGuard guard(&mutex_);
+    NativeModule* native_module = code_vec[0]->native_module();
+    DCHECK_EQ(1, native_modules_.count(native_module));
+    for (Isolate* isolate : native_modules_[native_module]->isolates) {
+      DCHECK_EQ(1, isolates_.count(isolate));
+      IsolateInfo* info = isolates_[isolate].get();
+      if (info->log_codes == false) continue;
+      if (info->log_codes_task == nullptr) {
+        auto new_task = std::make_unique<LogCodesTask>(
+            &mutex_, &info->log_codes_task, isolate, this);
+        info->log_codes_task = new_task.get();
+        // Store the LogCodeTasks to post them outside the WasmEngine::mutex_.
+        // Posting the task in the mutex can cause the following deadlock (only
+        // in d8): When d8 shuts down, it sets a terminate to the task runner.
+        // When the terminate flag in the taskrunner is set, all newly posted
+        // tasks get destroyed immediately. When the LogCodesTask gets
+        // destroyed, it takes the WasmEngine::mutex_ lock to deregister itself
+        // from the IsolateInfo. Therefore, as the LogCodesTask may get
+        // destroyed immediately when it gets posted, it cannot get posted when
+        // the WasmEngine::mutex_ lock is held.
+        to_schedule.emplace_back(info->foreground_task_runner,
+                                 std::move(new_task));
+      }
+      if (info->code_to_log.empty()) {
+        isolate->stack_guard()->RequestLogWasmCode();
+      }
+      for (WasmCode* code : code_vec) {
+        DCHECK_EQ(native_module, code->native_module());
+        code->IncRef();
+      }
+
+      auto script_it = info->scripts.find(native_module);
+      // If the script does not yet exist, logging will happen later. If the
+      // weak handle is cleared already, we also don't need to log any more.
+      if (script_it == info->scripts.end()) continue;
+      auto& log_entry = info->code_to_log[script_it->second.script_id()];
+      if (!log_entry.source_url) {
+        log_entry.source_url = script_it->second.source_url();
+      }
+      log_entry.code.insert(log_entry.code.end(), code_vec.begin(),
+                            code_vec.end());
     }
-    log_entry.code.insert(log_entry.code.end(), code_vec.begin(),
-                          code_vec.end());
+  }
+  for (auto& [runner, task] : to_schedule) {
+    runner->PostTask(std::move(task));
   }
 }
 
diff -r -u --color up/v8/src/wasm/wasm-engine.h nw/v8/src/wasm/wasm-engine.h
--- up/v8/src/wasm/wasm-engine.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/wasm-engine.h	2023-01-19 16:46:36.438942841 -0500
@@ -360,6 +360,12 @@
 
   TypeCanonicalizer* type_canonicalizer() { return &type_canonicalizer_; }
 
+  // Returns either the compressed tagged pointer representing a null value or
+  // 0 if pointer compression is not available.
+  Tagged_t compressed_null_value_or_zero() const {
+    return null_tagged_compressed_;
+  }
+
   // Call on process start and exit.
   static void InitializeOncePerProcess();
   static void GlobalTearDown();
@@ -395,6 +401,9 @@
 
   std::atomic<int> next_compilation_id_{0};
 
+  // Compressed tagged pointer to null value.
+  std::atomic<Tagged_t> null_tagged_compressed_{0};
+
   TypeCanonicalizer type_canonicalizer_;
 
   // This mutex protects all information which is mutated concurrently or
diff -r -u --color up/v8/src/wasm/wasm-external-refs.cc nw/v8/src/wasm/wasm-external-refs.cc
--- up/v8/src/wasm/wasm-external-refs.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/wasm-external-refs.cc	2023-01-19 16:46:36.438942841 -0500
@@ -588,12 +588,81 @@
   }
 }
 
-void array_fill_with_zeroes_wrapper(Address raw_array, uint32_t length,
-                                    uint32_t element_size_bytes) {
+void array_fill_with_number_or_null_wrapper(Address raw_array, uint32_t length,
+                                            uint32_t raw_type,
+                                            Address initial_value_addr) {
   ThreadNotInWasmScope thread_not_in_wasm_scope;
   DisallowGarbageCollection no_gc;
-  std::memset(ArrayElementAddress(raw_array, 0, element_size_bytes), 0,
-              length * element_size_bytes);
+  ValueType type = ValueType::FromRawBitField(raw_type);
+  int8_t* initial_element_address = reinterpret_cast<int8_t*>(
+      ArrayElementAddress(raw_array, 0, type.value_kind_size()));
+  int64_t initial_value = *reinterpret_cast<int64_t*>(initial_value_addr);
+  int bytes_to_set = length * type.value_kind_size();
+
+  // If the initial value is zero, we memset the array.
+  if (type.is_numeric() && initial_value == 0) {
+    std::memset(initial_element_address, 0, bytes_to_set);
+    return;
+  }
+
+  // We implement the general case by setting the first 8 bytes manually, then
+  // filling the rest by exponentially growing {memmove}s.
+
+  DCHECK_GE(static_cast<size_t>(bytes_to_set), sizeof(int64_t));
+
+  switch (type.kind()) {
+    case kI64:
+    case kF64: {
+      *reinterpret_cast<int64_t*>(initial_element_address) = initial_value;
+      break;
+    }
+    case kI32:
+    case kF32: {
+      int32_t* base = reinterpret_cast<int32_t*>(initial_element_address);
+      base[0] = base[1] = static_cast<int32_t>(initial_value);
+      break;
+    }
+    case kI16: {
+      int16_t* base = reinterpret_cast<int16_t*>(initial_element_address);
+      base[0] = base[1] = base[2] = base[3] =
+          static_cast<int16_t>(initial_value);
+      break;
+    }
+    case kI8: {
+      int8_t* base = reinterpret_cast<int8_t*>(initial_element_address);
+      for (size_t i = 0; i < sizeof(int64_t); i++) {
+        base[i] = static_cast<int8_t>(initial_value);
+      }
+      break;
+    }
+    case kRefNull:
+      if constexpr (kTaggedSize == 4) {
+        int32_t* base = reinterpret_cast<int32_t*>(initial_element_address);
+        base[0] = base[1] = static_cast<int32_t>(initial_value);
+      } else {
+        *reinterpret_cast<int64_t*>(initial_element_address) = initial_value;
+      }
+      break;
+    case kS128:
+    case kRtt:
+    case kRef:
+    case kVoid:
+    case kBottom:
+      UNREACHABLE();
+  }
+
+  int bytes_already_set = sizeof(int64_t);
+
+  while (bytes_already_set * 2 <= bytes_to_set) {
+    std::memcpy(initial_element_address + bytes_already_set,
+                initial_element_address, bytes_already_set);
+    bytes_already_set *= 2;
+  }
+
+  if (bytes_already_set < bytes_to_set) {
+    std::memcpy(initial_element_address + bytes_already_set,
+                initial_element_address, bytes_to_set - bytes_already_set);
+  }
 }
 
 static WasmTrapCallbackForTesting wasm_trap_callback_for_testing = nullptr;
diff -r -u --color up/v8/src/wasm/wasm-external-refs.h nw/v8/src/wasm/wasm-external-refs.h
--- up/v8/src/wasm/wasm-external-refs.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/wasm-external-refs.h	2023-01-19 16:46:36.438942841 -0500
@@ -118,8 +118,10 @@
                         uint32_t dst_index, Address raw_src_array,
                         uint32_t src_index, uint32_t length);
 
-void array_fill_with_zeroes_wrapper(Address raw_array, uint32_t length,
-                                    uint32_t element_size_bytes);
+// The initial value is passed as an int64_t on the stack.
+void array_fill_with_number_or_null_wrapper(Address raw_array, uint32_t length,
+                                            uint32_t raw_type,
+                                            Address initial_value_addr);
 
 using WasmTrapCallbackForTesting = void (*)();
 
diff -r -u --color up/v8/src/wasm/wasm-feature-flags.h nw/v8/src/wasm/wasm-feature-flags.h
--- up/v8/src/wasm/wasm-feature-flags.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/wasm-feature-flags.h	2023-01-19 16:46:36.438942841 -0500
@@ -100,32 +100,8 @@
 // #############################################################################
 // Shipped features (enabled by default). Remove the feature flag once they hit
 // stable and are expected to stay enabled.
-#define FOREACH_WASM_SHIPPED_FEATURE_FLAG(V) /*          (force 80 columns) */ \
-  /* Fixed-width SIMD operations. */                                           \
-  /* https://github.com/webassembly/simd */                                    \
-  /* V8 side owner: gdeepti, zhin */                                           \
-  /* Staged in v8.7 * */                                                       \
-  /* Shipped in v9.1 * */                                                      \
-  V(simd, "SIMD opcodes", true)                                                \
-                                                                               \
-  /* Threads proposal. */                                                      \
-  /* https://github.com/webassembly/threads */                                 \
-  /* NOTE: This is enabled via chromium flag on desktop systems since v7.4, */ \
-  /* and on android from 9.1. Threads are only available when */               \
-  /* SharedArrayBuffers are enabled as well, and are gated by COOP/COEP */     \
-  /* headers, more fine grained control is in the chromium codebase */         \
-  /* ITS: https://groups.google.com/a/chromium.org/d/msg/blink-dev/ */         \
-  /* tD6np-OG2PU/rcNGROOMFQAJ */                                               \
-  /* V8 side owner: gdeepti */                                                 \
-  V(threads, "thread opcodes", true)                                           \
-                                                                               \
-  /* Exception handling proposal. */                                           \
-  /* https://github.com/WebAssembly/exception-handling */                      \
-  /* V8 side owner: thibaudm */                                                \
-  /* Staged in v8.9 */                                                         \
-  /* Shipped in v9.5 */                                                        \
-  V(eh, "exception handling opcodes", true)                                    \
-                                                                               \
+#define FOREACH_WASM_SHIPPED_FEATURE_FLAG(V) /*          (force 80 columns) */
+
 // Combination of all available wasm feature flags.
 #define FOREACH_WASM_FEATURE_FLAG(V)        \
   FOREACH_WASM_EXPERIMENTAL_FEATURE_FLAG(V) \
diff -r -u --color up/v8/src/wasm/wasm-features.cc nw/v8/src/wasm/wasm-features.cc
--- up/v8/src/wasm/wasm-features.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/wasm-features.cc	2023-01-19 16:46:36.438942841 -0500
@@ -33,12 +33,7 @@
 WasmFeatures WasmFeatures::FromContext(Isolate* isolate,
                                        Handle<Context> context) {
   WasmFeatures features = WasmFeatures::FromFlags();
-  if (isolate->IsWasmSimdEnabled(context)) {
-    features.Add(kFeature_simd);
-  }
-  if (isolate->AreWasmExceptionsEnabled(context)) {
-    features.Add(kFeature_eh);
-  }
+  // This space intentionally left blank for future Wasm origin trials.
   return features;
 }
 
diff -r -u --color up/v8/src/wasm/wasm-features.h nw/v8/src/wasm/wasm-features.h
--- up/v8/src/wasm/wasm-features.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/wasm-features.h	2023-01-19 16:46:36.438942841 -0500
@@ -15,8 +15,11 @@
 #include "src/wasm/wasm-feature-flags.h"
 
 // Features that are always enabled and do not have a flag.
-#define FOREACH_WASM_NON_FLAG_FEATURE(V) \
-  V(reftypes, "reference type opcodes", true)
+#define FOREACH_WASM_NON_FLAG_FEATURE(V)      \
+  V(eh, "exception handling opcodes", true)   \
+  V(reftypes, "reference type opcodes", true) \
+  V(simd, "SIMD opcodes", true)               \
+  V(threads, "thread opcodes", true)
 
 // All features, including features that do not have flags.
 #define FOREACH_WASM_FEATURE(V) \
diff -r -u --color up/v8/src/wasm/wasm-import-wrapper-cache.h nw/v8/src/wasm/wasm-import-wrapper-cache.h
--- up/v8/src/wasm/wasm-import-wrapper-cache.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/wasm-import-wrapper-cache.h	2023-01-19 16:46:36.438942841 -0500
@@ -32,9 +32,7 @@
              uint32_t canonical_type_index, int expected_arity, Suspend suspend)
         : kind(kind),
           canonical_type_index(canonical_type_index),
-          expected_arity(expected_arity == kDontAdaptArgumentsSentinel
-                             ? 0
-                             : expected_arity),
+          expected_arity(expected_arity),
           suspend(suspend) {}
 
     bool operator==(const CacheKey& rhs) const {
diff -r -u --color up/v8/src/wasm/wasm-js.cc nw/v8/src/wasm/wasm-js.cc
--- up/v8/src/wasm/wasm-js.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/wasm-js.cc	2023-01-19 16:46:36.438942841 -0500
@@ -1174,8 +1174,8 @@
                string->StringEquals(v8_str(isolate, "eqref"))) {
       type = i::wasm::kWasmEqRef;
     } else if (enabled_features.has_gc() &&
-               string->StringEquals(v8_str(isolate, "dataref"))) {
-      type = i::wasm::kWasmDataRef;
+               string->StringEquals(v8_str(isolate, "structref"))) {
+      type = i::wasm::kWasmStructRef;
     } else if (enabled_features.has_gc() &&
                string->StringEquals(v8_str(isolate, "arrayref"))) {
       type = i::wasm::kWasmArrayRef;
@@ -1293,27 +1293,22 @@
   }
 
   auto shared = i::SharedFlag::kNotShared;
-  auto enabled_features = i::wasm::WasmFeatures::FromIsolate(i_isolate);
-  if (enabled_features.has_threads()) {
-    // Shared property of descriptor
-    Local<String> shared_key = v8_str(isolate, "shared");
-    v8::MaybeLocal<v8::Value> maybe_value =
-        descriptor->Get(context, shared_key);
-    v8::Local<v8::Value> value;
-    if (maybe_value.ToLocal(&value)) {
-      shared = value->BooleanValue(isolate) ? i::SharedFlag::kShared
-                                            : i::SharedFlag::kNotShared;
-    } else {
-      DCHECK(i_isolate->has_scheduled_exception());
-      return;
-    }
+  // Shared property of descriptor
+  Local<String> shared_key = v8_str(isolate, "shared");
+  v8::MaybeLocal<v8::Value> maybe_value = descriptor->Get(context, shared_key);
+  v8::Local<v8::Value> value;
+  if (maybe_value.ToLocal(&value)) {
+    shared = value->BooleanValue(isolate) ? i::SharedFlag::kShared
+                                          : i::SharedFlag::kNotShared;
+  } else {
+    DCHECK(i_isolate->has_scheduled_exception());
+    return;
+  }
 
-    // Throw TypeError if shared is true, and the descriptor has no "maximum"
-    if (shared == i::SharedFlag::kShared && maximum == -1) {
-      thrower.TypeError(
-          "If shared is true, maximum property should be defined.");
-      return;
-    }
+  // Throw TypeError if shared is true, and the descriptor has no "maximum"
+  if (shared == i::SharedFlag::kShared && maximum == -1) {
+    thrower.TypeError("If shared is true, maximum property should be defined.");
+    return;
   }
 
   i::Handle<i::JSObject> memory_obj;
@@ -1390,8 +1385,8 @@
              string->StringEquals(v8_str(isolate, "anyref"))) {
     *type = i::wasm::kWasmAnyRef;
   } else if (enabled_features.has_gc() &&
-             string->StringEquals(v8_str(isolate, "dataref"))) {
-    *type = i::wasm::kWasmDataRef;
+             string->StringEquals(v8_str(isolate, "structref"))) {
+    *type = i::wasm::kWasmStructRef;
   } else if (enabled_features.has_gc() &&
              string->StringEquals(v8_str(isolate, "arrayref"))) {
     *type = i::wasm::kWasmArrayRef;
@@ -1747,7 +1742,7 @@
           case i::wasm::HeapType::kAny:
           case i::wasm::HeapType::kEq:
           case i::wasm::HeapType::kI31:
-          case i::wasm::HeapType::kData:
+          case i::wasm::HeapType::kStruct:
           case i::wasm::HeapType::kArray:
           case i::wasm::HeapType::kString:
           case i::wasm::HeapType::kStringViewWtf8:
@@ -2263,19 +2258,10 @@
       return;
     case i::wasm::HeapType::kBottom:
       UNREACHABLE();
-    case i::wasm::HeapType::kData:
+    case i::wasm::HeapType::kStruct:
     case i::wasm::HeapType::kArray:
     case i::wasm::HeapType::kEq:
     case i::wasm::HeapType::kAny: {
-      if (!i::v8_flags.wasm_gc_js_interop && value->IsWasmObject()) {
-        // Transform wasm object into JS-compliant representation.
-        i::Handle<i::JSObject> wrapper =
-            isolate->factory()->NewJSObject(isolate->object_function());
-        i::JSObject::AddProperty(
-            isolate, wrapper, isolate->factory()->wasm_wrapped_object_symbol(),
-            value, i::NONE);
-        value = wrapper;
-      }
       return_value.Set(Utils::ToLocal(value));
       return;
     }
@@ -2287,17 +2273,6 @@
               i::Handle<i::WasmInternalFunction>::cast(value)->external(),
               isolate);
         }
-        return_value.Set(Utils::ToLocal(value));
-        return;
-      }
-      if (!i::v8_flags.wasm_gc_js_interop && value->IsWasmObject()) {
-        // Transform wasm object into JS-compliant representation.
-        i::Handle<i::JSObject> wrapper =
-            isolate->factory()->NewJSObject(isolate->object_function());
-        i::JSObject::AddProperty(
-            isolate, wrapper, isolate->factory()->wasm_wrapped_object_symbol(),
-            value, i::NONE);
-        value = wrapper;
       }
       return_value.Set(Utils::ToLocal(value));
       return;
@@ -2572,7 +2547,7 @@
           case i::wasm::HeapType::kAny:
           case i::wasm::HeapType::kEq:
           case i::wasm::HeapType::kI31:
-          case i::wasm::HeapType::kData:
+          case i::wasm::HeapType::kStruct:
           case i::wasm::HeapType::kArray:
           case i::wasm::HeapType::kString:
           case i::wasm::HeapType::kStringViewWtf8:
@@ -2636,7 +2611,7 @@
         case i::wasm::HeapType::kEq:
         case i::wasm::HeapType::kI31:
         case i::wasm::HeapType::kArray:
-        case i::wasm::HeapType::kData:
+        case i::wasm::HeapType::kStruct:
         case i::wasm::HeapType::kString:
         case i::wasm::HeapType::kStringViewWtf8:
         case i::wasm::HeapType::kStringViewWtf16:
@@ -3094,29 +3069,27 @@
   }
 
   // Setup Exception
-  if (enabled_features.has_eh()) {
-    Handle<JSFunction> tag_constructor =
-        InstallConstructorFunc(isolate, webassembly, "Tag", WebAssemblyTag);
-    Handle<JSObject> tag_proto =
-        SetupConstructor(isolate, tag_constructor, i::WASM_TAG_OBJECT_TYPE,
-                         WasmTagObject::kHeaderSize, "WebAssembly.Tag");
-    context->set_wasm_tag_constructor(*tag_constructor);
-
-    if (enabled_features.has_type_reflection()) {
-      InstallFunc(isolate, tag_proto, "type", WebAssemblyTagType, 0);
-    }
-    // Set up runtime exception constructor.
-    Handle<JSFunction> exception_constructor = InstallConstructorFunc(
-        isolate, webassembly, "Exception", WebAssemblyException);
-    SetDummyInstanceTemplate(isolate, exception_constructor);
-    Handle<JSObject> exception_proto = SetupConstructor(
-        isolate, exception_constructor, i::WASM_EXCEPTION_PACKAGE_TYPE,
-        WasmExceptionPackage::kHeaderSize, "WebAssembly.Exception");
-    InstallFunc(isolate, exception_proto, "getArg", WebAssemblyExceptionGetArg,
-                2);
-    InstallFunc(isolate, exception_proto, "is", WebAssemblyExceptionIs, 1);
-    context->set_wasm_exception_constructor(*exception_constructor);
+  Handle<JSFunction> tag_constructor =
+      InstallConstructorFunc(isolate, webassembly, "Tag", WebAssemblyTag);
+  Handle<JSObject> tag_proto =
+      SetupConstructor(isolate, tag_constructor, i::WASM_TAG_OBJECT_TYPE,
+                       WasmTagObject::kHeaderSize, "WebAssembly.Tag");
+  context->set_wasm_tag_constructor(*tag_constructor);
+
+  if (enabled_features.has_type_reflection()) {
+    InstallFunc(isolate, tag_proto, "type", WebAssemblyTagType, 0);
   }
+  // Set up runtime exception constructor.
+  Handle<JSFunction> exception_constructor = InstallConstructorFunc(
+      isolate, webassembly, "Exception", WebAssemblyException);
+  SetDummyInstanceTemplate(isolate, exception_constructor);
+  Handle<JSObject> exception_proto = SetupConstructor(
+      isolate, exception_constructor, i::WASM_EXCEPTION_PACKAGE_TYPE,
+      WasmExceptionPackage::kHeaderSize, "WebAssembly.Exception");
+  InstallFunc(isolate, exception_proto, "getArg", WebAssemblyExceptionGetArg,
+              2);
+  InstallFunc(isolate, exception_proto, "is", WebAssemblyExceptionIs, 1);
+  context->set_wasm_exception_constructor(*exception_constructor);
 
   // Setup Suspender.
   if (enabled_features.has_stack_switching()) {
@@ -3176,55 +3149,7 @@
 // static
 void WasmJs::InstallConditionalFeatures(Isolate* isolate,
                                         Handle<Context> context) {
-  // Exception handling may have been enabled by an origin trial. If so, make
-  // sure that the {WebAssembly.Tag} constructor is set up.
-  auto enabled_features = i::wasm::WasmFeatures::FromContext(isolate, context);
-  if (enabled_features.has_eh()) {
-    Handle<JSGlobalObject> global = handle(context->global_object(), isolate);
-    MaybeHandle<Object> maybe_webassembly =
-        JSObject::GetProperty(isolate, global, "WebAssembly");
-    Handle<Object> webassembly_obj;
-    if (!maybe_webassembly.ToHandle(&webassembly_obj) ||
-        !webassembly_obj->IsJSObject()) {
-      // There is no {WebAssembly} object, or it's not what we expect.
-      // Just return without adding the {Tag} constructor.
-      return;
-    }
-    Handle<JSObject> webassembly = Handle<JSObject>::cast(webassembly_obj);
-    // Setup Tag.
-    Handle<String> tag_name = v8_str(isolate, "Tag");
-    // The {WebAssembly} object may already have been modified. The following
-    // code is designed to:
-    //  - check for existing {Tag} properties on the object itself, and avoid
-    //    overwriting them or adding duplicate properties
-    //  - disregard any setters or read-only properties on the prototype chain
-    //  - only make objects accessible to user code after all internal setup
-    //    has been completed.
-    if (JSObject::HasOwnProperty(isolate, webassembly, tag_name)
-            .FromMaybe(true)) {
-      // Existing property, or exception.
-      return;
-    }
-
-    bool has_prototype = true;
-    Handle<JSFunction> tag_constructor =
-        CreateFunc(isolate, tag_name, WebAssemblyTag, has_prototype,
-                   SideEffectType::kHasNoSideEffect);
-    tag_constructor->shared().set_length(1);
-    context->set_wasm_tag_constructor(*tag_constructor);
-    Handle<JSObject> tag_proto =
-        SetupConstructor(isolate, tag_constructor, i::WASM_TAG_OBJECT_TYPE,
-                         WasmTagObject::kHeaderSize, "WebAssembly.Tag");
-    if (enabled_features.has_type_reflection()) {
-      InstallFunc(isolate, tag_proto, "type", WebAssemblyTagType, 0);
-    }
-    LookupIterator it(isolate, webassembly, tag_name, LookupIterator::OWN);
-    Maybe<bool> result = JSObject::DefineOwnPropertyIgnoreAttributes(
-        &it, tag_constructor, DONT_ENUM, Just(kDontThrow));
-    // This could still fail if the object was non-extensible, but now we
-    // return anyway so there's no need to even check.
-    USE(result);
-  }
+  // This space left blank for future origin trials.
 }
 #undef ASSIGN
 #undef EXTRACT_THIS
diff -r -u --color up/v8/src/wasm/wasm-module.h nw/v8/src/wasm/wasm-module.h
--- up/v8/src/wasm/wasm-module.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/wasm-module.h	2023-01-19 16:46:36.438942841 -0500
@@ -59,13 +59,13 @@
 
 // Static representation of a wasm function.
 struct WasmFunction {
-  const FunctionSig* sig;  // signature of the function.
-  uint32_t func_index;     // index into the function table.
-  uint32_t sig_index;      // index into the signature table.
-  WireBytesRef code;       // code of this function.
-  bool imported;
-  bool exported;
-  bool declared;
+  const FunctionSig* sig = nullptr;  // signature of the function.
+  uint32_t func_index = 0;           // index into the function table.
+  uint32_t sig_index = 0;            // index into the signature table.
+  WireBytesRef code = {};            // code of this function.
+  bool imported = false;
+  bool exported = false;
+  bool declared = false;
 };
 
 // Static representation of a wasm global variable.
@@ -494,9 +494,11 @@
   // mutable.
   uint32_t untagged_globals_buffer_size = 0;
   uint32_t tagged_globals_buffer_size = 0;
+  uint32_t num_imported_globals = 0;
   uint32_t num_imported_mutable_globals = 0;
   uint32_t num_imported_functions = 0;
   uint32_t num_imported_tables = 0;
+  uint32_t num_imported_tags = 0;
   uint32_t num_declared_functions = 0;  // excluding imported
   uint32_t num_exported_functions = 0;
   uint32_t num_declared_data_segments = 0;  // From the DataCount section.
@@ -508,6 +510,8 @@
   // ID and length).
   WireBytesRef name_section = {0, 0};
 
+  AccountingAllocator* allocator() const { return signature_zone->allocator(); }
+
   void add_type(TypeDefinition type) {
     types.push_back(type);
     // Isorecursive canonical type will be computed later.
@@ -568,6 +572,36 @@
                              isorecursive_canonical_type_ids.end());
   }
 
+  bool function_was_validated(int func_index) const {
+    DCHECK_NOT_NULL(validated_functions);
+    static_assert(sizeof(validated_functions[0]) == 1);
+    DCHECK_LE(num_imported_functions, func_index);
+    int pos = func_index - num_imported_functions;
+    DCHECK_LE(pos, num_declared_functions);
+    uint8_t byte =
+        validated_functions[pos >> 3].load(std::memory_order_relaxed);
+    return byte & (1 << (pos & 7));
+  }
+
+  void set_function_validated(int func_index) const {
+    DCHECK_NOT_NULL(validated_functions);
+    DCHECK_LE(num_imported_functions, func_index);
+    int pos = func_index - num_imported_functions;
+    DCHECK_LE(pos, num_declared_functions);
+    std::atomic<uint8_t>* atomic_byte = &validated_functions[pos >> 3];
+    uint8_t old_byte = atomic_byte->load(std::memory_order_relaxed);
+    uint8_t new_bit = 1 << (pos & 7);
+    while ((old_byte & new_bit) == 0 &&
+           !atomic_byte->compare_exchange_weak(old_byte, old_byte | new_bit,
+                                               std::memory_order_relaxed)) {
+      // Retry with updated {old_byte}.
+    }
+  }
+
+  base::Vector<const WasmFunction> declared_functions() const {
+    return base::VectorOf(functions) + num_imported_functions;
+  }
+
   std::vector<TypeDefinition> types;  // by type index
   // Maps each type index to its global (cross-module) canonical index as per
   // isorecursive type canonicalization.
@@ -595,6 +629,12 @@
   // from asm.js.
   std::unique_ptr<AsmJsOffsetInformation> asm_js_offset_information;
 
+  // {validated_functions} is atomically updated when functions get validated
+  // (during compilation, streaming decoding, or via explicit validation).
+  static_assert(sizeof(std::atomic<uint8_t>) == 1);
+  static_assert(alignof(std::atomic<uint8_t>) == 1);
+  mutable std::unique_ptr<std::atomic<uint8_t>[]> validated_functions;
+
   explicit WasmModule(std::unique_ptr<Zone> signature_zone = nullptr);
   WasmModule(const WasmModule&) = delete;
   WasmModule& operator=(const WasmModule&) = delete;
diff -r -u --color up/v8/src/wasm/wasm-objects-inl.h nw/v8/src/wasm/wasm-objects-inl.h
--- up/v8/src/wasm/wasm-objects-inl.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/wasm-objects-inl.h	2023-01-19 16:46:36.438942841 -0500
@@ -238,8 +238,8 @@
 OPTIONAL_ACCESSORS(WasmInstanceObject, indirect_function_table_refs, FixedArray,
                    kIndirectFunctionTableRefsOffset)
 OPTIONAL_ACCESSORS(WasmInstanceObject, tags_table, FixedArray, kTagsTableOffset)
-OPTIONAL_ACCESSORS(WasmInstanceObject, wasm_internal_functions, FixedArray,
-                   kWasmInternalFunctionsOffset)
+ACCESSORS(WasmInstanceObject, wasm_internal_functions, FixedArray,
+          kWasmInternalFunctionsOffset)
 ACCESSORS(WasmInstanceObject, managed_object_maps, FixedArray,
           kManagedObjectMapsOffset)
 ACCESSORS(WasmInstanceObject, feedback_vectors, FixedArray,
diff -r -u --color up/v8/src/wasm/wasm-objects.cc nw/v8/src/wasm/wasm-objects.cc
--- up/v8/src/wasm/wasm-objects.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/wasm-objects.cc	2023-01-19 16:46:36.438942841 -0500
@@ -267,8 +267,6 @@
     const char** error_message) {
   // Any `entry` has to be in its JS representation.
   DCHECK(!entry->IsWasmInternalFunction());
-  DCHECK_IMPLIES(!v8_flags.wasm_gc_js_interop,
-                 !entry->IsWasmArray() && !entry->IsWasmStruct());
   const WasmModule* module =
       !table->instance().IsUndefined()
           ? WasmInstanceObject::cast(table->instance()).module()
@@ -325,7 +323,7 @@
     case wasm::HeapType::kStringViewWtf16:
     case wasm::HeapType::kStringViewIter:
     case wasm::HeapType::kEq:
-    case wasm::HeapType::kData:
+    case wasm::HeapType::kStruct:
     case wasm::HeapType::kArray:
     case wasm::HeapType::kAny:
     case wasm::HeapType::kI31:
@@ -373,7 +371,7 @@
     case wasm::HeapType::kString:
     case wasm::HeapType::kEq:
     case wasm::HeapType::kI31:
-    case wasm::HeapType::kData:
+    case wasm::HeapType::kStruct:
     case wasm::HeapType::kArray:
     case wasm::HeapType::kAny:
       return entry;
@@ -917,7 +915,7 @@
   // Check if the non-shared memory could grow in-place.
   if (result_inplace.has_value()) {
     // Detach old and create a new one with the grown backing store.
-    old_buffer->Detach(true);
+    JSArrayBuffer::Detach(old_buffer, true).Check();
     Handle<JSArrayBuffer> new_buffer =
         isolate->factory()->NewJSArrayBuffer(std::move(backing_store));
     memory_object->update_instances(isolate, new_buffer);
@@ -957,7 +955,7 @@
   }
 
   // Detach old and create a new one with the new backing store.
-  old_buffer->Detach(true);
+  JSArrayBuffer::Detach(old_buffer, true).Check();
   Handle<JSArrayBuffer> new_buffer =
       isolate->factory()->NewJSArrayBuffer(std::move(new_backing_store));
   memory_object->update_instances(isolate, new_buffer);
@@ -1182,6 +1180,11 @@
   instance->set_hook_on_function_call_address(
       isolate->debug()->hook_on_function_call_address());
   instance->set_managed_object_maps(*isolate->factory()->empty_fixed_array());
+  // TODO(manoskouk): Initialize this array with zeroes, and check for zero in
+  // wasm-compiler.
+  Handle<FixedArray> functions = isolate->factory()->NewFixedArray(
+      static_cast<int>(module->functions.size()));
+  instance->set_wasm_internal_functions(*functions);
   instance->set_feedback_vectors(*isolate->factory()->empty_fixed_array());
   instance->set_tiering_budget_array(
       module_object->native_module()->tiering_budget_array());
@@ -1325,15 +1328,10 @@
 
 MaybeHandle<WasmInternalFunction> WasmInstanceObject::GetWasmInternalFunction(
     Isolate* isolate, Handle<WasmInstanceObject> instance, int index) {
-  MaybeHandle<WasmInternalFunction> result;
-  if (instance->has_wasm_internal_functions()) {
-    Object val = instance->wasm_internal_functions().get(index);
-    if (!val.IsUndefined(isolate)) {
-      result = Handle<WasmInternalFunction>(WasmInternalFunction::cast(val),
-                                            isolate);
-    }
-  }
-  return result;
+  Object val = instance->wasm_internal_functions().get(index);
+  return val.IsWasmInternalFunction()
+             ? handle(WasmInternalFunction::cast(val), isolate)
+             : MaybeHandle<WasmInternalFunction>();
 }
 
 Handle<WasmInternalFunction>
@@ -1342,10 +1340,8 @@
   MaybeHandle<WasmInternalFunction> maybe_result =
       WasmInstanceObject::GetWasmInternalFunction(isolate, instance,
                                                   function_index);
-
-  Handle<WasmInternalFunction> result;
-  if (maybe_result.ToHandle(&result)) {
-    return result;
+  if (!maybe_result.is_null()) {
+    return maybe_result.ToHandleChecked();
   }
 
   Handle<WasmModuleObject> module_object(instance->module_object(), isolate);
@@ -1378,28 +1374,17 @@
   auto external = Handle<WasmExternalFunction>::cast(WasmExportedFunction::New(
       isolate, instance, function_index,
       static_cast<int>(function.sig->parameter_count()), wrapper));
-  result =
+  Handle<WasmInternalFunction> result =
       WasmInternalFunction::FromExternal(external, isolate).ToHandleChecked();
 
-  WasmInstanceObject::SetWasmInternalFunction(isolate, instance, function_index,
-                                              result);
+  WasmInstanceObject::SetWasmInternalFunction(instance, function_index, result);
   return result;
 }
 
 void WasmInstanceObject::SetWasmInternalFunction(
-    Isolate* isolate, Handle<WasmInstanceObject> instance, int index,
+    Handle<WasmInstanceObject> instance, int index,
     Handle<WasmInternalFunction> val) {
-  Handle<FixedArray> functions;
-  if (!instance->has_wasm_internal_functions()) {
-    // Lazily allocate the wasm external functions array.
-    functions = isolate->factory()->NewFixedArray(
-        static_cast<int>(instance->module()->functions.size()));
-    instance->set_wasm_internal_functions(*functions);
-  } else {
-    functions =
-        Handle<FixedArray>(instance->wasm_internal_functions(), isolate);
-  }
-  functions->set(index, *val);
+  instance->wasm_internal_functions().set(index, *val);
 }
 
 // static
@@ -2214,24 +2199,6 @@
   return result;
 }
 
-namespace {
-// If {in_out_value} is a wrapped wasm struct/array, it gets unwrapped in-place
-// and this returns {true}. Otherwise, the value remains unchanged and this
-// returns {false}.
-bool TryUnpackObjectWrapper(Isolate* isolate, Handle<Object>& in_out_value) {
-  if (in_out_value->IsUndefined(isolate) || in_out_value->IsNull(isolate) ||
-      !in_out_value->IsJSObject()) {
-    return false;
-  }
-  Handle<Name> key = isolate->factory()->wasm_wrapped_object_symbol();
-  LookupIterator it(isolate, in_out_value, key,
-                    LookupIterator::OWN_SKIP_INTERCEPTOR);
-  if (it.state() != LookupIterator::DATA) return false;
-  in_out_value = it.GetDataValue();
-  return true;
-}
-}  // namespace
-
 namespace wasm {
 MaybeHandle<Object> JSToWasmObject(Isolate* isolate, const WasmModule* module,
                                    Handle<Object> value, ValueType expected,
@@ -2257,8 +2224,6 @@
       }
       V8_FALLTHROUGH;
     case kRef: {
-      // TODO(7748): Follow any changes in proposed JS API. In particular,
-      //             finalize the v8_flags.wasm_gc_js_interop situation.
       // TODO(7748): Allow all in-range numbers for i31. Make sure to convert
       //             Smis to i31refs if needed.
       // TODO(7748): Streamline interaction of undefined and (ref any).
@@ -2284,28 +2249,21 @@
           return {};
         }
         case HeapType::kAny: {
-          if (!v8_flags.wasm_gc_js_interop) {
-            TryUnpackObjectWrapper(isolate, value);
-          }
           if (!value->IsNull(isolate)) return value;
           *error_message = "null is not allowed for (ref any)";
           return {};
         }
-        case HeapType::kData: {
-          if (v8_flags.wasm_gc_js_interop
-                  ? value->IsWasmStruct() || value->IsWasmArray()
-                  : TryUnpackObjectWrapper(isolate, value)) {
+        case HeapType::kStruct: {
+          if (value->IsWasmStruct() ||
+              (value->IsWasmArray() && v8_flags.wasm_gc_structref_as_dataref)) {
             return value;
           }
           *error_message =
-              "dataref object must be null (if nullable) or a wasm "
-              "struct/array";
+              "structref object must be null (if nullable) or a wasm struct";
           return {};
         }
         case HeapType::kArray: {
-          if ((v8_flags.wasm_gc_js_interop ||
-               TryUnpackObjectWrapper(isolate, value)) &&
-              value->IsWasmArray()) {
+          if (value->IsWasmArray()) {
             return value;
           }
           *error_message =
@@ -2313,10 +2271,7 @@
           return {};
         }
         case HeapType::kEq: {
-          if (value->IsSmi() ||
-              (v8_flags.wasm_gc_js_interop
-                   ? value->IsWasmStruct() || value->IsWasmArray()
-                   : TryUnpackObjectWrapper(isolate, value))) {
+          if (value->IsSmi() || value->IsWasmStruct() || value->IsWasmArray()) {
             return value;
           }
           *error_message =
@@ -2403,9 +2358,7 @@
             // A struct or array type with index is expected.
             DCHECK(module->has_struct(expected.ref_index()) ||
                    module->has_array(expected.ref_index()));
-            if (v8_flags.wasm_gc_js_interop
-                    ? !value->IsWasmStruct() && !value->IsWasmArray()
-                    : !TryUnpackObjectWrapper(isolate, value)) {
+            if (!value->IsWasmStruct() && !value->IsWasmArray()) {
               *error_message = "object incompatible with wasm type";
               return {};
             }
diff -r -u --color up/v8/src/wasm/wasm-objects.h nw/v8/src/wasm/wasm-objects.h
--- up/v8/src/wasm/wasm-objects.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/wasm-objects.h	2023-01-19 16:46:36.438942841 -0500
@@ -342,7 +342,7 @@
   DECL_ACCESSORS(imported_function_targets, FixedAddressArray)
   DECL_OPTIONAL_ACCESSORS(indirect_function_table_refs, FixedArray)
   DECL_OPTIONAL_ACCESSORS(tags_table, FixedArray)
-  DECL_OPTIONAL_ACCESSORS(wasm_internal_functions, FixedArray)
+  DECL_ACCESSORS(wasm_internal_functions, FixedArray)
   DECL_ACCESSORS(managed_object_maps, FixedArray)
   DECL_ACCESSORS(feedback_vectors, FixedArray)
   DECL_SANDBOXED_POINTER_ACCESSORS(memory_start, byte*)
@@ -510,8 +510,7 @@
       Isolate* isolate, Handle<WasmInstanceObject> instance,
       int function_index);
 
-  static void SetWasmInternalFunction(Isolate* isolate,
-                                      Handle<WasmInstanceObject> instance,
+  static void SetWasmInternalFunction(Handle<WasmInstanceObject> instance,
                                       int index,
                                       Handle<WasmInternalFunction> val);
 
diff -r -u --color up/v8/src/wasm/wasm-opcodes.h nw/v8/src/wasm/wasm-opcodes.h
--- up/v8/src/wasm/wasm-opcodes.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/wasm-opcodes.h	2023-01-19 16:46:36.438942841 -0500
@@ -60,9 +60,9 @@
   V(CallIndirect, 0x11, _, "call_indirect")                                  \
   V(ReturnCall, 0x12, _, "return_call")                                      \
   V(ReturnCallIndirect, 0x13, _, "return_call_indirect")                     \
-  V(CallRefDeprecated, 0x14, _, "call_ref")    /* typed_funcref prototype */ \
+  V(CallRef, 0x14, _, "call_ref")              /* typed_funcref prototype */ \
   V(ReturnCallRef, 0x15, _, "return_call_ref") /* typed_funcref prototype */ \
-  V(CallRef, 0x17, _, "call_ref")              /* temporary, for compat.*/   \
+  V(CallRefDeprecated, 0x17, _, "call_ref")    /* temporary, for compat.*/   \
   V(Drop, 0x1a, _, "drop")                                                   \
   V(Select, 0x1b, _, "select")                                               \
   V(SelectWithType, 0x1c, _, "select")                                       \
@@ -710,20 +710,22 @@
   V(RefTest, 0xfb40, _, "ref.test")                                            \
   V(RefTestNull, 0xfb48, _, "ref.test null")                                   \
   V(RefTestDeprecated, 0xfb44, _, "ref.test")                                  \
-  V(RefCast, 0xfb45, _, "ref.cast")                                            \
+  V(RefCast, 0xfb41, _, "ref.cast")                                            \
+  V(RefCastNull, 0xfb49, _, "ref.cast null")                                   \
+  V(RefCastDeprecated, 0xfb45, _, "ref.cast")                                  \
   V(BrOnCast, 0xfb46, _, "br_on_cast")                                         \
   V(BrOnCastFail, 0xfb47, _, "br_on_cast_fail")                                \
   V(RefCastNop, 0xfb4c, _, "ref.cast_nop")                                     \
-  V(RefIsData, 0xfb51, _, "ref.is_data")                                       \
+  V(RefIsStruct, 0xfb51, _, "ref.is_struct")                                   \
   V(RefIsI31, 0xfb52, _, "ref.is_i31")                                         \
   V(RefIsArray, 0xfb53, _, "ref.is_array")                                     \
-  V(RefAsData, 0xfb59, _, "ref.as_data")                                       \
+  V(RefAsStruct, 0xfb59, _, "ref.as_struct")                                   \
   V(RefAsI31, 0xfb5a, _, "ref.as_i31")                                         \
   V(RefAsArray, 0xfb5b, _, "ref.as_array")                                     \
-  V(BrOnData, 0xfb61, _, "br_on_data")                                         \
+  V(BrOnStruct, 0xfb61, _, "br_on_struct")                                     \
   V(BrOnI31, 0xfb62, _, "br_on_i31")                                           \
   V(BrOnArray, 0xfb66, _, "br_on_array")                                       \
-  V(BrOnNonData, 0xfb64, _, "br_on_non_data")                                  \
+  V(BrOnNonStruct, 0xfb64, _, "br_on_non_struct")                              \
   V(BrOnNonI31, 0xfb65, _, "br_on_non_i31")                                    \
   V(BrOnNonArray, 0xfb67, _, "br_on_non_array")                                \
   V(ExternInternalize, 0xfb70, _, "extern.internalize")                        \
diff -r -u --color up/v8/src/wasm/wasm-subtyping.cc nw/v8/src/wasm/wasm-subtyping.cc
--- up/v8/src/wasm/wasm-subtyping.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/wasm/wasm-subtyping.cc	2023-01-19 16:46:36.438942841 -0500
@@ -4,6 +4,7 @@
 
 #include "src/wasm/wasm-subtyping.h"
 
+#include "src/base/v8-fallthrough.h"
 #include "src/wasm/canonical-types.h"
 #include "src/wasm/wasm-module.h"
 
@@ -104,7 +105,7 @@
     case HeapType::kI31:
     case HeapType::kNone:
     case HeapType::kEq:
-    case HeapType::kData:
+    case HeapType::kStruct:
     case HeapType::kArray:
     case HeapType::kAny:
     case HeapType::kString:
@@ -211,12 +212,17 @@
     case HeapType::kExtern:
       return super_heap == HeapType::kExtern;
     case HeapType::kI31:
-    case HeapType::kData:
+    case HeapType::kStruct:
+    case HeapType::kArray:
+      if (v8_flags.wasm_gc_structref_as_dataref &&
+          sub_heap.representation() == HeapType::kArray) {
+        // TODO(7748): Remove temporary workaround for backwards compatibility.
+        return super_heap == HeapType::kArray ||
+               super_heap == HeapType::kStruct || super_heap == HeapType::kEq ||
+               super_heap == HeapType::kAny;
+      }
       return super_heap == sub_heap || super_heap == HeapType::kEq ||
              super_heap == HeapType::kAny;
-    case HeapType::kArray:
-      return super_heap == HeapType::kArray || super_heap == HeapType::kData ||
-             super_heap == HeapType::kEq || super_heap == HeapType::kAny;
     case HeapType::kString:
       // stringref is a subtype of anyref under wasm-gc.
       return sub_heap == super_heap ||
@@ -256,8 +262,12 @@
   switch (super_heap.representation()) {
     case HeapType::kFunc:
       return sub_module->has_signature(sub_index);
+    case HeapType::kStruct:
+      if (!v8_flags.wasm_gc_structref_as_dataref) {
+        return sub_module->has_struct(sub_index);
+      }
+      V8_FALLTHROUGH;
     case HeapType::kEq:
-    case HeapType::kData:
     case HeapType::kAny:
       return !sub_module->has_signature(sub_index);
     case HeapType::kArray:
@@ -345,14 +355,25 @@
       DCHECK_EQ(kind2, kind1);
       return HeapType::kFunc;
     case TypeDefinition::kStruct:
-      DCHECK_NE(kind2, TypeDefinition::kFunction);
-      return HeapType::kData;
+      if (v8_flags.wasm_gc_structref_as_dataref) {
+        DCHECK_NE(kind2, TypeDefinition::kFunction);
+        return HeapType::kStruct;
+      }
+      switch (kind2) {
+        case TypeDefinition::kFunction:
+          UNREACHABLE();
+        case TypeDefinition::kStruct:
+          return HeapType::kStruct;
+        case TypeDefinition::kArray:
+          return HeapType::kEq;
+      }
     case TypeDefinition::kArray:
       switch (kind2) {
         case TypeDefinition::kFunction:
           UNREACHABLE();
         case TypeDefinition::kStruct:
-          return HeapType::kData;
+          return v8_flags.wasm_gc_structref_as_dataref ? HeapType::kStruct
+                                                       : HeapType::kEq;
         case TypeDefinition::kArray:
           return HeapType::kArray;
       }
@@ -361,6 +382,9 @@
 
 // Returns the least common ancestor of a generic HeapType {heap1}, and
 // another HeapType {heap2}.
+// TODO(7748): This function sometimes assumes that incompatible types cannot be
+// compared, in some cases explicitly and in others implicitly. Make it
+// consistent.
 HeapType::Representation CommonAncestorWithGeneric(HeapType heap1,
                                                    HeapType heap2,
                                                    const WasmModule* module2) {
@@ -380,7 +404,7 @@
         case HeapType::kNone:
           return HeapType::kI31;
         case HeapType::kEq:
-        case HeapType::kData:
+        case HeapType::kStruct:
         case HeapType::kArray:
           return HeapType::kEq;
         case HeapType::kAny:
@@ -394,12 +418,14 @@
           return module2->has_signature(heap2.ref_index()) ? HeapType::kBottom
                                                            : HeapType::kEq;
       }
-    case HeapType::kData:
+    case HeapType::kStruct:
       switch (heap2.representation()) {
-        case HeapType::kData:
-        case HeapType::kArray:
+        case HeapType::kStruct:
         case HeapType::kNone:
-          return HeapType::kData;
+          return HeapType::kStruct;
+        case HeapType::kArray:
+          return v8_flags.wasm_gc_structref_as_dataref ? HeapType::kStruct
+                                                       : HeapType::kEq;
         case HeapType::kI31:
         case HeapType::kEq:
           return HeapType::kEq;
@@ -412,15 +438,18 @@
           UNREACHABLE();
         default:
           return module2->has_signature(heap2.ref_index()) ? HeapType::kBottom
-                                                           : HeapType::kData;
+                 : module2->has_struct(heap2.ref_index())  ? HeapType::kStruct
+                 : v8_flags.wasm_gc_structref_as_dataref   ? HeapType::kStruct
+                                                           : HeapType::kEq;
       }
     case HeapType::kArray:
       switch (heap2.representation()) {
         case HeapType::kArray:
         case HeapType::kNone:
           return HeapType::kArray;
-        case HeapType::kData:
-          return HeapType::kData;
+        case HeapType::kStruct:
+          return v8_flags.wasm_gc_structref_as_dataref ? HeapType::kStruct
+                                                       : HeapType::kEq;
         case HeapType::kI31:
         case HeapType::kEq:
           return HeapType::kEq;
@@ -432,9 +461,12 @@
         case HeapType::kNoFunc:
           UNREACHABLE();
         default:
-          return module2->has_array(heap2.ref_index())    ? HeapType::kArray
-                 : module2->has_struct(heap2.ref_index()) ? HeapType::kData
-                                                          : HeapType::kBottom;
+          return module2->has_array(heap2.ref_index()) ? HeapType::kArray
+                 : module2->has_struct(heap2.ref_index())
+                     ? (v8_flags.wasm_gc_structref_as_dataref
+                            ? HeapType::kStruct
+                            : HeapType::kEq)
+                     : HeapType::kBottom;
       }
     case HeapType::kAny:
       return HeapType::kAny;
@@ -446,7 +478,7 @@
       switch (heap2.representation()) {
         case HeapType::kArray:
         case HeapType::kNone:
-        case HeapType::kData:
+        case HeapType::kStruct:
         case HeapType::kI31:
         case HeapType::kEq:
         case HeapType::kAny:
diff -r -u --color up/v8/src/web-snapshot/web-snapshot.cc nw/v8/src/web-snapshot/web-snapshot.cc
--- up/v8/src/web-snapshot/web-snapshot.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/web-snapshot/web-snapshot.cc	2023-01-19 16:46:36.438942841 -0500
@@ -3565,7 +3565,7 @@
     }
     Handle<JSArrayBuffer> array_buffer = Handle<JSArrayBuffer>::cast(
         isolate_->factory()->NewJSObjectFromMap(map, AllocationType::kYoung));
-    array_buffer->Setup(shared, resizable, nullptr);
+    array_buffer->Setup(shared, resizable, nullptr, isolate_);
 
     std::unique_ptr<BackingStore> backing_store;
     if (was_detached) {
diff -r -u --color up/v8/src/zone/accounting-allocator.cc nw/v8/src/zone/accounting-allocator.cc
--- up/v8/src/zone/accounting-allocator.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/zone/accounting-allocator.cc	2023-01-19 16:46:36.438942841 -0500
@@ -91,7 +91,9 @@
                            kZonePageSize, PageAllocator::kReadWrite);
 
   } else {
-    memory = AllocWithRetry(bytes, zone_backing_malloc_);
+    auto result = AllocAtLeastWithRetry(bytes);
+    memory = result.ptr;
+    bytes = result.count;
   }
   if (memory == nullptr) return nullptr;
 
@@ -115,7 +117,7 @@
   if (COMPRESS_ZONES_BOOL && supports_compression) {
     FreePages(bounded_page_allocator_.get(), segment, segment_size);
   } else {
-    zone_backing_free_(segment);
+    free(segment);
   }
 }
 
diff -r -u --color up/v8/src/zone/zone-handle-set.h nw/v8/src/zone/zone-handle-set.h
--- up/v8/src/zone/zone-handle-set.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/src/zone/zone-handle-set.h	2023-01-19 16:46:36.438942841 -0500
@@ -81,6 +81,12 @@
     }
   }
 
+  void Union(ZoneHandleSet<T> const& other, Zone* zone) {
+    for (size_t i = 0; i < other.size(); ++i) {
+      insert(other.at(i), zone);
+    }
+  }
+
   bool contains(ZoneHandleSet<T> const& other) const {
     if (data_ == other.data_) return true;
     if (data_ == kEmptyTag) return false;
diff -r -u --color up/v8/test/cctest/BUILD.gn nw/v8/test/cctest/BUILD.gn
--- up/v8/test/cctest/BUILD.gn	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/BUILD.gn	2023-01-19 16:46:36.438942841 -0500
@@ -144,6 +144,7 @@
     "heap/test-unmapper.cc",
     "heap/test-weak-references.cc",
     "heap/test-write-barrier.cc",
+    "jsonstream-helper.h",
     "manually-externalized-buffer.h",
     "print-extension.cc",
     "print-extension.h",
diff -r -u --color up/v8/test/cctest/cctest.cc nw/v8/test/cctest/cctest.cc
--- up/v8/test/cctest/cctest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/cctest.cc	2023-01-19 16:46:36.438942841 -0500
@@ -197,30 +197,40 @@
   env->Global()->Set(env, v8_str(name), func).FromJust();
 }
 
-void CcTest::CollectGarbage(i::AllocationSpace space, i::Isolate* isolate) {
+void CcTest::CollectGarbage(i::AllocationSpace space, i::Isolate* isolate,
+                            i::Heap::ScanStackMode mode) {
   i::Isolate* iso = isolate ? isolate : i_isolate();
+  i::ScanStackModeScopeForTesting scope(iso->heap(), mode);
   iso->heap()->CollectGarbage(space, i::GarbageCollectionReason::kTesting);
 }
 
-void CcTest::CollectAllGarbage(i::Isolate* isolate) {
+void CcTest::CollectAllGarbage(i::Isolate* isolate,
+                               i::Heap::ScanStackMode mode) {
   i::Isolate* iso = isolate ? isolate : i_isolate();
+  i::ScanStackModeScopeForTesting scope(iso->heap(), mode);
   iso->heap()->CollectAllGarbage(i::Heap::kNoGCFlags,
                                  i::GarbageCollectionReason::kTesting);
 }
 
-void CcTest::CollectAllAvailableGarbage(i::Isolate* isolate) {
+void CcTest::CollectAllAvailableGarbage(i::Isolate* isolate,
+                                        i::Heap::ScanStackMode mode) {
   i::Isolate* iso = isolate ? isolate : i_isolate();
+  i::ScanStackModeScopeForTesting scope(iso->heap(), mode);
   iso->heap()->CollectAllAvailableGarbage(i::GarbageCollectionReason::kTesting);
 }
 
-void CcTest::PreciseCollectAllGarbage(i::Isolate* isolate) {
+void CcTest::PreciseCollectAllGarbage(i::Isolate* isolate,
+                                      i::Heap::ScanStackMode mode) {
   i::Isolate* iso = isolate ? isolate : i_isolate();
+  i::ScanStackModeScopeForTesting scope(iso->heap(), mode);
   iso->heap()->PreciseCollectAllGarbage(i::Heap::kNoGCFlags,
                                         i::GarbageCollectionReason::kTesting);
 }
 
-void CcTest::CollectSharedGarbage(i::Isolate* isolate) {
+void CcTest::CollectSharedGarbage(i::Isolate* isolate,
+                                  i::Heap::ScanStackMode mode) {
   i::Isolate* iso = isolate ? isolate : i_isolate();
+  i::ScanStackModeScopeForTesting scope(iso->heap(), mode);
   iso->heap()->CollectGarbageShared(iso->main_thread_local_heap(),
                                     i::GarbageCollectionReason::kTesting);
 }
@@ -440,8 +450,6 @@
       flag_concurrent_sweeping_(i::v8_flags.concurrent_sweeping),
       flag_concurrent_minor_mc_marking_(
           i::v8_flags.concurrent_minor_mc_marking),
-      flag_concurrent_minor_mc_sweeping_(
-          i::v8_flags.concurrent_minor_mc_sweeping),
       flag_stress_concurrent_allocation_(
           i::v8_flags.stress_concurrent_allocation),
       flag_stress_incremental_marking_(i::v8_flags.stress_incremental_marking),
@@ -458,7 +466,6 @@
   i::v8_flags.concurrent_marking = false;
   i::v8_flags.concurrent_sweeping = false;
   i::v8_flags.concurrent_minor_mc_marking = false;
-  i::v8_flags.concurrent_minor_mc_sweeping = false;
   i::v8_flags.stress_incremental_marking = false;
   i::v8_flags.stress_concurrent_allocation = false;
   // Parallel marking has a dependency on concurrent marking.
@@ -470,7 +477,6 @@
   i::v8_flags.concurrent_marking = flag_concurrent_marking_;
   i::v8_flags.concurrent_sweeping = flag_concurrent_sweeping_;
   i::v8_flags.concurrent_minor_mc_marking = flag_concurrent_minor_mc_marking_;
-  i::v8_flags.concurrent_minor_mc_sweeping = flag_concurrent_minor_mc_sweeping_;
   i::v8_flags.stress_concurrent_allocation = flag_stress_concurrent_allocation_;
   i::v8_flags.stress_incremental_marking = flag_stress_incremental_marking_;
   i::v8_flags.parallel_marking = flag_parallel_marking_;
diff -r -u --color up/v8/test/cctest/cctest.h nw/v8/test/cctest/cctest.h
--- up/v8/test/cctest/cctest.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/cctest.h	2023-01-19 16:46:36.449776171 -0500
@@ -170,12 +170,22 @@
 
   static void AddGlobalFunction(v8::Local<v8::Context> env, const char* name,
                                 v8::FunctionCallback callback);
-  static void CollectGarbage(i::AllocationSpace space,
-                             i::Isolate* isolate = nullptr);
-  static void CollectAllGarbage(i::Isolate* isolate = nullptr);
-  static void CollectAllAvailableGarbage(i::Isolate* isolate = nullptr);
-  static void PreciseCollectAllGarbage(i::Isolate* isolate = nullptr);
-  static void CollectSharedGarbage(i::Isolate* isolate = nullptr);
+  // By default, the GC methods do not scan the stack conservatively.
+  static void CollectGarbage(
+      i::AllocationSpace space, i::Isolate* isolate = nullptr,
+      i::Heap::ScanStackMode mode = i::Heap::ScanStackMode::kNone);
+  static void CollectAllGarbage(
+      i::Isolate* isolate = nullptr,
+      i::Heap::ScanStackMode mode = i::Heap::ScanStackMode::kNone);
+  static void CollectAllAvailableGarbage(
+      i::Isolate* isolate = nullptr,
+      i::Heap::ScanStackMode mode = i::Heap::ScanStackMode::kNone);
+  static void PreciseCollectAllGarbage(
+      i::Isolate* isolate = nullptr,
+      i::Heap::ScanStackMode mode = i::Heap::ScanStackMode::kNone);
+  static void CollectSharedGarbage(
+      i::Isolate* isolate = nullptr,
+      i::Heap::ScanStackMode mode = i::Heap::ScanStackMode::kNone);
 
   static i::Handle<i::String> MakeString(const char* str);
   static i::Handle<i::String> MakeName(const char* str, int suffix);
@@ -706,7 +716,6 @@
   const bool flag_concurrent_marking_;
   const bool flag_concurrent_sweeping_;
   const bool flag_concurrent_minor_mc_marking_;
-  const bool flag_concurrent_minor_mc_sweeping_;
   const bool flag_stress_concurrent_allocation_;
   const bool flag_stress_incremental_marking_;
   const bool flag_parallel_marking_;
diff -r -u --color up/v8/test/cctest/cctest.status nw/v8/test/cctest/cctest.status
--- up/v8/test/cctest/cctest.status	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/cctest.status	2023-01-19 16:46:36.449776171 -0500
@@ -513,6 +513,7 @@
   'test-api/WasmI32AtomicWaitCallback': [SKIP],
   'test-api/WasmI64AtomicWaitCallback': [SKIP],
   'test-api/WasmSetJitCodeEventHandler': [SKIP],
+  'test-api-array-buffer/ArrayBuffer_NonDetachableWasDetached': [SKIP],
   'test-backing-store/Run_WasmModule_Buffer_Externalized_Regression_UseAfterFree': [SKIP],
   'test-c-wasm-entry/*': [SKIP],
   'test-compilation-cache/*': [SKIP],
@@ -1095,6 +1096,7 @@
   'test-run-wasm-relaxed-simd/RunWasm_F32x4Qfms_liftoff': [SKIP],
   'test-run-wasm-relaxed-simd/RunWasm_F64x2Qfma_liftoff': [SKIP],
   'test-run-wasm-relaxed-simd/RunWasm_F64x2Qfms_liftoff': [SKIP],
+  'test-run-wasm-relaxed-simd/RunWasm_RegressFmaReg_liftoff': [SKIP],
 }],
 
 ]
diff -r -u --color up/v8/test/cctest/compiler/test-representation-change.cc nw/v8/test/cctest/compiler/test-representation-change.cc
--- up/v8/test/cctest/compiler/test-representation-change.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/compiler/test-representation-change.cc	2023-01-19 16:46:36.449776171 -0500
@@ -29,6 +29,7 @@
         jsgraph_(main_isolate(), main_graph_, &main_common_, &javascript_,
                  &main_simplified_, &main_machine_),
         broker_(main_isolate(), main_zone()),
+        canonical_(main_isolate()),
         changer_(&jsgraph_, &broker_, nullptr) {
     Node* s = graph()->NewNode(common()->Start(num_parameters));
     graph()->SetStart(s);
@@ -37,6 +38,7 @@
   JSOperatorBuilder javascript_;
   JSGraph jsgraph_;
   JSHeapBroker broker_;
+  CanonicalHandleScope canonical_;
   RepresentationChanger changer_;
 
   Isolate* isolate() { return main_isolate(); }
diff -r -u --color up/v8/test/cctest/heap/heap-utils.cc nw/v8/test/cctest/heap/heap-utils.cc
--- up/v8/test/cctest/heap/heap-utils.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/heap/heap-utils.cc	2023-01-19 16:46:36.449776171 -0500
@@ -10,6 +10,7 @@
 #include "src/execution/isolate.h"
 #include "src/heap/factory.h"
 #include "src/heap/free-list.h"
+#include "src/heap/gc-tracer-inl.h"
 #include "src/heap/heap-inl.h"
 #include "src/heap/incremental-marking.h"
 #include "src/heap/mark-compact.h"
@@ -136,12 +137,20 @@
 namespace {
 void FillPageInPagedSpace(Page* page,
                           std::vector<Handle<FixedArray>>* out_handles) {
+  Heap* heap = page->heap();
   DCHECK(page->SweepingDone());
   PagedSpaceBase* paged_space = static_cast<PagedSpaceBase*>(page->owner());
   // Make sure the LAB is empty to guarantee that all free space is accounted
   // for in the freelist.
   DCHECK_EQ(paged_space->limit(), paged_space->top());
 
+  PauseAllocationObserversScope no_observers_scope(heap);
+
+  CollectionEpoch full_epoch =
+      heap->tracer()->CurrentEpoch(GCTracer::Scope::ScopeId::MARK_COMPACTOR);
+  CollectionEpoch young_epoch = heap->tracer()->CurrentEpoch(
+      GCTracer::Scope::ScopeId::MINOR_MARK_COMPACTOR);
+
   for (Page* p : *paged_space) {
     if (p != page) paged_space->UnlinkFreeListCategories(p);
   }
@@ -158,56 +167,67 @@
       [&available_sizes](FreeListCategory* category) {
         category->IterateNodesForTesting([&available_sizes](FreeSpace node) {
           int node_size = node.Size();
-          DCHECK_LT(0, FixedArrayLenFromSize(node_size));
-          available_sizes.push_back(node_size);
+          if (node_size >= kMaxRegularHeapObjectSize) {
+            available_sizes.push_back(node_size);
+          }
         });
       });
 
-  Isolate* isolate = page->heap()->isolate();
+  Isolate* isolate = heap->isolate();
 
   // Allocate as many max size arrays as possible, while making sure not to
   // leave behind a block too small to fit a FixedArray.
   const int max_array_length = FixedArrayLenFromSize(kMaxRegularHeapObjectSize);
   for (size_t i = 0; i < available_sizes.size(); ++i) {
     int available_size = available_sizes[i];
-    while (available_size >
-           kMaxRegularHeapObjectSize + FixedArray::kHeaderSize) {
+    while (available_size > kMaxRegularHeapObjectSize) {
       Handle<FixedArray> fixed_array = isolate->factory()->NewFixedArray(
           max_array_length, AllocationType::kYoung);
       if (out_handles) out_handles->push_back(fixed_array);
       available_size -= kMaxRegularHeapObjectSize;
     }
-    if (available_size > kMaxRegularHeapObjectSize) {
-      // Allocate less than kMaxRegularHeapObjectSize to ensure remaining space
-      // can be used to allcoate another FixedArray.
-      int array_size = kMaxRegularHeapObjectSize - FixedArray::kHeaderSize;
+  }
+
+  paged_space->FreeLinearAllocationArea();
+
+  // Allocate FixedArrays in remaining free list blocks, from largest
+  // category to smallest.
+  std::vector<std::vector<int>> remaining_sizes;
+  page->ForAllFreeListCategories(
+      [&remaining_sizes](FreeListCategory* category) {
+        remaining_sizes.push_back({});
+        std::vector<int>& sizes_in_category =
+            remaining_sizes[remaining_sizes.size() - 1];
+        category->IterateNodesForTesting([&sizes_in_category](FreeSpace node) {
+          int node_size = node.Size();
+          DCHECK_LT(0, FixedArrayLenFromSize(node_size));
+          sizes_in_category.push_back(node_size);
+        });
+      });
+  for (auto it = remaining_sizes.rbegin(); it != remaining_sizes.rend(); ++it) {
+    std::vector<int> sizes_in_category = *it;
+    for (int size : sizes_in_category) {
+      DCHECK_LE(size, kMaxRegularHeapObjectSize);
+      int array_length = FixedArrayLenFromSize(size);
+      DCHECK_LT(0, array_length);
       Handle<FixedArray> fixed_array = isolate->factory()->NewFixedArray(
-          FixedArrayLenFromSize(array_size), AllocationType::kYoung);
+          array_length, AllocationType::kYoung);
       if (out_handles) out_handles->push_back(fixed_array);
-      available_size -= array_size;
     }
-    DCHECK_LE(available_size, kMaxRegularHeapObjectSize);
-    DCHECK_LT(0, FixedArrayLenFromSize(available_size));
-    available_sizes[i] = available_size;
   }
 
-  // Allocate FixedArrays in remaining free list blocks, from largest to
-  // smallest.
-  std::sort(available_sizes.begin(), available_sizes.end(),
-            [](size_t a, size_t b) { return a > b; });
-  for (size_t i = 0; i < available_sizes.size(); ++i) {
-    int available_size = available_sizes[i];
-    DCHECK_LE(available_size, kMaxRegularHeapObjectSize);
-    int array_length = FixedArrayLenFromSize(available_size);
-    DCHECK_LT(0, array_length);
-    Handle<FixedArray> fixed_array =
-        isolate->factory()->NewFixedArray(array_length, AllocationType::kYoung);
-    if (out_handles) out_handles->push_back(fixed_array);
-  }
+  DCHECK_EQ(0, page->AvailableInFreeList());
+  DCHECK_EQ(0, page->AvailableInFreeListFromAllocatedBytes());
 
   for (Page* p : *paged_space) {
     if (p != page) paged_space->RelinkFreeListCategories(p);
   }
+
+  // Allocations in this method should not require a GC.
+  CHECK_EQ(full_epoch, heap->tracer()->CurrentEpoch(
+                           GCTracer::Scope::ScopeId::MARK_COMPACTOR));
+  CHECK_EQ(young_epoch, heap->tracer()->CurrentEpoch(
+                            GCTracer::Scope::ScopeId::MINOR_MARK_COMPACTOR));
 }
 }  // namespace
 
@@ -217,6 +237,8 @@
     PauseAllocationObserversScope pause_observers(space->heap());
     if (space->top() == kNullAddress) return;
     Page* page = Page::FromAllocationAreaAddress(space->top());
+    space->heap()->EnsureSweepingCompleted(
+        Heap::SweepingForcedFinalizationMode::kV8Only);
     space->FreeLinearAllocationArea();
     FillPageInPagedSpace(page, out_handles);
   } else {
@@ -261,7 +283,7 @@
   i::IncrementalMarking* marking = heap->incremental_marking();
 
   if (heap->sweeping_in_progress()) {
-    SafepointScope scope(heap);
+    IsolateSafepointScope scope(heap);
     heap->EnsureSweepingCompleted(
         Heap::SweepingForcedFinalizationMode::kV8Only);
   }
@@ -270,6 +292,7 @@
     // If minor incremental marking is running, we need to finalize it first
     // because of the AdvanceForTesting call in this function which is currently
     // only possible for MajorMC.
+    ScanStackModeScopeForTesting scope(heap, Heap::ScanStackMode::kNone);
     heap->CollectGarbage(NEW_SPACE, GarbageCollectionReason::kFinalizeMinorMC);
   }
 
@@ -280,7 +303,7 @@
   CHECK(marking->IsMarking());
   if (!force_completion) return;
 
-  SafepointScope scope(heap);
+  IsolateSafepointScope scope(heap);
   MarkingBarrier::PublishAll(heap);
   marking->MarkRootsForTesting();
 
@@ -311,9 +334,10 @@
 }
 
 void GcAndSweep(Heap* heap, AllocationSpace space) {
+  ScanStackModeScopeForTesting scope(heap, Heap::ScanStackMode::kNone);
   heap->CollectGarbage(space, GarbageCollectionReason::kTesting);
   if (heap->sweeping_in_progress()) {
-    SafepointScope scope(heap);
+    IsolateSafepointScope scope(heap);
     heap->EnsureSweepingCompleted(
         Heap::SweepingForcedFinalizationMode::kV8Only);
   }
@@ -341,17 +365,19 @@
 }
 
 void GrowNewSpace(Heap* heap) {
-  SafepointScope scope(heap);
+  IsolateSafepointScope scope(heap);
   if (!heap->new_space()->IsAtMaximumCapacity()) {
     heap->new_space()->Grow();
   }
+  CHECK(heap->new_space()->EnsureCurrentCapacity());
 }
 
 void GrowNewSpaceToMaximumCapacity(Heap* heap) {
-  SafepointScope scope(heap);
+  IsolateSafepointScope scope(heap);
   while (!heap->new_space()->IsAtMaximumCapacity()) {
     heap->new_space()->Grow();
   }
+  CHECK(heap->new_space()->EnsureCurrentCapacity());
 }
 
 }  // namespace heap
diff -r -u --color up/v8/test/cctest/heap/test-alloc.cc nw/v8/test/cctest/heap/test-alloc.cc
--- up/v8/test/cctest/heap/test-alloc.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/heap/test-alloc.cc	2023-01-19 16:46:36.449776171 -0500
@@ -78,7 +78,7 @@
   heap->CreateFillerObjectAt(obj.address(), size);
 
   // Map space.
-  heap::SimulateFullSpace(heap->space_for_maps());
+  heap::SimulateFullSpace(heap->old_space());
   obj = heap->AllocateRaw(Map::kSize, AllocationType::kMap).ToObjectChecked();
   heap->CreateFillerObjectAt(obj.address(), Map::kSize);
 
diff -r -u --color up/v8/test/cctest/heap/test-array-buffer-tracker.cc nw/v8/test/cctest/heap/test-array-buffer-tracker.cc
--- up/v8/test/cctest/heap/test-array-buffer-tracker.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/heap/test-array-buffer-tracker.cc	2023-01-19 16:46:36.449776171 -0500
@@ -203,7 +203,7 @@
     // |Detach| will cause the buffer to be |Unregister|ed. Without
     // barriers and proper synchronization this will trigger a data race on
     // TSAN.
-    ab->Detach();
+    ab->Detach(v8::Local<v8::Value>()).Check();
   }
 }
 
diff -r -u --color up/v8/test/cctest/heap/test-concurrent-allocation.cc nw/v8/test/cctest/heap/test-concurrent-allocation.cc
--- up/v8/test/cctest/heap/test-concurrent-allocation.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/heap/test-concurrent-allocation.cc	2023-01-19 16:46:36.449776171 -0500
@@ -144,6 +144,8 @@
   const int kThreads = 4;
 
   {
+    ScanStackModeScopeForTesting no_stack_scanning(i_isolate->heap(),
+                                                   Heap::ScanStackMode::kNone);
     ParkedScope scope(i_isolate->main_thread_local_isolate());
 
     for (int i = 0; i < kThreads; i++) {
@@ -174,22 +176,27 @@
   std::vector<std::unique_ptr<ConcurrentAllocationThread>> threads;
   const int kThreads = 4;
 
-  for (int i = 0; i < kThreads; i++) {
-    auto thread =
-        std::make_unique<ConcurrentAllocationThread>(i_isolate->heap());
-    CHECK(thread->Start());
-    threads.push_back(std::move(thread));
-  }
+  {
+    ScanStackModeScopeForTesting no_stack_scanning(i_isolate->heap(),
+                                                   Heap::ScanStackMode::kNone);
 
-  for (int i = 0; i < 300'000; i++) {
-    ParkedScope scope(i_isolate->main_thread_local_isolate());
-  }
+    for (int i = 0; i < kThreads; i++) {
+      auto thread =
+          std::make_unique<ConcurrentAllocationThread>(i_isolate->heap());
+      CHECK(thread->Start());
+      threads.push_back(std::move(thread));
+    }
 
-  {
-    ParkedScope scope(i_isolate->main_thread_local_isolate());
+    for (int i = 0; i < 300'000; i++) {
+      ParkedScope scope(i_isolate->main_thread_local_isolate());
+    }
 
-    for (auto& thread : threads) {
-      thread->Join();
+    {
+      ParkedScope scope(i_isolate->main_thread_local_isolate());
+
+      for (auto& thread : threads) {
+        thread->Join();
+      }
     }
   }
 
@@ -209,23 +216,29 @@
   std::vector<std::unique_ptr<ConcurrentAllocationThread>> threads;
   const int kThreads = 4;
 
-  for (int i = 0; i < kThreads; i++) {
-    auto thread =
-        std::make_unique<ConcurrentAllocationThread>(i_isolate->heap());
-    CHECK(thread->Start());
-    threads.push_back(std::move(thread));
-  }
+  {
+    ScanStackModeScopeForTesting no_stack_scanning(i_isolate->heap(),
+                                                   Heap::ScanStackMode::kNone);
 
-  // Some of the following Safepoint() invocations are supposed to perform a GC.
-  for (int i = 0; i < 1'000'000; i++) {
-    i_isolate->main_thread_local_heap()->Safepoint();
-  }
+    for (int i = 0; i < kThreads; i++) {
+      auto thread =
+          std::make_unique<ConcurrentAllocationThread>(i_isolate->heap());
+      CHECK(thread->Start());
+      threads.push_back(std::move(thread));
+    }
 
-  {
-    ParkedScope scope(i_isolate->main_thread_local_isolate());
+    // Some of the following Safepoint() invocations are supposed to perform a
+    // GC.
+    for (int i = 0; i < 1'000'000; i++) {
+      i_isolate->main_thread_local_heap()->Safepoint();
+    }
 
-    for (auto& thread : threads) {
-      thread->Join();
+    {
+      ParkedScope scope(i_isolate->main_thread_local_isolate());
+
+      for (auto& thread : threads) {
+        thread->Join();
+      }
     }
   }
 
diff -r -u --color up/v8/test/cctest/heap/test-heap.cc nw/v8/test/cctest/heap/test-heap.cc
--- up/v8/test/cctest/heap/test-heap.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/heap/test-heap.cc	2023-01-19 16:46:36.449776171 -0500
@@ -1136,6 +1136,72 @@
   }
 }
 
+TEST(TestMultiReferencedBytecodeFlushing) {
+#ifndef V8_LITE_MODE
+  v8_flags.turbofan = false;
+  v8_flags.always_turbofan = false;
+  i::v8_flags.optimize_for_size = false;
+#endif  // V8_LITE_MODE
+#if ENABLE_SPARKPLUG
+  v8_flags.always_sparkplug = false;
+#endif  // ENABLE_SPARKPLUG
+  i::v8_flags.flush_bytecode = true;
+  i::v8_flags.allow_natives_syntax = true;
+
+  CcTest::InitializeVM();
+  v8::Isolate* isolate = CcTest::isolate();
+  Isolate* i_isolate = CcTest::i_isolate();
+  Factory* factory = i_isolate->factory();
+
+  {
+    v8::HandleScope scope(isolate);
+    v8::Context::New(isolate)->Enter();
+    const char* source =
+        "function foo() {"
+        "  var x = 42;"
+        "  var y = 42;"
+        "  var z = x + y;"
+        "};"
+        "foo()";
+    Handle<String> foo_name = factory->InternalizeUtf8String("foo");
+
+    // This compile will add the code to the compilation cache.
+    {
+      v8::HandleScope new_scope(isolate);
+      CompileRun(source);
+    }
+
+    // Check function is compiled.
+    Handle<Object> func_value =
+        Object::GetProperty(i_isolate, i_isolate->global_object(), foo_name)
+            .ToHandleChecked();
+    CHECK(func_value->IsJSFunction());
+    Handle<JSFunction> function = Handle<JSFunction>::cast(func_value);
+    Handle<SharedFunctionInfo> shared = handle(function->shared(), i_isolate);
+    CHECK(shared->is_compiled());
+
+    // Make a copy of the SharedFunctionInfo which points to the same bytecode.
+    Handle<SharedFunctionInfo> copy =
+        i_isolate->factory()->CloneSharedFunctionInfo(shared);
+
+    // Simulate several GCs that use full marking.
+    const int kAgingThreshold = 7;
+    for (int i = 0; i < kAgingThreshold; i++) {
+      CcTest::CollectAllGarbage();
+    }
+
+    // foo should no longer be in the compilation cache
+    CHECK(!shared->is_compiled());
+    CHECK(!copy->is_compiled());
+    CHECK(!function->is_compiled());
+
+    // The feedback metadata for both SharedFunctionInfo instances should have
+    // been reset.
+    CHECK(!shared->HasFeedbackMetadata());
+    CHECK(!copy->HasFeedbackMetadata());
+  }
+}
+
 HEAP_TEST(Regress10560) {
   i::v8_flags.flush_bytecode = true;
   i::v8_flags.allow_natives_syntax = true;
@@ -5794,7 +5860,7 @@
     }
   }
 
-  SafepointScope safepoint_scope(heap);
+  IsolateSafepointScope safepoint_scope(heap);
   MarkingBarrier::PublishAll(heap);
 
   // Finish marking with bigger steps to speed up test.
@@ -6500,7 +6566,7 @@
   heap->tracer()->StopFullCycleIfNeeded();
   i::IncrementalMarking* marking = CcTest::heap()->incremental_marking();
   if (marking->IsStopped()) {
-    SafepointScope safepoint_scope(heap);
+    IsolateSafepointScope safepoint_scope(heap);
     heap->tracer()->StartCycle(
         GarbageCollector::MARK_COMPACTOR, GarbageCollectionReason::kTesting,
         "collector cctest", GCTracer::MarkingType::kIncremental);
diff -r -u --color up/v8/test/cctest/heap/test-incremental-marking.cc nw/v8/test/cctest/heap/test-incremental-marking.cc
--- up/v8/test/cctest/heap/test-incremental-marking.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/heap/test-incremental-marking.cc	2023-01-19 16:46:36.449776171 -0500
@@ -118,7 +118,7 @@
     i::IncrementalMarking* marking = heap->incremental_marking();
     marking->Stop();
     {
-      SafepointScope scope(heap);
+      IsolateSafepointScope scope(heap);
       heap->tracer()->StartCycle(
           GarbageCollector::MARK_COMPACTOR, GarbageCollectionReason::kTesting,
           "collector cctest", GCTracer::MarkingType::kIncremental);
diff -r -u --color up/v8/test/cctest/heap/test-iterators.cc nw/v8/test/cctest/heap/test-iterators.cc
--- up/v8/test/cctest/heap/test-iterators.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/heap/test-iterators.cc	2023-01-19 16:46:36.449776171 -0500
@@ -18,31 +18,30 @@
 namespace internal {
 namespace heap {
 
-TEST(HeapObjectIteratorNullPastEnd) {
-  HeapObjectIterator iterator(CcTest::heap());
-  while (!iterator.Next().is_null()) {
+namespace {
+template <typename T>
+void TestIterator(T it) {
+  while (!it.Next().is_null()) {
   }
   for (int i = 0; i < 20; i++) {
-    CHECK(iterator.Next().is_null());
+    CHECK(it.Next().is_null());
   }
 }
+}  // namespace
+
+TEST(HeapObjectIteratorNullPastEnd) {
+  TestIterator<HeapObjectIterator>(
+      static_cast<v8::internal::HeapObjectIterator>(CcTest::heap()));
+}
 
 TEST(ReadOnlyHeapObjectIteratorNullPastEnd) {
-  ReadOnlyHeapObjectIterator iterator(CcTest::read_only_heap());
-  while (!iterator.Next().is_null()) {
-  }
-  for (int i = 0; i < 20; i++) {
-    CHECK(iterator.Next().is_null());
-  }
+  TestIterator<ReadOnlyHeapObjectIterator>(
+      static_cast<v8::internal::ReadOnlyHeapObjectIterator>(
+          CcTest::read_only_heap()));
 }
 
 TEST(CombinedHeapObjectIteratorNullPastEnd) {
-  CombinedHeapObjectIterator iterator(CcTest::heap());
-  while (!iterator.Next().is_null()) {
-  }
-  for (int i = 0; i < 20; i++) {
-    CHECK(iterator.Next().is_null());
-  }
+  TestIterator<CombinedHeapObjectIterator>(CcTest::heap());
 }
 
 namespace {
@@ -103,7 +102,6 @@
   PagedSpaceIterator iterator(heap);
   CHECK_EQ(iterator.Next(), reinterpret_cast<PagedSpace*>(heap->old_space()));
   CHECK_EQ(iterator.Next(), reinterpret_cast<PagedSpace*>(heap->code_space()));
-  CHECK_EQ(iterator.Next(), reinterpret_cast<PagedSpace*>(heap->map_space()));
   for (int i = 0; i < 20; i++) {
     CHECK_NULL(iterator.Next());
   }
diff -r -u --color up/v8/test/cctest/heap/test-mark-compact.cc nw/v8/test/cctest/heap/test-mark-compact.cc
--- up/v8/test/cctest/heap/test-mark-compact.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/heap/test-mark-compact.cc	2023-01-19 16:46:36.449776171 -0500
@@ -138,7 +138,7 @@
   do {
     allocation = AllocateMapForTest(isolate);
   } while (!allocation.IsFailure());
-  CcTest::CollectGarbage(MAP_SPACE);
+  CcTest::CollectGarbage(OLD_SPACE);
   AllocateMapForTest(isolate).ToObjectChecked();
 
   { HandleScope scope(isolate);
@@ -248,7 +248,7 @@
 
   {
     // We need a safepoint for calling FindBasePtr.
-    SafepointScope scope(heap);
+    IsolateSafepointScope scope(heap);
 
     for (int k = 0; k < obj1.Size(); ++k) {
       Address obj1_inner_ptr = obj1.address() + k;
@@ -285,7 +285,7 @@
 
   {
     // We need a safepoint for calling FindBasePtr.
-    SafepointScope scope(heap);
+    IsolateSafepointScope scope(heap);
 
     // After FindBasePtr, the bits should be properly set again.
     for (int k = 0; k < obj1.Size(); ++k) {
diff -r -u --color up/v8/test/cctest/heap/test-spaces.cc nw/v8/test/cctest/heap/test-spaces.cc
--- up/v8/test/cctest/heap/test-spaces.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/heap/test-spaces.cc	2023-01-19 16:46:36.449776171 -0500
@@ -161,7 +161,7 @@
 TEST(MemoryChunk) {
   Isolate* isolate = CcTest::i_isolate();
   Heap* heap = isolate->heap();
-  SafepointScope safepoint(heap);
+  IsolateSafepointScope safepoint(heap);
 
   v8::PageAllocator* page_allocator = GetPlatformPageAllocator();
   size_t area_size;
@@ -329,6 +329,7 @@
       CcTest::heap()->InitialSemiSpaceSize(), allocation_info);
   CHECK(new_space->MaximumCapacity());
   CHECK(new_space->EnsureCurrentCapacity());
+  CHECK_LT(0, new_space->Capacity());
   CHECK_LT(0, new_space->TotalCapacity());
 
   AllocationResult allocation_result;
Only in nw/v8/test/cctest: jsonstream-helper.h
diff -r -u --color up/v8/test/cctest/test-allocation.cc nw/v8/test/cctest/test-allocation.cc
--- up/v8/test/cctest/test-allocation.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/test-allocation.cc	2023-01-19 16:46:36.449776171 -0500
@@ -96,6 +96,8 @@
   CHECK_EQ(result == nullptr, platform.oom_callback_called);
 }
 
+// We use |AllocateAtLeast| in the accounting allocator, so we check only that
+// we have _at least_ the expected amount of memory allocated.
 TEST_WITH_PLATFORM(AccountingAllocatorCurrentAndMax, AllocationPlatform) {
   v8::internal::AccountingAllocator allocator;
   static constexpr size_t kAllocationSizes[] = {51, 231, 27};
@@ -108,8 +110,8 @@
   for (size_t size : kAllocationSizes) {
     segments.push_back(allocator.AllocateSegment(size, support_compression));
     CHECK_NOT_NULL(segments.back());
-    CHECK_EQ(size, segments.back()->total_size());
-    expected_current += size;
+    CHECK_LE(size, segments.back()->total_size());
+    expected_current += segments.back()->total_size();
     if (expected_current > expected_max) expected_max = expected_current;
     CHECK_EQ(expected_current, allocator.GetCurrentMemoryUsage());
     CHECK_EQ(expected_max, allocator.GetMaxMemoryUsage());
diff -r -u --color up/v8/test/cctest/test-api-array-buffer.cc nw/v8/test/cctest/test-api-array-buffer.cc
--- up/v8/test/cctest/test-api-array-buffer.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/test-api-array-buffer.cc	2023-01-19 16:46:36.449776171 -0500
@@ -180,7 +180,7 @@
   CHECK_EQ(1023, dv->ByteLength());
 
   Externalize(buffer);
-  buffer->Detach();
+  buffer->Detach(v8::Local<v8::Value>()).Check();
   CHECK_EQ(0, buffer->ByteLength());
   CheckIsDetached(u8a);
   CheckIsDetached(u8c);
@@ -216,7 +216,7 @@
   v8::Local<v8::DataView> dv = CompileRun("dv").As<v8::DataView>();
 
   Externalize(ab);
-  ab->Detach();
+  ab->Detach(v8::Local<v8::Value>()).Check();
   CHECK_EQ(0, ab->ByteLength());
   CHECK_EQ(0, v8_run_int32value(v8_compile("ab.byteLength")));
 
@@ -245,6 +245,37 @@
   CheckDataViewIsDetached(dv);
 }
 
+THREADED_TEST(ArrayBuffer_WasDetached) {
+  LocalContext env;
+  v8::Isolate* isolate = env->GetIsolate();
+  v8::HandleScope handle_scope(isolate);
+
+  Local<v8::ArrayBuffer> ab = v8::ArrayBuffer::New(isolate, 0);
+  CHECK(!ab->WasDetached());
+
+  ab->Detach(v8::Local<v8::Value>()).Check();
+  CHECK(ab->WasDetached());
+}
+
+THREADED_TEST(ArrayBuffer_NonDetachableWasDetached) {
+  LocalContext env;
+  v8::Isolate* isolate = env->GetIsolate();
+  v8::HandleScope handle_scope(isolate);
+
+  CompileRun(R"JS(
+    var wasmMemory = new WebAssembly.Memory({initial: 1, maximum: 2});
+  )JS");
+
+  Local<v8::ArrayBuffer> non_detachable =
+      CompileRun("wasmMemory.buffer").As<v8::ArrayBuffer>();
+  CHECK(!non_detachable->IsDetachable());
+  CHECK(!non_detachable->WasDetached());
+
+  CompileRun("wasmMemory.grow(1)");
+  CHECK(!non_detachable->IsDetachable());
+  CHECK(non_detachable->WasDetached());
+}
+
 THREADED_TEST(ArrayBuffer_ExternalizeEmpty) {
   LocalContext env;
   v8::Isolate* isolate = env->GetIsolate();
diff -r -u --color up/v8/test/cctest/test-api.cc nw/v8/test/cctest/test-api.cc
--- up/v8/test/cctest/test-api.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/test-api.cc	2023-01-19 16:46:36.460609504 -0500
@@ -752,6 +752,8 @@
 
   // Trigger GCs and force evacuation.
   CcTest::CollectAllGarbage();
+  i::ScanStackModeScopeForTesting no_stack_scanning(
+      CcTest::heap(), i::Heap::ScanStackMode::kNone);
   CcTest::heap()->CollectAllGarbage(i::Heap::kReduceMemoryFootprintMask,
                                     i::GarbageCollectionReason::kTesting);
 }
@@ -13012,9 +13014,6 @@
   {
     // ... get the V8 lock
     v8::Locker locker(CcTest::isolate());
-    // ... set the isolate stack to this thread
-    CcTest::i_isolate()->heap()->SetStackStart(
-        v8::base::Stack::GetStackStart());
     // ... and start running the test.
     CallTest();
   }
@@ -13082,9 +13081,6 @@
     v8::Unlocker unlocker(CcTest::isolate());
     // Wait till someone starts us again.
     gate_.Wait();
-    // Set the isolate stack to this thread.
-    CcTest::i_isolate()->heap()->SetStackStart(
-        v8::base::Stack::GetStackStart());
     // And we're off.
   }
 }
@@ -19790,7 +19786,7 @@
 static void MicrotaskOne(const v8::FunctionCallbackInfo<Value>& info) {
   CHECK(v8::MicrotasksScope::IsRunningMicrotasks(info.GetIsolate()));
   v8::HandleScope scope(info.GetIsolate());
-  v8::MicrotasksScope microtasks(info.GetIsolate(),
+  v8::MicrotasksScope microtasks(info.GetIsolate()->GetCurrentContext(),
                                  v8::MicrotasksScope::kDoNotRunMicrotasks);
   CompileRun("ext1Calls++;");
 }
@@ -19799,7 +19795,7 @@
 static void MicrotaskTwo(const v8::FunctionCallbackInfo<Value>& info) {
   CHECK(v8::MicrotasksScope::IsRunningMicrotasks(info.GetIsolate()));
   v8::HandleScope scope(info.GetIsolate());
-  v8::MicrotasksScope microtasks(info.GetIsolate(),
+  v8::MicrotasksScope microtasks(info.GetIsolate()->GetCurrentContext(),
                                  v8::MicrotasksScope::kDoNotRunMicrotasks);
   CompileRun("ext2Calls++;");
 }
@@ -20101,31 +20097,31 @@
   v8::HandleScope handles(env->GetIsolate());
   env->GetIsolate()->SetMicrotasksPolicy(v8::MicrotasksPolicy::kScoped);
   {
-    v8::MicrotasksScope scope1(env->GetIsolate(),
+    v8::MicrotasksScope scope1(env.local(),
                                v8::MicrotasksScope::kRunMicrotasks);
     env->GetIsolate()->EnqueueMicrotask(
         Function::New(env.local(), MicrotaskOne).ToLocalChecked());
     CompileRun("var ext1Calls = 0;");
   }
   {
-    v8::MicrotasksScope scope1(env->GetIsolate(),
+    v8::MicrotasksScope scope1(env.local(),
                                v8::MicrotasksScope::kRunMicrotasks);
     ExpectInt32("ext1Calls", 1);
   }
   {
-    v8::MicrotasksScope scope1(env->GetIsolate(),
+    v8::MicrotasksScope scope1(env.local(),
                                v8::MicrotasksScope::kRunMicrotasks);
     env->GetIsolate()->EnqueueMicrotask(
         Function::New(env.local(), MicrotaskOne).ToLocalChecked());
     CompileRun("throw new Error()");
   }
   {
-    v8::MicrotasksScope scope1(env->GetIsolate(),
+    v8::MicrotasksScope scope1(env.local(),
                                v8::MicrotasksScope::kRunMicrotasks);
     ExpectInt32("ext1Calls", 2);
   }
   {
-    v8::MicrotasksScope scope1(env->GetIsolate(),
+    v8::MicrotasksScope scope1(env.local(),
                                v8::MicrotasksScope::kRunMicrotasks);
     env->GetIsolate()->EnqueueMicrotask(
         Function::New(env.local(), MicrotaskOne).ToLocalChecked());
@@ -20133,18 +20129,18 @@
     CompileRun("throw new Error()");
   }
   {
-    v8::MicrotasksScope scope1(env->GetIsolate(),
+    v8::MicrotasksScope scope1(env.local(),
                                v8::MicrotasksScope::kRunMicrotasks);
     ExpectInt32("ext1Calls", 3);
   }
   {
-    v8::MicrotasksScope scope1(env->GetIsolate(),
+    v8::MicrotasksScope scope1(env.local(),
                                v8::MicrotasksScope::kRunMicrotasks);
     env->GetIsolate()->EnqueueMicrotask(
         Function::New(env.local(), MicrotaskOne).ToLocalChecked());
     env->GetIsolate()->TerminateExecution();
     {
-      v8::MicrotasksScope scope2(env->GetIsolate(),
+      v8::MicrotasksScope scope2(env.local(),
                                  v8::MicrotasksScope::kRunMicrotasks);
       env->GetIsolate()->EnqueueMicrotask(
           Function::New(env.local(), MicrotaskOne).ToLocalChecked());
@@ -20152,21 +20148,21 @@
   }
   env->GetIsolate()->CancelTerminateExecution();
   {
-    v8::MicrotasksScope scope1(env->GetIsolate(),
+    v8::MicrotasksScope scope1(env.local(),
                                v8::MicrotasksScope::kRunMicrotasks);
     ExpectInt32("ext1Calls", 3);
     env->GetIsolate()->EnqueueMicrotask(
         Function::New(env.local(), MicrotaskOne).ToLocalChecked());
   }
   {
-    v8::MicrotasksScope scope1(env->GetIsolate(),
+    v8::MicrotasksScope scope1(env.local(),
                                v8::MicrotasksScope::kRunMicrotasks);
 
     ExpectInt32("ext1Calls", 4);
   }
 
   {
-    v8::MicrotasksScope scope1(env->GetIsolate(),
+    v8::MicrotasksScope scope1(env.local(),
                                v8::MicrotasksScope::kDoNotRunMicrotasks);
     env->GetIsolate()->EnqueueMicrotask(
         Function::New(env.local(), MicrotaskOne).ToLocalChecked());
@@ -20177,13 +20173,13 @@
     CHECK_EQ(0, CompileRun("ext1Calls")->Int32Value(env.local()).FromJust());
     CHECK_EQ(0, CompileRun("ext2Calls")->Int32Value(env.local()).FromJust());
     {
-      v8::MicrotasksScope scope2(env->GetIsolate(),
+      v8::MicrotasksScope scope2(env.local(),
                                  v8::MicrotasksScope::kRunMicrotasks);
       CompileRun("1+1;");
       CHECK_EQ(0, CompileRun("ext1Calls")->Int32Value(env.local()).FromJust());
       CHECK_EQ(0, CompileRun("ext2Calls")->Int32Value(env.local()).FromJust());
       {
-        v8::MicrotasksScope scope3(env->GetIsolate(),
+        v8::MicrotasksScope scope3(env.local(),
                                    v8::MicrotasksScope::kRunMicrotasks);
         CompileRun("1+1;");
         CHECK_EQ(0,
@@ -20201,20 +20197,20 @@
   }
 
   {
-    v8::MicrotasksScope scope(env->GetIsolate(),
+    v8::MicrotasksScope scope(env.local(),
                               v8::MicrotasksScope::kDoNotRunMicrotasks);
     CHECK_EQ(1, CompileRun("ext1Calls")->Int32Value(env.local()).FromJust());
     CHECK_EQ(0, CompileRun("ext2Calls")->Int32Value(env.local()).FromJust());
   }
 
   {
-    v8::MicrotasksScope scope1(env->GetIsolate(),
+    v8::MicrotasksScope scope1(env.local(),
                                v8::MicrotasksScope::kRunMicrotasks);
     CompileRun("1+1;");
     CHECK_EQ(1, CompileRun("ext1Calls")->Int32Value(env.local()).FromJust());
     CHECK_EQ(0, CompileRun("ext2Calls")->Int32Value(env.local()).FromJust());
     {
-      v8::MicrotasksScope scope2(env->GetIsolate(),
+      v8::MicrotasksScope scope2(env.local(),
                                  v8::MicrotasksScope::kDoNotRunMicrotasks);
     }
     CHECK_EQ(1, CompileRun("ext1Calls")->Int32Value(env.local()).FromJust());
@@ -20222,7 +20218,7 @@
   }
 
   {
-    v8::MicrotasksScope scope(env->GetIsolate(),
+    v8::MicrotasksScope scope(env.local(),
                               v8::MicrotasksScope::kDoNotRunMicrotasks);
     CHECK_EQ(1, CompileRun("ext1Calls")->Int32Value(env.local()).FromJust());
     CHECK_EQ(1, CompileRun("ext2Calls")->Int32Value(env.local()).FromJust());
@@ -20233,17 +20229,17 @@
   {
     v8::Isolate::SuppressMicrotaskExecutionScope scope1(env->GetIsolate());
     {
-      v8::MicrotasksScope scope2(env->GetIsolate(),
+      v8::MicrotasksScope scope2(env.local(),
                                  v8::MicrotasksScope::kRunMicrotasks);
     }
-    v8::MicrotasksScope scope3(env->GetIsolate(),
+    v8::MicrotasksScope scope3(env.local(),
                                v8::MicrotasksScope::kDoNotRunMicrotasks);
     CHECK_EQ(1, CompileRun("ext1Calls")->Int32Value(env.local()).FromJust());
     CHECK_EQ(1, CompileRun("ext2Calls")->Int32Value(env.local()).FromJust());
   }
 
   {
-    v8::MicrotasksScope scope1(env->GetIsolate(),
+    v8::MicrotasksScope scope1(env.local(),
                                v8::MicrotasksScope::kRunMicrotasks);
     v8::MicrotasksScope::PerformCheckpoint(env->GetIsolate());
     CHECK_EQ(1, CompileRun("ext1Calls")->Int32Value(env.local()).FromJust());
@@ -20251,7 +20247,7 @@
   }
 
   {
-    v8::MicrotasksScope scope(env->GetIsolate(),
+    v8::MicrotasksScope scope(env.local(),
                               v8::MicrotasksScope::kDoNotRunMicrotasks);
     CHECK_EQ(1, CompileRun("ext1Calls")->Int32Value(env.local()).FromJust());
     CHECK_EQ(2, CompileRun("ext2Calls")->Int32Value(env.local()).FromJust());
@@ -20260,7 +20256,7 @@
   v8::MicrotasksScope::PerformCheckpoint(env->GetIsolate());
 
   {
-    v8::MicrotasksScope scope(env->GetIsolate(),
+    v8::MicrotasksScope scope(env.local(),
                               v8::MicrotasksScope::kDoNotRunMicrotasks);
     CHECK_EQ(1, CompileRun("ext1Calls")->Int32Value(env.local()).FromJust());
     CHECK_EQ(2, CompileRun("ext2Calls")->Int32Value(env.local()).FromJust());
@@ -20271,7 +20267,7 @@
   v8::MicrotasksScope::PerformCheckpoint(env->GetIsolate());
 
   {
-    v8::MicrotasksScope scope(env->GetIsolate(),
+    v8::MicrotasksScope scope(env.local(),
                               v8::MicrotasksScope::kDoNotRunMicrotasks);
     CHECK_EQ(1, CompileRun("ext1Calls")->Int32Value(env.local()).FromJust());
     CHECK_EQ(3, CompileRun("ext2Calls")->Int32Value(env.local()).FromJust());
@@ -20282,7 +20278,7 @@
   {
     v8::Isolate::SuppressMicrotaskExecutionScope scope1(env->GetIsolate());
     v8::MicrotasksScope::PerformCheckpoint(env->GetIsolate());
-    v8::MicrotasksScope scope2(env->GetIsolate(),
+    v8::MicrotasksScope scope2(env.local(),
                                v8::MicrotasksScope::kDoNotRunMicrotasks);
     CHECK_EQ(1, CompileRun("ext1Calls")->Int32Value(env.local()).FromJust());
     CHECK_EQ(3, CompileRun("ext2Calls")->Int32Value(env.local()).FromJust());
@@ -20291,7 +20287,7 @@
   v8::MicrotasksScope::PerformCheckpoint(env->GetIsolate());
 
   {
-    v8::MicrotasksScope scope(env->GetIsolate(),
+    v8::MicrotasksScope scope(env.local(),
                               v8::MicrotasksScope::kDoNotRunMicrotasks);
     CHECK_EQ(2, CompileRun("ext1Calls")->Int32Value(env.local()).FromJust());
     CHECK_EQ(3, CompileRun("ext2Calls")->Int32Value(env.local()).FromJust());
@@ -21032,6 +21028,8 @@
 
   static void CollectAllGarbage(v8::Isolate* isolate, void* data) {
     i::Isolate* i_isolate = reinterpret_cast<i::Isolate*>(isolate);
+    i::ScanStackModeScopeForTesting no_stack_scanning(
+        CcTest::heap(), i::Heap::ScanStackMode::kNone);
     i_isolate->heap()->PreciseCollectAllGarbage(
         i::Heap::kNoGCFlags, i::GarbageCollectionReason::kRuntime);
   }
@@ -25591,6 +25589,10 @@
   v8::Isolate* isolate = CcTest::isolate();
   WeakCallCounter counter(1234);
 
+  // Conservative stack scanning might break results.
+  i::ScanStackModeScopeForTesting no_stack_scanning(
+      CcTest::heap(), i::Heap::ScanStackMode::kNone);
+
   // Check that critical memory pressure notification sets GC interrupt.
   auto garbage = CreateGarbageWithWeakCallCounter(isolate, &counter);
   CHECK(!v8::Locker::IsLocked(isolate));
@@ -26748,8 +26750,6 @@
 namespace internal {
 namespace wasm {
 TEST(WasmI32AtomicWaitCallback) {
-  FlagScope<bool> wasm_threads_flag(&i::v8_flags.experimental_wasm_threads,
-                                    true);
   WasmRunner<int32_t, int32_t, int32_t, double> r(TestExecutionTier::kTurbofan);
   r.builder().AddMemory(kWasmPageSize, SharedFlag::kShared);
   r.builder().SetHasSharedMemory();
@@ -26785,8 +26785,6 @@
 }
 
 TEST(WasmI64AtomicWaitCallback) {
-  FlagScope<bool> wasm_threads_flag(&i::v8_flags.experimental_wasm_threads,
-                                    true);
   WasmRunner<int32_t, int32_t, double, double> r(TestExecutionTier::kTurbofan);
   r.builder().AddMemory(kWasmPageSize, SharedFlag::kShared);
   r.builder().SetHasSharedMemory();
@@ -27456,9 +27454,11 @@
   v8::Local<v8::Context> context =
       v8::Local<v8::Context>::New(isolate_2, context_2);
   v8::Context::Scope context_scope(context);
-  reinterpret_cast<i::Isolate*>(isolate_2)->heap()->CollectAllGarbage(
-      i::Heap::kForcedGC, i::GarbageCollectionReason::kTesting,
-      v8::kNoGCCallbackFlags);
+  i::Heap* heap_2 = reinterpret_cast<i::Isolate*>(isolate_2)->heap();
+  i::ScanStackModeScopeForTesting no_stack_scanning(
+      heap_2, i::Heap::ScanStackMode::kNone);
+  heap_2->CollectAllGarbage(i::Heap::kForcedGC,
+                            i::GarbageCollectionReason::kTesting);
   CompileRun("f2() //# sourceURL=isolate2b");
 }
 
@@ -29316,6 +29316,20 @@
   CHECK_EQ(context->GetMicrotaskQueue(), microtask_queue.get());
 }
 
+THREADED_TEST(SetMicrotaskQueueOfContext) {
+  auto microtask_queue = v8::MicrotaskQueue::New(CcTest::isolate());
+  v8::HandleScope scope(CcTest::isolate());
+  v8::Local<Context> context = Context::New(
+      CcTest::isolate(), nullptr, v8::MaybeLocal<ObjectTemplate>(),
+      v8::MaybeLocal<Value>(), v8::DeserializeInternalFieldsCallback(),
+      microtask_queue.get());
+  CHECK_EQ(context->GetMicrotaskQueue(), microtask_queue.get());
+
+  auto new_microtask_queue = v8::MicrotaskQueue::New(CcTest::isolate());
+  context->SetMicrotaskQueue(new_microtask_queue.get());
+  CHECK_EQ(context->GetMicrotaskQueue(), new_microtask_queue.get());
+}
+
 namespace {
 
 bool sab_constructor_enabled_value = false;
diff -r -u --color up/v8/test/cctest/test-code-stub-assembler.cc nw/v8/test/cctest/test-code-stub-assembler.cc
--- up/v8/test/cctest/test-code-stub-assembler.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/test-code-stub-assembler.cc	2023-01-19 16:46:36.471442835 -0500
@@ -1750,7 +1750,7 @@
     {
       std::shared_ptr<v8::BackingStore> backing_store =
           buffer->GetBackingStore();
-      buffer->Detach();
+      buffer->Detach(v8::Local<v8::Value>()).Check();
     }
     CHECK_ABSENT(object, 0);
     CHECK_ABSENT(object, 1);
diff -r -u --color up/v8/test/cctest/test-cpu-profiler.cc nw/v8/test/cctest/test-cpu-profiler.cc
--- up/v8/test/cctest/test-cpu-profiler.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/test-cpu-profiler.cc	2023-01-19 16:46:36.471442835 -0500
@@ -33,6 +33,7 @@
 #include "include/libplatform/v8-tracing.h"
 #include "include/v8-fast-api-calls.h"
 #include "include/v8-function.h"
+#include "include/v8-json.h"
 #include "include/v8-locker.h"
 #include "include/v8-profiler.h"
 #include "src/api/api-inl.h"
@@ -53,6 +54,7 @@
 #include "src/utils/utils.h"
 #include "test/cctest/cctest.h"
 #include "test/cctest/heap/heap-utils.h"
+#include "test/cctest/jsonstream-helper.h"
 #include "test/cctest/profiler-extension.h"
 #include "test/common/flag-utils.h"
 
@@ -4705,6 +4707,64 @@
   CHECK_GT(profiler.GetEstimatedMemoryUsage(), 0);
 }
 
+TEST(CpuProfileJSONSerialization) {
+  LocalContext env;
+  v8::HandleScope scope(env->GetIsolate());
+  v8::CpuProfiler* cpu_profiler = v8::CpuProfiler::New(env->GetIsolate());
+
+  v8::Local<v8::String> name = v8_str("1");
+  cpu_profiler->StartProfiling(name);
+  v8::CpuProfile* profile = cpu_profiler->StopProfiling(name);
+  CHECK(profile);
+
+  TestJSONStream stream;
+  profile->Serialize(&stream, v8::CpuProfile::kJSON);
+  profile->Delete();
+  cpu_profiler->Dispose();
+  CHECK_GT(stream.size(), 0);
+  CHECK_EQ(1, stream.eos_signaled());
+  base::ScopedVector<char> json(stream.size());
+  stream.WriteTo(json);
+
+  // Verify that snapshot string is valid JSON.
+  OneByteResource* json_res = new OneByteResource(json);
+  v8::Local<v8::String> json_string =
+      v8::String::NewExternalOneByte(env->GetIsolate(), json_res)
+          .ToLocalChecked();
+  v8::Local<v8::Context> context = v8::Context::New(env->GetIsolate());
+  v8::Local<v8::Value> profile_parse_result =
+      v8::JSON::Parse(context, json_string).ToLocalChecked();
+
+  CHECK(!profile_parse_result.IsEmpty());
+  CHECK(profile_parse_result->IsObject());
+
+  v8::Local<v8::Object> profile_obj = profile_parse_result.As<v8::Object>();
+  CHECK(profile_obj->Get(env.local(), v8_str("nodes"))
+            .ToLocalChecked()
+            ->IsArray());
+  CHECK(profile_obj->Get(env.local(), v8_str("startTime"))
+            .ToLocalChecked()
+            ->IsNumber());
+  CHECK(profile_obj->Get(env.local(), v8_str("endTime"))
+            .ToLocalChecked()
+            ->IsNumber());
+  CHECK(profile_obj->Get(env.local(), v8_str("samples"))
+            .ToLocalChecked()
+            ->IsArray());
+  CHECK(profile_obj->Get(env.local(), v8_str("timeDeltas"))
+            .ToLocalChecked()
+            ->IsArray());
+
+  CHECK(profile_obj->Get(env.local(), v8_str("startTime"))
+            .ToLocalChecked()
+            .As<v8::Number>()
+            ->Value() > 0);
+  CHECK(profile_obj->Get(env.local(), v8_str("endTime"))
+            .ToLocalChecked()
+            .As<v8::Number>()
+            ->Value() > 0);
+}
+
 }  // namespace test_cpu_profiler
 }  // namespace internal
 }  // namespace v8
diff -r -u --color up/v8/test/cctest/test-debug.cc nw/v8/test/cctest/test-debug.cc
--- up/v8/test/cctest/test-debug.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/test-debug.cc	2023-01-19 16:46:36.471442835 -0500
@@ -35,8 +35,10 @@
 #include "src/base/strings.h"
 #include "src/codegen/compilation-cache.h"
 #include "src/debug/debug-interface.h"
+#include "src/debug/debug-scopes.h"
 #include "src/debug/debug.h"
 #include "src/deoptimizer/deoptimizer.h"
+#include "src/execution/frames-inl.h"
 #include "src/execution/microtask-queue.h"
 #include "src/objects/objects-inl.h"
 #include "src/utils/utils.h"
@@ -113,7 +115,7 @@
                                    bool uncaught) {
   v8::internal::Debug* debug =
       reinterpret_cast<v8::internal::Isolate*>(isolate)->debug();
-  debug->ChangeBreakOnException(v8::internal::BreakException, caught);
+  debug->ChangeBreakOnException(v8::internal::BreakCaughtException, caught);
   debug->ChangeBreakOnException(v8::internal::BreakUncaughtException, uncaught);
 }
 
@@ -1400,6 +1402,73 @@
   CheckDebuggerUnloaded();
 }
 
+TEST(BreakPointOnLazyAccessorInNewContexts) {
+  // Check that breakpoints on a lazy accessor still get hit after creating new
+  // contexts.
+  // Regression test for parts of http://crbug.com/1368554.
+  v8::Isolate* isolate = CcTest::isolate();
+  v8::HandleScope scope(isolate);
+
+  DebugEventCounter delegate;
+  v8::debug::SetDebugDelegate(isolate, &delegate);
+
+  auto accessor_tmpl = v8::FunctionTemplate::New(isolate, NoOpFunctionCallback);
+  accessor_tmpl->SetClassName(v8_str("get f"));
+  auto object_tmpl = v8::ObjectTemplate::New(isolate);
+  object_tmpl->SetAccessorProperty(v8_str("f"), accessor_tmpl);
+
+  {
+    v8::Local<v8::Context> context1 = v8::Context::New(isolate);
+    context1->Global()
+        ->Set(context1, v8_str("o"),
+              object_tmpl->NewInstance(context1).ToLocalChecked())
+        .ToChecked();
+    v8::Context::Scope context_scope(context1);
+
+    // 1. Set the breakpoint
+    v8::Local<v8::Function> function =
+        CompileRun(context1, "Object.getOwnPropertyDescriptor(o, 'f').get")
+            .ToLocalChecked()
+            .As<v8::Function>();
+    SetBreakPoint(function, 0);
+
+    // 2. Run and check that we hit the breakpoint
+    break_point_hit_count = 0;
+    CompileRun(context1, "o.f");
+    CHECK_EQ(1, break_point_hit_count);
+  }
+
+  {
+    // Create a second context and check that we also hit the breakpoint
+    // without setting it again.
+    v8::Local<v8::Context> context2 = v8::Context::New(isolate);
+    context2->Global()
+        ->Set(context2, v8_str("o"),
+              object_tmpl->NewInstance(context2).ToLocalChecked())
+        .ToChecked();
+    v8::Context::Scope context_scope(context2);
+
+    CompileRun(context2, "o.f");
+    CHECK_EQ(2, break_point_hit_count);
+  }
+
+  {
+    // Create a third context, but this time we use a global template instead
+    // and let the bootstrapper initialize "o" instead.
+    auto global_tmpl = v8::ObjectTemplate::New(isolate);
+    global_tmpl->Set(v8_str("o"), object_tmpl);
+    v8::Local<v8::Context> context3 =
+        v8::Context::New(isolate, nullptr, global_tmpl);
+    v8::Context::Scope context_scope(context3);
+
+    CompileRun(context3, "o.f");
+    CHECK_EQ(3, break_point_hit_count);
+  }
+
+  v8::debug::SetDebugDelegate(isolate, nullptr);
+  CheckDebuggerUnloaded();
+}
+
 TEST(BreakPointInlineApiFunction) {
   i::v8_flags.allow_natives_syntax = true;
   LocalContext env;
@@ -5337,7 +5406,7 @@
 static void MicrotaskOne(const v8::FunctionCallbackInfo<v8::Value>& info) {
   CHECK(v8::MicrotasksScope::IsRunningMicrotasks(info.GetIsolate()));
   v8::HandleScope scope(info.GetIsolate());
-  v8::MicrotasksScope microtasks(info.GetIsolate(),
+  v8::MicrotasksScope microtasks(info.GetIsolate()->GetCurrentContext(),
                                  v8::MicrotasksScope::kDoNotRunMicrotasks);
   ExpectInt32("1 + 1", 2);
   microtask_one_ran = true;
@@ -5846,3 +5915,100 @@
   CHECK(!message.IsEmpty());
   CHECK(message->GetStackTrace().IsEmpty());
 }
+
+namespace {
+
+class ScopeListener : public v8::debug::DebugDelegate {
+ public:
+  void BreakProgramRequested(v8::Local<v8::Context> context,
+                             const std::vector<v8::debug::BreakpointId>&,
+                             v8::debug::BreakReasons break_reasons) override {
+    i::Isolate* isolate = CcTest::i_isolate();
+    i::StackTraceFrameIterator iterator_(isolate,
+                                         isolate->debug()->break_frame_id());
+    // Go up one frame so we are on the script level.
+    iterator_.Advance();
+
+    auto frame_inspector =
+        std::make_unique<i::FrameInspector>(iterator_.frame(), 0, isolate);
+    i::ScopeIterator scope_iterator(
+        isolate, frame_inspector.get(),
+        i::ScopeIterator::ReparseStrategy::kScriptIfNeeded);
+
+    // Iterate all scopes triggering block list creation along the way. This
+    // should not run into any CHECKs.
+    while (!scope_iterator.Done()) scope_iterator.Next();
+  }
+};
+
+}  // namespace
+
+TEST(ScopeIteratorDoesNotCreateBlocklistForScriptScope) {
+  i::v8_flags.experimental_reuse_locals_blocklists = true;
+
+  LocalContext env;
+  v8::Isolate* isolate = env->GetIsolate();
+  v8::HandleScope scope(isolate);
+
+  // Register a debug event listener which creates a ScopeIterator.
+  ScopeListener delegate;
+  v8::debug::SetDebugDelegate(isolate, &delegate);
+
+  CompileRun(R"javascript(
+    function foo() { debugger; }
+    foo();
+  )javascript");
+
+  // Get rid of the debug event listener.
+  v8::debug::SetDebugDelegate(isolate, nullptr);
+  CheckDebuggerUnloaded();
+}
+
+namespace {
+
+class DebugEvaluateListener : public v8::debug::DebugDelegate {
+ public:
+  void BreakProgramRequested(v8::Local<v8::Context> context,
+                             const std::vector<v8::debug::BreakpointId>&,
+                             v8::debug::BreakReasons break_reasons) override {
+    v8::Isolate* isolate = context->GetIsolate();
+    auto it = v8::debug::StackTraceIterator::Create(isolate);
+    v8::Local<v8::Value> result =
+        it->Evaluate(v8_str(isolate, "x"), /* throw_on_side_effect */ false)
+            .ToLocalChecked();
+    CHECK_EQ(42, result->ToInteger(context).ToLocalChecked()->Value());
+  }
+};
+
+}  // namespace
+
+// This test checks that the debug-evaluate blocklist logic correctly handles
+// scopes created by `ScriptCompiler::CompileFunction`. It creates a function
+// scope nested inside an eval scope with the exact same source positions.
+// This can confuse the blocklist mechanism if not handled correctly.
+TEST(DebugEvaluateInWrappedScript) {
+  i::v8_flags.experimental_reuse_locals_blocklists = true;
+
+  LocalContext env;
+  v8::Isolate* isolate = env->GetIsolate();
+  v8::HandleScope scope(isolate);
+
+  // Register a debug event listener which evaluates 'x'.
+  DebugEvaluateListener delegate;
+  v8::debug::SetDebugDelegate(isolate, &delegate);
+
+  static const char* source = "const x = 42; () => x; debugger;";
+
+  {
+    v8::ScriptCompiler::Source script_source(v8_str(source));
+    v8::Local<v8::Function> fun =
+        v8::ScriptCompiler::CompileFunction(env.local(), &script_source)
+            .ToLocalChecked();
+
+    fun->Call(env.local(), env->Global(), 0, nullptr).ToLocalChecked();
+  }
+
+  // Get rid of the debug event listener.
+  v8::debug::SetDebugDelegate(env->GetIsolate(), nullptr);
+  CheckDebuggerUnloaded();
+}
diff -r -u --color up/v8/test/cctest/test-heap-profiler.cc nw/v8/test/cctest/test-heap-profiler.cc
--- up/v8/test/cctest/test-heap-profiler.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/test-heap-profiler.cc	2023-01-19 16:46:36.471442835 -0500
@@ -32,6 +32,7 @@
 #include <memory>
 
 #include "include/v8-function.h"
+#include "include/v8-json.h"
 #include "include/v8-profiler.h"
 #include "src/api/api-inl.h"
 #include "src/base/hashmap.h"
@@ -48,6 +49,7 @@
 #include "test/cctest/cctest.h"
 #include "test/cctest/collector.h"
 #include "test/cctest/heap/heap-utils.h"
+#include "test/cctest/jsonstream-helper.h"
 
 using i::AllocationTraceNode;
 using i::AllocationTraceTree;
@@ -1043,52 +1045,6 @@
   CHECK_EQ(b1->GetId(), b2->GetId());
 }
 
-namespace {
-
-class TestJSONStream : public v8::OutputStream {
- public:
-  TestJSONStream() : eos_signaled_(0), abort_countdown_(-1) {}
-  explicit TestJSONStream(int abort_countdown)
-      : eos_signaled_(0), abort_countdown_(abort_countdown) {}
-  ~TestJSONStream() override = default;
-  void EndOfStream() override { ++eos_signaled_; }
-  WriteResult WriteAsciiChunk(char* buffer, int chars_written) override {
-    if (abort_countdown_ > 0) --abort_countdown_;
-    if (abort_countdown_ == 0) return kAbort;
-    CHECK_GT(chars_written, 0);
-    v8::base::Vector<char> chunk = buffer_.AddBlock(chars_written, '\0');
-    i::MemCopy(chunk.begin(), buffer, chars_written);
-    return kContinue;
-  }
-  virtual WriteResult WriteUint32Chunk(uint32_t* buffer, int chars_written) {
-    UNREACHABLE();
-  }
-  void WriteTo(v8::base::Vector<char> dest) { buffer_.WriteTo(dest); }
-  int eos_signaled() { return eos_signaled_; }
-  int size() { return buffer_.size(); }
-
- private:
-  i::Collector<char> buffer_;
-  int eos_signaled_;
-  int abort_countdown_;
-};
-
-class OneByteResource : public v8::String::ExternalOneByteStringResource {
- public:
-  explicit OneByteResource(v8::base::Vector<char> string)
-      : data_(string.begin()) {
-    length_ = string.length();
-  }
-  const char* data() const override { return data_; }
-  size_t length() const override { return length_; }
-
- private:
-  const char* data_;
-  size_t length_;
-};
-
-}  // namespace
-
 TEST(HeapSnapshotJSONSerialization) {
   v8::Isolate* isolate = CcTest::isolate();
   LocalContext env;
@@ -1105,7 +1061,7 @@
   const v8::HeapSnapshot* snapshot = heap_profiler->TakeHeapSnapshot();
   CHECK(ValidateSnapshot(snapshot));
 
-  TestJSONStream stream;
+  v8::internal::TestJSONStream stream;
   snapshot->Serialize(&stream, v8::HeapSnapshot::kJSON);
   CHECK_GT(stream.size(), 0);
   CHECK_EQ(1, stream.eos_signaled());
@@ -1113,31 +1069,37 @@
   stream.WriteTo(json);
 
   // Verify that snapshot string is valid JSON.
-  OneByteResource* json_res = new OneByteResource(json);
+  v8::internal::OneByteResource* json_res =
+      new v8::internal::OneByteResource(json);
   v8::Local<v8::String> json_string =
       v8::String::NewExternalOneByte(env->GetIsolate(), json_res)
           .ToLocalChecked();
-  env->Global()
-      ->Set(env.local(), v8_str("json_snapshot"), json_string)
-      .FromJust();
-  v8::Local<v8::Value> snapshot_parse_result = CompileRun(
-      "var parsed = JSON.parse(json_snapshot); true;");
-  CHECK(!snapshot_parse_result.IsEmpty());
+  v8::Local<v8::Context> context = v8::Context::New(env->GetIsolate());
+  v8::Local<v8::Value> snapshot_parse_result =
+      v8::JSON::Parse(context, json_string).ToLocalChecked();
+  CHECK(snapshot_parse_result->IsObject());
 
   // Verify that snapshot object has required fields.
   v8::Local<v8::Object> parsed_snapshot =
-      env->Global()
-          ->Get(env.local(), v8_str("parsed"))
-          .ToLocalChecked()
-          ->ToObject(env.local())
-          .ToLocalChecked();
-  CHECK(parsed_snapshot->Has(env.local(), v8_str("snapshot")).FromJust());
-  CHECK(parsed_snapshot->Has(env.local(), v8_str("nodes")).FromJust());
-  CHECK(parsed_snapshot->Has(env.local(), v8_str("edges")).FromJust());
-  CHECK(parsed_snapshot->Has(env.local(), v8_str("locations")).FromJust());
-  CHECK(parsed_snapshot->Has(env.local(), v8_str("strings")).FromJust());
+      snapshot_parse_result.As<v8::Object>();
+  CHECK(parsed_snapshot->Get(env.local(), v8_str("snapshot"))
+            .ToLocalChecked()
+            ->IsObject());
+  CHECK(parsed_snapshot->Get(env.local(), v8_str("nodes"))
+            .ToLocalChecked()
+            ->IsArray());
+  CHECK(parsed_snapshot->Get(env.local(), v8_str("edges"))
+            .ToLocalChecked()
+            ->IsArray());
+  CHECK(parsed_snapshot->Get(env.local(), v8_str("locations"))
+            .ToLocalChecked()
+            ->IsArray());
+  CHECK(parsed_snapshot->Get(env.local(), v8_str("strings"))
+            .ToLocalChecked()
+            ->IsArray());
 
   // Get node and edge "member" offsets.
+  env->Global()->Set(env.local(), v8_str("parsed"), parsed_snapshot).FromJust();
   v8::Local<v8::Value> meta_analysis_result = CompileRun(
       "var meta = parsed.snapshot.meta;\n"
       "var edge_count_offset = meta.node_fields.indexOf('edge_count');\n"
@@ -1224,7 +1186,7 @@
   v8::HeapProfiler* heap_profiler = env->GetIsolate()->GetHeapProfiler();
   const v8::HeapSnapshot* snapshot = heap_profiler->TakeHeapSnapshot();
   CHECK(ValidateSnapshot(snapshot));
-  TestJSONStream stream(5);
+  v8::internal::TestJSONStream stream(5);
   snapshot->Serialize(&stream, v8::HeapSnapshot::kJSON);
   CHECK_GT(stream.size(), 0);
   CHECK_EQ(0, stream.eos_signaled());
@@ -1296,8 +1258,10 @@
 
 
 TEST(HeapSnapshotObjectsStats) {
-  // Concurrent allocation might break results
+  // Concurrent allocation and conservative stack scanning might break results.
   i::v8_flags.stress_concurrent_allocation = false;
+  i::ScanStackModeScopeForTesting no_stack_scanning(
+      CcTest::heap(), i::Heap::ScanStackMode::kNone);
 
   LocalContext env;
   v8::HandleScope scope(env->GetIsolate());
@@ -2771,13 +2735,12 @@
   CHECK(ValidateSnapshot(snapshot));
 
   const char* builtin_path1[] = {
-    "::(GC roots)",
-    "::(Builtins)",
+      "::(GC roots)",
+      "::(Builtins)",
 #ifdef V8_EXTERNAL_CODE_SPACE
-    "::(KeyedLoadIC_PolymorphicName builtin handle)",
-#endif
-#if !V8_REMOVE_BUILTINS_CODE_OBJECTS
-    "::(KeyedLoadIC_PolymorphicName builtin)"
+      "::(KeyedLoadIC_PolymorphicName builtin handle)",
+#else
+      "::(KeyedLoadIC_PolymorphicName builtin)",
 #endif
   };
   const v8::HeapGraphNode* node = GetNodeByPath(
@@ -2785,20 +2748,19 @@
   CHECK(node);
 
   const char* builtin_path2[] = {
-    "::(GC roots)",
-    "::(Builtins)",
+      "::(GC roots)",
+      "::(Builtins)",
 #ifdef V8_EXTERNAL_CODE_SPACE
-    "::(CompileLazy builtin handle)",
-#endif
-#if !V8_REMOVE_BUILTINS_CODE_OBJECTS
-    "::(CompileLazy builtin)"
+      "::(CompileLazy builtin handle)",
+#else
+      "::(CompileLazy builtin)",
 #endif
   };
   node = GetNodeByPath(env->GetIsolate(), snapshot, builtin_path2,
                        arraysize(builtin_path2));
   CHECK(node);
   v8::String::Utf8Value node_name(env->GetIsolate(), node->GetName());
-  if (V8_REMOVE_BUILTINS_CODE_OBJECTS) {
+  if (V8_EXTERNAL_CODE_SPACE_BOOL) {
     CHECK_EQ(0, strcmp("(CompileLazy builtin handle)", *node_name));
   } else {
     CHECK_EQ(0, strcmp("(CompileLazy builtin)", *node_name));
diff -r -u --color up/v8/test/cctest/test-serialize.cc nw/v8/test/cctest/test-serialize.cc
--- up/v8/test/cctest/test-serialize.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/test-serialize.cc	2023-01-19 16:46:36.482276165 -0500
@@ -210,7 +210,7 @@
   Isolate* i_isolate = reinterpret_cast<Isolate*>(isolate);
   CcTest::CollectAllAvailableGarbage(i_isolate);
 
-  SafepointScope safepoint(i_isolate->heap());
+  IsolateSafepointScope safepoint(i_isolate->heap());
   HandleScope scope(i_isolate);
 
   DisallowGarbageCollection no_gc;
@@ -403,7 +403,7 @@
 
     env.Reset();
 
-    SafepointScope safepoint(heap);
+    IsolateSafepointScope safepoint(heap);
 
     DisallowGarbageCollection no_gc;
     SnapshotByteSink read_only_sink;
@@ -572,7 +572,7 @@
 
     env.Reset();
 
-    SafepointScope safepoint(isolate->heap());
+    IsolateSafepointScope safepoint(isolate->heap());
 
     DisallowGarbageCollection no_gc;
     SnapshotByteSink read_only_sink;
@@ -5113,8 +5113,15 @@
   Isolate* i_isolate2 = reinterpret_cast<Isolate*>(isolate2);
 
   CHECK_EQ(i_isolate1->string_table(), i_isolate2->string_table());
-  CheckObjectsAreInSharedHeap(i_isolate1);
-  CheckObjectsAreInSharedHeap(i_isolate2);
+  {
+    ParkedScope parked(i_isolate2->main_thread_local_heap());
+    CheckObjectsAreInSharedHeap(i_isolate1);
+  }
+
+  {
+    ParkedScope parked(i_isolate1->main_thread_local_heap());
+    CheckObjectsAreInSharedHeap(i_isolate2);
+  }
 
   {
     // Because both isolate1 and isolate2 are considered running on the main
@@ -5133,5 +5140,110 @@
   FreeCurrentEmbeddedBlob();
 }
 
+namespace {
+
+class DebugBreakCounter : public v8::debug::DebugDelegate {
+ public:
+  void BreakProgramRequested(v8::Local<v8::Context>,
+                             const std::vector<v8::debug::BreakpointId>&,
+                             v8::debug::BreakReasons break_reasons) override {
+    break_point_hit_count_++;
+  }
+
+  int break_point_hit_count() const { return break_point_hit_count_; }
+
+ private:
+  int break_point_hit_count_ = 0;
+};
+
+}  // namespace
+
+UNINITIALIZED_TEST(BreakPointAccessorContextSnapshot) {
+  // Tests that a breakpoint set in one deserialized context also gets hit in
+  // another for lazy accessors.
+  DisableAlwaysOpt();
+  DisableEmbeddedBlobRefcounting();
+  v8::StartupData blob;
+
+  {
+    v8::SnapshotCreator creator(original_external_references);
+    v8::Isolate* isolate = creator.GetIsolate();
+    {
+      // Add a context to the snapshot that adds an object with an accessor to
+      // the global template.
+      v8::HandleScope scope(isolate);
+
+      auto accessor_tmpl =
+          v8::FunctionTemplate::New(isolate, SerializedCallback);
+      accessor_tmpl->SetClassName(v8_str("get f"));
+      auto object_tmpl = v8::ObjectTemplate::New(isolate);
+      object_tmpl->SetAccessorProperty(v8_str("f"), accessor_tmpl);
+
+      auto global_tmpl = v8::ObjectTemplate::New(isolate);
+      global_tmpl->Set(v8_str("o"), object_tmpl);
+
+      creator.SetDefaultContext(v8::Context::New(isolate));
+
+      v8::Local<v8::Context> context =
+          v8::Context::New(isolate, nullptr, global_tmpl);
+      creator.AddContext(context);
+    }
+    blob =
+        creator.CreateBlob(v8::SnapshotCreator::FunctionCodeHandling::kClear);
+  }
+
+  v8::Isolate::CreateParams params;
+  params.snapshot_blob = &blob;
+  params.array_buffer_allocator = CcTest::array_buffer_allocator();
+  params.external_references = original_external_references;
+  // Test-appropriate equivalent of v8::Isolate::New.
+  v8::Isolate* isolate = TestSerializer::NewIsolate(params);
+  {
+    v8::Isolate::Scope isolate_scope(isolate);
+
+    DebugBreakCounter delegate;
+    v8::debug::SetDebugDelegate(isolate, &delegate);
+
+    {
+      // Create a new context from the snapshot, put a breakpoint on the
+      // accessor and make sure we hit the breakpoint.
+      v8::HandleScope scope(isolate);
+      v8::Local<v8::Context> context =
+          v8::Context::FromSnapshot(isolate, 0).ToLocalChecked();
+      v8::Context::Scope context_scope(context);
+
+      // 1. Set the breakpoint
+      v8::Local<v8::Function> function =
+          CompileRun(context, "Object.getOwnPropertyDescriptor(o, 'f').get")
+              .ToLocalChecked()
+              .As<v8::Function>();
+      debug::BreakpointId id;
+      debug::SetFunctionBreakpoint(function, v8::Local<v8::String>(), &id);
+
+      // 2. Run and check that we hit the breakpoint
+      CompileRun(context, "o.f");
+      CHECK_EQ(1, delegate.break_point_hit_count());
+    }
+
+    {
+      // Create a second context from the snapshot and make sure we still hit
+      // the breakpoint without setting it again.
+      v8::HandleScope scope(isolate);
+      v8::Local<v8::Context> context =
+          v8::Context::FromSnapshot(isolate, 0).ToLocalChecked();
+      v8::Context::Scope context_scope(context);
+
+      CompileRun(context, "o.f");
+      CHECK_EQ(2, delegate.break_point_hit_count());
+    }
+
+    v8::debug::SetDebugDelegate(isolate, nullptr);
+  }
+
+  isolate->Dispose();
+  delete[] blob.data;
+  FreeCurrentEmbeddedBlob();
+}
+
 }  // namespace internal
 }  // namespace v8
diff -r -u --color up/v8/test/cctest/wasm/test-backing-store.cc nw/v8/test/cctest/wasm/test-backing-store.cc
--- up/v8/test/cctest/wasm/test-backing-store.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/wasm/test-backing-store.cc	2023-01-19 16:46:36.482276165 -0500
@@ -28,7 +28,7 @@
     // Embedder requests contents.
     ManuallyExternalizedBuffer external(buffer);
 
-    buffer->Detach();
+    JSArrayBuffer::Detach(buffer).Check();
     CHECK(buffer->was_detached());
 
     // Make sure we can write to the buffer without crashing
diff -r -u --color up/v8/test/cctest/wasm/test-gc.cc nw/v8/test/cctest/wasm/test-gc.cc
--- up/v8/test/cctest/wasm/test-gc.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/wasm/test-gc.cc	2023-01-19 16:46:36.482276165 -0500
@@ -571,7 +571,7 @@
   const byte other_type_index = tester.DefineStruct({F(kWasmF32, true)});
 
   const byte kTestStructStatic = tester.DefineFunction(
-      tester.sigs.i_v(), {kWasmI32, kWasmDataRef},
+      tester.sigs.i_v(), {kWasmI32, kWasmStructRef},
       {WASM_BLOCK_R(
            ValueType::RefNull(type_index), WASM_LOCAL_SET(0, WASM_I32V(111)),
            // Pipe a struct through a local so it's statically typed
@@ -592,19 +592,30 @@
        WASM_GC_OP(kExprStructGet), type_index, 0, WASM_LOCAL_GET(0),
        kExprI32Add, kExprEnd});
 
-  const byte kTestNull = tester.DefineFunction(
-      tester.sigs.i_v(), {kWasmI32, kWasmDataRef},
-      {WASM_BLOCK_R(
-           ValueType::RefNull(type_index), WASM_LOCAL_SET(0, WASM_I32V(111)),
-           WASM_LOCAL_GET(1),  // Put a nullref onto the value stack.
-           // Not taken for nullref.
-           WASM_BR_ON_CAST(0, type_index), WASM_GC_OP(kExprRefCast), type_index,
+  const byte kTestNullDeprecated = tester.DefineFunction(
+      tester.sigs.i_v(), {kWasmI32, kWasmStructRef},
+      {WASM_BLOCK_R(ValueType::RefNull(type_index),
+                    WASM_LOCAL_SET(0, WASM_I32V(111)),
+                    WASM_LOCAL_GET(1),  // Put a nullref onto the value stack.
+                    // Not taken for nullref.
+                    WASM_BR_ON_CAST(0, type_index),
+                    WASM_GC_OP(kExprRefCastDeprecated), type_index,
 
-           WASM_LOCAL_SET(0, WASM_I32V(222))),  // Final result.
+                    WASM_LOCAL_SET(0, WASM_I32V(222))),  // Final result.
+       WASM_DROP, WASM_LOCAL_GET(0), kExprEnd});
+
+  const byte kTestNull = tester.DefineFunction(
+      tester.sigs.i_v(), {kWasmI32, kWasmStructRef},
+      {WASM_BLOCK_R(ValueType::RefNull(type_index),
+                    WASM_LOCAL_SET(0, WASM_I32V(111)),
+                    WASM_LOCAL_GET(1),  // Put a nullref onto the value stack.
+                    // Not taken for nullref.
+                    WASM_BR_ON_CAST(0, type_index), WASM_GC_OP(kExprRefCast),
+                    type_index),  // Traps
        WASM_DROP, WASM_LOCAL_GET(0), kExprEnd});
 
   const byte kTypedAfterBranch = tester.DefineFunction(
-      tester.sigs.i_v(), {kWasmI32, kWasmDataRef},
+      tester.sigs.i_v(), {kWasmI32, kWasmStructRef},
       {WASM_LOCAL_SET(1, WASM_STRUCT_NEW(type_index, WASM_I32V(42))),
        WASM_BLOCK_I(
            // The inner block should take the early branch with a struct
@@ -620,7 +631,8 @@
 
   tester.CompileModule();
   tester.CheckResult(kTestStructStatic, 222);
-  tester.CheckResult(kTestNull, 222);
+  tester.CheckResult(kTestNullDeprecated, 222);
+  tester.CheckHasThrown(kTestNull);
   tester.CheckResult(kTypedAfterBranch, 42);
 }
 
@@ -645,7 +657,7 @@
 #define FUNCTION_BODY(value)                                               \
   WASM_LOCAL_SET(0, WASM_SEQ(value)),                                      \
       WASM_BLOCK(                                                          \
-          WASM_BLOCK_R(kWasmDataRef, WASM_LOCAL_GET(0),                    \
+          WASM_BLOCK_R(kWasmStructRef, WASM_LOCAL_GET(0),                  \
                        WASM_BR_ON_CAST_FAIL(0, type0),                     \
                        WASM_GC_OP(kExprStructGet), type0, 0, kExprReturn), \
           kExprBrOnNull, 0, WASM_GC_OP(kExprRefCast), type1,               \
@@ -653,16 +665,17 @@
       WASM_I32V(null_value), kExprEnd
 
   const byte kBranchTaken = tester.DefineFunction(
-      tester.sigs.i_v(), {kWasmDataRef},
+      tester.sigs.i_v(), {kWasmStructRef},
       {FUNCTION_BODY(
           WASM_STRUCT_NEW(type1, WASM_I64V(10), WASM_I32V(field1_value)))});
 
   const byte kBranchNotTaken = tester.DefineFunction(
-      tester.sigs.i_v(), {kWasmDataRef},
+      tester.sigs.i_v(), {kWasmStructRef},
       {FUNCTION_BODY(WASM_STRUCT_NEW(type0, WASM_I32V(field0_value)))});
 
-  const byte kNull = tester.DefineFunction(
-      tester.sigs.i_v(), {kWasmDataRef}, {FUNCTION_BODY(WASM_REF_NULL(type0))});
+  const byte kNull =
+      tester.DefineFunction(tester.sigs.i_v(), {kWasmStructRef},
+                            {FUNCTION_BODY(WASM_REF_NULL(type0))});
 
   const byte kUnrelatedTypes = tester.DefineFunction(
       tester.sigs.i_v(), {ValueType::RefNull(type1)},
@@ -671,11 +684,11 @@
 #undef FUNCTION_BODY
 
   const byte kBranchTakenStatic = tester.DefineFunction(
-      tester.sigs.i_v(), {kWasmDataRef},
+      tester.sigs.i_v(), {kWasmStructRef},
       {WASM_LOCAL_SET(
            0, WASM_STRUCT_NEW(type1, WASM_I64V(10), WASM_I32V(field1_value))),
        WASM_BLOCK(
-           WASM_BLOCK_R(kWasmDataRef, WASM_LOCAL_GET(0),
+           WASM_BLOCK_R(kWasmStructRef, WASM_LOCAL_GET(0),
                         WASM_BR_ON_CAST_FAIL(0, type0),
                         WASM_GC_OP(kExprStructGet), type0, 0, kExprReturn),
            kExprBrOnNull, 0, WASM_GC_OP(kExprRefCast), type1,
@@ -1212,6 +1225,11 @@
       {WASM_REF_TEST_DEPRECATED(WASM_STRUCT_NEW_DEFAULT(type_index), sig_index),
        kExprEnd});
 
+  const byte kRefCastNullDeprecated =
+      tester.DefineFunction(tester.sigs.i_v(), {},
+                            {WASM_REF_IS_NULL(WASM_REF_CAST_DEPRECATED(
+                                 WASM_REF_NULL(type_index), subtype_index)),
+                             kExprEnd});
   const byte kRefCastNull =
       tester.DefineFunction(tester.sigs.i_v(), {},
                             {WASM_REF_IS_NULL(WASM_REF_CAST(
@@ -1222,24 +1240,32 @@
       {WASM_REF_IS_NULL(
            WASM_REF_CAST(WASM_STRUCT_NEW_DEFAULT(subtype_index), type_index)),
        kExprEnd});
+  const byte kRefCastUpcastNullDeprecated =
+      tester.DefineFunction(tester.sigs.i_v(), {},
+                            {WASM_REF_IS_NULL(WASM_REF_CAST_DEPRECATED(
+                                 WASM_REF_NULL(subtype_index), type_index)),
+                             kExprEnd});
   const byte kRefCastUpcastNull =
       tester.DefineFunction(tester.sigs.i_v(), {},
                             {WASM_REF_IS_NULL(WASM_REF_CAST(
                                  WASM_REF_NULL(subtype_index), type_index)),
                              kExprEnd});
+  // Note: Casting of types from different type hierarchies is only valid for
+  // the deprecated cast instruction.
   const byte kRefCastUnrelatedNullable = tester.DefineFunction(
       tester.sigs.i_v(), {refNull(subtype_index)},
       {WASM_LOCAL_SET(0, WASM_STRUCT_NEW_DEFAULT(subtype_index)),
-       WASM_REF_IS_NULL(WASM_REF_CAST(WASM_LOCAL_GET(0), sig_index)),
-       kExprEnd});
-  const byte kRefCastUnrelatedNull = tester.DefineFunction(
-      tester.sigs.i_v(), {},
-      {WASM_REF_IS_NULL(WASM_REF_CAST(WASM_REF_NULL(subtype_index), sig_index)),
+       WASM_REF_IS_NULL(WASM_REF_CAST_DEPRECATED(WASM_LOCAL_GET(0), sig_index)),
        kExprEnd});
+  const byte kRefCastUnrelatedNull =
+      tester.DefineFunction(tester.sigs.i_v(), {},
+                            {WASM_REF_IS_NULL(WASM_REF_CAST_DEPRECATED(
+                                 WASM_REF_NULL(subtype_index), sig_index)),
+                             kExprEnd});
   const byte kRefCastUnrelatedNonNullable = tester.DefineFunction(
       tester.sigs.i_v(), {},
-      {WASM_REF_IS_NULL(
-           WASM_REF_CAST(WASM_STRUCT_NEW_DEFAULT(type_index), sig_index)),
+      {WASM_REF_IS_NULL(WASM_REF_CAST_DEPRECATED(
+           WASM_STRUCT_NEW_DEFAULT(type_index), sig_index)),
        kExprEnd});
 
   const byte kBrOnCastNull = tester.DefineFunction(
@@ -1339,9 +1365,11 @@
   tester.CheckResult(kRefTestUnrelatedNullDeprecated, 0);
   tester.CheckResult(kRefTestUnrelatedNonNullableDeprecated, 0);
 
-  tester.CheckResult(kRefCastNull, 1);
+  tester.CheckResult(kRefCastNullDeprecated, 1);
+  tester.CheckHasThrown(kRefCastNull);
   tester.CheckResult(kRefCastUpcast, 0);
-  tester.CheckResult(kRefCastUpcastNull, 1);
+  tester.CheckResult(kRefCastUpcastNullDeprecated, 1);
+  tester.CheckHasThrown(kRefCastUpcastNull);
   tester.CheckHasThrown(kRefCastUnrelatedNullable);
   tester.CheckResult(kRefCastUnrelatedNull, 1);
   tester.CheckHasThrown(kRefCastUnrelatedNonNullable);
@@ -1559,7 +1587,7 @@
       {kWasmFuncRef, kNoFuncCode},
       {kWasmEqRef, kNoneCode},
       {kWasmI31Ref.AsNullable(), kNoneCode},
-      {kWasmDataRef.AsNullable(), kNoneCode},
+      {kWasmStructRef.AsNullable(), kNoneCode},
       {kWasmArrayRef.AsNullable(), kNoneCode},
       {kWasmAnyRef, kNoneCode},
       {kWasmExternRef, kNoExternCode},
@@ -1602,14 +1630,15 @@
       tester.sigs.i_v(), {},
       {WASM_REF_IS_NULL(WASM_REF_AS_ARRAY(WASM_REF_NULL(kNoneCode))),
        kExprEnd});
-  byte to_data = tester.DefineFunction(
+  byte to_struct = tester.DefineFunction(
       tester.sigs.i_v(), {},
-      {WASM_REF_IS_NULL(WASM_REF_AS_DATA(WASM_REF_NULL(kNoneCode))), kExprEnd});
+      {WASM_REF_IS_NULL(WASM_REF_AS_STRUCT(WASM_REF_NULL(kNoneCode))),
+       kExprEnd});
   byte to_i31 = tester.DefineFunction(
       tester.sigs.i_v(), {},
       {WASM_REF_IS_NULL(WASM_REF_AS_I31(WASM_REF_NULL(kNoneCode))), kExprEnd});
   byte struct_idx = tester.DefineStruct({F(wasm::kWasmI32, true)});
-  byte to_struct = tester.DefineFunction(
+  byte to_struct_idx = tester.DefineFunction(
       tester.sigs.i_v(), {},
       {WASM_REF_IS_NULL(WASM_REF_CAST(WASM_REF_NULL(kNoneCode), struct_idx)),
        kExprEnd});
@@ -1617,10 +1646,10 @@
   // Generic casts trap on null.
   tester.CheckHasThrown(to_non_null);
   tester.CheckHasThrown(to_array);
-  tester.CheckHasThrown(to_data);
+  tester.CheckHasThrown(to_struct);
   tester.CheckHasThrown(to_i31);
-  // Static ref.cast succeeds.
-  tester.CheckResult(to_struct, 1);
+  // ref.cast traps on null.
+  tester.CheckHasThrown(to_struct_idx);
 }
 
 WASM_COMPILED_EXEC_TEST(CallReftypeParameters) {
@@ -1673,9 +1702,9 @@
   tester.AddGlobal(ValueType::RefNull(sig_index), false,
                    WasmInitExpr::RefFuncConst(function_index));
 
-  byte kDataCheckNull = tester.DefineFunction(
+  byte kStructCheckNull = tester.DefineFunction(
       tester.sigs.i_v(), {},
-      {WASM_REF_IS_DATA(WASM_REF_NULL(kAnyRefCode)), kExprEnd});
+      {WASM_REF_IS_STRUCT(WASM_REF_NULL(kAnyRefCode)), kExprEnd});
   byte kArrayCheckNull = tester.DefineFunction(
       tester.sigs.i_v(), {},
       {WASM_REF_IS_ARRAY(WASM_REF_NULL(kAnyRefCode)), kExprEnd});
@@ -1683,9 +1712,9 @@
       tester.sigs.i_v(), {},
       {WASM_REF_IS_I31(WASM_REF_NULL(kAnyRefCode)), kExprEnd});
 
-  byte kDataCastNull =
+  byte kStructCastNull =
       tester.DefineFunction(tester.sigs.i_v(), {},
-                            {WASM_REF_AS_DATA(WASM_REF_NULL(kAnyRefCode)),
+                            {WASM_REF_AS_STRUCT(WASM_REF_NULL(kAnyRefCode)),
                              WASM_DROP, WASM_I32V(1), kExprEnd});
   byte kArrayCastNull =
       tester.DefineFunction(tester.sigs.i_v(), {},
@@ -1701,9 +1730,9 @@
                         {WASM_LOCAL_SET(0, WASM_SEQ(value)), \
                          WASM_REF_IS_##type(WASM_LOCAL_GET(0)), kExprEnd})
 
-  byte kDataCheckSuccess =
-      TYPE_CHECK(DATA, WASM_ARRAY_NEW_DEFAULT(array_index, WASM_I32V(10)));
-  byte kDataCheckFailure = TYPE_CHECK(DATA, WASM_I31_NEW(WASM_I32V(42)));
+  byte kStructCheckSuccess =
+      TYPE_CHECK(STRUCT, WASM_STRUCT_NEW_DEFAULT(struct_index));
+  byte kStructCheckFailure = TYPE_CHECK(STRUCT, WASM_I31_NEW(WASM_I32V(42)));
   byte kArrayCheckSuccess =
       TYPE_CHECK(ARRAY, WASM_ARRAY_NEW_DEFAULT(array_index, WASM_I32V(10)));
   byte kArrayCheckFailure =
@@ -1719,12 +1748,12 @@
                          WASM_REF_AS_##type(WASM_LOCAL_GET(0)), WASM_DROP, \
                          WASM_I32V(1), kExprEnd})
 
-  byte kDataCastSuccess =
-      TYPE_CAST(DATA, WASM_ARRAY_NEW_DEFAULT(array_index, WASM_I32V(10)));
-  byte kDataCastFailure = TYPE_CAST(DATA, WASM_I31_NEW(WASM_I32V(42)));
+  byte kStructCastSuccess =
+      TYPE_CAST(STRUCT, WASM_STRUCT_NEW_DEFAULT(struct_index));
+  byte kStructCastFailure = TYPE_CAST(STRUCT, WASM_I31_NEW(WASM_I32V(42)));
   byte kArrayCastSuccess =
-      TYPE_CAST(DATA, WASM_ARRAY_NEW_DEFAULT(array_index, WASM_I32V(10)));
-  byte kArrayCastFailure = TYPE_CAST(DATA, WASM_I31_NEW(WASM_I32V(42)));
+      TYPE_CAST(ARRAY, WASM_ARRAY_NEW_DEFAULT(array_index, WASM_I32V(10)));
+  byte kArrayCastFailure = TYPE_CAST(ARRAY, WASM_I31_NEW(WASM_I32V(42)));
   byte kI31CastSuccess = TYPE_CAST(I31, WASM_I31_NEW(WASM_I32V(42)));
   byte kI31CastFailure =
       TYPE_CAST(I31, WASM_ARRAY_NEW_DEFAULT(array_index, WASM_I32V(10)));
@@ -1741,9 +1770,9 @@
                                        WASM_RETURN(WASM_I32V(0)))),         \
        kExprEnd})
 
-  byte kBrOnDataTaken =
-      BR_ON(DATA, Data, WASM_ARRAY_NEW_DEFAULT(array_index, WASM_I32V(10)));
-  byte kBrOnDataNotTaken = BR_ON(DATA, Data, WASM_REF_NULL(kNoneCode));
+  byte kBrOnStructTaken =
+      BR_ON(STRUCT, Struct, WASM_STRUCT_NEW_DEFAULT(struct_index));
+  byte kBrOnStructNotTaken = BR_ON(STRUCT, Struct, WASM_REF_NULL(kNoneCode));
   byte kBrOnArrayTaken =
       BR_ON(ARRAY, Array, WASM_ARRAY_NEW_DEFAULT(array_index, WASM_I32V(10)));
   byte kBrOnArrayNotTaken = BR_ON(ARRAY, Array, WASM_I31_NEW(WASM_I32V(42)));
@@ -1763,9 +1792,10 @@
                                        WASM_RETURN(WASM_I32V(1)))),    \
        kExprEnd})
 
-  byte kBrOnNonDataNotTaken =
-      BR_ON_NON(DATA, Data, WASM_ARRAY_NEW_DEFAULT(array_index, WASM_I32V(10)));
-  byte kBrOnNonDataTaken = BR_ON_NON(DATA, Data, WASM_REF_NULL(kNoneCode));
+  byte kBrOnNonStructNotTaken =
+      BR_ON_NON(STRUCT, Struct, WASM_STRUCT_NEW_DEFAULT(struct_index));
+  byte kBrOnNonStructTaken =
+      BR_ON_NON(STRUCT, Struct, WASM_REF_NULL(kNoneCode));
   byte kBrOnNonArrayNotTaken = BR_ON_NON(
       ARRAY, Array, WASM_ARRAY_NEW_DEFAULT(array_index, WASM_I32V(10)));
   byte kBrOnNonArrayTaken =
@@ -1777,39 +1807,39 @@
 
   tester.CompileModule();
 
-  tester.CheckResult(kDataCheckNull, 0);
+  tester.CheckResult(kStructCheckNull, 0);
   tester.CheckResult(kArrayCheckNull, 0);
   tester.CheckResult(kI31CheckNull, 0);
 
-  tester.CheckHasThrown(kDataCastNull);
+  tester.CheckHasThrown(kStructCastNull);
   tester.CheckHasThrown(kArrayCastNull);
   tester.CheckHasThrown(kI31CastNull);
 
-  tester.CheckResult(kDataCheckSuccess, 1);
+  tester.CheckResult(kStructCheckSuccess, 1);
   tester.CheckResult(kArrayCheckSuccess, 1);
   tester.CheckResult(kI31CheckSuccess, 1);
 
-  tester.CheckResult(kDataCheckFailure, 0);
+  tester.CheckResult(kStructCheckFailure, 0);
   tester.CheckResult(kArrayCheckFailure, 0);
   tester.CheckResult(kI31CheckFailure, 0);
 
-  tester.CheckResult(kDataCastSuccess, 1);
+  tester.CheckResult(kStructCastSuccess, 1);
   tester.CheckResult(kArrayCastSuccess, 1);
   tester.CheckResult(kI31CastSuccess, 1);
 
-  tester.CheckHasThrown(kDataCastFailure);
+  tester.CheckHasThrown(kStructCastFailure);
   tester.CheckHasThrown(kArrayCastFailure);
   tester.CheckHasThrown(kI31CastFailure);
 
-  tester.CheckResult(kBrOnDataTaken, 1);
-  tester.CheckResult(kBrOnDataNotTaken, 0);
+  tester.CheckResult(kBrOnStructTaken, 1);
+  tester.CheckResult(kBrOnStructNotTaken, 0);
   tester.CheckResult(kBrOnArrayTaken, 1);
   tester.CheckResult(kBrOnArrayNotTaken, 0);
   tester.CheckResult(kBrOnI31Taken, 1);
   tester.CheckResult(kBrOnI31NotTaken, 0);
 
-  tester.CheckResult(kBrOnNonDataTaken, 0);
-  tester.CheckResult(kBrOnNonDataNotTaken, 1);
+  tester.CheckResult(kBrOnNonStructTaken, 0);
+  tester.CheckResult(kBrOnNonStructNotTaken, 1);
   tester.CheckResult(kBrOnNonArrayTaken, 0);
   tester.CheckResult(kBrOnNonArrayNotTaken, 1);
   tester.CheckResult(kBrOnNonI31Taken, 0);
@@ -1824,7 +1854,7 @@
   const byte SubType = tester.DefineStruct(
       {F(wasm::kWasmI32, true), F(wasm::kWasmI32, true)}, SuperType);
 
-  const byte ListType = tester.DefineArray(kWasmDataRef, true);
+  const byte ListType = tester.DefineArray(kWasmStructRef, true);
 
   const byte List =
       tester.AddGlobal(ValueType::RefNull(ListType), true,
@@ -2014,9 +2044,9 @@
   WasmGCTester tester(execution_tier);
   const byte type_index = tester.DefineStruct({F(wasm::kWasmI32, true)});
   ValueType kRefType = ref(type_index);
-  ValueType kSupertypeToI[] = {kWasmI32, kWasmDataRef};
+  ValueType kSupertypeToI[] = {kWasmI32, kWasmStructRef};
   FunctionSig sig_t_v(1, 0, &kRefType);
-  FunctionSig sig_super_v(1, 0, &kWasmDataRef);
+  FunctionSig sig_super_v(1, 0, &kWasmStructRef);
   FunctionSig sig_i_super(1, 1, kSupertypeToI);
 
   tester.DefineExportedFunction(
diff -r -u --color up/v8/test/cctest/wasm/test-liftoff-inspection.cc nw/v8/test/cctest/wasm/test-liftoff-inspection.cc
--- up/v8/test/cctest/wasm/test-liftoff-inspection.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/wasm/test-liftoff-inspection.cc	2023-01-19 16:46:36.482276165 -0500
@@ -435,7 +435,6 @@
 }
 
 TEST(Liftoff_debug_side_table_catch_all) {
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   LiftoffCompileEnvironment env;
   TestSignatures sigs;
   int ex = env.builder()->AddException(sigs.v_v());
@@ -452,17 +451,16 @@
       {
           // function entry.
           {1, {Register(0, kWasmI32)}},
+          // throw.
+          {2, {Stack(0, kWasmI32), Constant(1, kWasmI32, 0)}},
           // breakpoint.
-          {3,
-           {Stack(0, kWasmI32), Register(1, exception_type),
-            Constant(2, kWasmI32, 1)}},
+          {3, {Register(1, exception_type), Constant(2, kWasmI32, 1)}},
           {1, {}},
       },
       debug_side_table.get());
 }
 
 TEST(Regress1199526) {
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   LiftoffCompileEnvironment env;
   ValueType exception_type = ValueType::Ref(HeapType::kAny);
   auto debug_side_table = env.GenerateDebugSideTable(
diff -r -u --color up/v8/test/cctest/wasm/test-run-wasm-atomics.cc nw/v8/test/cctest/wasm/test-run-wasm-atomics.cc
--- up/v8/test/cctest/wasm/test-run-wasm-atomics.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/wasm/test-run-wasm-atomics.cc	2023-01-19 16:46:36.482276165 -0500
@@ -12,7 +12,6 @@
 
 void RunU32BinOp(TestExecutionTier execution_tier, WasmOpcode wasm_op,
                  Uint32BinOp expected_op) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t, uint32_t> r(execution_tier);
   uint32_t* memory =
       r.builder().AddMemoryElems<uint32_t>(kWasmPageSize / sizeof(uint32_t));
@@ -41,7 +40,6 @@
 
 void RunU16BinOp(TestExecutionTier tier, WasmOpcode wasm_op,
                  Uint16BinOp expected_op) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t, uint32_t> r(tier);
   r.builder().SetHasSharedMemory();
   uint16_t* memory =
@@ -70,7 +68,6 @@
 
 void RunU8BinOp(TestExecutionTier execution_tier, WasmOpcode wasm_op,
                 Uint8BinOp expected_op) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t, uint32_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint8_t* memory = r.builder().AddMemoryElems<uint8_t>(kWasmPageSize);
@@ -97,7 +94,6 @@
 #undef TEST_OPERATION
 
 WASM_EXEC_TEST(I32AtomicCompareExchange) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t, uint32_t, uint32_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint32_t* memory =
@@ -118,7 +114,6 @@
 }
 
 WASM_EXEC_TEST(I32AtomicCompareExchange16U) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t, uint32_t, uint32_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint16_t* memory =
@@ -140,7 +135,6 @@
 }
 
 WASM_EXEC_TEST(I32AtomicCompareExchange8U) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t, uint32_t, uint32_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint8_t* memory = r.builder().AddMemoryElems<uint8_t>(kWasmPageSize);
@@ -161,7 +155,6 @@
 }
 
 WASM_EXEC_TEST(I32AtomicCompareExchange_fail) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t, uint32_t, uint32_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint32_t* memory =
@@ -183,7 +176,6 @@
 }
 
 WASM_EXEC_TEST(I32AtomicLoad) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint32_t* memory =
@@ -199,7 +191,6 @@
 }
 
 WASM_EXEC_TEST(I32AtomicLoad16U) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint16_t* memory =
@@ -215,7 +206,6 @@
 }
 
 WASM_EXEC_TEST(I32AtomicLoad8U) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint8_t* memory = r.builder().AddMemoryElems<uint8_t>(kWasmPageSize);
@@ -230,7 +220,6 @@
 }
 
 WASM_EXEC_TEST(I32AtomicStoreLoad) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t, uint32_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint32_t* memory =
@@ -250,7 +239,6 @@
 }
 
 WASM_EXEC_TEST(I32AtomicStoreLoad16U) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t, uint32_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint16_t* memory =
@@ -271,7 +259,6 @@
 }
 
 WASM_EXEC_TEST(I32AtomicStoreLoad8U) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t, uint32_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint8_t* memory = r.builder().AddMemoryElems<uint8_t>(kWasmPageSize);
@@ -290,7 +277,6 @@
 }
 
 WASM_EXEC_TEST(I32AtomicStoreParameter) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t, uint32_t> r(execution_tier);
   uint32_t* memory =
       r.builder().AddMemoryElems<uint32_t>(kWasmPageSize / sizeof(uint32_t));
@@ -306,7 +292,6 @@
 }
 
 WASM_EXEC_TEST(AtomicFence) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t> r(execution_tier);
   // Note that this test specifically doesn't use a shared memory, as the fence
   // instruction does not target a particular linear memory. It may occur in
@@ -318,7 +303,6 @@
 }
 
 WASM_EXEC_TEST(AtomicStoreNoConsideredEffectful) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   // Use {Load} instead of {ProtectedLoad}.
   FLAG_SCOPE(wasm_enforce_bounds_checks);
   WasmRunner<uint32_t> r(execution_tier);
@@ -332,7 +316,6 @@
 }
 
 void RunNoEffectTest(TestExecutionTier execution_tier, WasmOpcode wasm_op) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   // Use {Load} instead of {ProtectedLoad}.
   FLAG_SCOPE(wasm_enforce_bounds_checks);
   WasmRunner<uint32_t> r(execution_tier);
@@ -354,7 +337,6 @@
 }
 
 WASM_EXEC_TEST(AtomicCompareExchangeNoConsideredEffectful) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   // Use {Load} instead of {ProtectedLoad}.
   FLAG_SCOPE(wasm_enforce_bounds_checks);
   WasmRunner<uint32_t> r(execution_tier);
@@ -369,7 +351,6 @@
 }
 
 WASM_EXEC_TEST(I32AtomicLoad_trap) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   r.builder().AddMemory(kWasmPageSize);
@@ -379,7 +360,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicLoad_trap) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   r.builder().AddMemory(kWasmPageSize);
@@ -389,7 +369,6 @@
 }
 
 WASM_EXEC_TEST(I32AtomicStore_trap) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   r.builder().AddMemory(kWasmPageSize);
@@ -401,7 +380,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicStore_trap) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   r.builder().AddMemory(kWasmPageSize);
@@ -413,7 +391,6 @@
 }
 
 WASM_EXEC_TEST(I32AtomicLoad_NotOptOut) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   r.builder().AddMemory(kWasmPageSize);
@@ -425,7 +402,6 @@
 }
 
 void RunU32BinOp_OOB(TestExecutionTier execution_tier, WasmOpcode wasm_op) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t> r(execution_tier);
   r.builder().AddMemory(kWasmPageSize);
   r.builder().SetHasSharedMemory();
@@ -444,7 +420,6 @@
 #undef TEST_OPERATION
 
 void RunU64BinOp_OOB(TestExecutionTier execution_tier, WasmOpcode wasm_op) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t> r(execution_tier);
   r.builder().AddMemory(kWasmPageSize);
   r.builder().SetHasSharedMemory();
@@ -463,7 +438,6 @@
 #undef TEST_OPERATION
 
 WASM_EXEC_TEST(I32AtomicCompareExchange_trap) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t, uint32_t, uint32_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint32_t* memory =
@@ -483,7 +457,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicCompareExchange_trap) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   r.builder().AddMemory(kWasmPageSize);
diff -r -u --color up/v8/test/cctest/wasm/test-run-wasm-atomics64.cc nw/v8/test/cctest/wasm/test-run-wasm-atomics64.cc
--- up/v8/test/cctest/wasm/test-run-wasm-atomics64.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/wasm/test-run-wasm-atomics64.cc	2023-01-19 16:46:36.482276165 -0500
@@ -12,7 +12,6 @@
 
 void RunU64BinOp(TestExecutionTier execution_tier, WasmOpcode wasm_op,
                  Uint64BinOp expected_op) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t, uint64_t> r(execution_tier);
   uint64_t* memory =
       r.builder().AddMemoryElems<uint64_t>(kWasmPageSize / sizeof(uint64_t));
@@ -41,7 +40,6 @@
 
 void RunU32BinOp(TestExecutionTier execution_tier, WasmOpcode wasm_op,
                  Uint32BinOp expected_op) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t, uint64_t> r(execution_tier);
   uint32_t* memory =
       r.builder().AddMemoryElems<uint32_t>(kWasmPageSize / sizeof(uint32_t));
@@ -70,7 +68,6 @@
 
 void RunU16BinOp(TestExecutionTier tier, WasmOpcode wasm_op,
                  Uint16BinOp expected_op) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t, uint64_t> r(tier);
   r.builder().SetHasSharedMemory();
   uint16_t* memory =
@@ -99,7 +96,6 @@
 
 void RunU8BinOp(TestExecutionTier execution_tier, WasmOpcode wasm_op,
                 Uint8BinOp expected_op) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t, uint64_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint8_t* memory = r.builder().AddMemoryElems<uint8_t>(kWasmPageSize);
@@ -126,7 +122,6 @@
 #undef TEST_OPERATION
 
 WASM_EXEC_TEST(I64AtomicCompareExchange) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t, uint64_t, uint64_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint64_t* memory =
@@ -147,7 +142,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicCompareExchange32U) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t, uint64_t, uint64_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint32_t* memory =
@@ -169,7 +163,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicCompareExchange16U) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t, uint64_t, uint64_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint16_t* memory =
@@ -191,7 +184,6 @@
 }
 
 WASM_EXEC_TEST(I32AtomicCompareExchange8U) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t, uint64_t, uint64_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint8_t* memory = r.builder().AddMemoryElems<uint8_t>(kWasmPageSize);
@@ -211,7 +203,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicLoad) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint64_t* memory =
@@ -227,7 +218,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicLoad32U) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint32_t* memory =
@@ -243,7 +233,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicLoad16U) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint16_t* memory =
@@ -259,7 +248,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicLoad8U) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint8_t* memory = r.builder().AddMemoryElems<uint8_t>(kWasmPageSize);
@@ -274,7 +262,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicStoreLoad) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t, uint64_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint64_t* memory =
@@ -294,7 +281,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicStoreLoad32U) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t, uint64_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint32_t* memory =
@@ -315,7 +301,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicStoreLoad16U) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t, uint64_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint16_t* memory =
@@ -336,7 +321,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicStoreLoad8U) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t, uint64_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint8_t* memory = r.builder().AddMemoryElems<uint8_t>(kWasmPageSize);
@@ -358,7 +342,6 @@
 // entire 64-bit output is optimized out
 void RunDropTest(TestExecutionTier execution_tier, WasmOpcode wasm_op,
                  Uint64BinOp op) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t, uint64_t> r(execution_tier);
   uint64_t* memory =
       r.builder().AddMemoryElems<uint64_t>(kWasmPageSize / sizeof(uint64_t));
@@ -384,7 +367,6 @@
 #undef TEST_OPERATION
 
 WASM_EXEC_TEST(I64AtomicSub16UDrop) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t, uint64_t> r(execution_tier);
   uint16_t* memory =
       r.builder().AddMemoryElems<uint16_t>(kWasmPageSize / sizeof(uint16_t));
@@ -403,7 +385,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicCompareExchangeDrop) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t, uint64_t, uint64_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint64_t* memory =
@@ -422,7 +403,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicStoreLoadDrop) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t, uint64_t, uint64_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint64_t* memory =
@@ -441,7 +421,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicAddConvertDrop) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t, uint64_t> r(execution_tier);
   uint64_t* memory =
       r.builder().AddMemoryElems<uint64_t>(kWasmPageSize / sizeof(uint64_t));
@@ -460,7 +439,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicLoadConvertDrop) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t, uint64_t> r(execution_tier);
   uint64_t* memory =
       r.builder().AddMemoryElems<uint64_t>(kWasmPageSize / sizeof(uint64_t));
@@ -478,7 +456,6 @@
 // upper half of the 64-bit output is optimized out
 void RunConvertTest(TestExecutionTier execution_tier, WasmOpcode wasm_op,
                     Uint64BinOp op) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t, uint64_t> r(execution_tier);
   uint64_t* memory =
       r.builder().AddMemoryElems<uint64_t>(kWasmPageSize / sizeof(uint64_t));
@@ -503,7 +480,6 @@
 #undef TEST_OPERATION
 
 WASM_EXEC_TEST(I64AtomicConvertCompareExchange) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t, uint64_t, uint64_t> r(execution_tier);
   uint64_t* memory =
       r.builder().AddMemoryElems<uint64_t>(kWasmPageSize / sizeof(uint64_t));
@@ -524,7 +500,6 @@
 // is lowered correctly.
 void RunNonConstIndexTest(TestExecutionTier execution_tier, WasmOpcode wasm_op,
                           Uint64BinOp op) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t, uint64_t> r(execution_tier);
   uint64_t* memory =
       r.builder().AddMemoryElems<uint64_t>(kWasmPageSize / sizeof(uint64_t));
@@ -558,7 +533,6 @@
 #undef TEST_OPERATION
 
 WASM_EXEC_TEST(I64AtomicNonConstIndexCompareExchangeNarrow) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t, uint64_t, uint64_t> r(execution_tier);
   uint64_t* memory =
       r.builder().AddMemoryElems<uint64_t>(kWasmPageSize / sizeof(uint64_t));
@@ -577,7 +551,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicNonConstIndexCompareExchange) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t, uint64_t, uint64_t> r(execution_tier);
   uint64_t* memory =
       r.builder().AddMemoryElems<uint64_t>(kWasmPageSize / sizeof(uint64_t));
@@ -596,7 +569,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicNonConstIndexLoad8U) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint64_t* memory =
@@ -611,7 +583,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicCompareExchangeFail) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t, uint64_t, uint64_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint64_t* memory =
@@ -629,7 +600,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicCompareExchange32UFail) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint64_t, uint64_t, uint64_t> r(execution_tier);
   r.builder().SetHasSharedMemory();
   uint64_t* memory =
@@ -647,7 +617,6 @@
 }
 
 WASM_EXEC_TEST(AtomicStoreNoConsideredEffectful) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   // Use {Load} instead of {ProtectedLoad}.
   FLAG_SCOPE(wasm_enforce_bounds_checks);
   WasmRunner<uint32_t> r(execution_tier);
@@ -661,7 +630,6 @@
 }
 
 void RunNoEffectTest(TestExecutionTier execution_tier, WasmOpcode wasm_op) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   // Use {Load} instead of {ProtectedLoad}.
   FLAG_SCOPE(wasm_enforce_bounds_checks);
   WasmRunner<uint32_t> r(execution_tier);
@@ -683,7 +651,6 @@
 }
 
 WASM_EXEC_TEST(AtomicCompareExchangeNoConsideredEffectful) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   // Use {Load} instead of {ProtectedLoad}.
   FLAG_SCOPE(wasm_enforce_bounds_checks);
   WasmRunner<uint32_t> r(execution_tier);
@@ -698,7 +665,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicLoadUseOnlyLowWord) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t> r(execution_tier);
   uint64_t* memory =
       r.builder().AddMemoryElems<uint64_t>(kWasmPageSize / sizeof(uint64_t));
@@ -713,7 +679,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicLoadUseOnlyHighWord) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t> r(execution_tier);
   uint64_t* memory =
       r.builder().AddMemoryElems<uint64_t>(kWasmPageSize / sizeof(uint64_t));
@@ -729,7 +694,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicAddUseOnlyLowWord) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t> r(execution_tier);
   uint64_t* memory =
       r.builder().AddMemoryElems<uint64_t>(kWasmPageSize / sizeof(uint64_t));
@@ -744,7 +708,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicAddUseOnlyHighWord) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t> r(execution_tier);
   uint64_t* memory =
       r.builder().AddMemoryElems<uint64_t>(kWasmPageSize / sizeof(uint64_t));
@@ -760,7 +723,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicCompareExchangeUseOnlyLowWord) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t> r(execution_tier);
   uint64_t* memory =
       r.builder().AddMemoryElems<uint64_t>(kWasmPageSize / sizeof(uint64_t));
@@ -775,7 +737,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicCompareExchangeUseOnlyHighWord) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t> r(execution_tier);
   uint64_t* memory =
       r.builder().AddMemoryElems<uint64_t>(kWasmPageSize / sizeof(uint64_t));
@@ -792,7 +753,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicExchangeUseOnlyLowWord) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t> r(execution_tier);
   uint64_t* memory =
       r.builder().AddMemoryElems<uint64_t>(kWasmPageSize / sizeof(uint64_t));
@@ -807,7 +767,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicExchangeUseOnlyHighWord) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t> r(execution_tier);
   uint64_t* memory =
       r.builder().AddMemoryElems<uint64_t>(kWasmPageSize / sizeof(uint64_t));
@@ -823,7 +782,6 @@
 }
 
 WASM_EXEC_TEST(I64AtomicCompareExchange32UZeroExtended) {
-  EXPERIMENTAL_FLAG_SCOPE(threads);
   WasmRunner<uint32_t> r(execution_tier);
   uint64_t* memory =
       r.builder().AddMemoryElems<uint64_t>(kWasmPageSize / sizeof(uint64_t));
diff -r -u --color up/v8/test/cctest/wasm/test-run-wasm-exceptions.cc nw/v8/test/cctest/wasm/test-run-wasm-exceptions.cc
--- up/v8/test/cctest/wasm/test-run-wasm-exceptions.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/wasm/test-run-wasm-exceptions.cc	2023-01-19 16:46:36.482276165 -0500
@@ -15,7 +15,6 @@
 
 WASM_EXEC_TEST(TryCatchThrow) {
   TestSignatures sigs;
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   WasmRunner<uint32_t, uint32_t> r(execution_tier);
   byte except = r.builder().AddException(sigs.v_v());
   constexpr uint32_t kResult0 = 23;
@@ -40,7 +39,6 @@
 
 WASM_EXEC_TEST(TryCatchThrowWithValue) {
   TestSignatures sigs;
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   WasmRunner<uint32_t, uint32_t> r(execution_tier);
   byte except = r.builder().AddException(sigs.v_i());
   constexpr uint32_t kResult0 = 23;
@@ -66,7 +64,6 @@
 
 WASM_EXEC_TEST(TryMultiCatchThrow) {
   TestSignatures sigs;
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   WasmRunner<uint32_t, uint32_t> r(execution_tier);
   byte except1 = r.builder().AddException(sigs.v_v());
   byte except2 = r.builder().AddException(sigs.v_v());
@@ -98,7 +95,6 @@
 
 WASM_EXEC_TEST(TryCatchAllThrow) {
   TestSignatures sigs;
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   WasmRunner<uint32_t, uint32_t> r(execution_tier);
   byte except = r.builder().AddException(sigs.v_v());
   constexpr uint32_t kResult0 = 23;
@@ -122,7 +118,6 @@
 
 WASM_EXEC_TEST(TryCatchCatchAllThrow) {
   TestSignatures sigs;
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   WasmRunner<uint32_t, uint32_t> r(execution_tier);
   byte except1 = r.builder().AddException(sigs.v_v());
   byte except2 = r.builder().AddException(sigs.v_v());
@@ -154,7 +149,6 @@
 
 WASM_EXEC_TEST(TryImplicitRethrow) {
   TestSignatures sigs;
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   WasmRunner<uint32_t, uint32_t> r(execution_tier);
   byte except1 = r.builder().AddException(sigs.v_v());
   byte except2 = r.builder().AddException(sigs.v_v());
@@ -185,7 +179,6 @@
 
 WASM_EXEC_TEST(TryDelegate) {
   TestSignatures sigs;
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   WasmRunner<uint32_t, uint32_t> r(execution_tier);
   byte except = r.builder().AddException(sigs.v_v());
   constexpr uint32_t kResult0 = 23;
@@ -214,7 +207,6 @@
 
 WASM_EXEC_TEST(TestCatchlessTry) {
   TestSignatures sigs;
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   WasmRunner<uint32_t> r(execution_tier);
   byte except = r.builder().AddException(sigs.v_i());
   BUILD(r,
@@ -231,7 +223,6 @@
 
 WASM_EXEC_TEST(TryCatchRethrow) {
   TestSignatures sigs;
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   WasmRunner<uint32_t, uint32_t> r(execution_tier);
   byte except1 = r.builder().AddException(sigs.v_v());
   byte except2 = r.builder().AddException(sigs.v_v());
@@ -266,7 +257,6 @@
 
 WASM_EXEC_TEST(TryDelegateToCaller) {
   TestSignatures sigs;
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   WasmRunner<uint32_t, uint32_t> r(execution_tier);
   byte except = r.builder().AddException(sigs.v_v());
   constexpr uint32_t kResult0 = 23;
@@ -297,7 +287,6 @@
 
 WASM_EXEC_TEST(TryCatchCallDirect) {
   TestSignatures sigs;
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   WasmRunner<uint32_t, uint32_t> r(execution_tier);
   byte except = r.builder().AddException(sigs.v_v());
   constexpr uint32_t kResult0 = 23;
@@ -330,7 +319,6 @@
 
 WASM_EXEC_TEST(TryCatchAllCallDirect) {
   TestSignatures sigs;
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   WasmRunner<uint32_t, uint32_t> r(execution_tier);
   byte except = r.builder().AddException(sigs.v_v());
   constexpr uint32_t kResult0 = 23;
@@ -363,7 +351,6 @@
 
 WASM_EXEC_TEST(TryCatchCallIndirect) {
   TestSignatures sigs;
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   WasmRunner<uint32_t, uint32_t> r(execution_tier);
   byte except = r.builder().AddException(sigs.v_v());
   constexpr uint32_t kResult0 = 23;
@@ -403,7 +390,6 @@
 
 WASM_EXEC_TEST(TryCatchAllCallIndirect) {
   TestSignatures sigs;
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   WasmRunner<uint32_t, uint32_t> r(execution_tier);
   byte except = r.builder().AddException(sigs.v_v());
   constexpr uint32_t kResult0 = 23;
@@ -443,7 +429,6 @@
 
 WASM_COMPILED_EXEC_TEST(TryCatchCallExternal) {
   TestSignatures sigs;
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   HandleScope scope(CcTest::InitIsolateOnce());
   const char* source = "(function() { throw 'ball'; })";
   Handle<JSFunction> js_function =
@@ -473,7 +458,6 @@
 
 WASM_COMPILED_EXEC_TEST(TryCatchAllCallExternal) {
   TestSignatures sigs;
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   HandleScope scope(CcTest::InitIsolateOnce());
   const char* source = "(function() { throw 'ball'; })";
   Handle<JSFunction> js_function =
@@ -506,7 +490,6 @@
 void TestTrapNotCaught(byte* code, size_t code_size,
                        TestExecutionTier execution_tier) {
   TestSignatures sigs;
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   WasmRunner<uint32_t> r(execution_tier, nullptr, "main",
                          kRuntimeExceptionSupport);
   r.builder().AddMemory(kWasmPageSize);
@@ -596,7 +579,6 @@
 
 UNINITIALIZED_WASM_EXEC_TEST(TestStackOverflowNotCaught) {
   TestSignatures sigs;
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   // v8_flags.stack_size must be set before isolate initialization.
   FlagScope<int32_t> stack_size(&v8_flags.stack_size, 8);
 
@@ -632,7 +614,6 @@
 
 TEST(Regress1180457) {
   TestSignatures sigs;
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   WasmRunner<uint32_t> r(TestExecutionTier::kInterpreter);
   constexpr uint32_t kResult0 = 23;
   constexpr uint32_t kUnreachable = 42;
@@ -647,7 +628,6 @@
 
 TEST(Regress1187896) {
   TestSignatures sigs;
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   WasmRunner<uint32_t> r(TestExecutionTier::kInterpreter);
   byte try_sig = r.builder().AddSignature(sigs.v_i());
   constexpr uint32_t kResult = 23;
@@ -658,7 +638,6 @@
 
 TEST(Regress1190291) {
   TestSignatures sigs;
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   WasmRunner<uint32_t> r(TestExecutionTier::kInterpreter);
   byte try_sig = r.builder().AddSignature(sigs.v_i());
   BUILD(r, kExprUnreachable, kExprTry, try_sig, kExprCatchAll, kExprEnd,
@@ -668,7 +647,6 @@
 
 TEST(Regress1186795) {
   TestSignatures sigs;
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   WasmRunner<uint32_t> r(TestExecutionTier::kInterpreter);
   byte except = r.builder().AddException(sigs.v_i());
   BUILD(r,
@@ -686,7 +664,6 @@
 
 TEST(Regress1197408) {
   TestSignatures sigs;
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   WasmRunner<int32_t, int32_t, int32_t, int32_t> r(
       TestExecutionTier::kInterpreter);
   byte sig_id = r.builder().AddSignature(sigs.i_iii());
@@ -698,7 +675,6 @@
 
 TEST(Regress1212396) {
   TestSignatures sigs;
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   WasmRunner<int32_t> r(TestExecutionTier::kInterpreter);
   byte except = r.builder().AddException(sigs.v_v());
   BUILD(r, kExprTry, kVoidCode, kExprTry, kVoidCode, kExprI32Const, 0,
@@ -709,7 +685,6 @@
 
 TEST(Regress1219746) {
   TestSignatures sigs;
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   WasmRunner<int32_t> r(TestExecutionTier::kInterpreter);
   BUILD(r, kExprTry, kVoidCode, kExprI32Const, 0, kExprEnd);
   CHECK_EQ(0, r.CallInterpreter());
diff -r -u --color up/v8/test/cctest/wasm/test-run-wasm-relaxed-simd.cc nw/v8/test/cctest/wasm/test-run-wasm-relaxed-simd.cc
--- up/v8/test/cctest/wasm/test-run-wasm-relaxed-simd.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/wasm/test-run-wasm-relaxed-simd.cc	2023-01-19 16:46:36.482276165 -0500
@@ -226,6 +226,36 @@
     }
   }
 }
+
+TEST(RunWasm_RegressFmaReg_liftoff) {
+  EXPERIMENTAL_FLAG_SCOPE(relaxed_simd);
+  FLAG_SCOPE(liftoff_only);
+  TestExecutionTier execution_tier = TestExecutionTier::kLiftoff;
+  WasmRunner<int32_t, float, float, float> r(execution_tier);
+  byte local = r.AllocateLocal(kWasmS128);
+  float* g = r.builder().AddGlobal<float>(kWasmS128);
+  byte value1 = 0, value2 = 1, value3 = 2;
+  BUILD(r,
+        // Get the first arg from a local so that the register is blocked even
+        // after the arguments have been popped off the stack. This ensures that
+        // the first source register is not also the destination.
+        WASM_LOCAL_SET(local, WASM_SIMD_F32x4_SPLAT(WASM_LOCAL_GET(value1))),
+        WASM_GLOBAL_SET(0, WASM_SIMD_F32x4_QFMA(
+                               WASM_LOCAL_GET(local),
+                               WASM_SIMD_F32x4_SPLAT(WASM_LOCAL_GET(value2)),
+                               WASM_SIMD_F32x4_SPLAT(WASM_LOCAL_GET(value3)))),
+        WASM_ONE);
+
+  for (FMOperation<float> x : qfma_vector<float>()) {
+    r.Call(x.a, x.b, x.c);
+    float expected =
+        ExpectFused(execution_tier) ? x.fused_result : x.unfused_result;
+    for (int i = 0; i < 4; i++) {
+      float actual = LANE(g, i);
+      CheckFloatResult(x.a, x.b, expected, actual, true /* exact */);
+    }
+  }
+}
 #endif  // V8_TARGET_ARCH_X64 || V8_TARGET_ARCH_ARM64 || V8_TARGET_ARCH_S390X ||
         // V8_TARGET_ARCH_PPC64 || V8_TARGET_ARCH_IA32 || V8_TARGET_ARCH_RISCV64
 
diff -r -u --color up/v8/test/cctest/wasm/test-run-wasm-simd-liftoff.cc nw/v8/test/cctest/wasm/test-run-wasm-simd-liftoff.cc
--- up/v8/test/cctest/wasm/test-run-wasm-simd-liftoff.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/wasm/test-run-wasm-simd-liftoff.cc	2023-01-19 16:46:36.493109495 -0500
@@ -21,22 +21,14 @@
 namespace wasm {
 namespace test_run_wasm_simd_liftoff {
 
-#define WASM_SIMD_LIFTOFF_TEST(name) \
-  void RunWasm_##name##_Impl();      \
-  TEST(RunWasm_##name##_liftoff) {   \
-    EXPERIMENTAL_FLAG_SCOPE(simd);   \
-    RunWasm_##name##_Impl();         \
-  }                                  \
-  void RunWasm_##name##_Impl()
-
-WASM_SIMD_LIFTOFF_TEST(S128Local) {
+TEST(S128Local) {
   WasmRunner<int32_t> r(TestExecutionTier::kLiftoff);
   byte temp1 = r.AllocateLocal(kWasmS128);
   BUILD(r, WASM_LOCAL_SET(temp1, WASM_LOCAL_GET(temp1)), WASM_ONE);
   CHECK_EQ(1, r.Call());
 }
 
-WASM_SIMD_LIFTOFF_TEST(S128Global) {
+TEST(S128Global) {
   WasmRunner<int32_t> r(TestExecutionTier::kLiftoff);
 
   int32_t* g0 = r.builder().AddGlobal<int32_t>(kWasmS128);
@@ -54,7 +46,7 @@
   }
 }
 
-WASM_SIMD_LIFTOFF_TEST(S128Param) {
+TEST(S128Param) {
   // Test how SIMD parameters in functions are processed. There is no easy way
   // to specify a SIMD value when initializing a WasmRunner, so we manually
   // add a new function with the right signature, and call it from main.
@@ -72,7 +64,7 @@
   CHECK_EQ(1, r.Call());
 }
 
-WASM_SIMD_LIFTOFF_TEST(S128Return) {
+TEST(S128Return) {
   // Test how functions returning SIMD values are processed.
   WasmRunner<int32_t> r(TestExecutionTier::kLiftoff);
   TestSignatures sigs;
@@ -86,7 +78,7 @@
   CHECK_EQ(1, r.Call());
 }
 
-WASM_SIMD_LIFTOFF_TEST(REGRESS_1088273) {
+TEST(REGRESS_1088273) {
   // TODO(v8:9418): This is a regression test for Liftoff, translated from a
   // mjsunit test. We do not have I64x2Mul lowering yet, so this will cause a
   // crash on arch that don't support SIMD 128 and require lowering, thus
@@ -108,7 +100,7 @@
 // A test to exercise logic in Liftoff's implementation of shuffle. The
 // implementation in Liftoff is a bit more tricky due to shuffle requiring
 // adjacent registers in ARM/ARM64.
-WASM_SIMD_LIFTOFF_TEST(I8x16Shuffle) {
+TEST(I8x16Shuffle) {
   WasmRunner<int32_t> r(TestExecutionTier::kLiftoff);
   // Temps to use up registers and force non-adjacent registers for shuffle.
   byte local0 = r.AllocateLocal(kWasmS128);
@@ -153,7 +145,7 @@
 
 // Exercise logic in Liftoff's implementation of shuffle when inputs to the
 // shuffle are the same register.
-WASM_SIMD_LIFTOFF_TEST(I8x16Shuffle_SingleOperand) {
+TEST(I8x16Shuffle_SingleOperand) {
   WasmRunner<int32_t> r(TestExecutionTier::kLiftoff);
   byte local0 = r.AllocateLocal(kWasmS128);
 
@@ -189,7 +181,7 @@
 // Exercise Liftoff's logic for zero-initializing stack slots. We were using an
 // incorrect instruction for storing zeroes into the slot when the slot offset
 // was too large to fit in the instruction as an immediate.
-WASM_SIMD_LIFTOFF_TEST(FillStackSlotsWithZero_CheckStartOffset) {
+TEST(FillStackSlotsWithZero_CheckStartOffset) {
   WasmRunner<int64_t> r(TestExecutionTier::kLiftoff);
   // Function that takes in 32 i64 arguments, returns i64. This gets us a large
   // enough starting offset from which we spill locals.
@@ -221,8 +213,6 @@
   CHECK_EQ(1, r.Call());
 }
 
-#undef WASM_SIMD_LIFTOFF_TEST
-
 }  // namespace test_run_wasm_simd_liftoff
 }  // namespace wasm
 }  // namespace internal
diff -r -u --color up/v8/test/cctest/wasm/test-run-wasm-simd.cc nw/v8/test/cctest/wasm/test-run-wasm-simd.cc
--- up/v8/test/cctest/wasm/test-run-wasm-simd.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/wasm/test-run-wasm-simd.cc	2023-01-19 16:46:36.493109495 -0500
@@ -48,22 +48,6 @@
 
 using Shuffle = std::array<int8_t, kSimd128Size>;
 
-#define WASM_SIMD_TEST(name)                                    \
-  void RunWasm_##name##_Impl(TestExecutionTier execution_tier); \
-  TEST(RunWasm_##name##_turbofan) {                             \
-    EXPERIMENTAL_FLAG_SCOPE(simd);                              \
-    RunWasm_##name##_Impl(TestExecutionTier::kTurbofan);        \
-  }                                                             \
-  TEST(RunWasm_##name##_liftoff) {                              \
-    EXPERIMENTAL_FLAG_SCOPE(simd);                              \
-    RunWasm_##name##_Impl(TestExecutionTier::kLiftoff);         \
-  }                                                             \
-  TEST(RunWasm_##name##_interpreter) {                          \
-    EXPERIMENTAL_FLAG_SCOPE(simd);                              \
-    RunWasm_##name##_Impl(TestExecutionTier::kInterpreter);     \
-  }                                                             \
-  void RunWasm_##name##_Impl(TestExecutionTier execution_tier)
-
 // For signed integral types, use base::AddWithWraparound.
 template <typename T, typename = typename std::enable_if<
                           std::is_floating_point<T>::value>::type>
@@ -249,7 +233,7 @@
                                     lane_index, WASM_LOCAL_GET(value))),       \
           WASM_RETURN(WASM_ZERO))
 
-WASM_SIMD_TEST(S128Globals) {
+WASM_EXEC_TEST(S128Globals) {
   WasmRunner<int32_t> r(execution_tier);
   // Set up a global to hold input and output vectors.
   int32_t* g0 = r.builder().AddGlobal<int32_t>(kWasmS128);
@@ -269,7 +253,7 @@
   }
 }
 
-WASM_SIMD_TEST(F32x4Splat) {
+WASM_EXEC_TEST(F32x4Splat) {
   WasmRunner<int32_t, float> r(execution_tier);
   // Set up a global to hold output vector.
   float* g = r.builder().AddGlobal<float>(kWasmS128);
@@ -291,7 +275,7 @@
   }
 }
 
-WASM_SIMD_TEST(F32x4ReplaceLane) {
+WASM_EXEC_TEST(F32x4ReplaceLane) {
   WasmRunner<int32_t> r(execution_tier);
   // Set up a global to hold input/output vector.
   float* g = r.builder().AddGlobal<float>(kWasmS128);
@@ -315,7 +299,7 @@
 }
 
 // Tests both signed and unsigned conversion.
-WASM_SIMD_TEST(F32x4ConvertI32x4) {
+WASM_EXEC_TEST(F32x4ConvertI32x4) {
   WasmRunner<int32_t, int32_t> r(execution_tier);
   // Create two output vectors to hold signed and unsigned results.
   float* g0 = r.builder().AddGlobal<float>(kWasmS128);
@@ -384,82 +368,82 @@
   }
 }
 
-WASM_SIMD_TEST(F32x4Abs) {
+WASM_EXEC_TEST(F32x4Abs) {
   RunF32x4UnOpTest(execution_tier, kExprF32x4Abs, std::abs);
 }
 
-WASM_SIMD_TEST(F32x4Neg) {
+WASM_EXEC_TEST(F32x4Neg) {
   RunF32x4UnOpTest(execution_tier, kExprF32x4Neg, Negate);
 }
 
-WASM_SIMD_TEST(F32x4Sqrt) {
+WASM_EXEC_TEST(F32x4Sqrt) {
   RunF32x4UnOpTest(execution_tier, kExprF32x4Sqrt, std::sqrt);
 }
 
-WASM_SIMD_TEST(F32x4Ceil) {
+WASM_EXEC_TEST(F32x4Ceil) {
   RunF32x4UnOpTest(execution_tier, kExprF32x4Ceil, ceilf, true);
 }
 
-WASM_SIMD_TEST(F32x4Floor) {
+WASM_EXEC_TEST(F32x4Floor) {
   RunF32x4UnOpTest(execution_tier, kExprF32x4Floor, floorf, true);
 }
 
-WASM_SIMD_TEST(F32x4Trunc) {
+WASM_EXEC_TEST(F32x4Trunc) {
   RunF32x4UnOpTest(execution_tier, kExprF32x4Trunc, truncf, true);
 }
 
-WASM_SIMD_TEST(F32x4NearestInt) {
+WASM_EXEC_TEST(F32x4NearestInt) {
   RunF32x4UnOpTest(execution_tier, kExprF32x4NearestInt, nearbyintf, true);
 }
 
-WASM_SIMD_TEST(F32x4Add) {
+WASM_EXEC_TEST(F32x4Add) {
   RunF32x4BinOpTest(execution_tier, kExprF32x4Add, Add);
 }
-WASM_SIMD_TEST(F32x4Sub) {
+WASM_EXEC_TEST(F32x4Sub) {
   RunF32x4BinOpTest(execution_tier, kExprF32x4Sub, Sub);
 }
-WASM_SIMD_TEST(F32x4Mul) {
+WASM_EXEC_TEST(F32x4Mul) {
   RunF32x4BinOpTest(execution_tier, kExprF32x4Mul, Mul);
 }
-WASM_SIMD_TEST(F32x4Div) {
+WASM_EXEC_TEST(F32x4Div) {
   RunF32x4BinOpTest(execution_tier, kExprF32x4Div, base::Divide);
 }
-WASM_SIMD_TEST(F32x4Min) {
+WASM_EXEC_TEST(F32x4Min) {
   RunF32x4BinOpTest(execution_tier, kExprF32x4Min, JSMin);
 }
-WASM_SIMD_TEST(F32x4Max) {
+WASM_EXEC_TEST(F32x4Max) {
   RunF32x4BinOpTest(execution_tier, kExprF32x4Max, JSMax);
 }
 
-WASM_SIMD_TEST(F32x4Pmin) {
+WASM_EXEC_TEST(F32x4Pmin) {
   RunF32x4BinOpTest(execution_tier, kExprF32x4Pmin, Minimum);
 }
 
-WASM_SIMD_TEST(F32x4Pmax) {
+WASM_EXEC_TEST(F32x4Pmax) {
   RunF32x4BinOpTest(execution_tier, kExprF32x4Pmax, Maximum);
 }
 
-WASM_SIMD_TEST(F32x4Eq) {
+WASM_EXEC_TEST(F32x4Eq) {
   RunF32x4CompareOpTest(execution_tier, kExprF32x4Eq, Equal);
 }
 
-WASM_SIMD_TEST(F32x4Ne) {
+WASM_EXEC_TEST(F32x4Ne) {
   RunF32x4CompareOpTest(execution_tier, kExprF32x4Ne, NotEqual);
 }
 
-WASM_SIMD_TEST(F32x4Gt) {
+WASM_EXEC_TEST(F32x4Gt) {
   RunF32x4CompareOpTest(execution_tier, kExprF32x4Gt, Greater);
 }
 
-WASM_SIMD_TEST(F32x4Ge) {
+WASM_EXEC_TEST(F32x4Ge) {
   RunF32x4CompareOpTest(execution_tier, kExprF32x4Ge, GreaterEqual);
 }
 
-WASM_SIMD_TEST(F32x4Lt) {
+WASM_EXEC_TEST(F32x4Lt) {
   RunF32x4CompareOpTest(execution_tier, kExprF32x4Lt, Less);
 }
 
-WASM_SIMD_TEST(F32x4Le) {
+WASM_EXEC_TEST(F32x4Le) {
   RunF32x4CompareOpTest(execution_tier, kExprF32x4Le, LessEqual);
 }
 
@@ -503,37 +487,37 @@
   }
 }
 
-WASM_SIMD_TEST(F32x4EqZero) {
+WASM_EXEC_TEST(F32x4EqZero) {
   RunF128CompareOpConstImmTest<float, int32_t>(execution_tier, kExprF32x4Eq,
                                                kExprF32x4Splat, Equal);
 }
 
-WASM_SIMD_TEST(F32x4NeZero) {
+WASM_EXEC_TEST(F32x4NeZero) {
   RunF128CompareOpConstImmTest<float, int32_t>(execution_tier, kExprF32x4Ne,
                                                kExprF32x4Splat, NotEqual);
 }
 
-WASM_SIMD_TEST(F32x4GtZero) {
+WASM_EXEC_TEST(F32x4GtZero) {
   RunF128CompareOpConstImmTest<float, int32_t>(execution_tier, kExprF32x4Gt,
                                                kExprF32x4Splat, Greater);
 }
 
-WASM_SIMD_TEST(F32x4GeZero) {
+WASM_EXEC_TEST(F32x4GeZero) {
   RunF128CompareOpConstImmTest<float, int32_t>(execution_tier, kExprF32x4Ge,
                                                kExprF32x4Splat, GreaterEqual);
 }
 
-WASM_SIMD_TEST(F32x4LtZero) {
+WASM_EXEC_TEST(F32x4LtZero) {
   RunF128CompareOpConstImmTest<float, int32_t>(execution_tier, kExprF32x4Lt,
                                                kExprF32x4Splat, Less);
 }
 
-WASM_SIMD_TEST(F32x4LeZero) {
+WASM_EXEC_TEST(F32x4LeZero) {
   RunF128CompareOpConstImmTest<float, int32_t>(execution_tier, kExprF32x4Le,
                                                kExprF32x4Splat, LessEqual);
 }
 
-WASM_SIMD_TEST(I64x2Splat) {
+WASM_EXEC_TEST(I64x2Splat) {
   WasmRunner<int32_t, int64_t> r(execution_tier);
   // Set up a global to hold output vector.
   int64_t* g = r.builder().AddGlobal<int64_t>(kWasmS128);
@@ -551,7 +535,7 @@
   }
 }
 
-WASM_SIMD_TEST(I64x2ExtractLane) {
+WASM_EXEC_TEST(I64x2ExtractLane) {
   WasmRunner<int64_t> r(execution_tier);
   r.AllocateLocal(kWasmI64);
   r.AllocateLocal(kWasmS128);
@@ -564,7 +548,7 @@
   CHECK_EQ(0xFFFFFFFFFF, r.Call());
 }
 
-WASM_SIMD_TEST(I64x2ReplaceLane) {
+WASM_EXEC_TEST(I64x2ReplaceLane) {
   WasmRunner<int32_t> r(execution_tier);
   // Set up a global to hold input/output vector.
   int64_t* g = r.builder().AddGlobal<int64_t>(kWasmS128);
@@ -583,27 +567,27 @@
   }
 }
 
-WASM_SIMD_TEST(I64x2Neg) {
+WASM_EXEC_TEST(I64x2Neg) {
   RunI64x2UnOpTest(execution_tier, kExprI64x2Neg, base::NegateWithWraparound);
 }
 
-WASM_SIMD_TEST(I64x2Abs) {
+WASM_EXEC_TEST(I64x2Abs) {
   RunI64x2UnOpTest(execution_tier, kExprI64x2Abs, std::abs);
 }
 
-WASM_SIMD_TEST(I64x2Shl) {
+WASM_EXEC_TEST(I64x2Shl) {
   RunI64x2ShiftOpTest(execution_tier, kExprI64x2Shl, LogicalShiftLeft);
 }
 
-WASM_SIMD_TEST(I64x2ShrS) {
+WASM_EXEC_TEST(I64x2ShrS) {
   RunI64x2ShiftOpTest(execution_tier, kExprI64x2ShrS, ArithmeticShiftRight);
 }
 
-WASM_SIMD_TEST(I64x2ShrU) {
+WASM_EXEC_TEST(I64x2ShrU) {
   RunI64x2ShiftOpTest(execution_tier, kExprI64x2ShrU, LogicalShiftRight);
 }
 
-WASM_SIMD_TEST(I64x2ShiftAdd) {
+WASM_EXEC_TEST(I64x2ShiftAdd) {
   for (int imm = 0; imm <= 64; imm++) {
     RunShiftAddTestSequence<int64_t>(execution_tier, kExprI64x2ShrU,
                                      kExprI64x2Add, kExprI64x2Splat, imm,
@@ -614,35 +598,35 @@
   }
 }
 
-WASM_SIMD_TEST(I64x2Add) {
+WASM_EXEC_TEST(I64x2Add) {
   RunI64x2BinOpTest(execution_tier, kExprI64x2Add, base::AddWithWraparound);
 }
 
-WASM_SIMD_TEST(I64x2Sub) {
+WASM_EXEC_TEST(I64x2Sub) {
   RunI64x2BinOpTest(execution_tier, kExprI64x2Sub, base::SubWithWraparound);
 }
 
-WASM_SIMD_TEST(I64x2Eq) {
+WASM_EXEC_TEST(I64x2Eq) {
   RunI64x2BinOpTest(execution_tier, kExprI64x2Eq, Equal);
 }
 
-WASM_SIMD_TEST(I64x2Ne) {
+WASM_EXEC_TEST(I64x2Ne) {
   RunI64x2BinOpTest(execution_tier, kExprI64x2Ne, NotEqual);
 }
 
-WASM_SIMD_TEST(I64x2LtS) {
+WASM_EXEC_TEST(I64x2LtS) {
   RunI64x2BinOpTest(execution_tier, kExprI64x2LtS, Less);
 }
 
-WASM_SIMD_TEST(I64x2LeS) {
+WASM_EXEC_TEST(I64x2LeS) {
   RunI64x2BinOpTest(execution_tier, kExprI64x2LeS, LessEqual);
 }
 
-WASM_SIMD_TEST(I64x2GtS) {
+WASM_EXEC_TEST(I64x2GtS) {
   RunI64x2BinOpTest(execution_tier, kExprI64x2GtS, Greater);
 }
 
-WASM_SIMD_TEST(I64x2GeS) {
+WASM_EXEC_TEST(I64x2GeS) {
   RunI64x2BinOpTest(execution_tier, kExprI64x2GeS, GreaterEqual);
 }
 
@@ -690,37 +674,37 @@
 
 }  // namespace
 
-WASM_SIMD_TEST(I64x2EqZero) {
+WASM_EXEC_TEST(I64x2EqZero) {
   RunICompareOpConstImmTest<int64_t>(execution_tier, kExprI64x2Eq,
                                      kExprI64x2Splat, Equal);
 }
 
-WASM_SIMD_TEST(I64x2NeZero) {
+WASM_EXEC_TEST(I64x2NeZero) {
   RunICompareOpConstImmTest<int64_t>(execution_tier, kExprI64x2Ne,
                                      kExprI64x2Splat, NotEqual);
 }
 
-WASM_SIMD_TEST(I64x2GtZero) {
+WASM_EXEC_TEST(I64x2GtZero) {
   RunICompareOpConstImmTest<int64_t>(execution_tier, kExprI64x2GtS,
                                      kExprI64x2Splat, Greater);
 }
 
-WASM_SIMD_TEST(I64x2GeZero) {
+WASM_EXEC_TEST(I64x2GeZero) {
   RunICompareOpConstImmTest<int64_t>(execution_tier, kExprI64x2GeS,
                                      kExprI64x2Splat, GreaterEqual);
 }
 
-WASM_SIMD_TEST(I64x2LtZero) {
+WASM_EXEC_TEST(I64x2LtZero) {
   RunICompareOpConstImmTest<int64_t>(execution_tier, kExprI64x2LtS,
                                      kExprI64x2Splat, Less);
 }
 
-WASM_SIMD_TEST(I64x2LeZero) {
+WASM_EXEC_TEST(I64x2LeZero) {
   RunICompareOpConstImmTest<int64_t>(execution_tier, kExprI64x2LeS,
                                      kExprI64x2Splat, LessEqual);
 }
 
-WASM_SIMD_TEST(F64x2Splat) {
+WASM_EXEC_TEST(F64x2Splat) {
   WasmRunner<int32_t, double> r(execution_tier);
   // Set up a global to hold output vector.
   double* g = r.builder().AddGlobal<double>(kWasmS128);
@@ -742,7 +726,7 @@
   }
 }
 
-WASM_SIMD_TEST(F64x2ExtractLane) {
+WASM_EXEC_TEST(F64x2ExtractLane) {
   WasmRunner<double, double> r(execution_tier);
   byte param1 = 0;
   byte temp1 = r.AllocateLocal(kWasmF64);
@@ -764,7 +748,7 @@
   }
 }
 
-WASM_SIMD_TEST(F64x2ReplaceLane) {
+WASM_EXEC_TEST(F64x2ReplaceLane) {
   WasmRunner<int32_t> r(execution_tier);
   // Set up globals to hold input/output vector.
   double* g0 = r.builder().AddGlobal<double>(kWasmS128);
@@ -787,7 +771,7 @@
   CHECK_EQ(1., LANE(g1, 1));
 }
 
-WASM_SIMD_TEST(F64x2ExtractLaneWithI64x2) {
+WASM_EXEC_TEST(F64x2ExtractLaneWithI64x2) {
   WasmRunner<int64_t> r(execution_tier);
   BUILD(r, WASM_IF_ELSE_L(
                WASM_F64_EQ(WASM_SIMD_F64x2_EXTRACT_LANE(
@@ -797,7 +781,7 @@
   CHECK_EQ(1, r.Call());
 }
 
-WASM_SIMD_TEST(I64x2ExtractWithF64x2) {
+WASM_EXEC_TEST(I64x2ExtractWithF64x2) {
   WasmRunner<int64_t> r(execution_tier);
   BUILD(r, WASM_IF_ELSE_L(
                WASM_I64_EQ(WASM_SIMD_I64x2_EXTRACT_LANE(
@@ -807,31 +791,31 @@
   CHECK_EQ(1, r.Call());
 }
 
-WASM_SIMD_TEST(F64x2Abs) {
+WASM_EXEC_TEST(F64x2Abs) {
   RunF64x2UnOpTest(execution_tier, kExprF64x2Abs, std::abs);
 }
 
-WASM_SIMD_TEST(F64x2Neg) {
+WASM_EXEC_TEST(F64x2Neg) {
   RunF64x2UnOpTest(execution_tier, kExprF64x2Neg, Negate);
 }
 
-WASM_SIMD_TEST(F64x2Sqrt) {
+WASM_EXEC_TEST(F64x2Sqrt) {
   RunF64x2UnOpTest(execution_tier, kExprF64x2Sqrt, std::sqrt);
 }
 
-WASM_SIMD_TEST(F64x2Ceil) {
+WASM_EXEC_TEST(F64x2Ceil) {
   RunF64x2UnOpTest(execution_tier, kExprF64x2Ceil, ceil, true);
 }
 
-WASM_SIMD_TEST(F64x2Floor) {
+WASM_EXEC_TEST(F64x2Floor) {
   RunF64x2UnOpTest(execution_tier, kExprF64x2Floor, floor, true);
 }
 
-WASM_SIMD_TEST(F64x2Trunc) {
+WASM_EXEC_TEST(F64x2Trunc) {
   RunF64x2UnOpTest(execution_tier, kExprF64x2Trunc, trunc, true);
 }
 
-WASM_SIMD_TEST(F64x2NearestInt) {
+WASM_EXEC_TEST(F64x2NearestInt) {
   RunF64x2UnOpTest(execution_tier, kExprF64x2NearestInt, nearbyint, true);
 }
 
@@ -860,12 +844,12 @@
   }
 }
 
-WASM_SIMD_TEST(F64x2ConvertLowI32x4S) {
+WASM_EXEC_TEST(F64x2ConvertLowI32x4S) {
   RunF64x2ConvertLowI32x4Test<int32_t>(execution_tier,
                                        kExprF64x2ConvertLowI32x4S);
 }
 
-WASM_SIMD_TEST(F64x2ConvertLowI32x4U) {
+WASM_EXEC_TEST(F64x2ConvertLowI32x4U) {
   RunF64x2ConvertLowI32x4Test<uint32_t>(execution_tier,
                                         kExprF64x2ConvertLowI32x4U);
 }
@@ -895,17 +879,17 @@
   }
 }
 
-WASM_SIMD_TEST(I32x4TruncSatF64x2SZero) {
+WASM_EXEC_TEST(I32x4TruncSatF64x2SZero) {
   RunI32x4TruncSatF64x2Test<int32_t>(execution_tier,
                                      kExprI32x4TruncSatF64x2SZero);
 }
 
-WASM_SIMD_TEST(I32x4TruncSatF64x2UZero) {
+WASM_EXEC_TEST(I32x4TruncSatF64x2UZero) {
   RunI32x4TruncSatF64x2Test<uint32_t>(execution_tier,
                                       kExprI32x4TruncSatF64x2UZero);
 }
 
-WASM_SIMD_TEST(F32x4DemoteF64x2Zero) {
+WASM_EXEC_TEST(F32x4DemoteF64x2Zero) {
   WasmRunner<int32_t, double> r(execution_tier);
   float* g = r.builder().AddGlobal<float>(kWasmS128);
   BUILD(r,
@@ -928,7 +912,7 @@
   }
 }
 
-WASM_SIMD_TEST(F64x2PromoteLowF32x4) {
+WASM_EXEC_TEST(F64x2PromoteLowF32x4) {
   WasmRunner<int32_t, float> r(execution_tier);
   double* g = r.builder().AddGlobal<double>(kWasmS128);
   BUILD(r,
@@ -951,7 +935,7 @@
 // architectures). These 2 opcodes should be fused into a single instruction
 // with memory operands, which is tested in instruction-selector tests. This
 // test checks that we get correct results.
-WASM_SIMD_TEST(F64x2PromoteLowF32x4WithS128Load64Zero) {
+WASM_EXEC_TEST(F64x2PromoteLowF32x4WithS128Load64Zero) {
   {
     WasmRunner<int32_t> r(execution_tier);
     double* g = r.builder().AddGlobal<double>(kWasmS128);
@@ -992,97 +976,97 @@
   }
 }
 
-WASM_SIMD_TEST(F64x2Add) {
+WASM_EXEC_TEST(F64x2Add) {
   RunF64x2BinOpTest(execution_tier, kExprF64x2Add, Add);
 }
 
-WASM_SIMD_TEST(F64x2Sub) {
+WASM_EXEC_TEST(F64x2Sub) {
   RunF64x2BinOpTest(execution_tier, kExprF64x2Sub, Sub);
 }
 
-WASM_SIMD_TEST(F64x2Mul) {
+WASM_EXEC_TEST(F64x2Mul) {
   RunF64x2BinOpTest(execution_tier, kExprF64x2Mul, Mul);
 }
 
-WASM_SIMD_TEST(F64x2Div) {
+WASM_EXEC_TEST(F64x2Div) {
   RunF64x2BinOpTest(execution_tier, kExprF64x2Div, base::Divide);
 }
 
-WASM_SIMD_TEST(F64x2Pmin) {
+WASM_EXEC_TEST(F64x2Pmin) {
   RunF64x2BinOpTest(execution_tier, kExprF64x2Pmin, Minimum);
 }
 
-WASM_SIMD_TEST(F64x2Pmax) {
+WASM_EXEC_TEST(F64x2Pmax) {
   RunF64x2BinOpTest(execution_tier, kExprF64x2Pmax, Maximum);
 }
 
-WASM_SIMD_TEST(F64x2Eq) {
+WASM_EXEC_TEST(F64x2Eq) {
   RunF64x2CompareOpTest(execution_tier, kExprF64x2Eq, Equal);
 }
 
-WASM_SIMD_TEST(F64x2Ne) {
+WASM_EXEC_TEST(F64x2Ne) {
   RunF64x2CompareOpTest(execution_tier, kExprF64x2Ne, NotEqual);
 }
 
-WASM_SIMD_TEST(F64x2Gt) {
+WASM_EXEC_TEST(F64x2Gt) {
   RunF64x2CompareOpTest(execution_tier, kExprF64x2Gt, Greater);
 }
 
-WASM_SIMD_TEST(F64x2Ge) {
+WASM_EXEC_TEST(F64x2Ge) {
   RunF64x2CompareOpTest(execution_tier, kExprF64x2Ge, GreaterEqual);
 }
 
-WASM_SIMD_TEST(F64x2Lt) {
+WASM_EXEC_TEST(F64x2Lt) {
   RunF64x2CompareOpTest(execution_tier, kExprF64x2Lt, Less);
 }
 
-WASM_SIMD_TEST(F64x2Le) {
+WASM_EXEC_TEST(F64x2Le) {
   RunF64x2CompareOpTest(execution_tier, kExprF64x2Le, LessEqual);
 }
 
-WASM_SIMD_TEST(F64x2EqZero) {
+WASM_EXEC_TEST(F64x2EqZero) {
   RunF128CompareOpConstImmTest<double, int64_t>(execution_tier, kExprF64x2Eq,
                                                 kExprF64x2Splat, Equal);
 }
 
-WASM_SIMD_TEST(F64x2NeZero) {
+WASM_EXEC_TEST(F64x2NeZero) {
   RunF128CompareOpConstImmTest<double, int64_t>(execution_tier, kExprF64x2Ne,
                                                 kExprF64x2Splat, NotEqual);
 }
 
-WASM_SIMD_TEST(F64x2GtZero) {
+WASM_EXEC_TEST(F64x2GtZero) {
   RunF128CompareOpConstImmTest<double, int64_t>(execution_tier, kExprF64x2Gt,
                                                 kExprF64x2Splat, Greater);
 }
 
-WASM_SIMD_TEST(F64x2GeZero) {
+WASM_EXEC_TEST(F64x2GeZero) {
   RunF128CompareOpConstImmTest<double, int64_t>(execution_tier, kExprF64x2Ge,
                                                 kExprF64x2Splat, GreaterEqual);
 }
 
-WASM_SIMD_TEST(F64x2LtZero) {
+WASM_EXEC_TEST(F64x2LtZero) {
   RunF128CompareOpConstImmTest<double, int64_t>(execution_tier, kExprF64x2Lt,
                                                 kExprF64x2Splat, Less);
 }
 
-WASM_SIMD_TEST(F64x2LeZero) {
+WASM_EXEC_TEST(F64x2LeZero) {
   RunF128CompareOpConstImmTest<double, int64_t>(execution_tier, kExprF64x2Le,
                                                 kExprF64x2Splat, LessEqual);
 }
 
-WASM_SIMD_TEST(F64x2Min) {
+WASM_EXEC_TEST(F64x2Min) {
   RunF64x2BinOpTest(execution_tier, kExprF64x2Min, JSMin);
 }
 
-WASM_SIMD_TEST(F64x2Max) {
+WASM_EXEC_TEST(F64x2Max) {
   RunF64x2BinOpTest(execution_tier, kExprF64x2Max, JSMax);
 }
 
-WASM_SIMD_TEST(I64x2Mul) {
+WASM_EXEC_TEST(I64x2Mul) {
   RunI64x2BinOpTest(execution_tier, kExprI64x2Mul, base::MulWithWraparound);
 }
 
-WASM_SIMD_TEST(I32x4Splat) {
+WASM_EXEC_TEST(I32x4Splat) {
   WasmRunner<int32_t, int32_t> r(execution_tier);
   // Set up a global to hold output vector.
   int32_t* g = r.builder().AddGlobal<int32_t>(kWasmS128);
@@ -1100,7 +1084,7 @@
   }
 }
 
-WASM_SIMD_TEST(I32x4ReplaceLane) {
+WASM_EXEC_TEST(I32x4ReplaceLane) {
   WasmRunner<int32_t> r(execution_tier);
   // Set up a global to hold input/output vector.
   int32_t* g = r.builder().AddGlobal<int32_t>(kWasmS128);
@@ -1123,7 +1107,7 @@
   }
 }
 
-WASM_SIMD_TEST(I16x8Splat) {
+WASM_EXEC_TEST(I16x8Splat) {
   WasmRunner<int32_t, int32_t> r(execution_tier);
   // Set up a global to hold output vector.
   int16_t* g = r.builder().AddGlobal<int16_t>(kWasmS128);
@@ -1151,7 +1135,7 @@
   }
 }
 
-WASM_SIMD_TEST(I16x8ReplaceLane) {
+WASM_EXEC_TEST(I16x8ReplaceLane) {
   WasmRunner<int32_t> r(execution_tier);
   // Set up a global to hold input/output vector.
   int16_t* g = r.builder().AddGlobal<int16_t>(kWasmS128);
@@ -1182,7 +1166,7 @@
   }
 }
 
-WASM_SIMD_TEST(I8x16BitMask) {
+WASM_EXEC_TEST(I8x16BitMask) {
   WasmRunner<int32_t, int32_t> r(execution_tier);
   byte value1 = r.AllocateLocal(kWasmS128);
 
@@ -1201,7 +1185,7 @@
   }
 }
 
-WASM_SIMD_TEST(I16x8BitMask) {
+WASM_EXEC_TEST(I16x8BitMask) {
   WasmRunner<int32_t, int32_t> r(execution_tier);
   byte value1 = r.AllocateLocal(kWasmS128);
 
@@ -1220,7 +1204,7 @@
   }
 }
 
-WASM_SIMD_TEST(I32x4BitMask) {
+WASM_EXEC_TEST(I32x4BitMask) {
   WasmRunner<int32_t, int32_t> r(execution_tier);
   byte value1 = r.AllocateLocal(kWasmS128);
 
@@ -1239,7 +1223,7 @@
   }
 }
 
-WASM_SIMD_TEST(I64x2BitMask) {
+WASM_EXEC_TEST(I64x2BitMask) {
   WasmRunner<int32_t, int64_t> r(execution_tier);
   byte value1 = r.AllocateLocal(kWasmS128);
 
@@ -1256,7 +1240,7 @@
   }
 }
 
-WASM_SIMD_TEST(I8x16Splat) {
+WASM_EXEC_TEST(I8x16Splat) {
   WasmRunner<int32_t, int32_t> r(execution_tier);
   // Set up a global to hold output vector.
   int8_t* g = r.builder().AddGlobal<int8_t>(kWasmS128);
@@ -1284,7 +1268,7 @@
   }
 }
 
-WASM_SIMD_TEST(I8x16ReplaceLane) {
+WASM_EXEC_TEST(I8x16ReplaceLane) {
   WasmRunner<int32_t> r(execution_tier);
   // Set up a global to hold input/output vector.
   int8_t* g = r.builder().AddGlobal<int8_t>(kWasmS128);
@@ -1346,7 +1330,7 @@
 }
 
 // Tests both signed and unsigned conversion.
-WASM_SIMD_TEST(I32x4ConvertF32x4) {
+WASM_EXEC_TEST(I32x4ConvertF32x4) {
   WasmRunner<int32_t, float> r(execution_tier);
   // Create two output vectors to hold signed and unsigned results.
   int32_t* g0 = r.builder().AddGlobal<int32_t>(kWasmS128);
@@ -1374,7 +1358,7 @@
 }
 
 // Tests both signed and unsigned conversion from I16x8 (unpacking).
-WASM_SIMD_TEST(I32x4ConvertI16x8) {
+WASM_EXEC_TEST(I32x4ConvertI16x8) {
   WasmRunner<int32_t, int32_t> r(execution_tier);
   // Create four output vectors to hold signed and unsigned results.
   int32_t* g0 = r.builder().AddGlobal<int32_t>(kWasmS128);
@@ -1409,7 +1393,7 @@
 }
 
 // Tests both signed and unsigned conversion from I32x4 (unpacking).
-WASM_SIMD_TEST(I64x2ConvertI32x4) {
+WASM_EXEC_TEST(I64x2ConvertI32x4) {
   WasmRunner<int32_t, int32_t> r(execution_tier);
   // Create four output vectors to hold signed and unsigned results.
   int64_t* g0 = r.builder().AddGlobal<int64_t>(kWasmS128);
@@ -1444,15 +1428,15 @@
   }
 }
 
-WASM_SIMD_TEST(I32x4Neg) {
+WASM_EXEC_TEST(I32x4Neg) {
   RunI32x4UnOpTest(execution_tier, kExprI32x4Neg, base::NegateWithWraparound);
 }
 
-WASM_SIMD_TEST(I32x4Abs) {
+WASM_EXEC_TEST(I32x4Abs) {
   RunI32x4UnOpTest(execution_tier, kExprI32x4Abs, std::abs);
 }
 
-WASM_SIMD_TEST(S128Not) {
+WASM_EXEC_TEST(S128Not) {
   RunI32x4UnOpTest(execution_tier, kExprS128Not, [](int32_t x) { return ~x; });
 }
 
@@ -1487,60 +1471,60 @@
 constexpr Shuffle interleave_8x16_shuffle = {0, 17, 2,  19, 4,  21, 6,  23,
                                              8, 25, 10, 27, 12, 29, 14, 31};
 
-WASM_SIMD_TEST(I32x4ExtAddPairwiseI16x8S) {
+WASM_EXEC_TEST(I32x4ExtAddPairwiseI16x8S) {
   RunExtAddPairwiseTest<int16_t, int32_t>(
       execution_tier, kExprI32x4ExtAddPairwiseI16x8S, kExprI16x8Splat,
       interleave_16x8_shuffle);
 }
 
-WASM_SIMD_TEST(I32x4ExtAddPairwiseI16x8U) {
+WASM_EXEC_TEST(I32x4ExtAddPairwiseI16x8U) {
   RunExtAddPairwiseTest<uint16_t, uint32_t>(
       execution_tier, kExprI32x4ExtAddPairwiseI16x8U, kExprI16x8Splat,
       interleave_16x8_shuffle);
 }
 
-WASM_SIMD_TEST(I16x8ExtAddPairwiseI8x16S) {
+WASM_EXEC_TEST(I16x8ExtAddPairwiseI8x16S) {
   RunExtAddPairwiseTest<int8_t, int16_t>(
       execution_tier, kExprI16x8ExtAddPairwiseI8x16S, kExprI8x16Splat,
       interleave_8x16_shuffle);
 }
 
-WASM_SIMD_TEST(I16x8ExtAddPairwiseI8x16U) {
+WASM_EXEC_TEST(I16x8ExtAddPairwiseI8x16U) {
   RunExtAddPairwiseTest<uint8_t, uint16_t>(
       execution_tier, kExprI16x8ExtAddPairwiseI8x16U, kExprI8x16Splat,
       interleave_8x16_shuffle);
 }
 
-WASM_SIMD_TEST(I32x4Add) {
+WASM_EXEC_TEST(I32x4Add) {
   RunI32x4BinOpTest(execution_tier, kExprI32x4Add, base::AddWithWraparound);
 }
 
-WASM_SIMD_TEST(I32x4Sub) {
+WASM_EXEC_TEST(I32x4Sub) {
   RunI32x4BinOpTest(execution_tier, kExprI32x4Sub, base::SubWithWraparound);
 }
 
-WASM_SIMD_TEST(I32x4Mul) {
+WASM_EXEC_TEST(I32x4Mul) {
   RunI32x4BinOpTest(execution_tier, kExprI32x4Mul, base::MulWithWraparound);
 }
 
-WASM_SIMD_TEST(I32x4MinS) {
+WASM_EXEC_TEST(I32x4MinS) {
   RunI32x4BinOpTest(execution_tier, kExprI32x4MinS, Minimum);
 }
 
-WASM_SIMD_TEST(I32x4MaxS) {
+WASM_EXEC_TEST(I32x4MaxS) {
   RunI32x4BinOpTest(execution_tier, kExprI32x4MaxS, Maximum);
 }
 
-WASM_SIMD_TEST(I32x4MinU) {
+WASM_EXEC_TEST(I32x4MinU) {
   RunI32x4BinOpTest(execution_tier, kExprI32x4MinU, UnsignedMinimum);
 }
-WASM_SIMD_TEST(I32x4MaxU) {
+WASM_EXEC_TEST(I32x4MaxU) {
   RunI32x4BinOpTest(execution_tier, kExprI32x4MaxU,
 
                     UnsignedMaximum);
 }
 
-WASM_SIMD_TEST(S128And) {
+WASM_EXEC_TEST(S128And) {
   RunI32x4BinOpTest(execution_tier, kExprS128And,
                     [](int32_t x, int32_t y) { return x & y; });
 }
@@ -1599,7 +1583,7 @@
   }
 }
 
-WASM_SIMD_TEST(S128AndImm) {
+WASM_EXEC_TEST(S128AndImm) {
   RunS128ConstBinOpTest<int32_t>(execution_tier, kConstLeft, kExprS128And,
                                  kExprI32x4Splat,
                                  [](int32_t x, int32_t y) { return x & y; });
@@ -1614,23 +1598,23 @@
       [](int16_t x, int16_t y) { return static_cast<int16_t>(x & y); });
 }
 
-WASM_SIMD_TEST(S128Or) {
+WASM_EXEC_TEST(S128Or) {
   RunI32x4BinOpTest(execution_tier, kExprS128Or,
                     [](int32_t x, int32_t y) { return x | y; });
 }
 
-WASM_SIMD_TEST(S128Xor) {
+WASM_EXEC_TEST(S128Xor) {
   RunI32x4BinOpTest(execution_tier, kExprS128Xor,
                     [](int32_t x, int32_t y) { return x ^ y; });
 }
 
 // Bitwise operation, doesn't really matter what simd type we test it with.
-WASM_SIMD_TEST(S128AndNot) {
+WASM_EXEC_TEST(S128AndNot) {
   RunI32x4BinOpTest(execution_tier, kExprS128AndNot,
                     [](int32_t x, int32_t y) { return x & ~y; });
 }
 
-WASM_SIMD_TEST(S128AndNotImm) {
+WASM_EXEC_TEST(S128AndNotImm) {
   RunS128ConstBinOpTest<int32_t>(execution_tier, kConstLeft, kExprS128AndNot,
                                  kExprI32x4Splat,
                                  [](int32_t x, int32_t y) { return x & ~y; });
@@ -1645,89 +1629,89 @@
       [](int16_t x, int16_t y) { return static_cast<int16_t>(x & ~y); });
 }
 
-WASM_SIMD_TEST(I32x4Eq) {
+WASM_EXEC_TEST(I32x4Eq) {
   RunI32x4BinOpTest(execution_tier, kExprI32x4Eq, Equal);
 }
 
-WASM_SIMD_TEST(I32x4Ne) {
+WASM_EXEC_TEST(I32x4Ne) {
   RunI32x4BinOpTest(execution_tier, kExprI32x4Ne, NotEqual);
 }
 
-WASM_SIMD_TEST(I32x4LtS) {
+WASM_EXEC_TEST(I32x4LtS) {
   RunI32x4BinOpTest(execution_tier, kExprI32x4LtS, Less);
 }
 
-WASM_SIMD_TEST(I32x4LeS) {
+WASM_EXEC_TEST(I32x4LeS) {
   RunI32x4BinOpTest(execution_tier, kExprI32x4LeS, LessEqual);
 }
 
-WASM_SIMD_TEST(I32x4GtS) {
+WASM_EXEC_TEST(I32x4GtS) {
   RunI32x4BinOpTest(execution_tier, kExprI32x4GtS, Greater);
 }
 
-WASM_SIMD_TEST(I32x4GeS) {
+WASM_EXEC_TEST(I32x4GeS) {
   RunI32x4BinOpTest(execution_tier, kExprI32x4GeS, GreaterEqual);
 }
 
-WASM_SIMD_TEST(I32x4LtU) {
+WASM_EXEC_TEST(I32x4LtU) {
   RunI32x4BinOpTest(execution_tier, kExprI32x4LtU, UnsignedLess);
 }
 
-WASM_SIMD_TEST(I32x4LeU) {
+WASM_EXEC_TEST(I32x4LeU) {
   RunI32x4BinOpTest(execution_tier, kExprI32x4LeU, UnsignedLessEqual);
 }
 
-WASM_SIMD_TEST(I32x4GtU) {
+WASM_EXEC_TEST(I32x4GtU) {
   RunI32x4BinOpTest(execution_tier, kExprI32x4GtU, UnsignedGreater);
 }
 
-WASM_SIMD_TEST(I32x4GeU) {
+WASM_EXEC_TEST(I32x4GeU) {
   RunI32x4BinOpTest(execution_tier, kExprI32x4GeU, UnsignedGreaterEqual);
 }
 
-WASM_SIMD_TEST(I32x4EqZero) {
+WASM_EXEC_TEST(I32x4EqZero) {
   RunICompareOpConstImmTest<int32_t>(execution_tier, kExprI32x4Eq,
                                      kExprI32x4Splat, Equal);
 }
 
-WASM_SIMD_TEST(I32x4NeZero) {
+WASM_EXEC_TEST(I32x4NeZero) {
   RunICompareOpConstImmTest<int32_t>(execution_tier, kExprI32x4Ne,
                                      kExprI32x4Splat, NotEqual);
 }
 
-WASM_SIMD_TEST(I32x4GtZero) {
+WASM_EXEC_TEST(I32x4GtZero) {
   RunICompareOpConstImmTest<int32_t>(execution_tier, kExprI32x4GtS,
                                      kExprI32x4Splat, Greater);
 }
 
-WASM_SIMD_TEST(I32x4GeZero) {
+WASM_EXEC_TEST(I32x4GeZero) {
   RunICompareOpConstImmTest<int32_t>(execution_tier, kExprI32x4GeS,
                                      kExprI32x4Splat, GreaterEqual);
 }
 
-WASM_SIMD_TEST(I32x4LtZero) {
+WASM_EXEC_TEST(I32x4LtZero) {
   RunICompareOpConstImmTest<int32_t>(execution_tier, kExprI32x4LtS,
                                      kExprI32x4Splat, Less);
 }
 
-WASM_SIMD_TEST(I32x4LeZero) {
+WASM_EXEC_TEST(I32x4LeZero) {
   RunICompareOpConstImmTest<int32_t>(execution_tier, kExprI32x4LeS,
                                      kExprI32x4Splat, LessEqual);
 }
 
-WASM_SIMD_TEST(I32x4Shl) {
+WASM_EXEC_TEST(I32x4Shl) {
   RunI32x4ShiftOpTest(execution_tier, kExprI32x4Shl, LogicalShiftLeft);
 }
 
-WASM_SIMD_TEST(I32x4ShrS) {
+WASM_EXEC_TEST(I32x4ShrS) {
   RunI32x4ShiftOpTest(execution_tier, kExprI32x4ShrS, ArithmeticShiftRight);
 }
 
-WASM_SIMD_TEST(I32x4ShrU) {
+WASM_EXEC_TEST(I32x4ShrU) {
   RunI32x4ShiftOpTest(execution_tier, kExprI32x4ShrU, LogicalShiftRight);
 }
 
-WASM_SIMD_TEST(I32x4ShiftAdd) {
+WASM_EXEC_TEST(I32x4ShiftAdd) {
   for (int imm = 0; imm <= 32; imm++) {
     RunShiftAddTestSequence<int32_t>(execution_tier, kExprI32x4ShrU,
                                      kExprI32x4Add, kExprI32x4Splat, imm,
@@ -1739,7 +1723,7 @@
 }
 
 // Tests both signed and unsigned conversion from I8x16 (unpacking).
-WASM_SIMD_TEST(I16x8ConvertI8x16) {
+WASM_EXEC_TEST(I16x8ConvertI8x16) {
   WasmRunner<int32_t, int32_t> r(execution_tier);
   // Create four output vectors to hold signed and unsigned results.
   int16_t* g0 = r.builder().AddGlobal<int16_t>(kWasmS128);
@@ -1774,7 +1758,7 @@
 }
 
 // Tests both signed and unsigned conversion from I32x4 (packing).
-WASM_SIMD_TEST(I16x8ConvertI32x4) {
+WASM_EXEC_TEST(I16x8ConvertI32x4) {
   WasmRunner<int32_t, int32_t> r(execution_tier);
   // Create output vectors to hold signed and unsigned results.
   int16_t* g0 = r.builder().AddGlobal<int16_t>(kWasmS128);
@@ -1802,136 +1786,136 @@
   }
 }
 
-WASM_SIMD_TEST(I16x8Neg) {
+WASM_EXEC_TEST(I16x8Neg) {
   RunI16x8UnOpTest(execution_tier, kExprI16x8Neg, base::NegateWithWraparound);
 }
 
-WASM_SIMD_TEST(I16x8Abs) {
+WASM_EXEC_TEST(I16x8Abs) {
   RunI16x8UnOpTest(execution_tier, kExprI16x8Abs, Abs);
 }
 
-WASM_SIMD_TEST(I16x8Add) {
+WASM_EXEC_TEST(I16x8Add) {
   RunI16x8BinOpTest(execution_tier, kExprI16x8Add, base::AddWithWraparound);
 }
 
-WASM_SIMD_TEST(I16x8AddSatS) {
+WASM_EXEC_TEST(I16x8AddSatS) {
   RunI16x8BinOpTest(execution_tier, kExprI16x8AddSatS, SaturateAdd<int16_t>);
 }
 
-WASM_SIMD_TEST(I16x8Sub) {
+WASM_EXEC_TEST(I16x8Sub) {
   RunI16x8BinOpTest(execution_tier, kExprI16x8Sub, base::SubWithWraparound);
 }
 
-WASM_SIMD_TEST(I16x8SubSatS) {
+WASM_EXEC_TEST(I16x8SubSatS) {
   RunI16x8BinOpTest(execution_tier, kExprI16x8SubSatS, SaturateSub<int16_t>);
 }
 
-WASM_SIMD_TEST(I16x8Mul) {
+WASM_EXEC_TEST(I16x8Mul) {
   RunI16x8BinOpTest(execution_tier, kExprI16x8Mul, base::MulWithWraparound);
 }
 
-WASM_SIMD_TEST(I16x8MinS) {
+WASM_EXEC_TEST(I16x8MinS) {
   RunI16x8BinOpTest(execution_tier, kExprI16x8MinS, Minimum);
 }
 
-WASM_SIMD_TEST(I16x8MaxS) {
+WASM_EXEC_TEST(I16x8MaxS) {
   RunI16x8BinOpTest(execution_tier, kExprI16x8MaxS, Maximum);
 }
 
-WASM_SIMD_TEST(I16x8AddSatU) {
+WASM_EXEC_TEST(I16x8AddSatU) {
   RunI16x8BinOpTest<uint16_t>(execution_tier, kExprI16x8AddSatU,
                               SaturateAdd<uint16_t>);
 }
 
-WASM_SIMD_TEST(I16x8SubSatU) {
+WASM_EXEC_TEST(I16x8SubSatU) {
   RunI16x8BinOpTest<uint16_t>(execution_tier, kExprI16x8SubSatU,
                               SaturateSub<uint16_t>);
 }
 
-WASM_SIMD_TEST(I16x8MinU) {
+WASM_EXEC_TEST(I16x8MinU) {
   RunI16x8BinOpTest(execution_tier, kExprI16x8MinU, UnsignedMinimum);
 }
 
-WASM_SIMD_TEST(I16x8MaxU) {
+WASM_EXEC_TEST(I16x8MaxU) {
   RunI16x8BinOpTest(execution_tier, kExprI16x8MaxU, UnsignedMaximum);
 }
 
-WASM_SIMD_TEST(I16x8Eq) {
+WASM_EXEC_TEST(I16x8Eq) {
   RunI16x8BinOpTest(execution_tier, kExprI16x8Eq, Equal);
 }
 
-WASM_SIMD_TEST(I16x8Ne) {
+WASM_EXEC_TEST(I16x8Ne) {
   RunI16x8BinOpTest(execution_tier, kExprI16x8Ne, NotEqual);
 }
 
-WASM_SIMD_TEST(I16x8LtS) {
+WASM_EXEC_TEST(I16x8LtS) {
   RunI16x8BinOpTest(execution_tier, kExprI16x8LtS, Less);
 }
 
-WASM_SIMD_TEST(I16x8LeS) {
+WASM_EXEC_TEST(I16x8LeS) {
   RunI16x8BinOpTest(execution_tier, kExprI16x8LeS, LessEqual);
 }
 
-WASM_SIMD_TEST(I16x8GtS) {
+WASM_EXEC_TEST(I16x8GtS) {
   RunI16x8BinOpTest(execution_tier, kExprI16x8GtS, Greater);
 }
 
-WASM_SIMD_TEST(I16x8GeS) {
+WASM_EXEC_TEST(I16x8GeS) {
   RunI16x8BinOpTest(execution_tier, kExprI16x8GeS, GreaterEqual);
 }
 
-WASM_SIMD_TEST(I16x8GtU) {
+WASM_EXEC_TEST(I16x8GtU) {
   RunI16x8BinOpTest(execution_tier, kExprI16x8GtU, UnsignedGreater);
 }
 
-WASM_SIMD_TEST(I16x8GeU) {
+WASM_EXEC_TEST(I16x8GeU) {
   RunI16x8BinOpTest(execution_tier, kExprI16x8GeU, UnsignedGreaterEqual);
 }
 
-WASM_SIMD_TEST(I16x8LtU) {
+WASM_EXEC_TEST(I16x8LtU) {
   RunI16x8BinOpTest(execution_tier, kExprI16x8LtU, UnsignedLess);
 }
 
-WASM_SIMD_TEST(I16x8LeU) {
+WASM_EXEC_TEST(I16x8LeU) {
   RunI16x8BinOpTest(execution_tier, kExprI16x8LeU, UnsignedLessEqual);
 }
 
-WASM_SIMD_TEST(I16x8EqZero) {
+WASM_EXEC_TEST(I16x8EqZero) {
   RunICompareOpConstImmTest<int16_t>(execution_tier, kExprI16x8Eq,
                                      kExprI16x8Splat, Equal);
 }
 
-WASM_SIMD_TEST(I16x8NeZero) {
+WASM_EXEC_TEST(I16x8NeZero) {
   RunICompareOpConstImmTest<int16_t>(execution_tier, kExprI16x8Ne,
                                      kExprI16x8Splat, NotEqual);
 }
 
-WASM_SIMD_TEST(I16x8GtZero) {
+WASM_EXEC_TEST(I16x8GtZero) {
   RunICompareOpConstImmTest<int16_t>(execution_tier, kExprI16x8GtS,
                                      kExprI16x8Splat, Greater);
 }
 
-WASM_SIMD_TEST(I16x8GeZero) {
+WASM_EXEC_TEST(I16x8GeZero) {
   RunICompareOpConstImmTest<int16_t>(execution_tier, kExprI16x8GeS,
                                      kExprI16x8Splat, GreaterEqual);
 }
 
-WASM_SIMD_TEST(I16x8LtZero) {
+WASM_EXEC_TEST(I16x8LtZero) {
   RunICompareOpConstImmTest<int16_t>(execution_tier, kExprI16x8LtS,
                                      kExprI16x8Splat, Less);
 }
 
-WASM_SIMD_TEST(I16x8LeZero) {
+WASM_EXEC_TEST(I16x8LeZero) {
   RunICompareOpConstImmTest<int16_t>(execution_tier, kExprI16x8LeS,
                                      kExprI16x8Splat, LessEqual);
 }
 
-WASM_SIMD_TEST(I16x8RoundingAverageU) {
+WASM_EXEC_TEST(I16x8RoundingAverageU) {
   RunI16x8BinOpTest<uint16_t>(execution_tier, kExprI16x8RoundingAverageU,
                               RoundingAverageUnsigned);
 }
 
-WASM_SIMD_TEST(I16x8Q15MulRSatS) {
+WASM_EXEC_TEST(I16x8Q15MulRSatS) {
   RunI16x8BinOpTest<int16_t>(execution_tier, kExprI16x8Q15MulRSatS,
                              SaturateRoundingQMul<int16_t>);
 }
@@ -1973,69 +1957,69 @@
 }
 }  // namespace
 
-WASM_SIMD_TEST(I16x8ExtMulLowI8x16S) {
+WASM_EXEC_TEST(I16x8ExtMulLowI8x16S) {
   RunExtMulTest<int8_t, int16_t>(execution_tier, kExprI16x8ExtMulLowI8x16S,
                                  MultiplyLong, kExprI8x16Splat, MulHalf::kLow);
 }
 
-WASM_SIMD_TEST(I16x8ExtMulHighI8x16S) {
+WASM_EXEC_TEST(I16x8ExtMulHighI8x16S) {
   RunExtMulTest<int8_t, int16_t>(execution_tier, kExprI16x8ExtMulHighI8x16S,
                                  MultiplyLong, kExprI8x16Splat, MulHalf::kHigh);
 }
 
-WASM_SIMD_TEST(I16x8ExtMulLowI8x16U) {
+WASM_EXEC_TEST(I16x8ExtMulLowI8x16U) {
   RunExtMulTest<uint8_t, uint16_t>(execution_tier, kExprI16x8ExtMulLowI8x16U,
                                    MultiplyLong, kExprI8x16Splat,
                                    MulHalf::kLow);
 }
 
-WASM_SIMD_TEST(I16x8ExtMulHighI8x16U) {
+WASM_EXEC_TEST(I16x8ExtMulHighI8x16U) {
   RunExtMulTest<uint8_t, uint16_t>(execution_tier, kExprI16x8ExtMulHighI8x16U,
                                    MultiplyLong, kExprI8x16Splat,
                                    MulHalf::kHigh);
 }
 
-WASM_SIMD_TEST(I32x4ExtMulLowI16x8S) {
+WASM_EXEC_TEST(I32x4ExtMulLowI16x8S) {
   RunExtMulTest<int16_t, int32_t>(execution_tier, kExprI32x4ExtMulLowI16x8S,
                                   MultiplyLong, kExprI16x8Splat, MulHalf::kLow);
 }
 
-WASM_SIMD_TEST(I32x4ExtMulHighI16x8S) {
+WASM_EXEC_TEST(I32x4ExtMulHighI16x8S) {
   RunExtMulTest<int16_t, int32_t>(execution_tier, kExprI32x4ExtMulHighI16x8S,
                                   MultiplyLong, kExprI16x8Splat,
                                   MulHalf::kHigh);
 }
 
-WASM_SIMD_TEST(I32x4ExtMulLowI16x8U) {
+WASM_EXEC_TEST(I32x4ExtMulLowI16x8U) {
   RunExtMulTest<uint16_t, uint32_t>(execution_tier, kExprI32x4ExtMulLowI16x8U,
                                     MultiplyLong, kExprI16x8Splat,
                                     MulHalf::kLow);
 }
 
-WASM_SIMD_TEST(I32x4ExtMulHighI16x8U) {
+WASM_EXEC_TEST(I32x4ExtMulHighI16x8U) {
   RunExtMulTest<uint16_t, uint32_t>(execution_tier, kExprI32x4ExtMulHighI16x8U,
                                     MultiplyLong, kExprI16x8Splat,
                                     MulHalf::kHigh);
 }
 
-WASM_SIMD_TEST(I64x2ExtMulLowI32x4S) {
+WASM_EXEC_TEST(I64x2ExtMulLowI32x4S) {
   RunExtMulTest<int32_t, int64_t>(execution_tier, kExprI64x2ExtMulLowI32x4S,
                                   MultiplyLong, kExprI32x4Splat, MulHalf::kLow);
 }
 
-WASM_SIMD_TEST(I64x2ExtMulHighI32x4S) {
+WASM_EXEC_TEST(I64x2ExtMulHighI32x4S) {
   RunExtMulTest<int32_t, int64_t>(execution_tier, kExprI64x2ExtMulHighI32x4S,
                                   MultiplyLong, kExprI32x4Splat,
                                   MulHalf::kHigh);
 }
 
-WASM_SIMD_TEST(I64x2ExtMulLowI32x4U) {
+WASM_EXEC_TEST(I64x2ExtMulLowI32x4U) {
   RunExtMulTest<uint32_t, uint64_t>(execution_tier, kExprI64x2ExtMulLowI32x4U,
                                     MultiplyLong, kExprI32x4Splat,
                                     MulHalf::kLow);
 }
 
-WASM_SIMD_TEST(I64x2ExtMulHighI32x4U) {
+WASM_EXEC_TEST(I64x2ExtMulHighI32x4U) {
   RunExtMulTest<uint32_t, uint64_t>(execution_tier, kExprI64x2ExtMulHighI32x4U,
                                     MultiplyLong, kExprI32x4Splat,
                                     MulHalf::kHigh);
@@ -2082,25 +2066,25 @@
 // optimization.
 #define EXTMUL_ADD_OPTIMIZATION_TEST(NarrowType, NarrowShape, WideType,  \
                                      WideShape)                          \
-  WASM_SIMD_TEST(WideShape##ExtMulLow##NarrowShape##SAddOptimization) {  \
+  WASM_EXEC_TEST(WideShape##ExtMulLow##NarrowShape##SAddOptimization) {  \
     RunExtMulAddOptimizationTest<NarrowType, WideType>(                  \
         execution_tier, kExpr##WideShape##ExtMulLow##NarrowShape##S,     \
         kExpr##NarrowShape##Splat, kExpr##WideShape##Splat,              \
         kExpr##WideShape##Add, base::AddWithWraparound<WideType>);       \
   }                                                                      \
-  WASM_SIMD_TEST(WideShape##ExtMulHigh##NarrowShape##SAddOptimization) { \
+  WASM_EXEC_TEST(WideShape##ExtMulHigh##NarrowShape##SAddOptimization) { \
     RunExtMulAddOptimizationTest<NarrowType, WideType>(                  \
         execution_tier, kExpr##WideShape##ExtMulHigh##NarrowShape##S,    \
         kExpr##NarrowShape##Splat, kExpr##WideShape##Splat,              \
         kExpr##WideShape##Add, base::AddWithWraparound<WideType>);       \
   }                                                                      \
-  WASM_SIMD_TEST(WideShape##ExtMulLow##NarrowShape##UAddOptimization) {  \
+  WASM_EXEC_TEST(WideShape##ExtMulLow##NarrowShape##UAddOptimization) {  \
     RunExtMulAddOptimizationTest<u##NarrowType, u##WideType>(            \
         execution_tier, kExpr##WideShape##ExtMulLow##NarrowShape##U,     \
         kExpr##NarrowShape##Splat, kExpr##WideShape##Splat,              \
         kExpr##WideShape##Add, std::plus<u##WideType>());                \
   }                                                                      \
-  WASM_SIMD_TEST(WideShape##ExtMulHigh##NarrowShape##UAddOptimization) { \
+  WASM_EXEC_TEST(WideShape##ExtMulHigh##NarrowShape##UAddOptimization) { \
     RunExtMulAddOptimizationTest<u##NarrowType, u##WideType>(            \
         execution_tier, kExpr##WideShape##ExtMulHigh##NarrowShape##U,    \
         kExpr##NarrowShape##Splat, kExpr##WideShape##Splat,              \
@@ -2112,7 +2096,7 @@
 
 #undef EXTMUL_ADD_OPTIMIZATION_TEST
 
-WASM_SIMD_TEST(I32x4DotI16x8S) {
+WASM_EXEC_TEST(I32x4DotI16x8S) {
   WasmRunner<int32_t, int16_t, int16_t> r(execution_tier);
   int32_t* g = r.builder().template AddGlobal<int32_t>(kWasmS128);
   byte value1 = 0, value2 = 1;
@@ -2137,19 +2121,19 @@
   }
 }
 
-WASM_SIMD_TEST(I16x8Shl) {
+WASM_EXEC_TEST(I16x8Shl) {
   RunI16x8ShiftOpTest(execution_tier, kExprI16x8Shl, LogicalShiftLeft);
 }
 
-WASM_SIMD_TEST(I16x8ShrS) {
+WASM_EXEC_TEST(I16x8ShrS) {
   RunI16x8ShiftOpTest(execution_tier, kExprI16x8ShrS, ArithmeticShiftRight);
 }
 
-WASM_SIMD_TEST(I16x8ShrU) {
+WASM_EXEC_TEST(I16x8ShrU) {
   RunI16x8ShiftOpTest(execution_tier, kExprI16x8ShrU, LogicalShiftRight);
 }
 
-WASM_SIMD_TEST(I16x8ShiftAdd) {
+WASM_EXEC_TEST(I16x8ShiftAdd) {
   for (int imm = 0; imm <= 16; imm++) {
     RunShiftAddTestSequence<int16_t>(execution_tier, kExprI16x8ShrU,
                                      kExprI16x8Add, kExprI16x8Splat, imm,
@@ -2160,15 +2144,15 @@
   }
 }
 
-WASM_SIMD_TEST(I8x16Neg) {
+WASM_EXEC_TEST(I8x16Neg) {
   RunI8x16UnOpTest(execution_tier, kExprI8x16Neg, base::NegateWithWraparound);
 }
 
-WASM_SIMD_TEST(I8x16Abs) {
+WASM_EXEC_TEST(I8x16Abs) {
   RunI8x16UnOpTest(execution_tier, kExprI8x16Abs, Abs);
 }
 
-WASM_SIMD_TEST(I8x16Popcnt) {
+WASM_EXEC_TEST(I8x16Popcnt) {
   WasmRunner<int32_t, int32_t> r(execution_tier);
   // Global to hold output.
   int8_t* g = r.builder().AddGlobal<int8_t>(kWasmS128);
@@ -2190,7 +2174,7 @@
 }
 
 // Tests both signed and unsigned conversion from I16x8 (packing).
-WASM_SIMD_TEST(I8x16ConvertI16x8) {
+WASM_EXEC_TEST(I8x16ConvertI16x8) {
   WasmRunner<int32_t, int32_t> r(execution_tier);
   // Create output vectors to hold signed and unsigned results.
   int8_t* g_s = r.builder().AddGlobal<int8_t>(kWasmS128);
@@ -2218,136 +2202,136 @@
   }
 }
 
-WASM_SIMD_TEST(I8x16Add) {
+WASM_EXEC_TEST(I8x16Add) {
   RunI8x16BinOpTest(execution_tier, kExprI8x16Add, base::AddWithWraparound);
 }
 
-WASM_SIMD_TEST(I8x16AddSatS) {
+WASM_EXEC_TEST(I8x16AddSatS) {
   RunI8x16BinOpTest(execution_tier, kExprI8x16AddSatS, SaturateAdd<int8_t>);
 }
 
-WASM_SIMD_TEST(I8x16Sub) {
+WASM_EXEC_TEST(I8x16Sub) {
   RunI8x16BinOpTest(execution_tier, kExprI8x16Sub, base::SubWithWraparound);
 }
 
-WASM_SIMD_TEST(I8x16SubSatS) {
+WASM_EXEC_TEST(I8x16SubSatS) {
   RunI8x16BinOpTest(execution_tier, kExprI8x16SubSatS, SaturateSub<int8_t>);
 }
 
-WASM_SIMD_TEST(I8x16MinS) {
+WASM_EXEC_TEST(I8x16MinS) {
   RunI8x16BinOpTest(execution_tier, kExprI8x16MinS, Minimum);
 }
 
-WASM_SIMD_TEST(I8x16MaxS) {
+WASM_EXEC_TEST(I8x16MaxS) {
   RunI8x16BinOpTest(execution_tier, kExprI8x16MaxS, Maximum);
 }
 
-WASM_SIMD_TEST(I8x16AddSatU) {
+WASM_EXEC_TEST(I8x16AddSatU) {
   RunI8x16BinOpTest<uint8_t>(execution_tier, kExprI8x16AddSatU,
                              SaturateAdd<uint8_t>);
 }
 
-WASM_SIMD_TEST(I8x16SubSatU) {
+WASM_EXEC_TEST(I8x16SubSatU) {
   RunI8x16BinOpTest<uint8_t>(execution_tier, kExprI8x16SubSatU,
                              SaturateSub<uint8_t>);
 }
 
-WASM_SIMD_TEST(I8x16MinU) {
+WASM_EXEC_TEST(I8x16MinU) {
   RunI8x16BinOpTest(execution_tier, kExprI8x16MinU, UnsignedMinimum);
 }
 
-WASM_SIMD_TEST(I8x16MaxU) {
+WASM_EXEC_TEST(I8x16MaxU) {
   RunI8x16BinOpTest(execution_tier, kExprI8x16MaxU, UnsignedMaximum);
 }
 
-WASM_SIMD_TEST(I8x16Eq) {
+WASM_EXEC_TEST(I8x16Eq) {
   RunI8x16BinOpTest(execution_tier, kExprI8x16Eq, Equal);
 }
 
-WASM_SIMD_TEST(I8x16Ne) {
+WASM_EXEC_TEST(I8x16Ne) {
   RunI8x16BinOpTest(execution_tier, kExprI8x16Ne, NotEqual);
 }
 
-WASM_SIMD_TEST(I8x16GtS) {
+WASM_EXEC_TEST(I8x16GtS) {
   RunI8x16BinOpTest(execution_tier, kExprI8x16GtS, Greater);
 }
 
-WASM_SIMD_TEST(I8x16GeS) {
+WASM_EXEC_TEST(I8x16GeS) {
   RunI8x16BinOpTest(execution_tier, kExprI8x16GeS, GreaterEqual);
 }
 
-WASM_SIMD_TEST(I8x16LtS) {
+WASM_EXEC_TEST(I8x16LtS) {
   RunI8x16BinOpTest(execution_tier, kExprI8x16LtS, Less);
 }
 
-WASM_SIMD_TEST(I8x16LeS) {
+WASM_EXEC_TEST(I8x16LeS) {
   RunI8x16BinOpTest(execution_tier, kExprI8x16LeS, LessEqual);
 }
 
-WASM_SIMD_TEST(I8x16GtU) {
+WASM_EXEC_TEST(I8x16GtU) {
   RunI8x16BinOpTest(execution_tier, kExprI8x16GtU, UnsignedGreater);
 }
 
-WASM_SIMD_TEST(I8x16GeU) {
+WASM_EXEC_TEST(I8x16GeU) {
   RunI8x16BinOpTest(execution_tier, kExprI8x16GeU, UnsignedGreaterEqual);
 }
 
-WASM_SIMD_TEST(I8x16LtU) {
+WASM_EXEC_TEST(I8x16LtU) {
   RunI8x16BinOpTest(execution_tier, kExprI8x16LtU, UnsignedLess);
 }
 
-WASM_SIMD_TEST(I8x16LeU) {
+WASM_EXEC_TEST(I8x16LeU) {
   RunI8x16BinOpTest(execution_tier, kExprI8x16LeU, UnsignedLessEqual);
 }
 
-WASM_SIMD_TEST(I8x16EqZero) {
+WASM_EXEC_TEST(I8x16EqZero) {
   RunICompareOpConstImmTest<int8_t>(execution_tier, kExprI8x16Eq,
                                     kExprI8x16Splat, Equal);
 }
 
-WASM_SIMD_TEST(I8x16NeZero) {
+WASM_EXEC_TEST(I8x16NeZero) {
   RunICompareOpConstImmTest<int8_t>(execution_tier, kExprI8x16Ne,
                                     kExprI8x16Splat, NotEqual);
 }
 
-WASM_SIMD_TEST(I8x16GtZero) {
+WASM_EXEC_TEST(I8x16GtZero) {
   RunICompareOpConstImmTest<int8_t>(execution_tier, kExprI8x16GtS,
                                     kExprI8x16Splat, Greater);
 }
 
-WASM_SIMD_TEST(I8x16GeZero) {
+WASM_EXEC_TEST(I8x16GeZero) {
   RunICompareOpConstImmTest<int8_t>(execution_tier, kExprI8x16GeS,
                                     kExprI8x16Splat, GreaterEqual);
 }
 
-WASM_SIMD_TEST(I8x16LtZero) {
+WASM_EXEC_TEST(I8x16LtZero) {
   RunICompareOpConstImmTest<int8_t>(execution_tier, kExprI8x16LtS,
                                     kExprI8x16Splat, Less);
 }
 
-WASM_SIMD_TEST(I8x16LeZero) {
+WASM_EXEC_TEST(I8x16LeZero) {
   RunICompareOpConstImmTest<int8_t>(execution_tier, kExprI8x16LeS,
                                     kExprI8x16Splat, LessEqual);
 }
 
-WASM_SIMD_TEST(I8x16RoundingAverageU) {
+WASM_EXEC_TEST(I8x16RoundingAverageU) {
   RunI8x16BinOpTest<uint8_t>(execution_tier, kExprI8x16RoundingAverageU,
                              RoundingAverageUnsigned);
 }
 
-WASM_SIMD_TEST(I8x16Shl) {
+WASM_EXEC_TEST(I8x16Shl) {
   RunI8x16ShiftOpTest(execution_tier, kExprI8x16Shl, LogicalShiftLeft);
 }
 
-WASM_SIMD_TEST(I8x16ShrS) {
+WASM_EXEC_TEST(I8x16ShrS) {
   RunI8x16ShiftOpTest(execution_tier, kExprI8x16ShrS, ArithmeticShiftRight);
 }
 
-WASM_SIMD_TEST(I8x16ShrU) {
+WASM_EXEC_TEST(I8x16ShrU) {
   RunI8x16ShiftOpTest(execution_tier, kExprI8x16ShrU, LogicalShiftRight);
 }
 
-WASM_SIMD_TEST(I8x16ShiftAdd) {
+WASM_EXEC_TEST(I8x16ShiftAdd) {
   for (int imm = 0; imm <= 8; imm++) {
     RunShiftAddTestSequence<int8_t>(execution_tier, kExprI8x16ShrU,
                                     kExprI8x16Add, kExprI8x16Splat, imm,
@@ -2362,7 +2346,7 @@
 // rest false, and comparing for non-equality with zero to convert to a boolean
 // vector.
 #define WASM_SIMD_SELECT_TEST(format)                                        \
-  WASM_SIMD_TEST(S##format##Select) {                                        \
+  WASM_EXEC_TEST(S##format##Select) {                                        \
     WasmRunner<int32_t, int32_t, int32_t> r(execution_tier);                 \
     byte val1 = 0;                                                           \
     byte val2 = 1;                                                           \
@@ -2401,7 +2385,7 @@
 // Test Select by making a mask where the 0th and 3rd lanes are non-zero and the
 // rest 0. The mask is not the result of a comparison op.
 #define WASM_SIMD_NON_CANONICAL_SELECT_TEST(format)                           \
-  WASM_SIMD_TEST(S##format##NonCanonicalSelect) {                             \
+  WASM_EXEC_TEST(S##format##NonCanonicalSelect) {                             \
     WasmRunner<int32_t, int32_t, int32_t, int32_t> r(execution_tier);         \
     byte val1 = 0;                                                            \
     byte val2 = 1;                                                            \
@@ -2597,7 +2581,7 @@
 };
 
 #define SHUFFLE_TEST(Name)                                           \
-  WASM_SIMD_TEST(Name) {                                             \
+  WASM_EXEC_TEST(Name) {                                             \
     ShuffleMap::const_iterator it = test_shuffles.find(k##Name);     \
     DCHECK_NE(it, test_shuffles.end());                              \
     RunShuffleOpTest(execution_tier, kExprI8x16Shuffle, it->second); \
@@ -2607,7 +2591,7 @@
 #undef SHUFFLE_LIST
 
 // Test shuffles that blend the two vectors (elements remain in their lanes.)
-WASM_SIMD_TEST(S8x16Blend) {
+WASM_EXEC_TEST(S8x16Blend) {
   std::array<int8_t, kSimd128Size> expected;
   for (int bias = 1; bias < kSimd128Size; bias++) {
     for (int i = 0; i < bias; i++) expected[i] = i;
@@ -2617,7 +2601,7 @@
 }
 
 // Test shuffles that concatenate the two vectors.
-WASM_SIMD_TEST(S8x16Concat) {
+WASM_EXEC_TEST(S8x16Concat) {
   std::array<int8_t, kSimd128Size> expected;
   // n is offset or bias of concatenation.
   for (int n = 1; n < kSimd128Size; ++n) {
@@ -2634,7 +2618,7 @@
   }
 }
 
-WASM_SIMD_TEST(ShuffleShufps) {
+WASM_EXEC_TEST(ShuffleShufps) {
   // We reverse engineer the shufps immediates into 8x16 shuffles.
   std::array<int8_t, kSimd128Size> expected;
   for (int mask = 0; mask < 256; mask++) {
@@ -2656,7 +2640,7 @@
   }
 }
 
-WASM_SIMD_TEST(I8x16ShuffleWithZeroInput) {
+WASM_EXEC_TEST(I8x16ShuffleWithZeroInput) {
   WasmRunner<int32_t> r(execution_tier);
   static const int kElems = kSimd128Size / sizeof(uint8_t);
   uint8_t* dst = r.builder().AddGlobal<uint8_t>(kWasmS128);
@@ -2709,7 +2693,7 @@
 static constexpr base::Vector<const SwizzleTestArgs> swizzle_test_vector =
     base::ArrayVector(swizzle_test_args);
 
-WASM_SIMD_TEST(I8x16Swizzle) {
+WASM_EXEC_TEST(I8x16Swizzle) {
   // RunBinaryLaneOpTest set up the two globals to be consecutive integers,
   // [0-15] and [16-31]. Using [0-15] as the indices will not sufficiently test
   // swizzle since the expected result is a no-op, using [16-31] will result in
@@ -2782,7 +2766,7 @@
 // Test shuffles that are random combinations of 3 test shuffles. Completely
 // random shuffles almost always generate the slow general shuffle code, so
 // don't exercise as many code paths.
-WASM_SIMD_TEST(I8x16ShuffleFuzz) {
+WASM_EXEC_TEST(I8x16ShuffleFuzz) {
   v8::base::RandomNumberGenerator* rng = CcTest::random_number_generator();
   static const int kTests = 100;
   for (int i = 0; i < kTests; ++i) {
@@ -2838,7 +2822,7 @@
 }
 
 // Test multiple shuffles executed in sequence.
-WASM_SIMD_TEST(S8x16MultiShuffleFuzz) {
+WASM_EXEC_TEST(S8x16MultiShuffleFuzz) {
   // Don't compare interpreter results with itself.
   if (execution_tier == TestExecutionTier::kInterpreter) {
     return;
@@ -2875,7 +2859,7 @@
 // result. Use relational ops on numeric vectors to create the boolean vector
 // test inputs. Test inputs with all true, all false, one true, and one false.
 #define WASM_SIMD_BOOL_REDUCTION_TEST(format, lanes, int_type)                 \
-  WASM_SIMD_TEST(ReductionTest##lanes) {                                       \
+  WASM_EXEC_TEST(ReductionTest##lanes) {                                       \
     WasmRunner<int32_t> r(execution_tier);                                     \
     if (lanes == 2) return;                                                    \
     byte zero = r.AllocateLocal(kWasmS128);                                    \
@@ -2950,7 +2934,7 @@
 WASM_SIMD_BOOL_REDUCTION_TEST(16x8, 8, WASM_I32V)
 WASM_SIMD_BOOL_REDUCTION_TEST(8x16, 16, WASM_I32V)
 
-WASM_SIMD_TEST(SimdI32x4ExtractWithF32x4) {
+WASM_EXEC_TEST(SimdI32x4ExtractWithF32x4) {
   WasmRunner<int32_t> r(execution_tier);
   BUILD(r, WASM_IF_ELSE_I(
                WASM_I32_EQ(WASM_SIMD_I32x4_EXTRACT_LANE(
@@ -2960,7 +2944,7 @@
   CHECK_EQ(1, r.Call());
 }
 
-WASM_SIMD_TEST(SimdF32x4ExtractWithI32x4) {
+WASM_EXEC_TEST(SimdF32x4ExtractWithI32x4) {
   WasmRunner<int32_t> r(execution_tier);
   BUILD(r,
         WASM_IF_ELSE_I(WASM_F32_EQ(WASM_SIMD_F32x4_EXTRACT_LANE(
@@ -2970,7 +2954,7 @@
   CHECK_EQ(1, r.Call());
 }
 
-WASM_SIMD_TEST(SimdF32x4ExtractLane) {
+WASM_EXEC_TEST(SimdF32x4ExtractLane) {
   WasmRunner<float> r(execution_tier);
   r.AllocateLocal(kWasmF32);
   r.AllocateLocal(kWasmS128);
@@ -2982,7 +2966,7 @@
   CHECK_EQ(30.5, r.Call());
 }
 
-WASM_SIMD_TEST(SimdF32x4AddWithI32x4) {
+WASM_EXEC_TEST(SimdF32x4AddWithI32x4) {
   // Choose two floating point values whose sum is normal and exactly
   // representable as a float.
   const int kOne = 0x3F800000;
@@ -3001,7 +2985,7 @@
   CHECK_EQ(1, r.Call());
 }
 
-WASM_SIMD_TEST(SimdI32x4AddWithF32x4) {
+WASM_EXEC_TEST(SimdI32x4AddWithF32x4) {
   WasmRunner<int32_t> r(execution_tier);
   BUILD(r,
         WASM_IF_ELSE_I(
@@ -3016,7 +3000,7 @@
   CHECK_EQ(1, r.Call());
 }
 
-WASM_SIMD_TEST(SimdI32x4Local) {
+WASM_EXEC_TEST(SimdI32x4Local) {
   WasmRunner<int32_t> r(execution_tier);
   r.AllocateLocal(kWasmS128);
   BUILD(r, WASM_LOCAL_SET(0, WASM_SIMD_I32x4_SPLAT(WASM_I32V(31))),
@@ -3025,7 +3009,7 @@
   CHECK_EQ(31, r.Call());
 }
 
-WASM_SIMD_TEST(SimdI32x4SplatFromExtract) {
+WASM_EXEC_TEST(SimdI32x4SplatFromExtract) {
   WasmRunner<int32_t> r(execution_tier);
   r.AllocateLocal(kWasmI32);
   r.AllocateLocal(kWasmS128);
@@ -3037,7 +3021,7 @@
   CHECK_EQ(76, r.Call());
 }
 
-WASM_SIMD_TEST(SimdI32x4For) {
+WASM_EXEC_TEST(SimdI32x4For) {
   WasmRunner<int32_t> r(execution_tier);
   r.AllocateLocal(kWasmI32);
   r.AllocateLocal(kWasmS128);
@@ -3071,7 +3055,7 @@
   CHECK_EQ(1, r.Call());
 }
 
-WASM_SIMD_TEST(SimdF32x4For) {
+WASM_EXEC_TEST(SimdF32x4For) {
   WasmRunner<int32_t> r(execution_tier);
   r.AllocateLocal(kWasmI32);
   r.AllocateLocal(kWasmS128);
@@ -3109,7 +3093,7 @@
   return LANE(v, lane);
 }
 
-WASM_SIMD_TEST(SimdI32x4GetGlobal) {
+WASM_EXEC_TEST(SimdI32x4GetGlobal) {
   WasmRunner<int32_t, int32_t> r(execution_tier);
   // Pad the globals with a few unused slots to get a non-zero offset.
   r.builder().AddGlobal<int32_t>(kWasmI32);  // purposefully unused
@@ -3137,7 +3121,7 @@
   CHECK_EQ(1, r.Call(0));
 }
 
-WASM_SIMD_TEST(SimdI32x4SetGlobal) {
+WASM_EXEC_TEST(SimdI32x4SetGlobal) {
   WasmRunner<int32_t, int32_t> r(execution_tier);
   // Pad the globals with a few unused slots to get a non-zero offset.
   r.builder().AddGlobal<int32_t>(kWasmI32);  // purposefully unused
@@ -3160,7 +3144,7 @@
   CHECK_EQ(GetScalar(global, 3), 56);
 }
 
-WASM_SIMD_TEST(SimdF32x4GetGlobal) {
+WASM_EXEC_TEST(SimdF32x4GetGlobal) {
   WasmRunner<int32_t, int32_t> r(execution_tier);
   float* global = r.builder().AddGlobal<float>(kWasmS128);
   SetVectorByLanes<float>(global, {{0.0, 1.5, 2.25, 3.5}});
@@ -3183,7 +3167,7 @@
   CHECK_EQ(1, r.Call(0));
 }
 
-WASM_SIMD_TEST(SimdF32x4SetGlobal) {
+WASM_EXEC_TEST(SimdF32x4SetGlobal) {
   WasmRunner<int32_t, int32_t> r(execution_tier);
   float* global = r.builder().AddGlobal<float>(kWasmS128);
   BUILD(r, WASM_GLOBAL_SET(0, WASM_SIMD_F32x4_SPLAT(WASM_F32(13.5))),
@@ -3201,7 +3185,7 @@
   CHECK_EQ(GetScalar(global, 3), 65.0f);
 }
 
-WASM_SIMD_TEST(SimdLoadStoreLoad) {
+WASM_EXEC_TEST(SimdLoadStoreLoad) {
   {
     WasmRunner<int32_t> r(execution_tier);
     int32_t* memory =
@@ -3247,7 +3231,7 @@
   }
 }
 
-WASM_SIMD_TEST(SimdLoadStoreLoadMemargOffset) {
+WASM_EXEC_TEST(SimdLoadStoreLoadMemargOffset) {
   {
     WasmRunner<int32_t> r(execution_tier);
     int32_t* memory =
@@ -3301,7 +3285,7 @@
 
 // Test a multi-byte opcode with offset values that encode into valid opcodes.
 // This is to exercise decoding logic and make sure we get the lengths right.
-WASM_SIMD_TEST(S128Load8SplatOffset) {
+WASM_EXEC_TEST(S128Load8SplatOffset) {
   // This offset is [82, 22] when encoded, which contains valid opcodes.
   constexpr int offset = 4354;
   WasmRunner<int32_t> r(execution_tier);
@@ -3360,19 +3344,19 @@
   }
 }
 
-WASM_SIMD_TEST(S128Load8Splat) {
+WASM_EXEC_TEST(S128Load8Splat) {
   RunLoadSplatTest<int8_t>(execution_tier, kExprS128Load8Splat);
 }
 
-WASM_SIMD_TEST(S128Load16Splat) {
+WASM_EXEC_TEST(S128Load16Splat) {
   RunLoadSplatTest<int16_t>(execution_tier, kExprS128Load16Splat);
 }
 
-WASM_SIMD_TEST(S128Load32Splat) {
+WASM_EXEC_TEST(S128Load32Splat) {
   RunLoadSplatTest<int32_t>(execution_tier, kExprS128Load32Splat);
 }
 
-WASM_SIMD_TEST(S128Load64Splat) {
+WASM_EXEC_TEST(S128Load64Splat) {
   RunLoadSplatTest<int64_t>(execution_tier, kExprS128Load64Splat);
 }
 
@@ -3450,26 +3434,26 @@
   }
 }
 
-WASM_SIMD_TEST(S128Load8x8U) {
+WASM_EXEC_TEST(S128Load8x8U) {
   RunLoadExtendTest<uint8_t, uint16_t>(execution_tier, kExprS128Load8x8U);
 }
 
-WASM_SIMD_TEST(S128Load8x8S) {
+WASM_EXEC_TEST(S128Load8x8S) {
   RunLoadExtendTest<int8_t, int16_t>(execution_tier, kExprS128Load8x8S);
 }
-WASM_SIMD_TEST(S128Load16x4U) {
+WASM_EXEC_TEST(S128Load16x4U) {
   RunLoadExtendTest<uint16_t, uint32_t>(execution_tier, kExprS128Load16x4U);
 }
 
-WASM_SIMD_TEST(S128Load16x4S) {
+WASM_EXEC_TEST(S128Load16x4S) {
   RunLoadExtendTest<int16_t, int32_t>(execution_tier, kExprS128Load16x4S);
 }
 
-WASM_SIMD_TEST(S128Load32x2U) {
+WASM_EXEC_TEST(S128Load32x2U) {
   RunLoadExtendTest<uint32_t, uint64_t>(execution_tier, kExprS128Load32x2U);
 }
 
-WASM_SIMD_TEST(S128Load32x2S) {
+WASM_EXEC_TEST(S128Load32x2S) {
   RunLoadExtendTest<int32_t, int64_t>(execution_tier, kExprS128Load32x2S);
 }
 
@@ -3543,11 +3527,11 @@
   }
 }
 
-WASM_SIMD_TEST(S128Load32Zero) {
+WASM_EXEC_TEST(S128Load32Zero) {
   RunLoadZeroTest<int32_t>(execution_tier, kExprS128Load32Zero);
 }
 
-WASM_SIMD_TEST(S128Load64Zero) {
+WASM_EXEC_TEST(S128Load64Zero) {
   RunLoadZeroTest<int64_t>(execution_tier, kExprS128Load64Zero);
 }
 
@@ -3630,21 +3614,21 @@
   }
 }
 
-WASM_SIMD_TEST(S128Load8Lane) {
+WASM_EXEC_TEST(S128Load8Lane) {
   RunLoadLaneTest<int8_t>(execution_tier, kExprS128Load8Lane, kExprI8x16Splat);
 }
 
-WASM_SIMD_TEST(S128Load16Lane) {
+WASM_EXEC_TEST(S128Load16Lane) {
   RunLoadLaneTest<int16_t>(execution_tier, kExprS128Load16Lane,
                            kExprI16x8Splat);
 }
 
-WASM_SIMD_TEST(S128Load32Lane) {
+WASM_EXEC_TEST(S128Load32Lane) {
   RunLoadLaneTest<int32_t>(execution_tier, kExprS128Load32Lane,
                            kExprI32x4Splat);
 }
 
-WASM_SIMD_TEST(S128Load64Lane) {
+WASM_EXEC_TEST(S128Load64Lane) {
   RunLoadLaneTest<int64_t>(execution_tier, kExprS128Load64Lane,
                            kExprI64x2Splat);
 }
@@ -3722,28 +3706,28 @@
   }
 }
 
-WASM_SIMD_TEST(S128Store8Lane) {
+WASM_EXEC_TEST(S128Store8Lane) {
   RunStoreLaneTest<int8_t>(execution_tier, kExprS128Store8Lane,
                            kExprI8x16Splat);
 }
 
-WASM_SIMD_TEST(S128Store16Lane) {
+WASM_EXEC_TEST(S128Store16Lane) {
   RunStoreLaneTest<int16_t>(execution_tier, kExprS128Store16Lane,
                             kExprI16x8Splat);
 }
 
-WASM_SIMD_TEST(S128Store32Lane) {
+WASM_EXEC_TEST(S128Store32Lane) {
   RunStoreLaneTest<int32_t>(execution_tier, kExprS128Store32Lane,
                             kExprI32x4Splat);
 }
 
-WASM_SIMD_TEST(S128Store64Lane) {
+WASM_EXEC_TEST(S128Store64Lane) {
   RunStoreLaneTest<int64_t>(execution_tier, kExprS128Store64Lane,
                             kExprI64x2Splat);
 }
 
 #define WASM_SIMD_ANYTRUE_TEST(format, lanes, max, param_type)                \
-  WASM_SIMD_TEST(S##format##AnyTrue) {                                        \
+  WASM_EXEC_TEST(S##format##AnyTrue) {                                        \
     WasmRunner<int32_t, param_type> r(execution_tier);                        \
     if (lanes == 2) return;                                                   \
     byte simd = r.AllocateLocal(kWasmS128);                                   \
@@ -3762,7 +3746,7 @@
 // Special any true test cases that splats a -0.0 double into a i64x2.
 // This is specifically to ensure that our implementation correct handles that
 // 0.0 and -0.0 will be different in an anytrue (IEEE753 says they are equals).
-WASM_SIMD_TEST(V128AnytrueWithNegativeZero) {
+WASM_EXEC_TEST(V128AnytrueWithNegativeZero) {
   WasmRunner<int32_t, int64_t> r(execution_tier);
   byte simd = r.AllocateLocal(kWasmS128);
   BUILD(r, WASM_LOCAL_SET(simd, WASM_SIMD_I64x2_SPLAT(WASM_LOCAL_GET(0))),
@@ -3772,7 +3756,7 @@
 }
 
 #define WASM_SIMD_ALLTRUE_TEST(format, lanes, max, param_type)                \
-  WASM_SIMD_TEST(I##format##AllTrue) {                                        \
+  WASM_EXEC_TEST(I##format##AllTrue) {                                        \
     WasmRunner<int32_t, param_type> r(execution_tier);                        \
     if (lanes == 2) return;                                                   \
     byte simd = r.AllocateLocal(kWasmS128);                                   \
@@ -3789,7 +3773,7 @@
 WASM_SIMD_ALLTRUE_TEST(16x8, 8, 0xffff, int32_t)
 WASM_SIMD_ALLTRUE_TEST(8x16, 16, 0xff, int32_t)
 
-WASM_SIMD_TEST(BitSelect) {
+WASM_EXEC_TEST(BitSelect) {
   WasmRunner<int32_t, int32_t> r(execution_tier);
   byte simd = r.AllocateLocal(kWasmS128);
   BUILD(r,
@@ -3814,7 +3798,7 @@
   }
 }
 
-WASM_SIMD_TEST(S128Const) {
+WASM_EXEC_TEST(S128Const) {
   std::array<uint8_t, kSimd128Size> expected;
   // Test for generic constant
   for (int i = 0; i < kSimd128Size; i++) {
@@ -3838,12 +3822,12 @@
   RunSimdConstTest(execution_tier, expected);
 }
 
-WASM_SIMD_TEST(S128ConstAllZero) {
+WASM_EXEC_TEST(S128ConstAllZero) {
   std::array<uint8_t, kSimd128Size> expected = {0};
   RunSimdConstTest(execution_tier, expected);
 }
 
-WASM_SIMD_TEST(S128ConstAllOnes) {
+WASM_EXEC_TEST(S128ConstAllOnes) {
   std::array<uint8_t, kSimd128Size> expected;
   // Test for generic constant
   for (int i = 0; i < kSimd128Size; i++) {
@@ -3852,37 +3836,37 @@
   RunSimdConstTest(execution_tier, expected);
 }
 
-WASM_SIMD_TEST(I8x16LeUMixed) {
+WASM_EXEC_TEST(I8x16LeUMixed) {
   RunI8x16MixedRelationalOpTest(execution_tier, kExprI8x16LeU,
                                 UnsignedLessEqual);
 }
-WASM_SIMD_TEST(I8x16LtUMixed) {
+WASM_EXEC_TEST(I8x16LtUMixed) {
   RunI8x16MixedRelationalOpTest(execution_tier, kExprI8x16LtU, UnsignedLess);
 }
-WASM_SIMD_TEST(I8x16GeUMixed) {
+WASM_EXEC_TEST(I8x16GeUMixed) {
   RunI8x16MixedRelationalOpTest(execution_tier, kExprI8x16GeU,
                                 UnsignedGreaterEqual);
 }
-WASM_SIMD_TEST(I8x16GtUMixed) {
+WASM_EXEC_TEST(I8x16GtUMixed) {
   RunI8x16MixedRelationalOpTest(execution_tier, kExprI8x16GtU, UnsignedGreater);
 }
 
-WASM_SIMD_TEST(I16x8LeUMixed) {
+WASM_EXEC_TEST(I16x8LeUMixed) {
   RunI16x8MixedRelationalOpTest(execution_tier, kExprI16x8LeU,
                                 UnsignedLessEqual);
 }
-WASM_SIMD_TEST(I16x8LtUMixed) {
+WASM_EXEC_TEST(I16x8LtUMixed) {
   RunI16x8MixedRelationalOpTest(execution_tier, kExprI16x8LtU, UnsignedLess);
 }
-WASM_SIMD_TEST(I16x8GeUMixed) {
+WASM_EXEC_TEST(I16x8GeUMixed) {
   RunI16x8MixedRelationalOpTest(execution_tier, kExprI16x8GeU,
                                 UnsignedGreaterEqual);
 }
-WASM_SIMD_TEST(I16x8GtUMixed) {
+WASM_EXEC_TEST(I16x8GtUMixed) {
   RunI16x8MixedRelationalOpTest(execution_tier, kExprI16x8GtU, UnsignedGreater);
 }
 
-WASM_SIMD_TEST(I16x8ExtractLaneU_I8x16Splat) {
+WASM_EXEC_TEST(I16x8ExtractLaneU_I8x16Splat) {
   // Test that we are correctly signed/unsigned extending when extracting.
   WasmRunner<int32_t, int32_t> r(execution_tier);
   byte simd_val = r.AllocateLocal(kWasmS128);
@@ -3940,21 +3924,21 @@
   }
 }
 
-WASM_SIMD_TEST(AddExtAddPairwiseI32Right) {
+WASM_EXEC_TEST(AddExtAddPairwiseI32Right) {
   RunAddExtAddPairwiseTest<int32_t, int16_t>(
       execution_tier, RIGHT, kExprI32x4Add, {1, 2, 3, 4},
       kExprI32x4ExtAddPairwiseI16x8S, {-1, -2, -3, -4, -5, -6, -7, -8},
       {-2, -5, -8, -11});
 }
 
-WASM_SIMD_TEST(AddExtAddPairwiseI32Left) {
+WASM_EXEC_TEST(AddExtAddPairwiseI32Left) {
   RunAddExtAddPairwiseTest<int32_t, int16_t>(
       execution_tier, LEFT, kExprI32x4Add, {1, 2, 3, 4},
       kExprI32x4ExtAddPairwiseI16x8S, {-1, -2, -3, -4, -5, -6, -7, -8},
       {-2, -5, -8, -11});
 }
 
-WASM_SIMD_TEST(AddExtAddPairwiseI16Right) {
+WASM_EXEC_TEST(AddExtAddPairwiseI16Right) {
   RunAddExtAddPairwiseTest<int16_t, int8_t>(
       execution_tier, RIGHT, kExprI16x8Add, {1, 2, 3, 4, 5, 6, 7, 8},
       kExprI16x8ExtAddPairwiseI8x16S,
@@ -3962,7 +3946,7 @@
       {-2, -5, -8, -11, -14, -17, -20, -23});
 }
 
-WASM_SIMD_TEST(AddExtAddPairwiseI16Left) {
+WASM_EXEC_TEST(AddExtAddPairwiseI16Left) {
   RunAddExtAddPairwiseTest<int16_t, int8_t>(
       execution_tier, LEFT, kExprI16x8Add, {1, 2, 3, 4, 5, 6, 7, 8},
       kExprI16x8ExtAddPairwiseI8x16S,
@@ -3970,13 +3954,13 @@
       {4, 9, 14, 19, 24, 29, 34, 39});
 }
 
-WASM_SIMD_TEST(AddExtAddPairwiseI32RightUnsigned) {
+WASM_EXEC_TEST(AddExtAddPairwiseI32RightUnsigned) {
   RunAddExtAddPairwiseTest<uint32_t, uint16_t>(
       execution_tier, RIGHT, kExprI32x4Add, {1, 2, 3, 4},
       kExprI32x4ExtAddPairwiseI16x8U, {1, 2, 3, 4, 5, 6, 7, 8}, {4, 9, 14, 19});
 }
 
-WASM_SIMD_TEST(AddExtAddPairwiseI32LeftUnsigned) {
+WASM_EXEC_TEST(AddExtAddPairwiseI32LeftUnsigned) {
   RunAddExtAddPairwiseTest<uint32_t, uint16_t>(
       execution_tier, LEFT, kExprI32x4Add, {1, 2, 3, 4},
       kExprI32x4ExtAddPairwiseI16x8U, {1, 2, 3, 4, 5, 6, 7, 8}, {4, 9, 14, 19});
@@ -3984,7 +3968,7 @@
 
 // Regression test from https://crbug.com/v8/12237 to exercise a codegen bug
 // for i64x2.gts which overwrote one of the inputs.
-WASM_SIMD_TEST(Regress_12237) {
+WASM_EXEC_TEST(Regress_12237) {
   WasmRunner<int32_t, int64_t> r(execution_tier);
   int64_t* g = r.builder().AddGlobal<int64_t>(kWasmS128);
   byte value = 0;
@@ -4007,7 +3991,7 @@
 }
 
 #define WASM_EXTRACT_I16x8_TEST(Sign, Type)                                    \
-  WASM_SIMD_TEST(I16X8ExtractLane##Sign) {                                     \
+  WASM_EXEC_TEST(I16X8ExtractLane##Sign) {                                     \
     WasmRunner<int32_t, int32_t> r(execution_tier);                            \
     byte int_val = r.AllocateLocal(kWasmI32);                                  \
     byte simd_val = r.AllocateLocal(kWasmS128);                                \
@@ -4024,7 +4008,7 @@
 #undef WASM_EXTRACT_I16x8_TEST
 
 #define WASM_EXTRACT_I8x16_TEST(Sign, Type)                               \
-  WASM_SIMD_TEST(I8x16ExtractLane##Sign) {                                \
+  WASM_EXEC_TEST(I8x16ExtractLane##Sign) {                                \
     WasmRunner<int32_t, int32_t> r(execution_tier);                       \
     byte int_val = r.AllocateLocal(kWasmI32);                             \
     byte simd_val = r.AllocateLocal(kWasmS128);                           \
@@ -4045,7 +4029,6 @@
     WASM_EXTRACT_I8x16_TEST(S, UINT8) WASM_EXTRACT_I8x16_TEST(I, INT8)
 #undef WASM_EXTRACT_I8x16_TEST
 
-#undef WASM_SIMD_TEST
 #undef WASM_SIMD_CHECK_LANE_S
 #undef WASM_SIMD_CHECK_LANE_U
 #undef TO_BYTE
@@ -4084,7 +4067,6 @@
 #undef WASM_SIMD_SELECT_TEST
 #undef WASM_SIMD_NON_CANONICAL_SELECT_TEST
 #undef WASM_SIMD_BOOL_REDUCTION_TEST
-#undef WASM_SIMD_TEST
 #undef WASM_SIMD_ANYTRUE_TEST
 #undef WASM_SIMD_ALLTRUE_TEST
 #undef WASM_SIMD_F64x2_QFMA
diff -r -u --color up/v8/test/cctest/wasm/wasm-run-utils.cc nw/v8/test/cctest/wasm/wasm-run-utils.cc
--- up/v8/test/cctest/wasm/wasm-run-utils.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/cctest/wasm/wasm-run-utils.cc	2023-01-19 16:46:36.493109495 -0500
@@ -152,6 +152,9 @@
     // TODO(titzer): Reserving space here to avoid the underlying WasmFunction
     // structs from moving.
     test_module_->functions.reserve(kMaxFunctions);
+    DCHECK_NULL(test_module_->validated_functions);
+    test_module_->validated_functions =
+        std::make_unique<std::atomic<uint8_t>[]>((kMaxFunctions + 7) / 8);
   }
   uint32_t index = static_cast<uint32_t>(test_module_->functions.size());
   test_module_->functions.push_back({sig,      // sig
@@ -181,6 +184,11 @@
     interpreter_->AddFunctionForTesting(&test_module_->functions.back());
   }
   DCHECK_LT(index, kMaxFunctions);  // limited for testing.
+  if (!instance_object_.is_null()) {
+    Handle<FixedArray> funcs = isolate_->factory()->NewFixedArray(
+        static_cast<int>(test_module_->functions.size()));
+    instance_object_->set_wasm_internal_functions(*funcs);
+  }
   return index;
 }
 
@@ -617,9 +625,10 @@
   } else {
     WasmCompilationUnit unit(function_->func_index, builder_->execution_tier(),
                              for_debugging);
+    WasmFeatures unused_detected_features;
     result.emplace(unit.ExecuteCompilation(
         &env, native_module->compilation_state()->GetWireBytesStorage().get(),
-        nullptr, nullptr, nullptr));
+        nullptr, nullptr, &unused_detected_features));
   }
   WasmCode* code = native_module->PublishCode(
       native_module->AddCompiledCode(std::move(*result)));
diff -r -u --color up/v8/test/common/flag-utils.h nw/v8/test/common/flag-utils.h
--- up/v8/test/common/flag-utils.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/common/flag-utils.h	2023-01-19 16:46:36.493109495 -0500
@@ -28,9 +28,9 @@
 }  // namespace internal
 }  // namespace v8
 
-#define FLAG_VALUE_SCOPE(flag, value)                              \
-  v8::internal::FlagScope<bool> UNIQUE_IDENTIFIER(__scope_##flag)( \
-      &v8::internal::FLAG_##flag, value)
+#define FLAG_VALUE_SCOPE(flag, value)                                \
+  ::v8::internal::FlagScope<bool> UNIQUE_IDENTIFIER(__scope_##flag)( \
+      &::v8::internal::v8_flags.flag, value)
 #define FLAG_SCOPE(flag) FLAG_VALUE_SCOPE(flag, true)
 
 #endif  // V8_TEST_COMMON_FLAG_UTILS_H
diff -r -u --color up/v8/test/common/wasm/wasm-interpreter.cc nw/v8/test/common/wasm/wasm-interpreter.cc
--- up/v8/test/common/wasm/wasm-interpreter.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/common/wasm/wasm-interpreter.cc	2023-01-19 16:46:36.493109495 -0500
@@ -585,6 +585,8 @@
   const byte* at(pc_t pc) { return start + pc; }
 };
 
+constexpr Decoder::NoValidationTag kNoValidate;
+
 // A helper class to compute the control transfers for each bytecode offset.
 // Control transfers allow Br, BrIf, BrTable, If, Else, and End bytecodes to
 // be directly executed without the need to dynamically track blocks.
@@ -792,8 +794,8 @@
         case kExprBlock:
         case kExprLoop: {
           bool is_loop = opcode == kExprLoop;
-          BlockTypeImmediate<Decoder::kNoValidation> imm(
-              WasmFeatures::All(), &i, i.pc() + 1, module);
+          BlockTypeImmediate imm(WasmFeatures::All(), &i, i.pc() + 1,
+                                 kNoValidate);
           if (imm.type == kWasmBottom) {
             imm.sig = module->signature(imm.sig_index);
           }
@@ -814,8 +816,8 @@
           break;
         }
         case kExprIf: {
-          BlockTypeImmediate<Decoder::kNoValidation> imm(
-              WasmFeatures::All(), &i, i.pc() + 1, module);
+          BlockTypeImmediate imm(WasmFeatures::All(), &i, i.pc() + 1,
+                                 kNoValidate);
           if (imm.type == kWasmBottom) {
             imm.sig = module->signature(imm.sig_index);
           }
@@ -875,8 +877,8 @@
           break;
         }
         case kExprTry: {
-          BlockTypeImmediate<Decoder::kNoValidation> imm(
-              WasmFeatures::All(), &i, i.pc() + 1, module);
+          BlockTypeImmediate imm(WasmFeatures::All(), &i, i.pc() + 1,
+                                 kNoValidate);
           if (imm.type == kWasmBottom) {
             imm.sig = module->signature(imm.sig_index);
           }
@@ -899,7 +901,7 @@
           break;
         }
         case kExprRethrow: {
-          BranchDepthImmediate<Decoder::kNoValidation> imm(&i, i.pc() + 1);
+          BranchDepthImmediate imm(&i, i.pc() + 1, kNoValidate);
           int index = static_cast<int>(control_stack.size()) - 1 - imm.depth;
           rethrow_map_.emplace(i.pc() - i.start(), index);
           break;
@@ -910,7 +912,7 @@
             // Only pop the exception stack once when we enter the first catch.
             exception_stack.pop_back();
           }
-          TagIndexImmediate<Decoder::kNoValidation> imm(&i, i.pc() + 1);
+          TagIndexImmediate imm(&i, i.pc() + 1, kNoValidate);
           Control* c = &control_stack.back();
           copy_unreachable();
           TRACE("control @%u: Catch\n", i.pc_offset());
@@ -978,7 +980,7 @@
           break;
         }
         case kExprDelegate: {
-          BranchDepthImmediate<Decoder::kNoValidation> imm(&i, i.pc() + 1);
+          BranchDepthImmediate imm(&i, i.pc() + 1, kNoValidate);
           TRACE("control @%u: Delegate[depth=%u]\n", i.pc_offset(), imm.depth);
           Control* c = &control_stack.back();
           const size_t new_stack_size = control_stack.size() - 1;
@@ -1016,22 +1018,22 @@
           break;
         }
         case kExprBr: {
-          BranchDepthImmediate<Decoder::kNoValidation> imm(&i, i.pc() + 1);
+          BranchDepthImmediate imm(&i, i.pc() + 1, kNoValidate);
           TRACE("control @%u: Br[depth=%u]\n", i.pc_offset(), imm.depth);
           Control* c = &control_stack[control_stack.size() - imm.depth - 1];
           if (!unreachable) c->end_label->Ref(i.pc(), stack_height);
           break;
         }
         case kExprBrIf: {
-          BranchDepthImmediate<Decoder::kNoValidation> imm(&i, i.pc() + 1);
+          BranchDepthImmediate imm(&i, i.pc() + 1, kNoValidate);
           TRACE("control @%u: BrIf[depth=%u]\n", i.pc_offset(), imm.depth);
           Control* c = &control_stack[control_stack.size() - imm.depth - 1];
           if (!unreachable) c->end_label->Ref(i.pc(), stack_height);
           break;
         }
         case kExprBrTable: {
-          BranchTableImmediate<Decoder::kNoValidation> imm(&i, i.pc() + 1);
-          BranchTableIterator<Decoder::kNoValidation> iterator(&i, imm);
+          BranchTableImmediate imm(&i, i.pc() + 1, kNoValidate);
+          BranchTableIterator<Decoder::NoValidationTag> iterator(&i, imm);
           TRACE("control @%u: BrTable[count=%u]\n", i.pc_offset(),
                 imm.table_count);
           if (!unreachable) {
@@ -1530,13 +1532,11 @@
   pc_t ReturnPc(Decoder* decoder, InterpreterCode* code, pc_t pc) {
     switch (code->start[pc]) {
       case kExprCallFunction: {
-        CallFunctionImmediate<Decoder::kNoValidation> imm(decoder,
-                                                          code->at(pc + 1));
+        CallFunctionImmediate imm(decoder, code->at(pc + 1), kNoValidate);
         return pc + 1 + imm.length;
       }
       case kExprCallIndirect: {
-        CallIndirectImmediate<Decoder::kNoValidation> imm(decoder,
-                                                          code->at(pc + 1));
+        CallIndirectImmediate imm(decoder, code->at(pc + 1), kNoValidate);
         return pc + 1 + imm.length;
       }
       default:
@@ -1688,9 +1688,8 @@
     // increment pc at the caller, because we want to keep pc to the start of
     // the operation to keep trap reporting and tracing accurate, otherwise
     // those will report at the middle of an opcode.
-    MemoryAccessImmediate<Decoder::kNoValidation> imm(
-        decoder, code->at(pc + prefix_len), sizeof(ctype),
-        module()->is_memory64);
+    MemoryAccessImmediate imm(decoder, code->at(pc + prefix_len), sizeof(ctype),
+                              module()->is_memory64, kNoValidate);
     uint64_t index = ToMemType(Pop());
     Address addr = BoundsCheckMem<mtype>(imm.offset, index);
     if (!addr) {
@@ -1721,9 +1720,8 @@
     // increment pc at the caller, because we want to keep pc to the start of
     // the operation to keep trap reporting and tracing accurate, otherwise
     // those will report at the middle of an opcode.
-    MemoryAccessImmediate<Decoder::kNoValidation> imm(
-        decoder, code->at(pc + prefix_len), sizeof(ctype),
-        module()->is_memory64);
+    MemoryAccessImmediate imm(decoder, code->at(pc + prefix_len), sizeof(ctype),
+                              module()->is_memory64, kNoValidate);
     ctype val = Pop().to<ctype>();
 
     uint64_t index = ToMemType(Pop());
@@ -1749,8 +1747,8 @@
   bool ExtractAtomicOpParams(Decoder* decoder, InterpreterCode* code,
                              Address* address, pc_t pc, int* const len,
                              type* val = nullptr, type* val2 = nullptr) {
-    MemoryAccessImmediate<Decoder::kNoValidation> imm(
-        decoder, code->at(pc + *len), sizeof(type), module()->is_memory64);
+    MemoryAccessImmediate imm(decoder, code->at(pc + *len), sizeof(type),
+                              module()->is_memory64, kNoValidate);
     if (val2) *val2 = static_cast<type>(Pop().to<op_type>());
     if (val) *val = static_cast<type>(Pop().to<op_type>());
     uint64_t index = ToMemType(Pop());
@@ -1773,8 +1771,8 @@
                                      uint64_t* buffer_offset, type* val,
                                      int64_t* timeout = nullptr) {
     // TODO(manoskouk): Introduce test which exposes wrong pc offset below.
-    MemoryAccessImmediate<Decoder::kFullValidation> imm(
-        decoder, code->at(pc + *len), sizeof(type), module()->is_memory64);
+    MemoryAccessImmediate imm(decoder, code->at(pc + *len), sizeof(type),
+                              module()->is_memory64, kNoValidate);
     if (timeout) {
       *timeout = Pop().to<int64_t>();
     }
@@ -1825,8 +1823,7 @@
         Push(WasmValue(base::saturated_cast<uint64_t>(Pop().to<double>())));
         return true;
       case kExprMemoryInit: {
-        MemoryInitImmediate<Decoder::kNoValidation> imm(decoder,
-                                                        code->at(pc + *len));
+        MemoryInitImmediate imm(decoder, code->at(pc + *len), kNoValidate);
         // The data segment index must be in bounds since it is required by
         // validation.
         DCHECK_LT(imm.data_segment.index, module()->num_declared_data_segments);
@@ -1850,8 +1847,8 @@
         return true;
       }
       case kExprDataDrop: {
-        IndexImmediate<Decoder::kNoValidation> imm(decoder, code->at(pc + *len),
-                                                   "data segment index");
+        IndexImmediate imm(decoder, code->at(pc + *len), "data segment index",
+                           kNoValidate);
         // The data segment index must be in bounds since it is required by
         // validation.
         DCHECK_LT(imm.index, module()->num_declared_data_segments);
@@ -1860,8 +1857,7 @@
         return true;
       }
       case kExprMemoryCopy: {
-        MemoryCopyImmediate<Decoder::kNoValidation> imm(decoder,
-                                                        code->at(pc + *len));
+        MemoryCopyImmediate imm(decoder, code->at(pc + *len), kNoValidate);
         *len += imm.length;
         uint64_t size = ToMemType(Pop());
         uint64_t src = ToMemType(Pop());
@@ -1879,8 +1875,7 @@
         return true;
       }
       case kExprMemoryFill: {
-        MemoryIndexImmediate<Decoder::kNoValidation> imm(decoder,
-                                                         code->at(pc + *len));
+        MemoryIndexImmediate imm(decoder, code->at(pc + *len), kNoValidate);
         *len += imm.length;
         uint64_t size = ToMemType(Pop());
         uint32_t value = Pop().to<uint32_t>();
@@ -1894,8 +1889,7 @@
         return true;
       }
       case kExprTableInit: {
-        TableInitImmediate<Decoder::kNoValidation> imm(decoder,
-                                                       code->at(pc + *len));
+        TableInitImmediate imm(decoder, code->at(pc + *len), kNoValidate);
         *len += imm.length;
         auto size = Pop().to<uint32_t>();
         auto src = Pop().to<uint32_t>();
@@ -1913,15 +1907,14 @@
         return true;
       }
       case kExprElemDrop: {
-        IndexImmediate<Decoder::kNoValidation> imm(decoder, code->at(pc + *len),
-                                                   "element segment index");
+        IndexImmediate imm(decoder, code->at(pc + *len),
+                           "element segment index", kNoValidate);
         *len += imm.length;
         instance_object_->dropped_elem_segments().set(imm.index, 1);
         return true;
       }
       case kExprTableCopy: {
-        TableCopyImmediate<Decoder::kNoValidation> imm(decoder,
-                                                       code->at(pc + *len));
+        TableCopyImmediate imm(decoder, code->at(pc + *len), kNoValidate);
         auto size = Pop().to<uint32_t>();
         auto src = Pop().to<uint32_t>();
         auto dst = Pop().to<uint32_t>();
@@ -1934,8 +1927,8 @@
         return ok;
       }
       case kExprTableGrow: {
-        IndexImmediate<Decoder::kNoValidation> imm(decoder, code->at(pc + *len),
-                                                   "table index");
+        IndexImmediate imm(decoder, code->at(pc + *len), "table index",
+                           kNoValidate);
         HandleScope handle_scope(isolate_);
         auto table = handle(
             WasmTableObject::cast(instance_object_->tables().get(imm.index)),
@@ -1948,8 +1941,8 @@
         return true;
       }
       case kExprTableSize: {
-        IndexImmediate<Decoder::kNoValidation> imm(decoder, code->at(pc + *len),
-                                                   "table index");
+        IndexImmediate imm(decoder, code->at(pc + *len), "table index",
+                           kNoValidate);
         HandleScope handle_scope(isolate_);
         auto table = handle(
             WasmTableObject::cast(instance_object_->tables().get(imm.index)),
@@ -1960,8 +1953,8 @@
         return true;
       }
       case kExprTableFill: {
-        IndexImmediate<Decoder::kNoValidation> imm(decoder, code->at(pc + *len),
-                                                   "table index");
+        IndexImmediate imm(decoder, code->at(pc + *len), "table index",
+                           kNoValidate);
         HandleScope handle_scope(isolate_);
         auto count = Pop().to<uint32_t>();
         auto value = Pop().to_ref();
@@ -2285,16 +2278,15 @@
       SPLAT_CASE(I16x8, int8, int32_t, 8)
       SPLAT_CASE(I8x16, int16, int32_t, 16)
 #undef SPLAT_CASE
-#define EXTRACT_LANE_CASE(format, name)                                 \
-  case kExpr##format##ExtractLane: {                                    \
-    SimdLaneImmediate<Decoder::kNoValidation> imm(decoder,              \
-                                                  code->at(pc + *len)); \
-    *len += 1;                                                          \
-    WasmValue val = Pop();                                              \
-    Simd128 s = val.to_s128();                                          \
-    auto ss = s.to_##name();                                            \
-    Push(WasmValue(ss.val[LANE(imm.lane, ss)]));                        \
-    return true;                                                        \
+#define EXTRACT_LANE_CASE(format, name)                               \
+  case kExpr##format##ExtractLane: {                                  \
+    SimdLaneImmediate imm(decoder, code->at(pc + *len), kNoValidate); \
+    *len += 1;                                                        \
+    WasmValue val = Pop();                                            \
+    Simd128 s = val.to_s128();                                        \
+    auto ss = s.to_##name();                                          \
+    Push(WasmValue(ss.val[LANE(imm.lane, ss)]));                      \
+    return true;                                                      \
   }
       EXTRACT_LANE_CASE(F64x2, f64x2)
       EXTRACT_LANE_CASE(F32x4, f32x4)
@@ -2310,8 +2302,7 @@
       // change this function.
 #define EXTRACT_LANE_EXTEND_CASE(format, name, sign, extended_type)      \
   case kExpr##format##ExtractLane##sign: {                               \
-    SimdLaneImmediate<Decoder::kNoValidation> imm(decoder,               \
-                                                  code->at(pc + *len));  \
+    SimdLaneImmediate imm(decoder, code->at(pc + *len), kNoValidate);    \
     *len += 1;                                                           \
     WasmValue val = Pop();                                               \
     Simd128 s = val.to_s128();                                           \
@@ -2568,17 +2559,16 @@
       CMPOP_CASE(I8x16LeU, i8x16, int16, int16, 16,
                  static_cast<uint8_t>(a) <= static_cast<uint8_t>(b))
 #undef CMPOP_CASE
-#define REPLACE_LANE_CASE(format, name, stype, ctype)                   \
-  case kExpr##format##ReplaceLane: {                                    \
-    SimdLaneImmediate<Decoder::kNoValidation> imm(decoder,              \
-                                                  code->at(pc + *len)); \
-    *len += 1;                                                          \
-    WasmValue new_val = Pop();                                          \
-    WasmValue simd_val = Pop();                                         \
-    stype s = simd_val.to_s128().to_##name();                           \
-    s.val[LANE(imm.lane, s)] = new_val.to<ctype>();                     \
-    Push(WasmValue(Simd128(s)));                                        \
-    return true;                                                        \
+#define REPLACE_LANE_CASE(format, name, stype, ctype)                 \
+  case kExpr##format##ReplaceLane: {                                  \
+    SimdLaneImmediate imm(decoder, code->at(pc + *len), kNoValidate); \
+    *len += 1;                                                        \
+    WasmValue new_val = Pop();                                        \
+    WasmValue simd_val = Pop();                                       \
+    stype s = simd_val.to_s128().to_##name();                         \
+    s.val[LANE(imm.lane, s)] = new_val.to<ctype>();                   \
+    Push(WasmValue(Simd128(s)));                                      \
+    return true;                                                      \
   }
       REPLACE_LANE_CASE(F64x2, f64x2, float2, double)
       REPLACE_LANE_CASE(F32x4, f32x4, float4, float)
@@ -2781,8 +2771,7 @@
         return true;
       }
       case kExprS128Const: {
-        Simd128Immediate<Decoder::kNoValidation> imm(decoder,
-                                                     code->at(pc + *len));
+        Simd128Immediate imm(decoder, code->at(pc + *len), kNoValidate);
         int16 res;
         for (size_t i = 0; i < kSimd128Size; ++i) {
           res.val[LANE(i, res)] = imm.value[i];
@@ -2838,8 +2827,7 @@
         return true;
       }
       case kExprI8x16Shuffle: {
-        Simd128Immediate<Decoder::kNoValidation> imm(decoder,
-                                                     code->at(pc + *len));
+        Simd128Immediate imm(decoder, code->at(pc + *len), kNoValidate);
         *len += 16;
         int16 v2 = Pop().to_s128().to_i8x16();
         int16 v1 = Pop().to_s128().to_i8x16();
@@ -3054,8 +3042,7 @@
       return false;
     }
 
-    SimdLaneImmediate<Decoder::kNoValidation> lane_imm(decoder,
-                                                       code->at(pc + *len));
+    SimdLaneImmediate lane_imm(decoder, code->at(pc + *len), kNoValidate);
     *len += lane_imm.length;
     result_type loaded = Pop().to<result_type>();
     value.val[LANE(lane_imm.lane, value)] = loaded;
@@ -3069,11 +3056,11 @@
     // Extract a single lane, push it onto the stack, then store the lane.
     s_type value = Pop().to_s128().to<s_type>();
 
-    MemoryAccessImmediate<Decoder::kNoValidation> imm(
-        decoder, code->at(pc + *len), sizeof(load_type), module()->is_memory64);
+    MemoryAccessImmediate imm(decoder, code->at(pc + *len), sizeof(load_type),
+                              module()->is_memory64, kNoValidate);
 
-    SimdLaneImmediate<Decoder::kNoValidation> lane_imm(
-        decoder, code->at(pc + *len + imm.length));
+    SimdLaneImmediate lane_imm(decoder, code->at(pc + *len + imm.length),
+                               kNoValidate);
 
     Push(WasmValue(
         static_cast<result_type>(value.val[LANE(lane_imm.lane, value)])));
@@ -3335,7 +3322,7 @@
 
       if (WasmOpcodes::IsPrefixOpcode(opcode)) {
         uint32_t prefixed_opcode_length = 0;
-        opcode = decoder.read_prefixed_opcode<Decoder::kNoValidation>(
+        opcode = decoder.read_prefixed_opcode<Decoder::NoValidationTag>(
             code->at(pc), &prefixed_opcode_length);
         // read_prefixed_opcode includes the prefix byte, overwrite len.
         len = prefixed_opcode_length;
@@ -3367,14 +3354,14 @@
         case kExprBlock:
         case kExprLoop:
         case kExprTry: {
-          BlockTypeImmediate<Decoder::kNoValidation> imm(
-              WasmFeatures::All(), &decoder, code->at(pc + 1), module());
+          BlockTypeImmediate imm(WasmFeatures::All(), &decoder,
+                                 code->at(pc + 1), kNoValidate);
           len = 1 + imm.length;
           break;
         }
         case kExprIf: {
-          BlockTypeImmediate<Decoder::kNoValidation> imm(
-              WasmFeatures::All(), &decoder, code->at(pc + 1), module());
+          BlockTypeImmediate imm(WasmFeatures::All(), &decoder,
+                                 code->at(pc + 1), kNoValidate);
           WasmValue cond = Pop();
           bool is_true = cond.to<uint32_t>() != 0;
           if (is_true) {
@@ -3395,8 +3382,7 @@
           break;
         }
         case kExprThrow: {
-          TagIndexImmediate<Decoder::kNoValidation> imm(&decoder,
-                                                        code->at(pc + 1));
+          TagIndexImmediate imm(&decoder, code->at(pc + 1), kNoValidate);
           CommitPc(pc);  // Needed for local unwinding.
           const WasmTag* tag = &module()->tags[imm.index];
           if (!DoThrowException(tag, imm.index)) return;
@@ -3404,8 +3390,7 @@
           continue;  // Do not bump pc.
         }
         case kExprRethrow: {
-          BranchDepthImmediate<Decoder::kNoValidation> imm(&decoder,
-                                                           code->at(pc + 1));
+          BranchDepthImmediate imm(&decoder, code->at(pc + 1), kNoValidate);
           HandleScope scope(isolate_);  // Avoid leaking handles.
           DCHECK(!frames_.back().caught_exception_stack.is_null());
           int index = code->side_table->rethrow_map_[pc];
@@ -3420,8 +3405,8 @@
           continue;  // Do not bump pc.
         }
         case kExprSelectWithType: {
-          SelectTypeImmediate<Decoder::kNoValidation> imm(
-              WasmFeatures::All(), &decoder, code->at(pc + 1), module());
+          SelectTypeImmediate imm(WasmFeatures::All(), &decoder,
+                                  code->at(pc + 1), kNoValidate);
           len = 1 + imm.length;
           V8_FALLTHROUGH;
         }
@@ -3434,15 +3419,13 @@
           break;
         }
         case kExprBr: {
-          BranchDepthImmediate<Decoder::kNoValidation> imm(&decoder,
-                                                           code->at(pc + 1));
+          BranchDepthImmediate imm(&decoder, code->at(pc + 1), kNoValidate);
           len = DoBreak(code, pc, imm.depth);
           TRACE("  br => @%zu\n", pc + len);
           break;
         }
         case kExprBrIf: {
-          BranchDepthImmediate<Decoder::kNoValidation> imm(&decoder,
-                                                           code->at(pc + 1));
+          BranchDepthImmediate imm(&decoder, code->at(pc + 1), kNoValidate);
           WasmValue cond = Pop();
           bool is_true = cond.to<uint32_t>() != 0;
           if (is_true) {
@@ -3455,9 +3438,8 @@
           break;
         }
         case kExprBrTable: {
-          BranchTableImmediate<Decoder::kNoValidation> imm(&decoder,
-                                                           code->at(pc + 1));
-          BranchTableIterator<Decoder::kNoValidation> iterator(&decoder, imm);
+          BranchTableImmediate imm(&decoder, code->at(pc + 1), kNoValidate);
+          BranchTableIterator<Decoder::NoValidationTag> iterator(&decoder, imm);
           uint32_t key = Pop().to<uint32_t>();
           uint32_t depth = 0;
           if (key >= imm.table_count) key = imm.table_count;
@@ -3478,8 +3460,7 @@
           return DoTrap(kTrapUnreachable, pc);
         }
         case kExprDelegate: {
-          BranchDepthImmediate<Decoder::kNoValidation> imm(&decoder,
-                                                           code->at(pc + 1));
+          BranchDepthImmediate imm(&decoder, code->at(pc + 1), kNoValidate);
           len = 1 + imm.length;
           break;
         }
@@ -3487,44 +3468,40 @@
           break;
         }
         case kExprI32Const: {
-          ImmI32Immediate<Decoder::kNoValidation> imm(&decoder,
-                                                      code->at(pc + 1));
+          ImmI32Immediate imm(&decoder, code->at(pc + 1), kNoValidate);
           Push(WasmValue(imm.value));
           len = 1 + imm.length;
           break;
         }
         case kExprI64Const: {
-          ImmI64Immediate<Decoder::kNoValidation> imm(&decoder,
-                                                      code->at(pc + 1));
+          ImmI64Immediate imm(&decoder, code->at(pc + 1), kNoValidate);
           Push(WasmValue(imm.value));
           len = 1 + imm.length;
           break;
         }
         case kExprF32Const: {
-          ImmF32Immediate<Decoder::kNoValidation> imm(&decoder,
-                                                      code->at(pc + 1));
+          ImmF32Immediate imm(&decoder, code->at(pc + 1), kNoValidate);
           Push(WasmValue(imm.value));
           len = 1 + imm.length;
           break;
         }
         case kExprF64Const: {
-          ImmF64Immediate<Decoder::kNoValidation> imm(&decoder,
-                                                      code->at(pc + 1));
+          ImmF64Immediate imm(&decoder, code->at(pc + 1), kNoValidate);
           Push(WasmValue(imm.value));
           len = 1 + imm.length;
           break;
         }
         case kExprRefNull: {
-          HeapTypeImmediate<Decoder::kNoValidation> imm(
-              WasmFeatures::All(), &decoder, code->at(pc + 1), module());
+          HeapTypeImmediate imm(WasmFeatures::All(), &decoder, code->at(pc + 1),
+                                kNoValidate);
           len = 1 + imm.length;
           Push(WasmValue(isolate_->factory()->null_value(),
                          ValueType::RefNull(imm.type)));
           break;
         }
         case kExprRefFunc: {
-          IndexImmediate<Decoder::kNoValidation> imm(&decoder, code->at(pc + 1),
-                                                     "function index");
+          IndexImmediate imm(&decoder, code->at(pc + 1), "function index",
+                             kNoValidate);
           HandleScope handle_scope(isolate_);  // Avoid leaking handles.
 
           Handle<WasmInternalFunction> function =
@@ -3535,16 +3512,16 @@
           break;
         }
         case kExprLocalGet: {
-          IndexImmediate<Decoder::kNoValidation> imm(&decoder, code->at(pc + 1),
-                                                     "local index");
+          IndexImmediate imm(&decoder, code->at(pc + 1), "local index",
+                             kNoValidate);
           HandleScope handle_scope(isolate_);  // Avoid leaking handles.
           Push(GetStackValue(frames_.back().sp + imm.index));
           len = 1 + imm.length;
           break;
         }
         case kExprLocalSet: {
-          IndexImmediate<Decoder::kNoValidation> imm(&decoder, code->at(pc + 1),
-                                                     "local index");
+          IndexImmediate imm(&decoder, code->at(pc + 1), "local index",
+                             kNoValidate);
           HandleScope handle_scope(isolate_);  // Avoid leaking handles.
           WasmValue val = Pop();
           SetStackValue(frames_.back().sp + imm.index, val);
@@ -3552,8 +3529,8 @@
           break;
         }
         case kExprLocalTee: {
-          IndexImmediate<Decoder::kNoValidation> imm(&decoder, code->at(pc + 1),
-                                                     "local index");
+          IndexImmediate imm(&decoder, code->at(pc + 1), "local index",
+                             kNoValidate);
           HandleScope handle_scope(isolate_);  // Avoid leaking handles.
           WasmValue val = Pop();
           SetStackValue(frames_.back().sp + imm.index, val);
@@ -3566,8 +3543,7 @@
           break;
         }
         case kExprCallFunction: {
-          CallFunctionImmediate<Decoder::kNoValidation> imm(&decoder,
-                                                            code->at(pc + 1));
+          CallFunctionImmediate imm(&decoder, code->at(pc + 1), kNoValidate);
           InterpreterCode* target = codemap_.GetCode(imm.index);
           CHECK(!target->function->imported);
           // Execute an internal call.
@@ -3577,8 +3553,7 @@
         }
 
         case kExprCallIndirect: {
-          CallIndirectImmediate<Decoder::kNoValidation> imm(&decoder,
-                                                            code->at(pc + 1));
+          CallIndirectImmediate imm(&decoder, code->at(pc + 1), kNoValidate);
           uint32_t entry_index = Pop().to<uint32_t>();
           CommitPc(pc);  // TODO(wasm): Be more disciplined about committing PC.
           CallResult result = CallIndirectFunction(
@@ -3601,8 +3576,7 @@
           // Make return calls more expensive, so that return call recursions
           // don't cause a timeout.
           if (max > 0) max = std::max(0, max - 100);
-          CallFunctionImmediate<Decoder::kNoValidation> imm(&decoder,
-                                                            code->at(pc + 1));
+          CallFunctionImmediate imm(&decoder, code->at(pc + 1), kNoValidate);
           InterpreterCode* target = codemap_.GetCode(imm.index);
 
           CHECK(!target->function->imported);
@@ -3616,8 +3590,7 @@
           // Make return calls more expensive, so that return call recursions
           // don't cause a timeout.
           if (max > 0) max = std::max(0, max - 100);
-          CallIndirectImmediate<Decoder::kNoValidation> imm(&decoder,
-                                                            code->at(pc + 1));
+          CallIndirectImmediate imm(&decoder, code->at(pc + 1), kNoValidate);
           uint32_t entry_index = Pop().to<uint32_t>();
           CommitPc(pc);  // TODO(wasm): Be more disciplined about committing PC.
 
@@ -3644,8 +3617,7 @@
         } break;
 
         case kExprGlobalGet: {
-          GlobalIndexImmediate<Decoder::kNoValidation> imm(&decoder,
-                                                           code->at(pc + 1));
+          GlobalIndexImmediate imm(&decoder, code->at(pc + 1), kNoValidate);
           HandleScope handle_scope(isolate_);
           Push(WasmInstanceObject::GetGlobalValue(
               instance_object_, module()->globals[imm.index]));
@@ -3653,8 +3625,7 @@
           break;
         }
         case kExprGlobalSet: {
-          GlobalIndexImmediate<Decoder::kNoValidation> imm(&decoder,
-                                                           code->at(pc + 1));
+          GlobalIndexImmediate imm(&decoder, code->at(pc + 1), kNoValidate);
           auto& global = module()->globals[imm.index];
           switch (global.type.kind()) {
 #define CASE_TYPE(valuetype, ctype)                                     \
@@ -3690,8 +3661,8 @@
           break;
         }
         case kExprTableGet: {
-          IndexImmediate<Decoder::kNoValidation> imm(&decoder, code->at(pc + 1),
-                                                     "table index");
+          IndexImmediate imm(&decoder, code->at(pc + 1), "table index",
+                             kNoValidate);
           HandleScope handle_scope(isolate_);
           auto table = handle(
               WasmTableObject::cast(instance_object_->tables().get(imm.index)),
@@ -3708,8 +3679,8 @@
           break;
         }
         case kExprTableSet: {
-          IndexImmediate<Decoder::kNoValidation> imm(&decoder, code->at(pc + 1),
-                                                     "table index");
+          IndexImmediate imm(&decoder, code->at(pc + 1), "table index",
+                             kNoValidate);
           HandleScope handle_scope(isolate_);
           auto table = handle(
               WasmTableObject::cast(instance_object_->tables().get(imm.index)),
@@ -3811,8 +3782,7 @@
           ASMJS_STORE_CASE(F64AsmjsStoreMem, double, double);
 #undef ASMJS_STORE_CASE
         case kExprMemoryGrow: {
-          MemoryIndexImmediate<Decoder::kNoValidation> imm(&decoder,
-                                                           code->at(pc + 1));
+          MemoryIndexImmediate imm(&decoder, code->at(pc + 1), kNoValidate);
           // TODO(clemensb): Fix this for memory64.
           uint32_t delta_pages = Pop().to<uint32_t>();
           HandleScope handle_scope(isolate_);  // Avoid leaking handles.
@@ -3828,8 +3798,7 @@
           break;
         }
         case kExprMemorySize: {
-          MemoryIndexImmediate<Decoder::kNoValidation> imm(&decoder,
-                                                           code->at(pc + 1));
+          MemoryIndexImmediate imm(&decoder, code->at(pc + 1), kNoValidate);
           uint64_t num_pages = instance_object_->memory_size() / kWasmPageSize;
           Push(module()->is_memory64
                    ? WasmValue(num_pages)
@@ -4231,7 +4200,7 @@
 }
 
 ControlTransferMap WasmInterpreter::ComputeControlTransfersForTesting(
-    Zone* zone, const WasmModule* module, const byte* start, const byte* end) {
+    Zone* zone, const byte* start, const byte* end) {
   // Create some dummy structures, to avoid special-casing the implementation
   // just for testing.
   FunctionSig sig(0, 0, nullptr);
@@ -4245,7 +4214,8 @@
   InterpreterCode code{&function, BodyLocalDecls{}, start, end, nullptr};
 
   // Now compute and return the control transfers.
-  SideTable side_table(zone, module, &code);
+  constexpr const WasmModule* kNoModule = nullptr;
+  SideTable side_table(zone, kNoModule, &code);
   return side_table.map_;
 }
 
diff -r -u --color up/v8/test/common/wasm/wasm-interpreter.h nw/v8/test/common/wasm/wasm-interpreter.h
--- up/v8/test/common/wasm/wasm-interpreter.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/common/wasm/wasm-interpreter.h	2023-01-19 16:46:36.493109495 -0500
@@ -111,8 +111,9 @@
 
   // Computes the control transfers for the given bytecode. Used internally in
   // the interpreter, but exposed for testing.
-  static ControlTransferMap ComputeControlTransfersForTesting(
-      Zone* zone, const WasmModule* module, const byte* start, const byte* end);
+  static ControlTransferMap ComputeControlTransfersForTesting(Zone* zone,
+                                                              const byte* start,
+                                                              const byte* end);
 
  private:
   Zone zone_;
diff -r -u --color up/v8/test/common/wasm/wasm-macro-gen.h nw/v8/test/common/wasm/wasm-macro-gen.h
--- up/v8/test/common/wasm/wasm-macro-gen.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/common/wasm/wasm-macro-gen.h	2023-01-19 16:46:36.493109495 -0500
@@ -525,6 +525,8 @@
   ref, WASM_GC_OP(kExprRefTestDeprecated), static_cast<byte>(typeidx)
 #define WASM_REF_TEST(ref, typeidx) \
   ref, WASM_GC_OP(kExprRefTest), static_cast<byte>(typeidx)
+#define WASM_REF_CAST_DEPRECATED(ref, typeidx) \
+  ref, WASM_GC_OP(kExprRefCastDeprecated), static_cast<byte>(typeidx)
 #define WASM_REF_CAST(ref, typeidx) \
   ref, WASM_GC_OP(kExprRefCast), static_cast<byte>(typeidx)
 // Takes a reference value from the value stack to allow sequences of
@@ -539,21 +541,21 @@
 #define WASM_GC_INTERNALIZE(extern) extern, WASM_GC_OP(kExprExternInternalize)
 #define WASM_GC_EXTERNALIZE(ref) ref, WASM_GC_OP(kExprExternExternalize)
 
-#define WASM_REF_IS_DATA(ref) ref, WASM_GC_OP(kExprRefIsData)
+#define WASM_REF_IS_STRUCT(ref) ref, WASM_GC_OP(kExprRefIsStruct)
 #define WASM_REF_IS_ARRAY(ref) ref, WASM_GC_OP(kExprRefIsArray)
 #define WASM_REF_IS_I31(ref) ref, WASM_GC_OP(kExprRefIsI31)
-#define WASM_REF_AS_DATA(ref) ref, WASM_GC_OP(kExprRefAsData)
+#define WASM_REF_AS_STRUCT(ref) ref, WASM_GC_OP(kExprRefAsStruct)
 #define WASM_REF_AS_ARRAY(ref) ref, WASM_GC_OP(kExprRefAsArray)
 #define WASM_REF_AS_I31(ref) ref, WASM_GC_OP(kExprRefAsI31)
 #define WASM_BR_ON_ARRAY(depth) \
   WASM_GC_OP(kExprBrOnArray), static_cast<byte>(depth)
-#define WASM_BR_ON_DATA(depth) \
-  WASM_GC_OP(kExprBrOnData), static_cast<byte>(depth)
+#define WASM_BR_ON_STRUCT(depth) \
+  WASM_GC_OP(kExprBrOnStruct), static_cast<byte>(depth)
 #define WASM_BR_ON_I31(depth) WASM_GC_OP(kExprBrOnI31), static_cast<byte>(depth)
 #define WASM_BR_ON_NON_ARRAY(depth) \
   WASM_GC_OP(kExprBrOnNonArray), static_cast<byte>(depth)
-#define WASM_BR_ON_NON_DATA(depth) \
-  WASM_GC_OP(kExprBrOnNonData), static_cast<byte>(depth)
+#define WASM_BR_ON_NON_STRUCT(depth) \
+  WASM_GC_OP(kExprBrOnNonStruct), static_cast<byte>(depth)
 #define WASM_BR_ON_NON_I31(depth) \
   WASM_GC_OP(kExprBrOnNonI31), static_cast<byte>(depth)
 
diff -r -u --color up/v8/test/fuzzer/fuzzer-support.cc nw/v8/test/fuzzer/fuzzer-support.cc
--- up/v8/test/fuzzer/fuzzer-support.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/fuzzer/fuzzer-support.cc	2023-01-19 16:46:36.514776154 -0500
@@ -19,13 +19,13 @@
 FuzzerSupport::FuzzerSupport(int* argc, char*** argv) {
   // Disable hard abort, which generates a trap instead of a proper abortion.
   // Traps by default do not cause libfuzzer to generate a crash file.
-  i::FLAG_hard_abort = false;
+  i::v8_flags.hard_abort = false;
 
-  i::FLAG_expose_gc = true;
+  i::v8_flags.expose_gc = true;
 
   // Allow changing flags in fuzzers.
   // TODO(12887): Refactor fuzzers to not change flags after initialization.
-  i::FLAG_freeze_flags_after_init = false;
+  i::v8_flags.freeze_flags_after_init = false;
 
 #if V8_ENABLE_WEBASSEMBLY
   if (V8_TRAP_HANDLER_SUPPORTED) {
diff -r -u --color up/v8/test/fuzzer/multi-return.cc nw/v8/test/fuzzer/multi-return.cc
--- up/v8/test/fuzzer/multi-return.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/fuzzer/multi-return.cc	2023-01-19 16:46:36.525609486 -0500
@@ -168,7 +168,7 @@
   CallDescriptor* desc =
       CreateRandomCallDescriptor(&zone, return_count, param_count, &input);
 
-  if (FLAG_wasm_fuzzer_gen_test) {
+  if (v8_flags.wasm_fuzzer_gen_test) {
     // Print some debugging output which describes the produced signature.
     printf("[");
     for (size_t j = 0; j < param_count; ++j) {
diff -r -u --color up/v8/test/fuzzer/regexp-builtins.cc nw/v8/test/fuzzer/regexp-builtins.cc
--- up/v8/test/fuzzer/regexp-builtins.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/fuzzer/regexp-builtins.cc	2023-01-19 16:46:36.525609486 -0500
@@ -453,7 +453,7 @@
 
   // Flag definitions.
 
-  FLAG_allow_natives_syntax = true;
+  v8_flags.allow_natives_syntax = true;
 
   // V8 setup.
 
diff -r -u --color up/v8/test/fuzzer/wasm-compile.cc nw/v8/test/fuzzer/wasm-compile.cc
--- up/v8/test/fuzzer/wasm-compile.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/fuzzer/wasm-compile.cc	2023-01-19 16:46:36.525609486 -0500
@@ -131,7 +131,7 @@
                   kWasmNullExternRef, kWasmNullFuncRef});
   }
   if (include_generics == kIncludeGenerics) {
-    types.insert(types.end(), {kWasmDataRef, kWasmAnyRef, kWasmEqRef});
+    types.insert(types.end(), {kWasmStructRef, kWasmAnyRef, kWasmEqRef});
   }
 
   // The last index of user-defined types allowed is different based on the
@@ -864,10 +864,8 @@
       bool can_be_defaultable = std::all_of(
           struct_gen->fields().begin(), struct_gen->fields().end(),
           [](ValueType type) -> bool { return type.is_defaultable(); });
-      bool is_mutable = std::all_of(
-          struct_gen->mutabilities().begin(), struct_gen->mutabilities().end(),
-          [](bool mutability) -> bool { return mutability; });
-      if (new_default && can_be_defaultable && is_mutable) {
+
+      if (new_default && can_be_defaultable) {
         builder_->EmitWithPrefix(kExprStructNewDefault);
         builder_->EmitU32V(index);
       } else {
@@ -2111,8 +2109,10 @@
         }
         random = data->get<uint8_t>() % (num_data_types + emit_i31ref);
       }
-      if (random < num_data_types) {
-        GenerateRef(HeapType(HeapType::kData), data, nullability);
+      if (random < num_structs_) {
+        GenerateRef(HeapType(HeapType::kStruct), data, nullability);
+      } else if (random < num_data_types) {
+        GenerateRef(HeapType(HeapType::kArray), data, nullability);
       } else {
         GenerateRef(HeapType(HeapType::kI31), data, nullability);
       }
@@ -2132,18 +2132,18 @@
       GenerateRef(HeapType(random), data, nullability);
       return;
     }
-    case HeapType::kData: {
+    case HeapType::kStruct: {
       DCHECK(liftoff_as_reference_);
       constexpr uint8_t fallback_to_dataref = 2;
-      uint8_t random = data->get<uint8_t>() %
-                       (num_arrays_ + num_structs_ + fallback_to_dataref);
+      uint8_t random =
+          data->get<uint8_t>() % (num_structs_ + fallback_to_dataref);
       // Try generating one of the alternatives
       // and continue to the rest of the methods in case it fails.
-      if (random >= num_arrays_ + num_structs_) {
+      if (random >= num_structs_) {
         if (GenerateOneOf(alternatives_other, type, data, nullability)) {
           return;
         }
-        random = data->get<uint8_t>() % (num_arrays_ + num_structs_);
+        random = data->get<uint8_t>() % num_structs_;
       }
       GenerateRef(HeapType(random), data, nullability);
       return;
@@ -2384,7 +2384,7 @@
     }
     case kRef: {
       switch (type.heap_type().representation()) {
-        case HeapType::kData:
+        case HeapType::kStruct:
         case HeapType::kAny:
         case HeapType::kEq: {
           // We materialize all these types with a struct because they are all
@@ -2612,8 +2612,6 @@
   constexpr bool require_valid = true;
   EXPERIMENTAL_FLAG_SCOPE(typed_funcref);
   EXPERIMENTAL_FLAG_SCOPE(gc);
-  EXPERIMENTAL_FLAG_SCOPE(simd);
-  EXPERIMENTAL_FLAG_SCOPE(eh);
   WasmCompileFuzzer().FuzzWasmModule({data, size}, require_valid);
   return 0;
 }
diff -r -u --color up/v8/test/fuzzer/wasm-fuzzer-common.cc nw/v8/test/fuzzer/wasm-fuzzer-common.cc
--- up/v8/test/fuzzer/wasm-fuzzer-common.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/fuzzer/wasm-fuzzer-common.cc	2023-01-19 16:46:36.525609486 -0500
@@ -231,8 +231,8 @@
       return "kEqRefCode";
     case HeapType::kI31:
       return "kI31RefCode";
-    case HeapType::kData:
-      return "kDataRefCode";
+    case HeapType::kStruct:
+      return "kStructRefCode";
     case HeapType::kArray:
       return "kArrayRefCode";
     case HeapType::kAny:
@@ -260,8 +260,8 @@
       return "kWasmEqRef";
     case HeapType::kI31:
       return "kWasmI31Ref";
-    case HeapType::kData:
-      return "kWasmDataRef";
+    case HeapType::kStruct:
+      return "kWasmStructRef";
     case HeapType::kArray:
       return "kWasmArrayRef";
     case HeapType::kExtern:
@@ -309,7 +309,7 @@
           return "kWasmAnyRef";
         case HeapType::kBottom:
           UNREACHABLE();
-        case HeapType::kData:
+        case HeapType::kStruct:
         case HeapType::kArray:
         case HeapType::kI31:
         default:
@@ -346,18 +346,18 @@
 // representation of the expression, compatible with wasm-module-builder.js.
 class InitExprInterface {
  public:
-  static constexpr Decoder::ValidateFlag validate = Decoder::kFullValidation;
+  using ValidationTag = Decoder::FullValidationTag;
   static constexpr DecodingMode decoding_mode = kConstantExpression;
 
-  struct Value : public ValueBase<validate> {
+  struct Value : public ValueBase<ValidationTag> {
     template <typename... Args>
     explicit Value(Args&&... args) V8_NOEXCEPT
         : ValueBase(std::forward<Args>(args)...) {}
   };
 
-  using Control = ControlBase<Value, validate>;
+  using Control = ControlBase<Value, ValidationTag>;
   using FullDecoder =
-      WasmFullDecoder<validate, InitExprInterface, decoding_mode>;
+      WasmFullDecoder<ValidationTag, InitExprInterface, decoding_mode>;
 
   explicit InitExprInterface(StdoutStream& os) : os_(os) { os_ << "["; }
 
@@ -386,8 +386,7 @@
     os_ << "...wasmF64Const(" << value << "), ";
   }
 
-  void S128Const(FullDecoder* decoder, Simd128Immediate<validate>& imm,
-                 Value* result) {
+  void S128Const(FullDecoder* decoder, Simd128Immediate& imm, Value* result) {
     os_ << "kSimdPrefix, kExprS128Const, " << std::hex;
     for (int i = 0; i < kSimd128Size; i++) {
       os_ << "0x" << static_cast<int>(imm.value[i]) << ", ";
@@ -411,37 +410,33 @@
   }
 
   void GlobalGet(FullDecoder* decoder, Value* result,
-                 const GlobalIndexImmediate<validate>& imm) {
+                 const GlobalIndexImmediate& imm) {
     os_ << "kWasmGlobalGet, " << index(imm.index);
   }
 
   // The following operations assume non-rtt versions of the instructions.
-  void StructNew(FullDecoder* decoder,
-                 const StructIndexImmediate<validate>& imm, const Value& rtt,
-                 const Value args[], Value* result) {
+  void StructNew(FullDecoder* decoder, const StructIndexImmediate& imm,
+                 const Value& rtt, const Value args[], Value* result) {
     os_ << "kGCPrefix, kExprStructNew, " << index(imm.index);
   }
 
-  void StructNewDefault(FullDecoder* decoder,
-                        const StructIndexImmediate<validate>& imm,
+  void StructNewDefault(FullDecoder* decoder, const StructIndexImmediate& imm,
                         const Value& rtt, Value* result) {
     os_ << "kGCPrefix, kExprStructNewDefault, " << index(imm.index);
   }
 
-  void ArrayNew(FullDecoder* decoder, const ArrayIndexImmediate<validate>& imm,
+  void ArrayNew(FullDecoder* decoder, const ArrayIndexImmediate& imm,
                 const Value& length, const Value& initial_value,
                 const Value& rtt, Value* result) {
     os_ << "kGCPrefix, kExprArrayNew, " << index(imm.index);
   }
 
-  void ArrayNewDefault(FullDecoder* decoder,
-                       const ArrayIndexImmediate<validate>& imm,
+  void ArrayNewDefault(FullDecoder* decoder, const ArrayIndexImmediate& imm,
                        const Value& length, const Value& rtt, Value* result) {
     os_ << "kGCPrefix, kExprArrayNewDefault, " << index(imm.index);
   }
 
-  void ArrayNewFixed(FullDecoder* decoder,
-                     const ArrayIndexImmediate<validate>& imm,
+  void ArrayNewFixed(FullDecoder* decoder, const ArrayIndexImmediate& imm,
                      const base::Vector<Value>& elements, const Value& rtt,
                      Value* result) {
     os_ << "kGCPrefix, kExprArrayNewFixed, " << index(imm.index)
@@ -449,8 +444,8 @@
   }
 
   void ArrayNewSegment(FullDecoder* decoder,
-                       const ArrayIndexImmediate<validate>& array_imm,
-                       const IndexImmediate<validate>& data_segment_imm,
+                       const ArrayIndexImmediate& array_imm,
+                       const IndexImmediate& data_segment_imm,
                        const Value& offset_value, const Value& length_value,
                        const Value& rtt, Value* result) {
     // TODO(7748): Implement.
@@ -464,8 +459,8 @@
   // Since we treat all instructions as rtt-less, we should not print rtts.
   void RttCanon(FullDecoder* decoder, uint32_t type_index, Value* result) {}
 
-  void StringConst(FullDecoder* decoder,
-                   const StringConstImmediate<validate>& imm, Value* result) {
+  void StringConst(FullDecoder* decoder, const StringConstImmediate& imm,
+                   Value* result) {
     os_ << "...GCInstr(kExprStringConst), " << index(imm.index);
   }
 
@@ -498,7 +493,7 @@
       FunctionBody body(&sig, ref.offset(), module_bytes.start() + ref.offset(),
                         module_bytes.start() + ref.end_offset());
       WasmFeatures detected;
-      WasmFullDecoder<Decoder::kFullValidation, InitExprInterface,
+      WasmFullDecoder<Decoder::FullValidationTag, InitExprInterface,
                       kConstantExpression>
           decoder(zone, module, WasmFeatures::All(), &detected, body, os);
       decoder.DecodeFunctionBody();
@@ -682,7 +677,7 @@
 
     // Add locals.
     BodyLocalDecls decls;
-    DecodeLocalDecls(enabled_features, &decls, module, func_code.begin(),
+    DecodeLocalDecls(enabled_features, &decls, func_code.begin(),
                      func_code.end(), &tmp_zone);
     if (decls.num_locals) {
       os << "  ";
diff -r -u --color up/v8/test/inspector/cpu-profiler/console-profile-wasm-expected.txt nw/v8/test/inspector/cpu-profiler/console-profile-wasm-expected.txt
--- up/v8/test/inspector/cpu-profiler/console-profile-wasm-expected.txt	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/inspector/cpu-profiler/console-profile-wasm-expected.txt	2023-01-19 16:46:36.525609486 -0500
@@ -1,26 +1,30 @@
 Test that console profiles contain wasm function names.
-testEnableProfilerEarly
+
+Running test: testEnableProfilerEarly
 Compiling wasm.
 Building wasm module with sentinel 1.
 Running fib with increasing input until it shows up in the profile.
 Found expected functions in profile.
 Wasm script id is set.
 Wasm position: wasm://wasm/6b211e7e@0:47
-testEnableProfilerLate
+
+Running test: testEnableProfilerLate
 Compiling wasm.
 Building wasm module with sentinel 2.
 Running fib with increasing input until it shows up in the profile.
 Found expected functions in profile.
 Wasm script id is set.
 Wasm position: wasm://wasm/d6029ed6@0:47
-testEnableProfilerAfterDebugger
+
+Running test: testEnableProfilerAfterDebugger
 Compiling wasm.
 Building wasm module with sentinel 3.
 Running fib with increasing input until it shows up in the profile.
 Found expected functions in profile.
 Wasm script id is set.
 Wasm position: wasm://wasm/6df1c11a@0:47
-testEnableProfilerBeforeDebugger
+
+Running test: testEnableProfilerBeforeDebugger
 Compiling wasm.
 Building wasm module with sentinel 4.
 Running fib with increasing input until it shows up in the profile.
diff -r -u --color up/v8/test/inspector/cpu-profiler/console-profile-wasm.js nw/v8/test/inspector/cpu-profiler/console-profile-wasm.js
--- up/v8/test/inspector/cpu-profiler/console-profile-wasm.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/inspector/cpu-profiler/console-profile-wasm.js	2023-01-19 16:46:36.525609486 -0500
@@ -63,12 +63,28 @@
 let found_wasm_script_id;
 let wasm_position;
 let finished_profiles = 0;
+
+// Remember which sequences of functions we have seen in profiles so far. This
+// is printed in the error case to aid debugging.
+let seen_profiles = [];
+function addSeenProfile(function_names) {
+  let arrays_equal = (a, b) =>
+      a.length == b.length && a.every((val, index) => val == b[index]);
+  if (seen_profiles.some(a => arrays_equal(a, function_names))) return false;
+  seen_profiles.push(function_names.slice());
+  return true;
+}
+
+function resetGlobalData() {
+  found_good_profile = false;
+  finished_profiles = 0;
+  seen_profiles = [];
+}
+
 Protocol.Profiler.onConsoleProfileFinished(e => {
   ++finished_profiles;
   let nodes = e.params.profile.nodes;
   let function_names = nodes.map(n => n.callFrame.functionName);
-  // Enable this line for debugging:
-  // InspectorTest.log(function_names.join(', '));
   // Check for at least one full cycle of
   // fib -> wasm-to-js -> imp -> js-to-wasm -> fib.
   // There are two different kinds of js-to-wasm-wrappers, so there are two
@@ -77,6 +93,7 @@
     ['fib'], ['wasm-to-js:i:i'], ['imp'],
     ['GenericJSToWasmWrapper', 'js-to-wasm:i:i'], ['fib']
   ];
+  if (!addSeenProfile(function_names)) return;
   for (let i = 0; i <= function_names.length - expected.length; ++i) {
     if (expected.every((val, idx) => val.includes(function_names[i + idx]))) {
       found_good_profile = true;
@@ -91,25 +108,35 @@
 async function runFibUntilProfileFound() {
   InspectorTest.log(
       'Running fib with increasing input until it shows up in the profile.');
-  found_good_profile = false;
-  finished_profiles = 0;
+  resetGlobalData();
+  const start = Date.now();
+  const kTimeoutMs = 30000;
   for (let i = 1; !found_good_profile; ++i) {
     checkError(await Protocol.Runtime.evaluate(
-        {expression: 'console.profile(\'profile\');'}));
+        {expression: `console.profile('profile');`}));
     checkError(await Protocol.Runtime.evaluate(
-        {expression: 'globalThis.instance.exports.fib(' + i + ');'}));
+        {expression: `globalThis.instance.exports.fib(${i});`}));
     checkError(await Protocol.Runtime.evaluate(
-        {expression: 'console.profileEnd(\'profile\');'}));
+        {expression: `console.profileEnd('profile');`}));
     if (finished_profiles != i) {
       InspectorTest.log(
-          'Missing consoleProfileFinished message (expected ' + i + ', got ' +
-          finished_profiles + ')');
+          `Missing consoleProfileFinished message (expected ${i}, got ` +
+          `${finished_profiles})`);
+    }
+    if (Date.now() - start > kTimeoutMs) {
+      InspectorTest.log('Seen profiles so far:');
+      for (let profile of seen_profiles) {
+        InspectorTest.log('  - ' + profile.join(" -> "));
+      }
+      throw new Error(
+          `fib did not show up in the profile within ` +
+          `${kTimeoutMs}ms (after ${i} executions)`);
     }
   }
   InspectorTest.log('Found expected functions in profile.');
   InspectorTest.log(
-      'Wasm script id is ' + (found_wasm_script_id ? 'set.' : 'NOT SET.'));
-  InspectorTest.log('Wasm position: ' + wasm_position);
+      `Wasm script id is ${found_wasm_script_id ? 'set.' : 'NOT SET.'}`);
+  InspectorTest.log(`Wasm position: ${wasm_position}`);
 }
 
 async function compileWasm() {
@@ -120,54 +147,40 @@
   }));
 }
 
-async function testEnableProfilerEarly() {
-  InspectorTest.log(arguments.callee.name);
-  checkError(await Protocol.Profiler.enable());
-  checkError(await Protocol.Profiler.start());
-  await compileWasm();
-  await runFibUntilProfileFound();
-  checkError(await Protocol.Profiler.disable());
-}
-
-async function testEnableProfilerLate() {
-  InspectorTest.log(arguments.callee.name);
-  await compileWasm();
-  checkError(await Protocol.Profiler.enable());
-  checkError(await Protocol.Profiler.start());
-  await runFibUntilProfileFound();
-  checkError(await Protocol.Profiler.disable());
-}
-
-async function testEnableProfilerAfterDebugger() {
-  InspectorTest.log(arguments.callee.name);
-  checkError(await Protocol.Debugger.enable());
-  await compileWasm();
-  checkError(await Protocol.Profiler.enable());
-  checkError(await Protocol.Profiler.start());
-  await runFibUntilProfileFound();
-  checkError(await Protocol.Profiler.disable());
-  checkError(await Protocol.Debugger.disable());
-}
-
-async function testEnableProfilerBeforeDebugger() {
-  InspectorTest.log(arguments.callee.name);
-  await compileWasm();
-  await Protocol.Profiler.enable();
-  await Protocol.Debugger.enable();
-  checkError(await Protocol.Profiler.start());
-  await runFibUntilProfileFound();
-  await Protocol.Debugger.disable();
-  await Protocol.Profiler.disable();
-}
-
-(async function test() {
-  try {
-    await testEnableProfilerEarly();
-    await testEnableProfilerLate();
-    await testEnableProfilerAfterDebugger();
-    await testEnableProfilerBeforeDebugger();
-  } catch (e) {
-    InspectorTest.log('caught: ' + e);
+InspectorTest.runAsyncTestSuite([
+  async function testEnableProfilerEarly() {
+    checkError(await Protocol.Profiler.enable());
+    checkError(await Protocol.Profiler.start());
+    await compileWasm();
+    await runFibUntilProfileFound();
+    checkError(await Protocol.Profiler.disable());
+  },
+
+  async function testEnableProfilerLate() {
+    await compileWasm();
+    checkError(await Protocol.Profiler.enable());
+    checkError(await Protocol.Profiler.start());
+    await runFibUntilProfileFound();
+    checkError(await Protocol.Profiler.disable());
+  },
+
+  async function testEnableProfilerAfterDebugger() {
+    checkError(await Protocol.Debugger.enable());
+    await compileWasm();
+    checkError(await Protocol.Profiler.enable());
+    checkError(await Protocol.Profiler.start());
+    await runFibUntilProfileFound();
+    checkError(await Protocol.Profiler.disable());
+    checkError(await Protocol.Debugger.disable());
+  },
+
+  async function testEnableProfilerBeforeDebugger() {
+    await compileWasm();
+    await Protocol.Profiler.enable();
+    await Protocol.Debugger.enable();
+    checkError(await Protocol.Profiler.start());
+    await runFibUntilProfileFound();
+    await Protocol.Debugger.disable();
+    await Protocol.Profiler.disable();
   }
-})().catch(e => InspectorTest.log('caught: ' + e))
-    .finally(InspectorTest.completeTest);
+]);
diff -r -u --color up/v8/test/inspector/debugger/break-on-exception-expected.txt nw/v8/test/inspector/debugger/break-on-exception-expected.txt
--- up/v8/test/inspector/debugger/break-on-exception-expected.txt	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/inspector/debugger/break-on-exception-expected.txt	2023-01-19 16:46:36.536442816 -0500
@@ -20,6 +20,23 @@
 
 evaluate 'caughtFinally()'..
 
+Running test: testBreakOnCaughtException
+
+evaluate 'caught()'..
+paused on exception:
+{
+    description : 1
+    type : number
+    uncaught : false
+    value : 1
+}
+
+evaluate 'uncaught()'..
+
+evaluate 'uncaughtFinally()'..
+
+evaluate 'caughtFinally()'..
+
 Running test: testBreakOnUncaughtException
 
 evaluate 'caught()'..
@@ -131,7 +148,7 @@
 paused on exception:
 {
     className : Error
-    description : Error     at f (<anonymous>:106:144)     at <anonymous>:137:154
+    description : Error     at f (<anonymous>:115:153)     at <anonymous>:146:163
     objectId : <objectId>
     subtype : error
     type : object
@@ -144,7 +161,7 @@
 paused on exception:
 {
     className : Error
-    description : Error     at f (<anonymous>:106:144)     at <anonymous>:137:154
+    description : Error     at f (<anonymous>:115:153)     at <anonymous>:146:163
     objectId : <objectId>
     subtype : error
     type : object
diff -r -u --color up/v8/test/inspector/debugger/break-on-exception.js nw/v8/test/inspector/debugger/break-on-exception.js
--- up/v8/test/inspector/debugger/break-on-exception.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/inspector/debugger/break-on-exception.js	2023-01-19 16:46:36.536442816 -0500
@@ -57,6 +57,15 @@
     await Protocol.Debugger.setPauseOnExceptions({state: 'none'});
   },
 
+  async function testBreakOnCaughtException() {
+    await Protocol.Debugger.setPauseOnExceptions({state: 'caught'});
+    await evaluate('caught()');
+    await evaluate('uncaught()');
+    await evaluate('uncaughtFinally()');
+    await evaluate('caughtFinally()');
+    await Protocol.Debugger.setPauseOnExceptions({state: 'none'});
+  },
+
   async function testBreakOnUncaughtException() {
     await Protocol.Debugger.setPauseOnExceptions({state: 'uncaught'});
     await evaluate('caught()');
Only in nw/v8/test/inspector/debugger: evaluate-on-call-frame-new-target-expected.txt
Only in nw/v8/test/inspector/debugger: evaluate-on-call-frame-new-target.js
Only in nw/v8/test/inspector/debugger: pause-on-instrumentation-expected.txt
Only in nw/v8/test/inspector/debugger: pause-on-instrumentation.js
Only in nw/v8/test/inspector/debugger: reuse-locals-blocklists-not-inside-function-expected.txt
Only in nw/v8/test/inspector/debugger: reuse-locals-blocklists-not-inside-function.js
diff -r -u --color up/v8/test/inspector/debugger/set-breakpoint-on-instrumentation.js nw/v8/test/inspector/debugger/set-breakpoint-on-instrumentation.js
--- up/v8/test/inspector/debugger/set-breakpoint-on-instrumentation.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/inspector/debugger/set-breakpoint-on-instrumentation.js	2023-01-19 16:46:36.558109476 -0500
@@ -31,7 +31,7 @@
   const url = session.getCallFrameUrl(top_frame);
   InspectorTest.log(`Paused at ${url} with reason "${reason}".`);
   InspectorTest.log(
-      `Hit breakpoints: ${JSON.stringify(msg.params.hitBreakpoints)}`)
+      `Hit breakpoints: ${JSON.stringify(msg.params.hitBreakpoints)}`);
   return Protocol.Debugger.resume();
 };
 
Only in nw/v8/test/inspector/debugger: set-script-source-repl-mode-expected.txt
Only in nw/v8/test/inspector/debugger: set-script-source-repl-mode.js
diff -r -u --color up/v8/test/inspector/frontend-channel.h nw/v8/test/inspector/frontend-channel.h
--- up/v8/test/inspector/frontend-channel.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/inspector/frontend-channel.h	2023-01-19 16:46:36.568942806 -0500
@@ -55,11 +55,11 @@
 
    private:
     void Run(InspectorIsolateData* data) override {
-      v8::MicrotasksScope microtasks_scope(data->isolate(),
-                                           v8::MicrotasksScope::kRunMicrotasks);
       v8::HandleScope handle_scope(data->isolate());
       v8::Local<v8::Context> context =
           data->GetDefaultContext(channel_->context_group_id_);
+      v8::MicrotasksScope microtasks_scope(context,
+                                           v8::MicrotasksScope::kRunMicrotasks);
       v8::Context::Scope context_scope(context);
       v8::Local<v8::Value> message = ToV8String(data->isolate(), message_);
       v8::MaybeLocal<v8::Value> result;
diff -r -u --color up/v8/test/inspector/inspector-test.cc nw/v8/test/inspector/inspector-test.cc
--- up/v8/test/inspector/inspector-test.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/inspector/inspector-test.cc	2023-01-19 16:46:36.568942806 -0500
@@ -806,7 +806,7 @@
   v8::V8::InitializeICUDefaultLocation(argv[0]);
   std::unique_ptr<Platform> platform(platform::NewDefaultPlatform());
   v8::V8::InitializePlatform(platform.get());
-  FLAG_abort_on_contradictory_flags = true;
+  v8_flags.abort_on_contradictory_flags = true;
   v8::V8::SetFlagsFromCommandLine(&argc, argv, true);
   v8::V8::InitializeExternalStartupData(argv[0]);
   v8::V8::Initialize();
diff -r -u --color up/v8/test/inspector/isolate-data.cc nw/v8/test/inspector/isolate-data.cc
--- up/v8/test/inspector/isolate-data.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/inspector/isolate-data.cc	2023-01-19 16:46:36.568942806 -0500
@@ -389,7 +389,7 @@
     v8::Local<v8::Object> object) {
   v8::Local<v8::Context> context = isolate()->GetCurrentContext();
   v8::MicrotasksScope microtasks_scope(
-      isolate(), v8::MicrotasksScope::kDoNotRunMicrotasks);
+      context, v8::MicrotasksScope::kDoNotRunMicrotasks);
   return !object->HasPrivate(context, not_inspectable_private_.Get(isolate()))
               .FromMaybe(false);
 }
Only in nw/v8/test/inspector/regress: regress-crbug-1085693-expected.txt
Only in nw/v8/test/inspector/regress: regress-crbug-1085693.js
Only in nw/v8/test/inspector/regress: regress-crbug-1209117-expected.txt
Only in nw/v8/test/inspector/regress: regress-crbug-1209117.js
Only in nw/v8/test/inspector/regress: regress-crbug-1246897-expected.txt
Only in nw/v8/test/inspector/regress: regress-crbug-1246897.js
Only in nw/v8/test/inspector/regress: regress-crbug-1352303-expected.txt
Only in nw/v8/test/inspector/regress: regress-crbug-1352303.js
diff -r -u --color up/v8/test/inspector/runtime/get-properties-expected.txt nw/v8/test/inspector/runtime/get-properties-expected.txt
--- up/v8/test/inspector/runtime/get-properties-expected.txt	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/inspector/runtime/get-properties-expected.txt	2023-01-19 16:46:36.579776138 -0500
@@ -194,3 +194,8 @@
 Running test: testTypedArrayNonIndexedPropertiesOnly
 Internal properties
   [[Prototype]] object undefined
+
+Running test: testWeakRef
+Internal properties
+  [[Prototype]] object undefined
+  [[WeakRefTarget]] object undefined
diff -r -u --color up/v8/test/inspector/runtime/get-properties.js nw/v8/test/inspector/runtime/get-properties.js
--- up/v8/test/inspector/runtime/get-properties.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/inspector/runtime/get-properties.js	2023-01-19 16:46:36.579776138 -0500
@@ -114,6 +114,10 @@
 
   function testTypedArrayNonIndexedPropertiesOnly() {
     return logExpressionProperties('new Int8Array(1)', {nonIndexedPropertiesOnly: true, ownProperties: true});
+  },
+
+  function testWeakRef() {
+    return logExpressionProperties('new WeakRef(globalThis)');
   }
 ]);
 
diff -r -u --color up/v8/test/inspector/task-runner.cc nw/v8/test/inspector/task-runner.cc
--- up/v8/test/inspector/task-runner.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/inspector/task-runner.cc	2023-01-19 16:46:36.579776138 -0500
@@ -88,7 +88,7 @@
     // This can be removed once https://crbug.com/v8/10747 is fixed.
     // TODO(10748): Enable --stress-incremental-marking after the existing
     // tests are fixed.
-    if (!i::FLAG_stress_incremental_marking) {
+    if (!i::v8_flags.stress_incremental_marking) {
       while (v8::platform::PumpMessageLoop(
           v8::internal::V8::GetCurrentPlatform(), isolate(),
           isolate()->HasPendingBackgroundTasks()
diff -r -u --color up/v8/test/inspector/tasks.cc nw/v8/test/inspector/tasks.cc
--- up/v8/test/inspector/tasks.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/inspector/tasks.cc	2023-01-19 16:46:36.579776138 -0500
@@ -15,10 +15,10 @@
 namespace internal {
 
 void ExecuteStringTask::Run(InspectorIsolateData* data) {
-  v8::MicrotasksScope microtasks_scope(data->isolate(),
-                                       v8::MicrotasksScope::kRunMicrotasks);
   v8::HandleScope handle_scope(data->isolate());
   v8::Local<v8::Context> context = data->GetDefaultContext(context_group_id_);
+  v8::MicrotasksScope microtasks_scope(context,
+                                       v8::MicrotasksScope::kRunMicrotasks);
   v8::Context::Scope context_scope(context);
   v8::ScriptOrigin origin(data->isolate(), ToV8String(data->isolate(), name_),
                           line_offset_, column_offset_,
diff -r -u --color up/v8/test/inspector/tasks.h nw/v8/test/inspector/tasks.h
--- up/v8/test/inspector/tasks.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/inspector/tasks.h	2023-01-19 16:46:36.579776138 -0500
@@ -129,10 +129,10 @@
 
  private:
   void Run(InspectorIsolateData* data) override {
-    v8::MicrotasksScope microtasks_scope(data->isolate(),
-                                         v8::MicrotasksScope::kRunMicrotasks);
     v8::HandleScope handle_scope(data->isolate());
     v8::Local<v8::Context> context = data->GetDefaultContext(context_group_id_);
+    v8::MicrotasksScope microtasks_scope(context,
+                                         v8::MicrotasksScope::kRunMicrotasks);
     v8::Context::Scope context_scope(context);
 
     v8::Local<v8::Function> function = function_.Get(data->isolate());
diff -r -u --color up/v8/test/message/fail/wasm-exception-rethrow.js nw/v8/test/message/fail/wasm-exception-rethrow.js
--- up/v8/test/message/fail/wasm-exception-rethrow.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/message/fail/wasm-exception-rethrow.js	2023-01-19 16:46:36.677276110 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --expose-wasm --experimental-wasm-eh
-
 d8.file.execute('test/mjsunit/wasm/wasm-module-builder.js');
 
 let builder = new WasmModuleBuilder();
diff -r -u --color up/v8/test/message/fail/wasm-exception-throw.js nw/v8/test/message/fail/wasm-exception-throw.js
--- up/v8/test/message/fail/wasm-exception-throw.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/message/fail/wasm-exception-throw.js	2023-01-19 16:46:36.677276110 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --expose-wasm --experimental-wasm-eh
-
 d8.file.execute('test/mjsunit/wasm/wasm-module-builder.js');
 
 let builder = new WasmModuleBuilder();
diff -r -u --color up/v8/test/message/wasm-trace-memory-liftoff.js nw/v8/test/message/wasm-trace-memory-liftoff.js
--- up/v8/test/message/wasm-trace-memory-liftoff.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/message/wasm-trace-memory-liftoff.js	2023-01-19 16:46:36.688109442 -0500
@@ -2,8 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --trace-wasm-memory --liftoff
-// Flags: --no-wasm-tier-up --experimental-wasm-simd
+// Flags: --trace-wasm-memory --liftoff --no-wasm-tier-up
 
 // Force enable sse3 and sse4-1, since that will determine which execution tier
 // we use, and thus the expected output message will differ.
diff -r -u --color up/v8/test/message/wasm-trace-memory.js nw/v8/test/message/wasm-trace-memory.js
--- up/v8/test/message/wasm-trace-memory.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/message/wasm-trace-memory.js	2023-01-19 16:46:36.688109442 -0500
@@ -3,7 +3,6 @@
 // found in the LICENSE file.
 
 // Flags: --trace-wasm-memory --no-liftoff
-// Flags: --experimental-wasm-simd
 
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 
diff -r -u --color up/v8/test/mjsunit/allocation-site-info.js nw/v8/test/mjsunit/allocation-site-info.js
--- up/v8/test/mjsunit/allocation-site-info.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/allocation-site-info.js	2023-01-19 16:46:36.688109442 -0500
@@ -214,7 +214,7 @@
 
 // Try to continue the transition to fast object.
 // TODO(mvstanton): re-enable commented out code when
-// FLAG_pretenuring_call_new is turned on in the build.
+// v8_flags.pretenuring_call_new is turned on in the build.
 obj = newarraycase_length_smidouble("coates");
 assertKind(elements_kind.fast, obj);
 obj = newarraycase_length_smidouble(2);
Only in nw/v8/test/mjsunit: array-buffer-detach-key.js
diff -r -u --color up/v8/test/mjsunit/code-coverage-block-async.js nw/v8/test/mjsunit/code-coverage-block-async.js
--- up/v8/test/mjsunit/code-coverage-block-async.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/code-coverage-block-async.js	2023-01-19 16:46:36.796442745 -0500
@@ -4,42 +4,45 @@
 
 // Flags: --allow-natives-syntax --no-always-turbofan --no-stress-flush-code
 // Flags: --no-stress-incremental-marking
+// Flags: --expose-gc
 // Files: test/mjsunit/code-coverage-utils.js
 
-%DebugToggleBlockCoverage(true);
+(async function () {
 
-TestCoverage(
-"await expressions",
-`
+  %DebugToggleBlockCoverage(true);
+
+  await TestCoverage(
+    "await expressions",
+    `
 async function f() {                      // 0000
   await 42;                               // 0050
   await 42;                               // 0100
 };                                        // 0150
 f();                                      // 0200
 %PerformMicrotaskCheckpoint();            // 0250
-`,
-[{"start":0,"end":299,"count":1},
-  {"start":0,"end":151,"count":1}]
-);
-
-TestCoverage(
-"for-await-of statements",
-`
+    `,
+    [ {"start":0,"end":299,"count":1},
+      {"start":0,"end":151,"count":1} ]
+  );
+
+  await TestCoverage(
+    "for-await-of statements",
+    `
 !async function() {                       // 0000
   for await (var x of [0,1,2,3]) {        // 0050
     nop();                                // 0100
   }                                       // 0150
 }();                                      // 0200
 %PerformMicrotaskCheckpoint();            // 0250
-`,
-[{"start":0,"end":299,"count":1},
-  {"start":1,"end":201,"count":1},
-  {"start":83,"end":153,"count":4}]
-);
-
-TestCoverage(
-"https://crbug.com/981313",
-`
+    `,
+    [ {"start":0,"end":299,"count":1},
+      {"start":1,"end":201,"count":1},
+      {"start":83,"end":153,"count":4} ]
+  );
+
+  await TestCoverage(
+    "https://crbug.com/981313",
+    `
 class Foo {                               // 0000
   async timeout() {                       // 0000
     return new Promise(                   // 0100
@@ -47,14 +50,15 @@
   }                                       // 0200
 }                                         // 0000
 new Foo().timeout();                      // 0300
-`,
-[ {"start":0,  "end":349, "count":1},
-  {"start":52, "end":203, "count":1},
-  {"start":158,"end":182, "count":1}]);
-
-TestCoverage(
-  "test async generator coverage",
-`
+    `,
+    [ {"start":0,  "end":349, "count":1},
+      {"start":52, "end":203, "count":1},
+      {"start":158,"end":182, "count":1} ]
+  );
+
+  await TestCoverage(
+    "test async generator coverage",
+    `
 class Foo {                               // 0000
   async *timeout() {                      // 0000
     return new Promise(                   // 0100
@@ -62,14 +66,15 @@
   }                                       // 0200
 }                                         // 0000
 new Foo().timeout();                      // 0300
-`,
-  [ {"start":0,  "end":349, "count":1},
-    {"start":52, "end":203, "count":1},
-    {"start":158,"end":182, "count":0}]);
-
-TestCoverage(
-  "test async generator coverage with next call",
-`
+    `,
+    [ {"start":0,  "end":349, "count":1},
+      {"start":52, "end":203, "count":1},
+      {"start":158,"end":182, "count":0} ]
+  );
+
+  await TestCoverage(
+    "test async generator coverage with next call",
+    `
 class Foo {                               // 0000
   async *timeout() {                      // 0000
     return new Promise(                   // 0100
@@ -77,14 +82,15 @@
   }                                       // 0200
 }                                         // 0000
 new Foo().timeout().next();               // 0300
-`,
-  [ {"start":0,  "end":349, "count":1},
-    {"start":52, "end":203, "count":1},
-    {"start":158,"end":182, "count":1}]);
-
-TestCoverage(
-  "test two consecutive returns",
-`
+    `,
+    [ {"start":0,  "end":349, "count":1},
+      {"start":52, "end":203, "count":1},
+      {"start":158,"end":182, "count":1} ]
+  );
+
+  await TestCoverage(
+    "test two consecutive returns",
+    `
 class Foo {                               // 0000
   timeout() {                             // 0000
     return new Promise(                   // 0100
@@ -94,16 +100,17 @@
   }                                       // 0300
 }                                         // 0000
 new Foo().timeout();                      // 0400
-`,
-[ {"start":0,"end":449,"count":1},
-  {"start":52,"end":303,"count":1},
-  {"start":184,"end":302,"count":0},
-  {"start":158,"end":182,"count":1}] );
+    `,
+    [ {"start":0,"end":449,"count":1},
+      {"start":52,"end":303,"count":1},
+      {"start":184,"end":302,"count":0},
+      {"start":158,"end":182,"count":1} ]
+  );
 
 
-TestCoverage(
-  "test async generator with two consecutive returns",
-`
+  await TestCoverage(
+    "test async generator with two consecutive returns",
+    `
 class Foo {                               // 0000
   async *timeout() {                      // 0000
     return new Promise(                   // 0100
@@ -113,15 +120,16 @@
   }                                       // 0300
 }                                         // 0000
 new Foo().timeout().next();               // 0400
-`,
-[ {"start":0,"end":449,"count":1},
-  {"start":52,"end":303,"count":1},
-  {"start":184,"end":302,"count":0},
-  {"start":158,"end":182,"count":1}] );
-
-TestCoverage(
-"https://crbug.com/v8/9952",
-`
+    `,
+    [ {"start":0,"end":449,"count":1},
+      {"start":52,"end":303,"count":1},
+      {"start":184,"end":302,"count":0},
+      {"start":158,"end":182,"count":1} ]
+  );
+
+  await TestCoverage(
+    "https://crbug.com/v8/9952",
+    `
 async function test(foo) {                // 0000
   return {bar};                           // 0050
                                           // 0100
@@ -130,15 +138,17 @@
   }                                       // 0250
 }                                         // 0300
 test().then(r => r.bar());                // 0350
-%PerformMicrotaskCheckpoint();            // 0400`,
-[{"start":0,"end":449,"count":1},
- {"start":0,"end":301,"count":1},
- {"start":152,"end":253,"count":1},
- {"start":362,"end":374,"count":1}]);
-
-TestCoverage(
-"https://crbug.com/v8/10628",
-`
+%PerformMicrotaskCheckpoint();            // 0400
+    `,
+    [ {"start":0,"end":449,"count":1},
+      {"start":0,"end":301,"count":1},
+      {"start":152,"end":253,"count":1},
+      {"start":362,"end":374,"count":1} ]
+  );
+
+  await TestCoverage(
+    "https://crbug.com/v8/10628",
+    `
 async function abc() {                    // 0000
  try {                                    // 0050
   return 'abc';                           // 0100
@@ -148,13 +158,14 @@
 }                                         // 0300
 abc();                                    // 0350
 %PerformMicrotaskCheckpoint();            // 0400
-`,
-[{"start":0,"end":449,"count":1},
- {"start":0,"end":301,"count":1}]);
-
-TestCoverage(
-"try/catch/finally statements async",
-`
+    `,
+    [ {"start":0,"end":449,"count":1},
+      {"start":0,"end":301,"count":1} ]
+  );
+
+  await TestCoverage(
+    "try/catch/finally statements async",
+    `
 !async function() {                       // 0000
   try { nop(); } catch (e) { nop(); }     // 0050
   try { nop(); } finally { nop(); }       // 0100
@@ -172,15 +183,16 @@
     nop();                                // 0700
   }                                       // 0750
 }();                                      // 0800
-`,
-[{"start":0,"end":849,"count":1},
-  {"start":1,"end":801,"count":1},
-  {"start":67,"end":87,"count":0},
-  {"start":254,"end":274,"count":0}]
-);
-
-TestCoverage("try/catch/finally statements with early return async",
-`
+    `,
+    [ {"start":0,"end":849,"count":1},
+      {"start":1,"end":801,"count":1},
+      {"start":67,"end":87,"count":0},
+      {"start":254,"end":274,"count":0} ]
+  );
+
+  await TestCoverage(
+    "try/catch/finally statements with early return async",
+    `
 !async function() {                       // 0000
   try { throw 42; } catch (e) { return; } // 0050
   nop();                                  // 0100
@@ -190,12 +202,14 @@
   finally { return; }                     // 0300
   nop();                                  // 0350
 }();                                      // 0400
-`,
-[{"start":0,"end":449,"count":1},
-  {"start":1,"end":151,"count":1},
-  {"start":91,"end":150,"count":0},
-  {"start":201,"end":401,"count":1},
-  {"start":321,"end":400,"count":0}]
-);
+    `,
+    [ {"start":0,"end":449,"count":1},
+      {"start":1,"end":151,"count":1},
+      {"start":91,"end":150,"count":0},
+      {"start":201,"end":401,"count":1},
+      {"start":321,"end":400,"count":0} ]
+  );
+
+  %DebugToggleBlockCoverage(false);
 
-%DebugToggleBlockCoverage(false);
+})();
diff -r -u --color up/v8/test/mjsunit/code-coverage-block-noopt.js nw/v8/test/mjsunit/code-coverage-block-noopt.js
--- up/v8/test/mjsunit/code-coverage-block-noopt.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/code-coverage-block-noopt.js	2023-01-19 16:46:36.796442745 -0500
@@ -4,28 +4,32 @@
 
 // Flags: --allow-natives-syntax --no-always-turbofan --no-stress-flush-code
 // Flags: --no-turbofan
+// Flags: --expose-gc
 // Files: test/mjsunit/code-coverage-utils.js
 
-%DebugToggleBlockCoverage(true);
+(async function () {
 
-TestCoverage(
-"optimized and inlined functions",
-`
+  %DebugToggleBlockCoverage(true);
+
+  await TestCoverage(
+    "optimized and inlined functions",
+    `
 function g() { if (true) nop(); }         // 0000
 function f() { g(); g(); }                // 0050
 %PrepareFunctionForOptimization(f);       // 0100
 f(); f(); %OptimizeFunctionOnNextCall(f); // 0150
 f(); f(); f(); f(); f(); f();             // 0200
-`,
-[{"start":0,"end":249,"count":1},
- {"start":0,"end":33,"count":16},
- {"start":50,"end":76,"count":8}]
-);
-
-// In contrast to the corresponding test in -opt.js, f is not optimized here
-// and therefore reports its invocation count correctly.
-TestCoverage("Partial coverage collection",
-`
+    `,
+    [ {"start":0,"end":249,"count":1},
+      {"start":0,"end":33,"count":16},
+      {"start":50,"end":76,"count":8} ]
+  );
+
+  // In contrast to the corresponding test in -opt.js, f is not optimized here
+  // and therefore reports its invocation count correctly.
+  await TestCoverage(
+    "Partial coverage collection",
+    `
 !function() {                             // 0000
   function f(x) {                         // 0050
     if (x) { nop(); } else { nop(); }     // 0100
@@ -36,10 +40,12 @@
   %DebugCollectCoverage();                // 0350
   f(false);                               // 0400
 }();                                      // 0450
-`,
-[{"start":52,"end":153,"count":1},
- {"start":111,"end":121,"count":0}]
-);
+    `,
+    [ {"start":52,"end":153,"count":1},
+      {"start":111,"end":121,"count":0} ]
+  );
+
 
+  %DebugToggleBlockCoverage(false);
 
-%DebugToggleBlockCoverage(false);
+})();
diff -r -u --color up/v8/test/mjsunit/code-coverage-block-opt.js nw/v8/test/mjsunit/code-coverage-block-opt.js
--- up/v8/test/mjsunit/code-coverage-block-opt.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/code-coverage-block-opt.js	2023-01-19 16:46:36.796442745 -0500
@@ -4,36 +4,40 @@
 
 // Flags: --allow-natives-syntax --no-always-turbofan --turbofan
 // Flags: --no-stress-flush-code --turbo-inlining
+// Flags: --expose-gc
 // Files: test/mjsunit/code-coverage-utils.js
 
-if (isNeverOptimizeLiteMode()) {
-  print("Warning: skipping test that requires optimization in Lite mode.");
-  testRunner.quit(0);
-}
-
-%DebugToggleBlockCoverage(true);
-
-TestCoverage(
-"optimized and inlined functions",
-`
+(async function () {
+
+  if (isNeverOptimizeLiteMode()) {
+    print("Warning: skipping test that requires optimization in Lite mode.");
+    testRunner.quit(0);
+  }
+
+  %DebugToggleBlockCoverage(true);
+
+  await TestCoverage(
+    "optimized and inlined functions",
+    `
 function g() { if (true) nop(); }         // 0000
 function f() { g(); g(); }                // 0050
 %PrepareFunctionForOptimization(f);       // 0100
 f(); f(); %OptimizeFunctionOnNextCall(f); // 0150
 f(); f(); f(); f(); f(); f();             // 0200
-`,
-[{"start":0,"end":249,"count":1},
- {"start":0,"end":33,"count":16},
- {"start":50,"end":76,"count":8}]
-);
-
-// This test is tricky: it requires a non-toplevel, optimized function.
-// After initial collection, counts are cleared. Further invocation_counts
-// are not collected for optimized functions, and on the next coverage
-// collection we and up with an uncovered function with an uncovered parent
-// but with non-trivial block coverage.
-TestCoverage("Partial coverage collection",
-`
+    `,
+    [ {"start":0,"end":249,"count":1},
+      {"start":0,"end":33,"count":16},
+      {"start":50,"end":76,"count":8} ]
+  );
+
+  // This test is tricky: it requires a non-toplevel, optimized function.
+  // After initial collection, counts are cleared. Further invocation_counts
+  // are not collected for optimized functions, and on the next coverage
+  // collection we and up with an uncovered function with an uncovered parent
+  // but with non-trivial block coverage.
+  await TestCoverage(
+    "Partial coverage collection",
+    `
 !function() {                             // 0000
   function f(x) {                         // 0050
     if (x) { nop(); } else { nop(); }     // 0100
@@ -44,9 +48,11 @@
   %DebugCollectCoverage();                // 0350
   f(false);                               // 0400
 }();                                      // 0450
-`,
-[{"start":52,"end":153,"count":1},
- {"start":111,"end":121,"count":0}]
-);
+    `,
+    [ {"start":52,"end":153,"count":1},
+      {"start":111,"end":121,"count":0} ]
+  );
+
+  %DebugToggleBlockCoverage(false);
 
-%DebugToggleBlockCoverage(false);
+})();
diff -r -u --color up/v8/test/mjsunit/code-coverage-block.js nw/v8/test/mjsunit/code-coverage-block.js
--- up/v8/test/mjsunit/code-coverage-block.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/code-coverage-block.js	2023-01-19 16:46:36.796442745 -0500
@@ -3,27 +3,34 @@
 // found in the LICENSE file.
 
 // Flags: --allow-natives-syntax --no-always-turbofan --no-stress-flush-code
+// Flags: --expose-gc
 // Files: test/mjsunit/code-coverage-utils.js
 
-%DebugToggleBlockCoverage(true);
+(async function () {
 
-TestCoverage(
-"call an IIFE",
-`
+  %DebugToggleBlockCoverage(true);
+
+  await TestCoverage(
+    "call an IIFE",
+    `
 (function f() {})();
-`,
-[{"start":0,"end":20,"count":1},{"start":1,"end":16,"count":1}]
-);
-
-TestCoverage(
-"call locally allocated function",
-`let f = () => 1; f();`,
-[{"start":0,"end":21,"count":1},{"start":8,"end":15,"count":1}]
-);
-
-TestCoverage(
-"if statements",
-`
+    `,
+    [ {"start":0,"end":20,"count":1},
+      {"start":1,"end":16,"count":1} ]
+  );
+
+  await TestCoverage(
+    "call locally allocated function",
+    `
+let f = () => 1; f();
+    `,
+    [ {"start":0,"end":21,"count":1},
+      {"start":8,"end":15,"count":1} ]
+  );
+
+  await TestCoverage(
+    "if statements",
+    `
 function g() {}                           // 0000
 function f(x) {                           // 0050
   if (x == 42) {                          // 0100
@@ -43,23 +50,23 @@
 } else {                                  // 0800
   const bar = 'foo';                      // 0850
 }                                         // 0900
-`,
-[{"start":0,"end":949,"count":1},
- {"start":801,"end":901,"count":0},
- {"start":0,"end":15,"count":11},
- {"start":50,"end":551,"count":2},
- {"start":115,"end":203,"count":1},
- {"start":167,"end":171,"count":0},
- {"start":265,"end":287,"count":1},
- {"start":315,"end":329,"count":1},
- {"start":363,"end":367,"count":0},
- {"start":413,"end":417,"count":0},
- {"start":466,"end":476,"count":0}]
-);
-
-TestCoverage(
-"if statement (early return)",
-`
+    `,
+    [ {"start":0,"end":949,"count":1},
+      {"start":801,"end":901,"count":0},
+      {"start":0,"end":15,"count":11},
+      {"start":50,"end":551,"count":2},
+      {"start":115,"end":203,"count":1},
+      {"start":167,"end":171,"count":0},
+      {"start":265,"end":287,"count":1},
+      {"start":315,"end":329,"count":1},
+      {"start":363,"end":367,"count":0},
+      {"start":413,"end":417,"count":0},
+      {"start":466,"end":476,"count":0} ]
+  );
+
+  await TestCoverage(
+    "if statement (early return)",
+    `
 !function() {                             // 0000
   if (true) {                             // 0050
     nop();                                // 0100
@@ -68,29 +75,29 @@
   }                                       // 0250
   nop();                                  // 0300
 }()                                       // 0350
-`,
-[{"start":0,"end":399,"count":1},
- {"start":1,"end":351,"count":1},
- {"start":161,"end":350,"count":0}]
-);
-
-TestCoverage(
-"if statement (no semi-colon)",
-`
+    `,
+    [ {"start":0,"end":399,"count":1},
+      {"start":1,"end":351,"count":1},
+      {"start":161,"end":350,"count":0} ]
+  );
+
+  await TestCoverage(
+    "if statement (no semi-colon)",
+    `
 !function() {                             // 0000
   if (true) nop()                         // 0050
   if (true) nop(); else nop()             // 0100
   nop();                                  // 0150
 }()                                       // 0200
-`,
-[{"start":0,"end":249,"count":1},
- {"start":1,"end":201,"count":1},
- {"start":118,"end":129,"count":0}]
-);
-
-TestCoverage(
-"for statements",
-`
+    `,
+    [ {"start":0,"end":249,"count":1},
+      {"start":1,"end":201,"count":1},
+      {"start":118,"end":129,"count":0} ]
+  );
+
+  await TestCoverage(
+    "for statements",
+    `
 function g() {}                           // 0000
 !function() {                             // 0050
   for (var i = 0; i < 12; i++) g();       // 0100
@@ -103,21 +110,21 @@
     if (i % 3 == 0) g(); else g();        // 0450
   }                                       // 0500
 }();                                      // 0550
-`,
-[{"start":0,"end":599,"count":1},
- {"start":0,"end":15,"count":36},
- {"start":51,"end":551,"count":1},
- {"start":131,"end":135,"count":12},
- {"start":181,"end":253,"count":12},
- {"start":330,"end":334,"count":0},
- {"start":431,"end":503,"count":12},
- {"start":470,"end":474,"count":4},
- {"start":474,"end":484,"count":8}]
-);
-
-TestCoverage(
-"for statements pt. 2",
-`
+    `,
+    [ {"start":0,"end":599,"count":1},
+      {"start":0,"end":15,"count":36},
+      {"start":51,"end":551,"count":1},
+      {"start":131,"end":135,"count":12},
+      {"start":181,"end":253,"count":12},
+      {"start":330,"end":334,"count":0},
+      {"start":431,"end":503,"count":12},
+      {"start":470,"end":474,"count":4},
+      {"start":474,"end":484,"count":8} ]
+  );
+
+  await TestCoverage(
+    "for statements pt. 2",
+    `
 function g() {}                           // 0000
 !function() {                             // 0050
   let j = 0;                              // 0100
@@ -126,34 +133,34 @@
   for (j = 0; j < 12; j++) g();           // 0250
   for (;;) break;                         // 0300
 }();                                      // 0350
-`,
-[{"start":0,"end":399,"count":1},
- {"start":0,"end":15,"count":36},
- {"start":51,"end":351,"count":1},
- {"start":181,"end":185,"count":12},
- {"start":233,"end":237,"count":12},
- {"start":277,"end":281,"count":12}]
-);
-
-TestCoverage(
-"for statements (no semicolon)",
-`
+    `,
+    [ {"start":0,"end":399,"count":1},
+      {"start":0,"end":15,"count":36},
+      {"start":51,"end":351,"count":1},
+      {"start":181,"end":185,"count":12},
+      {"start":233,"end":237,"count":12},
+      {"start":277,"end":281,"count":12} ]
+  );
+
+  await TestCoverage(
+    "for statements (no semicolon)",
+    `
 function g() {}                           // 0000
 !function() {                             // 0050
   for (let i = 0; i < 12; i++) g()        // 0100
   for (let i = 0; i < 12; i++) break      // 0150
   for (let i = 0; i < 12; i++) break; g() // 0200
 }();                                      // 0250
-`,
-[{"start":0,"end":299,"count":1},
- {"start":0,"end":15,"count":13},
- {"start":51,"end":251,"count":1},
- {"start":131,"end":134,"count":12}]
-);
-
-TestCoverage(
-"for statement (early return)",
-`
+    `,
+    [ {"start":0,"end":299,"count":1},
+      {"start":0,"end":15,"count":13},
+      {"start":51,"end":251,"count":1},
+      {"start":131,"end":134,"count":12} ]
+  );
+
+  await TestCoverage(
+    "for statement (early return)",
+    `
 !function() {                             // 0000
   for (var i = 0; i < 10; i++) {          // 0050
     nop();                                // 0100
@@ -174,18 +181,18 @@
   }                                       // 0850
   nop();                                  // 0900
 }()                                       // 0950
-`,
-[{"start":0,"end":999,"count":1},
- {"start":1,"end":951,"count":1},
- {"start":81,"end":253,"count":10},
- {"start":163,"end":253,"count":0},
- {"start":460,"end":553,"count":0},
- {"start":761,"end":950,"count":0}]
-);
-
-TestCoverage(
-"for-of and for-in statements",
-`
+    `,
+    [ {"start":0,"end":999,"count":1},
+      {"start":1,"end":951,"count":1},
+      {"start":81,"end":253,"count":10},
+      {"start":163,"end":253,"count":0},
+      {"start":460,"end":553,"count":0},
+      {"start":761,"end":950,"count":0} ]
+  );
+
+  await TestCoverage(
+    "for-of and for-in statements",
+    `
 !function() {                             // 0000
   var i;                                  // 0050
   for (i of [0,1,2,3]) { nop(); }         // 0100
@@ -195,19 +202,19 @@
   var xs = [{a:0, b:1}, {a:1,b:0}];       // 0300
   for (var {a: x, b: y} of xs) { nop(); } // 0350
 }();                                      // 0400
-`,
-[{"start":0,"end":449,"count":1},
- {"start":1,"end":401,"count":1},
- {"start":123,"end":133,"count":4},
- {"start":177,"end":187,"count":4},
- {"start":223,"end":233,"count":4},
- {"start":277,"end":287,"count":4},
- {"start":381,"end":391,"count":2}]
-);
-
-TestCoverage(
-"while and do-while statements",
-`
+    `,
+    [ {"start":0,"end":449,"count":1},
+      {"start":1,"end":401,"count":1},
+      {"start":123,"end":133,"count":4},
+      {"start":177,"end":187,"count":4},
+      {"start":223,"end":233,"count":4},
+      {"start":277,"end":287,"count":4},
+      {"start":381,"end":391,"count":2} ]
+  );
+
+  await TestCoverage(
+    "while and do-while statements",
+    `
 function g() {}                           // 0000
 !function() {                             // 0050
   var i;                                  // 0100
@@ -222,20 +229,20 @@
   i = 0; do { g(); } while (false);       // 0550
   i = 0; do { break; } while (true);      // 0600
 }();                                      // 0650
-`,
-[{"start":0,"end":699,"count":1},
- {"start":0,"end":15,"count":25},
- {"start":51,"end":651,"count":1},
- {"start":174,"end":178,"count":12},
- {"start":224,"end":237,"count":12},
- {"start":273,"end":277,"count":0},
- {"start":412,"end":416,"count":12},
- {"start":462,"end":475,"count":12}]
-);
-
-TestCoverage(
-"while statement (early return)",
-`
+    `,
+    [ {"start":0,"end":699,"count":1},
+      {"start":0,"end":15,"count":25},
+      {"start":51,"end":651,"count":1},
+      {"start":174,"end":178,"count":12},
+      {"start":224,"end":237,"count":12},
+      {"start":273,"end":277,"count":0},
+      {"start":412,"end":416,"count":12},
+      {"start":462,"end":475,"count":12} ]
+  );
+
+  await TestCoverage(
+    "while statement (early return)",
+    `
 !function() {                             // 0000
   let i = 0;                              // 0050
   while (i < 10) {                        // 0100
@@ -257,18 +264,18 @@
   }                                       // 0900
   nop();                                  // 0950
 }()                                       // 1000
-`,
-[{"start":0,"end":1049,"count":1},
- {"start":1,"end":1001,"count":1},
- {"start":117,"end":303,"count":10},
- {"start":213,"end":303,"count":0},
- {"start":510,"end":603,"count":0},
- {"start":811,"end":1000,"count":0}]
-);
-
-TestCoverage(
-"do-while statement (early return)",
-`
+    `,
+    [ {"start":0,"end":1049,"count":1},
+      {"start":1,"end":1001,"count":1},
+      {"start":117,"end":303,"count":10},
+      {"start":213,"end":303,"count":0},
+      {"start":510,"end":603,"count":0},
+      {"start":811,"end":1000,"count":0} ]
+  );
+
+  await TestCoverage(
+    "do-while statement (early return)",
+    `
 !function() {                             // 0000
   let i = 0;                              // 0050
   do {                                    // 0100
@@ -290,32 +297,32 @@
   } while (true);                         // 0900
   nop();                                  // 0950
 }()                                       // 1000
-`,
-[{"start":0,"end":1049,"count":1},
- {"start":1,"end":1001,"count":1},
- {"start":105,"end":303,"count":10},
- {"start":213,"end":303,"count":0},
- {"start":510,"end":603,"count":0},
- {"start":811,"end":1000,"count":0}]
-);
-
-TestCoverage(
-"return statements",
-`
+    `,
+    [ {"start":0,"end":1049,"count":1},
+      {"start":1,"end":1001,"count":1},
+      {"start":105,"end":303,"count":10},
+      {"start":213,"end":303,"count":0},
+      {"start":510,"end":603,"count":0},
+      {"start":811,"end":1000,"count":0} ]
+  );
+
+  await TestCoverage(
+    "return statements",
+    `
 !function() { nop(); return; nop(); }();  // 0000
 !function() { nop(); return 42;           // 0050
               nop(); }();                 // 0100
-`,
-[{"start":0,"end":149,"count":1},
- {"start":1,"end":37,"count":1},
- {"start":28,"end":36,"count":0},
- {"start":51,"end":122,"count":1},
- {"start":81,"end":121,"count":0}]
-);
-
-TestCoverage(
-"try/catch/finally statements",
-`
+    `,
+    [ {"start":0,"end":149,"count":1},
+      {"start":1,"end":37,"count":1},
+      {"start":28,"end":36,"count":0},
+      {"start":51,"end":122,"count":1},
+      {"start":81,"end":121,"count":0} ]
+  );
+
+  await TestCoverage(
+    "try/catch/finally statements",
+    `
 !function() {                             // 0000
   try { nop(); } catch (e) { nop(); }     // 0050
   try { nop(); } finally { nop(); }       // 0100
@@ -333,15 +340,16 @@
     nop();                                // 0700
   }                                       // 0750
 }();                                      // 0800
-`,
-[{"start":0,"end":849,"count":1},
- {"start":1,"end":801,"count":1},
- {"start":67,"end":87,"count":0},
- {"start":254,"end":274,"count":0}]
-);
-
-TestCoverage("try/catch/finally statements with early return",
-`
+    `,
+    [ {"start":0,"end":849,"count":1},
+      {"start":1,"end":801,"count":1},
+      {"start":67,"end":87,"count":0},
+      {"start":254,"end":274,"count":0} ]
+  );
+
+  await TestCoverage(
+    "try/catch/finally statements with early return",
+    `
 !function() {                             // 0000
   try { throw 42; } catch (e) { return; } // 0050
   nop();                                  // 0100
@@ -351,17 +359,17 @@
   finally { return; }                     // 0300
   nop();                                  // 0350
 }();                                      // 0400
-`,
-[{"start":0,"end":449,"count":1},
- {"start":1,"end":151,"count":1},
- {"start":91,"end":150,"count":0},
- {"start":201,"end":401,"count":1},
- {"start":321,"end":400,"count":0}]
-);
-
-TestCoverage(
-"early return in blocks",
-`
+    `,
+    [ {"start":0,"end":449,"count":1},
+      {"start":1,"end":151,"count":1},
+      {"start":91,"end":150,"count":0},
+      {"start":201,"end":401,"count":1},
+      {"start":321,"end":400,"count":0} ]
+  );
+
+  await TestCoverage(
+    "early return in blocks",
+    `
 !function() {                             // 0000
   try { throw 42; } catch (e) { return; } // 0050
   nop();                                  // 0100
@@ -384,22 +392,22 @@
   }                                       // 0950
   nop();                                  // 1000
 }();                                      // 1050
-`,
-[{"start":0,"end":1099,"count":1},
- {"start":1,"end":151,"count":1},
- {"start":91,"end":150,"count":0},
- {"start":201,"end":351,"count":1},
- {"start":286,"end":350,"count":0},
- {"start":401,"end":701,"count":1},
- {"start":603,"end":700,"count":0},
- {"start":561,"end":568,"count":0},
- {"start":751,"end":1051,"count":1},
- {"start":861,"end":1050,"count":0}]
-);
-
-TestCoverage(
-"switch statements",
-`
+    `,
+    [ {"start":0,"end":1099,"count":1},
+      {"start":1,"end":151,"count":1},
+      {"start":91,"end":150,"count":0},
+      {"start":201,"end":351,"count":1},
+      {"start":286,"end":350,"count":0},
+      {"start":401,"end":701,"count":1},
+      {"start":603,"end":700,"count":0},
+      {"start":561,"end":568,"count":0},
+      {"start":751,"end":1051,"count":1},
+      {"start":861,"end":1050,"count":0} ]
+  );
+
+  await TestCoverage(
+    "switch statements",
+    `
 !function() {                             // 0000
   var x = 42;                             // 0050
   switch (x) {                            // 0100
@@ -408,16 +416,16 @@
     default: nop(); break;                // 0250
   }                                       // 0300
 }();                                      // 0350
-`,
-[{"start":0,"end":399,"count":1},
- {"start":1,"end":351,"count":1},
- {"start":154,"end":176,"count":0},
- {"start":254,"end":276,"count":0}]
-);
-
-TestCoverage(
-"labeled break statements",
-`
+    `,
+    [ {"start":0,"end":399,"count":1},
+      {"start":1,"end":351,"count":1},
+      {"start":154,"end":176,"count":0},
+      {"start":254,"end":276,"count":0} ]
+  );
+
+  await TestCoverage(
+    "labeled break statements",
+    `
 !function() {                             // 0000
   var x = 42;                             // 0050
   l0: switch (x) {                        // 0100
@@ -438,16 +446,16 @@
   l4: { break l4; }                       // 0850
   l5: for (;;) for (;;) break l5;         // 0900
 }();                                      // 0950
-`,
-[{"start":0,"end":999,"count":1},
- {"start":1,"end":951,"count":1},
- {"start":152,"end":168,"count":0},
- {"start":287,"end":310,"count":0}]
-);
-
-TestCoverage(
-"labeled continue statements",
-`
+    `,
+    [ {"start":0,"end":999,"count":1},
+      {"start":1,"end":951,"count":1},
+      {"start":152,"end":168,"count":0},
+      {"start":287,"end":310,"count":0} ]
+  );
+
+  await TestCoverage(
+    "labeled continue statements",
+    `
 !function() {                             // 0000
   l0: for (var i0 = 0; i0 < 2; i0++) {    // 0050
     for (;;) continue l0;                 // 0100
@@ -463,17 +471,17 @@
     do { continue l2; } while (true);     // 0600
   } while (i2 < 2);                       // 0650
 }();                                      // 0700
-`,
-[{"start":0,"end":749,"count":1},
- {"start":1,"end":701,"count":1},
- {"start":87,"end":153,"count":2},
- {"start":271,"end":403,"count":2},
- {"start":509,"end":653,"count":2}]
-);
-
-TestCoverage(
-"conditional expressions",
-`
+    `,
+    [ {"start":0,"end":749,"count":1},
+      {"start":1,"end":701,"count":1},
+      {"start":87,"end":153,"count":2},
+      {"start":271,"end":403,"count":2},
+      {"start":509,"end":653,"count":2} ]
+  );
+
+  await TestCoverage(
+    "conditional expressions",
+    `
 var TRUE = true;                          // 0000
 var FALSE = false;                        // 0050
 !function() {                             // 0100
@@ -491,39 +499,39 @@
   FALSE ? nop() : TRUE ? nop()            // 0700
                        : nop();           // 0750
 }();                                      // 0800
-`,
-[{"start":0,"end":849,"count":1},
- {"start":101,"end":801,"count":1},
- {"start":165,"end":172,"count":0},
- {"start":215,"end":222,"count":0},
- {"start":258,"end":265,"count":0},
- {"start":308,"end":372,"count":0},
- {"start":465,"end":472,"count":0},
- {"start":557,"end":564,"count":0},
- {"start":615,"end":680,"count":0},
- {"start":708,"end":715,"count":0},
- {"start":773,"end":780,"count":0}]
-);
-
-TestCoverage(
-"yield expressions",
-`
+    `,
+    [ {"start":0,"end":849,"count":1},
+      {"start":101,"end":801,"count":1},
+      {"start":165,"end":172,"count":0},
+      {"start":215,"end":222,"count":0},
+      {"start":258,"end":265,"count":0},
+      {"start":308,"end":372,"count":0},
+      {"start":465,"end":472,"count":0},
+      {"start":557,"end":564,"count":0},
+      {"start":615,"end":680,"count":0},
+      {"start":708,"end":715,"count":0},
+      {"start":773,"end":780,"count":0} ]
+  );
+
+  await TestCoverage(
+    "yield expressions",
+    `
 const it = function*() {                  // 0000
   yield nop();                            // 0050
   yield nop() ? nop() : nop()             // 0100
   return nop();                           // 0150
 }();                                      // 0200
 it.next(); it.next();                     // 0250
-`,
-[{"start":0,"end":299,"count":1},
- {"start":11,"end":201,"count":1},
- {"start":114,"end":121,"count":0},
- {"start":129,"end":200,"count":0}]
-);
-
-TestCoverage(
-"yield expressions twice",
-`
+    `,
+    [ {"start":0,"end":299,"count":1},
+      {"start":11,"end":201,"count":1},
+      {"start":114,"end":121,"count":0},
+      {"start":129,"end":200,"count":0} ]
+  );
+
+  await TestCoverage(
+    "yield expressions twice",
+    `
 function* gen() {                         // 0000
   yield nop();                            // 0050
   yield nop() ? nop() : nop()             // 0100
@@ -531,16 +539,16 @@
 };                                        // 0200
 {const it = gen(); it.next(); it.next();} // 0250
 {const it = gen(); it.next(); it.next();} // 0300
-`,
-[{"start":0,"end":349,"count":1},
- {"start":0,"end":201,"count":2},
- {"start":114,"end":121,"count":0},
- {"start":129,"end":200,"count":0}]
-);
-
-TestCoverage(
-"yield expressions (.return and .throw)",
-`
+    `,
+    [ {"start":0,"end":349,"count":1},
+      {"start":0,"end":201,"count":2},
+      {"start":114,"end":121,"count":0},
+      {"start":129,"end":200,"count":0} ]
+  );
+
+  await TestCoverage(
+    "yield expressions (.return and .throw)",
+    `
 const it0 = function*() {                 // 0000
   yield 1; yield 2; yield 3;              // 0050
 }();                                      // 0100
@@ -551,16 +559,17 @@
   }();                                    // 0350
   it1.next(); it1.throw();                // 0400
 } catch (e) {}                            // 0450
-`,
-[{"start":0,"end":499,"count":1},
- {"start":12,"end":101,"count":1},
- {"start":60,"end":100,"count":0},
- {"start":264,"end":353,"count":1},
- {"start":312,"end":352,"count":0}]
-);
-
-TestCoverage("yield expressions (.return and try/catch/finally)",
-`
+    `,
+    [ {"start":0,"end":499,"count":1},
+      {"start":12,"end":101,"count":1},
+      {"start":60,"end":100,"count":0},
+      {"start":264,"end":353,"count":1},
+      {"start":312,"end":352,"count":0} ]
+  );
+
+  await TestCoverage(
+    "yield expressions (.return and try/catch/finally)",
+    `
 const it = function*() {                  // 0000
   try {                                   // 0050
     yield 1; yield 2; yield 3;            // 0100
@@ -570,15 +579,16 @@
   yield 4;                                // 0300
 }();                                      // 0350
 it.next(); it.return();                   // 0450
-`,
-[{"start":0,"end":449,"count":1},
- {"start":11,"end":351,"count":1},
- {"start":112,"end":254,"count":0},
- {"start":272,"end":350,"count":0}]
-);
-
-TestCoverage("yield expressions (.throw and try/catch/finally)",
-`
+    `,
+    [ {"start":0,"end":449,"count":1},
+      {"start":11,"end":351,"count":1},
+      {"start":112,"end":254,"count":0},
+      {"start":272,"end":350,"count":0} ]
+  );
+
+  await TestCoverage(
+    "yield expressions (.throw and try/catch/finally)",
+    `
 const it = function*() {                  // 0000
   try {                                   // 0050
     yield 1; yield 2; yield 3;            // 0100
@@ -588,16 +598,16 @@
   yield 4;                                // 0300
 }();                                      // 0350
 it.next(); it.throw(42);                  // 0550
-`,
-[{"start":0,"end":449,"count":1},
- {"start":11,"end":351,"count":1},
- {"start":112,"end":154,"count":0},
- {"start":310,"end":350,"count":0}]
-);
-
-TestCoverage(
-"yield* expressions",
-`
+    `,
+    [ {"start":0,"end":449,"count":1},
+      {"start":11,"end":351,"count":1},
+      {"start":112,"end":154,"count":0},
+      {"start":310,"end":350,"count":0} ]
+  );
+
+  await TestCoverage(
+    "yield* expressions",
+    `
 const it = function*() {                  // 0000
   yield* gen();                           // 0050
   yield* nop() ? gen() : gen()            // 0100
@@ -605,16 +615,16 @@
 }();                                      // 0200
 it.next(); it.next(); it.next();          // 0250
 it.next(); it.next(); it.next();          // 0300
-`,
-[{"start":0,"end":349,"count":1},
- {"start":11,"end":201,"count":1},
- {"start":115,"end":122,"count":0},
- {"start":130,"end":200,"count":0}]
-);
-
-TestCoverage(
-"yield* expressions (.return and .throw)",
-`
+    `,
+    [ {"start":0,"end":349,"count":1},
+      {"start":11,"end":201,"count":1},
+      {"start":115,"end":122,"count":0},
+      {"start":130,"end":200,"count":0} ]
+  );
+
+  await TestCoverage(
+    "yield* expressions (.return and .throw)",
+    `
 const it0 = function*() {                 // 0000
   yield* gen(); yield* gen(); yield 3;    // 0050
 }();                                      // 0100
@@ -625,46 +635,47 @@
   }();                                    // 0350
   it1.next(); it1.throw();                // 0400
 } catch (e) {}                            // 0450
-`,
-[{"start":0,"end":499,"count":1},
- {"start":12,"end":101,"count":1},
- {"start":65,"end":100,"count":0},
- {"start":264,"end":353,"count":1},
- {"start":317,"end":352,"count":0}]
-);
-
-TestCoverage(
-"LogicalOrExpression assignment",
-`
+    `,
+    [ {"start":0,"end":499,"count":1},
+      {"start":12,"end":101,"count":1},
+      {"start":65,"end":100,"count":0},
+      {"start":264,"end":353,"count":1},
+      {"start":317,"end":352,"count":0} ]
+  );
+
+  await TestCoverage(
+    "LogicalOrExpression assignment",
+    `
 const a = true || 99                      // 0000
 function b () {                           // 0050
   const b = a || 2                        // 0100
 }                                         // 0150
 b()                                       // 0200
 b()                                       // 0250
-`,
-[{"start":0,"end":299,"count":1},
- {"start":15,"end":20,"count":0},
- {"start":50,"end":151,"count":2},
- {"start":114,"end":118,"count":0}]
-);
-
-TestCoverage(
-"LogicalOrExpression IsTest()",
-`
+    `,
+    [ {"start":0,"end":299,"count":1},
+      {"start":15,"end":20,"count":0},
+      {"start":50,"end":151,"count":2},
+      {"start":114,"end":118,"count":0} ]
+  );
+
+  await TestCoverage(
+    "LogicalOrExpression IsTest()",
+    `
 true || false                             // 0000
 const a = 99                              // 0050
 a || 50                                   // 0100
 const b = false                           // 0150
 if (b || true) {}                         // 0200
-`,
-[{"start":0,"end":249,"count":1},
- {"start":5,"end":13,"count":0},
- {"start":102,"end":107,"count":0}]);
-
-TestCoverage(
-"LogicalAndExpression assignment",
-`
+    `,
+    [ {"start":0,"end":249,"count":1},
+      {"start":5,"end":13,"count":0},
+      {"start":102,"end":107,"count":0} ]
+  );
+
+  await TestCoverage(
+    "LogicalAndExpression assignment",
+    `
 const a = false && 99                     // 0000
 function b () {                           // 0050
   const b = a && 2                        // 0100
@@ -672,30 +683,31 @@
 b()                                       // 0200
 b()                                       // 0250
 const c = true && 50                      // 0300
-`,
-[{"start":0,"end":349,"count":1},
- {"start":16,"end":21,"count":0},
- {"start":50,"end":151,"count":2},
- {"start":114,"end":118,"count":0}]
-);
-
-TestCoverage(
-"LogicalAndExpression IsTest()",
-`
+    `,
+    [ {"start":0,"end":349,"count":1},
+      {"start":16,"end":21,"count":0},
+      {"start":50,"end":151,"count":2},
+      {"start":114,"end":118,"count":0} ]
+  );
+
+  await TestCoverage(
+    "LogicalAndExpression IsTest()",
+    `
 false && true                             // 0000
 const a = 0                               // 0050
 a && 50                                   // 0100
 const b = true                            // 0150
 if (b && true) {}                         // 0200
 true && true                              // 0250
-`,
-[{"start":0,"end":299,"count":1},
- {"start":6,"end":13,"count":0},
- {"start":102,"end":107,"count":0}]);
-
-TestCoverage(
-"NaryLogicalOr assignment",
-`
+    `,
+    [ {"start":0,"end":299,"count":1},
+      {"start":6,"end":13,"count":0},
+      {"start":102,"end":107,"count":0} ]
+  );
+
+  await TestCoverage(
+    "NaryLogicalOr assignment",
+    `
 const a = true                            // 0000
 const b = false                           // 0050
 const c = false || false || 99            // 0100
@@ -705,20 +717,21 @@
 const g = b || a || 99                    // 0300
 const h = a || a || 99                    // 0350
 const i = a || (b || c) || d              // 0400
-`,
-[{"start":0,"end":449,"count":1},
- {"start":174,"end":179,"count":0},
- {"start":215,"end":222,"count":0},
- {"start":223,"end":228,"count":0},
- {"start":317,"end":322,"count":0},
- {"start":362,"end":366,"count":0},
- {"start":367,"end":372,"count":0},
- {"start":412,"end":423,"count":0},
- {"start":424,"end":428,"count":0}]);
-
-TestCoverage(
-"NaryLogicalOr IsTest()",
-`
+    `,
+    [ {"start":0,"end":449,"count":1},
+      {"start":174,"end":179,"count":0},
+      {"start":215,"end":222,"count":0},
+      {"start":223,"end":228,"count":0},
+      {"start":317,"end":322,"count":0},
+      {"start":362,"end":366,"count":0},
+      {"start":367,"end":372,"count":0},
+      {"start":412,"end":423,"count":0},
+      {"start":424,"end":428,"count":0} ]
+  );
+
+  await TestCoverage(
+    "NaryLogicalOr IsTest()",
+    `
 const a = true                            // 0000
 const b = false                           // 0050
 false || false || 99                      // 0100
@@ -727,18 +740,19 @@
 b || b || 99                              // 0250
 b || a || 99                              // 0300
 a || a || 99                              // 0350
-`,
-[{"start":0,"end":399,"count":1},
- {"start":164,"end":169,"count":0},
- {"start":205,"end":212,"count":0},
- {"start":213,"end":218,"count":0},
- {"start":307,"end":312,"count":0},
- {"start":352,"end":356,"count":0},
- {"start":357,"end":362,"count":0}]);
-
-TestCoverage(
-"NaryLogicalAnd assignment",
-`
+    `,
+    [ {"start":0,"end":399,"count":1},
+      {"start":164,"end":169,"count":0},
+      {"start":205,"end":212,"count":0},
+      {"start":213,"end":218,"count":0},
+      {"start":307,"end":312,"count":0},
+      {"start":352,"end":356,"count":0},
+      {"start":357,"end":362,"count":0} ]
+  );
+
+  await TestCoverage(
+    "NaryLogicalAnd assignment",
+    `
 const a = true                            // 0000
 const b = false                           // 0050
 const c = false && false && 99            // 0100
@@ -746,18 +760,18 @@
 const e = true && true && 99              // 0200
 const f = true && false || true           // 0250
 const g = true || false && true           // 0300
-`,
-[{"start":0,"end":349,"count":1},
- {"start":116,"end":124,"count":0},
- {"start":125,"end":130,"count":0},
- {"start":166,"end":173,"count":0},
- {"start":174,"end":179,"count":0},
- {"start":315,"end":331,"count":0}
-]);
-
-TestCoverage(
-"NaryLogicalAnd IsTest()",
-`
+    `,
+    [ {"start":0,"end":349,"count":1},
+      {"start":116,"end":124,"count":0},
+      {"start":125,"end":130,"count":0},
+      {"start":166,"end":173,"count":0},
+      {"start":174,"end":179,"count":0},
+      {"start":315,"end":331,"count":0}
+  ]);
+
+  await TestCoverage(
+    "NaryLogicalAnd IsTest()",
+    `
 const a = true                            // 0000
 const b = false                           // 0050
 false && false && 99                      // 0100
@@ -766,19 +780,20 @@
 true && false || true                     // 0250
 true || false && true                     // 0300
 false || false || 99 || 55                // 0350
-`,
-[{"start":0,"end":399,"count":1},
- {"start":106,"end":114,"count":0},
- {"start":115,"end":120,"count":0},
- {"start":156,"end":163,"count":0},
- {"start":164,"end":169,"count":0},
- {"start":305,"end":321,"count":0},
- {"start":371,"end":376,"count":0}]);
-
-// see regression: https://bugs.chromium.org/p/chromium/issues/detail?id=785778
-TestCoverage(
-"logical expressions + conditional expressions",
-`
+    `,
+    [ {"start":0,"end":399,"count":1},
+      {"start":106,"end":114,"count":0},
+      {"start":115,"end":120,"count":0},
+      {"start":156,"end":163,"count":0},
+      {"start":164,"end":169,"count":0},
+      {"start":305,"end":321,"count":0},
+      {"start":371,"end":376,"count":0} ]
+  );
+
+  // see regression: https://crbug.com/785778
+  await TestCoverage(
+    "logical expressions + conditional expressions",
+    `
 const a = true                            // 0000
 const b = 99                              // 0050
 const c = false                           // 0100
@@ -788,18 +803,18 @@
 const g = c || d ? 'left' : 'right'       // 0300
 const h = a && b && (b ? 'left' : 'right')// 0350
 const i = d || c || (c ? 'left' : 'right')// 0400
-`,
-[{"start":0,"end":449,"count":1},
- {"start":227,"end":236,"count":0},
- {"start":262,"end":287,"count":0},
- {"start":317,"end":325,"count":0},
- {"start":382,"end":391,"count":0},
- {"start":423,"end":431,"count":0}
-]);
-
-TestCoverage(
-"https://crbug.com/827530",
-`
+    `,
+    [ {"start":0,"end":449,"count":1},
+      {"start":227,"end":236,"count":0},
+      {"start":262,"end":287,"count":0},
+      {"start":317,"end":325,"count":0},
+      {"start":382,"end":391,"count":0},
+      {"start":423,"end":431,"count":0} ]
+  );
+
+  await TestCoverage(
+    "https://crbug.com/827530",
+    `
 Util = {};                                // 0000
 Util.escape = function UtilEscape(str) {  // 0050
   if (!str) {                             // 0100
@@ -809,15 +824,15 @@
   }                                       // 0300
 };                                        // 0350
 Util.escape("foo.bar");                   // 0400
-`,
-[{"start":0,"end":449,"count":1},
- {"start":64,"end":351,"count":1},
- {"start":112,"end":203,"count":0}]
-);
-
-TestCoverage(
-"https://crbug.com/v8/8237",
-`
+    `,
+    [ {"start":0,"end":449,"count":1},
+      {"start":64,"end":351,"count":1},
+      {"start":112,"end":203,"count":0} ]
+  );
+
+  await TestCoverage(
+    "https://crbug.com/v8/8237",
+    `
 !function() {                             // 0000
   if (true)                               // 0050
     while (false) return; else nop();     // 0100
@@ -839,27 +854,27 @@
   if(true){if(false){return}}else         // 0900
     if(nop()){}                           // 0950
 }();                                      // 1000
-`,
-[{"start":0,"end":1049,"count":1},
- {"start":1,"end":151,"count":1},
- {"start":118,"end":137,"count":0},
- {"start":201,"end":351,"count":1},
- {"start":279,"end":318,"count":0},
- {"start":401,"end":525,"count":1},
- {"start":475,"end":486,"count":0},
- {"start":503,"end":523,"count":0},
- {"start":551,"end":651,"count":1},
- {"start":622,"end":639,"count":0},
- {"start":701,"end":801,"count":1},
- {"start":774,"end":791,"count":0},
- {"start":851,"end":1001,"count":1},
- {"start":920,"end":928,"count":0},
- {"start":929,"end":965,"count":0}]
-);
-
-TestCoverage(
-"terminal break statement",
-`
+    `,
+    [ {"start":0,"end":1049,"count":1},
+      {"start":1,"end":151,"count":1},
+      {"start":118,"end":137,"count":0},
+      {"start":201,"end":351,"count":1},
+      {"start":279,"end":318,"count":0},
+      {"start":401,"end":525,"count":1},
+      {"start":475,"end":486,"count":0},
+      {"start":503,"end":523,"count":0},
+      {"start":551,"end":651,"count":1},
+      {"start":622,"end":639,"count":0},
+      {"start":701,"end":801,"count":1},
+      {"start":774,"end":791,"count":0},
+      {"start":851,"end":1001,"count":1},
+      {"start":920,"end":928,"count":0},
+      {"start":929,"end":965,"count":0} ]
+  );
+
+  await TestCoverage(
+    "terminal break statement",
+    `
 while (true) {                            // 0000
   const b = false                         // 0050
   break                                   // 0100
@@ -871,15 +886,15 @@
   }                                       // 0400
   stop = true                             // 0450
 }                                         // 0500
-`,
-[{"start":0,"end":549,"count":1},
- {"start":263,"end":501,"count":2},
- {"start":312,"end":501,"count":1}]
-);
-
-TestCoverage(
-"terminal return statement",
-`
+    `,
+    [ {"start":0,"end":549,"count":1},
+      {"start":263,"end":501,"count":2},
+      {"start":312,"end":501,"count":1} ]
+  );
+
+  await TestCoverage(
+    "terminal return statement",
+    `
 function a () {                           // 0000
   const b = false                         // 0050
   return 1                                // 0100
@@ -896,17 +911,17 @@
   }                                       // 0650
 }                                         // 0700
 a(); b(false); b(true); c()               // 0750
-`,
-[{"start":0,"end":799,"count":1},
- {"start":0,"end":151,"count":1},
- {"start":210,"end":451,"count":2},
- {"start":263,"end":450,"count":1},
- {"start":510,"end":701,"count":1}]
-);
-
-TestCoverage(
-"terminal blocks",
-`
+    `,
+    [ {"start":0,"end":799,"count":1},
+      {"start":0,"end":151,"count":1},
+      {"start":210,"end":451,"count":2},
+      {"start":263,"end":450,"count":1},
+      {"start":510,"end":701,"count":1} ]
+  );
+
+  await TestCoverage(
+    "terminal blocks",
+    `
 function a () {                           // 0000
   {                                       // 0050
     return 'a'                            // 0100
@@ -920,15 +935,15 @@
   }                                       // 0500
 }                                         // 0550
 a(); b()                                  // 0600
-`,
-[{"start":0,"end":649,"count":1},
- {"start":0,"end":201,"count":1},
- {"start":250,"end":551,"count":1}]
-);
-
-TestCoverage(
-"terminal if statements",
-`
+    `,
+    [ {"start":0,"end":649,"count":1},
+      {"start":0,"end":201,"count":1},
+      {"start":250,"end":551,"count":1} ]
+  );
+
+  await TestCoverage(
+    "terminal if statements",
+    `
 function a (branch) {                     // 0000
   if (branch) {                           // 0050
     return 'a'                            // 0100
@@ -963,35 +978,35 @@
 }                                         // 1550
 a(true); a(false); b(true); b(false)      // 1600
 c(true); d(true);                         // 1650
-`,
-[{"start":0,"end":1699,"count":1},
- {"start":0,"end":301,"count":2},
- {"start":64,"end":253,"count":1},
- {"start":350,"end":651,"count":2},
- {"start":414,"end":603,"count":1},
- {"start":700,"end":1001,"count":1},
- {"start":853,"end":953,"count":0},
- {"start":1050,"end":1551,"count":1},
- {"start":1167,"end":1255,"count":0},
- {"start":1403,"end":1503,"count":0}]
-);
-
-TestCoverage(
-"https://crbug.com/927464",
-`
+    `,
+    [ {"start":0,"end":1699,"count":1},
+      {"start":0,"end":301,"count":2},
+      {"start":64,"end":253,"count":1},
+      {"start":350,"end":651,"count":2},
+      {"start":414,"end":603,"count":1},
+      {"start":700,"end":1001,"count":1},
+      {"start":853,"end":953,"count":0},
+      {"start":1050,"end":1551,"count":1},
+      {"start":1167,"end":1255,"count":0},
+      {"start":1403,"end":1503,"count":0} ]
+  );
+
+  await TestCoverage(
+    "https://crbug.com/927464",
+    `
 !function f() {                           // 0000
   function unused() { nop(); }            // 0050
   nop();                                  // 0100
 }();                                      // 0150
-`,
-[{"start":0,"end":199,"count":1},
- {"start":1,"end":151,"count":1},
- {"start":52,"end":80,"count":0}]
-);
-
-TestCoverage(
-"https://crbug.com/v8/8691",
-`
+    `,
+    [ {"start":0,"end":199,"count":1},
+      {"start":1,"end":151,"count":1},
+      {"start":52,"end":80,"count":0} ]
+  );
+
+  await TestCoverage(
+    "https://crbug.com/v8/8691",
+    `
 function f(shouldThrow) {                 // 0000
   if (shouldThrow) {                      // 0050
     throw Error('threw')                  // 0100
@@ -1005,16 +1020,16 @@
 try {                                     // 0500
   f(false)                                // 0550
 } catch (err) {}                          // 0600
-`,
-[{"start":0,"end":649,"count":1},
- {"start":602,"end":616,"count":0},
- {"start":0,"end":201,"count":2},
- {"start":69,"end":153,"count":1}]
-);
-
-TestCoverage(
-"https://crbug.com/v8/9705",
-`
+    `,
+    [ {"start":0,"end":649,"count":1},
+      {"start":602,"end":616,"count":0},
+      {"start":0,"end":201,"count":2},
+      {"start":69,"end":153,"count":1} ]
+  );
+
+  await TestCoverage(
+    "https://crbug.com/v8/9705",
+    `
 function f(x) {                           // 0000
   switch (x) {                            // 0050
     case 40: nop();                       // 0100
@@ -1027,18 +1042,18 @@
 f(41);                                    // 0450
 f(42);                                    // 0500
 f(43);                                    // 0550
-`,
-[{"start":0,"end":599,"count":1},
- {"start":0,"end":351,"count":4},
- {"start":104,"end":119,"count":1},
- {"start":154,"end":179,"count":2},
- {"start":204,"end":226,"count":1},
- {"start":253,"end":350,"count":2}]
-);
-
-TestCoverage(
-"https://crbug.com/v8/9705",
-`
+    `,
+    [ {"start":0,"end":599,"count":1},
+      {"start":0,"end":351,"count":4},
+      {"start":104,"end":119,"count":1},
+      {"start":154,"end":179,"count":2},
+      {"start":204,"end":226,"count":1},
+      {"start":253,"end":350,"count":2} ]
+  );
+
+  await TestCoverage(
+    "https://crbug.com/v8/9705",
+    `
 function f(x) {                           // 0000
   switch (x) {                            // 0050
     case 40: nop();                       // 0100
@@ -1049,32 +1064,32 @@
 };                                        // 0350
 f(42);                                    // 0400
 f(43);                                    // 0450
-`,
-[{"start":0,"end":499,"count":1},
- {"start":0,"end":351,"count":2},
- {"start":104,"end":119,"count":0},
- {"start":154,"end":179,"count":0},
- {"start":204,"end":226,"count":1}]
-);
-
-TestCoverage(
-"https://crbug.com/v8/9857",
-`function foo() {}`,
-[{"start":0,"end":17,"count":1},
- {"start":0,"end":17,"count":0}]
-);
-
-TestCoverage(
-"https://crbug.com/v8/9857",
-`function foo() {function bar() {}}; foo()`,
-[{"start":0,"end":41,"count":1},
- {"start":0,"end":34,"count":1},
- {"start":16,"end":33,"count":0}]
-);
-
-TestCoverage(
-"https://crbug.com/v8/9952",
-`
+    `,
+    [ {"start":0,"end":499,"count":1},
+      {"start":0,"end":351,"count":2},
+      {"start":104,"end":119,"count":0},
+      {"start":154,"end":179,"count":0},
+      {"start":204,"end":226,"count":1} ]
+  );
+
+  await TestCoverage(
+    "https://crbug.com/v8/9857",
+    `function foo() {}`,
+    [ {"start":0,"end":17,"count":1},
+      {"start":0,"end":17,"count":0} ]
+  );
+
+  await TestCoverage(
+    "https://crbug.com/v8/9857",
+    `function foo() {function bar() {}}; foo()`,
+    [ {"start":0,"end":41,"count":1},
+      {"start":0,"end":34,"count":1},
+      {"start":16,"end":33,"count":0} ]
+  );
+
+  await TestCoverage(
+    "https://crbug.com/v8/9952",
+    `
 function test(foo = "foodef") {           // 0000
   return {bar};                           // 0050
                                           // 0100
@@ -1083,29 +1098,30 @@
   }                                       // 0250
 }                                         // 0300
 test().bar();                             // 0350
-`,
-[{"start":0,"end":399,"count":1},
- {"start":0,"end":301,"count":1},
- {"start":152,"end":253,"count":1}]);
-
-TestCoverage(
-"https://crbug.com/v8/9952",
-`
+    `,
+    [ {"start":0,"end":399,"count":1},
+      {"start":0,"end":301,"count":1},
+      {"start":152,"end":253,"count":1} ]
+  );
+
+  await TestCoverage(
+    "https://crbug.com/v8/9952",
+    `
 function test(foo = (()=>{})) {           // 0000
   return {foo};                           // 0050
 }                                         // 0100
                                           // 0150
 test(()=>{}).foo();                       // 0200
-`,
-[{"start":0,"end":249,"count":1},
- {"start":0,"end":101,"count":1},
- {"start":21,"end":27,"count":0},
- {"start":205,"end":211,"count":1}]
-);
-
-TestCoverage(
-"https://crbug.com/v8/10030 - original",
-`
+    `,
+    [ {"start":0,"end":249,"count":1},
+      {"start":0,"end":101,"count":1},
+      {"start":21,"end":27,"count":0},
+      {"start":205,"end":211,"count":1} ]
+  );
+
+  await TestCoverage(
+    "https://crbug.com/v8/10030 - original",
+    `
 function a (shouldThrow) {                // 0000
   try {                                   // 0050
     if (shouldThrow)                      // 0100
@@ -1117,15 +1133,15 @@
 }                                         // 0400
 a(false);                                 // 0450
 a(true);                                  // 0500
-`,
-[{"start":0,"end":549,"count":1},
- {"start":0,"end":401,"count":2},
- {"start":156,"end":353,"count":1}]
-);
-
-TestCoverage(
-"https://crbug.com/v8/10030 - only throw",
-`
+    `,
+    [ {"start":0,"end":549,"count":1},
+      {"start":0,"end":401,"count":2},
+      {"start":156,"end":353,"count":1} ]
+  );
+
+  await TestCoverage(
+    "https://crbug.com/v8/10030 - only throw",
+    `
 function a (shouldThrow) {                // 0000
   try {                                   // 0050
     if (shouldThrow)                      // 0100
@@ -1136,15 +1152,15 @@
   }                                       // 0350
 }                                         // 0400
 a(true);                                  // 0450
-`,
-[{"start":0,"end":499,"count":1},
- {"start":0,"end":401,"count":1},
- {"start":180,"end":254,"count":0}]
-);
-
-TestCoverage(
-"https://crbug.com/v8/10030 - finally",
-`
+    `,
+    [ {"start":0,"end":499,"count":1},
+      {"start":0,"end":401,"count":1},
+      {"start":180,"end":254,"count":0} ]
+  );
+
+  await TestCoverage(
+    "https://crbug.com/v8/10030 - finally",
+    `
 function a (shouldThrow) {                // 0000
   try {                                   // 0050
     return 'I ran';                       // 0100
@@ -1154,13 +1170,14 @@
 }                                         // 0300
 a(false);                                 // 0350
 a(true);                                  // 0400
-`,
-[{"start":0,"end":449,"count":1},
- {"start":0,"end":301,"count":2}]);
-
-TestCoverage(
-"https://crbug.com/v8/10030 - catch & finally",
-`
+    `,
+    [ {"start":0,"end":449,"count":1},
+      {"start":0,"end":301,"count":2} ]
+  );
+
+  await TestCoverage(
+    "https://crbug.com/v8/10030 - catch & finally",
+    `
 function a (shouldThrow) {                // 0000
   try {                                   // 0050
     return 'I ran';                       // 0100
@@ -1172,14 +1189,15 @@
 }                                         // 0400
 a(false);                                 // 0450
 a(true);                                  // 0500
-`,
-[{"start":0,"end":549,"count":1},
- {"start":0,"end":401,"count":2},
- {"start":154,"end":254,"count":0}]);
-
-TestCoverage(
-"https://crbug.com/v8/11231 - nullish coalescing",
-`
+    `,
+    [ {"start":0,"end":549,"count":1},
+    {"start":0,"end":401,"count":2},
+    {"start":154,"end":254,"count":0} ]
+  );
+
+  await TestCoverage(
+    "https://crbug.com/v8/11231 - nullish coalescing",
+    `
 const a = true                            // 0000
 const b = false                           // 0050
 const c = undefined                       // 0100
@@ -1189,15 +1207,16 @@
 const g = 33                              // 0300
 const h = c ?? (c ?? 'hello')             // 0350
 const i = c ?? b ?? 'hello'               // 0400
-`,
-[{"start":0,"end":449,"count":1},
- {"start":162,"end":167,"count":0},
- {"start":262,"end":274,"count":0},
- {"start":417,"end":427,"count":0}]);
-
-TestCoverage(
-"Optional Chaining",
-`
+    `,
+    [ {"start":0,"end":449,"count":1},
+      {"start":162,"end":167,"count":0},
+      {"start":262,"end":274,"count":0},
+      {"start":417,"end":427,"count":0} ]
+  );
+
+  await TestCoverage(
+    "Optional Chaining",
+    `
 const a = undefined || null               // 0000
 const b = a?.b                            // 0050
 const c = a?.['b']                        // 0100
@@ -1216,20 +1235,23 @@
 const n = d?.[d?.x?.f]                    // 0750
 if (a?.[d?.x?.f]) { const p = 99 } else {}// 0800
 const p = d?.[d?.x?.f]?.x                 // 0850
-`,
-[{"start":0,"end":899,"count":1},
- {"start":61,"end":64,"count":0},
- {"start":111,"end":118,"count":0},
- {"start":470,"end":473,"count":0},
- {"start":518,"end":532,"count":0},
- {"start":561,"end":568,"count":0},
- {"start":671,"end":677,"count":0},
- {"start":708,"end":711,"count":0},
- {"start":768,"end":771,"count":0},
- {"start":805,"end":816,"count":0},
- {"start":818,"end":834,"count":0},
- {"start":868,"end":871,"count":0},
- {"start":872,"end":875,"count":0},
- {"start":216,"end":240,"count":2}]);
+    `,
+    [ {"start":0,"end":899,"count":1},
+      {"start":61,"end":64,"count":0},
+      {"start":111,"end":118,"count":0},
+      {"start":470,"end":473,"count":0},
+      {"start":518,"end":532,"count":0},
+      {"start":561,"end":568,"count":0},
+      {"start":671,"end":677,"count":0},
+      {"start":708,"end":711,"count":0},
+      {"start":768,"end":771,"count":0},
+      {"start":805,"end":816,"count":0},
+      {"start":818,"end":834,"count":0},
+      {"start":868,"end":871,"count":0},
+      {"start":872,"end":875,"count":0},
+      {"start":216,"end":240,"count":2} ]
+  );
+
+  %DebugToggleBlockCoverage(false);
 
-%DebugToggleBlockCoverage(false);
+})();
diff -r -u --color up/v8/test/mjsunit/code-coverage-class-fields.js nw/v8/test/mjsunit/code-coverage-class-fields.js
--- up/v8/test/mjsunit/code-coverage-class-fields.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/code-coverage-class-fields.js	2023-01-19 16:46:36.796442745 -0500
@@ -4,60 +4,63 @@
 
 // Flags: --allow-natives-syntax --no-always-turbofan --harmony-public-fields
 // Flags: --harmony-static-fields --no-stress-flush-code
+// Flags: --expose-gc
 // Files: test/mjsunit/code-coverage-utils.js
 
-%DebugToggleBlockCoverage(true);
+(async function () {
 
-TestCoverage(
-"class with no fields",
-`
+  %DebugToggleBlockCoverage(true);
+
+  await TestCoverage(
+    "class with no fields",
+    `
 class X {                                  // 000
 };                                         // 050
-`,
-[{"start":0,"end":99,"count":1}]
-);
-
-TestCoverage(
-"class that's not created",
-`
+    `,
+    [ {"start":0,"end":99,"count":1} ]
+  );
+
+  await TestCoverage(
+    "class that's not created",
+    `
 class X {                                  // 000
   x = function() { }                       // 050
 };                                         // 100
-`,
-[{"start":0,"end":149,"count":1},
- {"start":0,"end":101,"count":0}]
-);
-
-TestCoverage(
-"class with field thats not called",
-`
+    `,
+    [ {"start":0,"end":149,"count":1},
+      {"start":0,"end":101,"count":0} ]
+  );
+
+  await TestCoverage(
+    "class with field thats not called",
+    `
 class X {                                  // 000
   x = function() { }                       // 050
 };                                         // 100
 let x = new X();                           // 150
-`,
-[{"start":0,"end":199,"count":1},
- {"start":0,"end":101,"count":1},
- {"start":56,"end":70,"count":0}]
-);
-
-TestCoverage(
-"class field",
-`
+    `,
+    [ {"start":0,"end":199,"count":1},
+      {"start":0,"end":101,"count":1},
+      {"start":56,"end":70,"count":0} ]
+  );
+
+  await TestCoverage(
+    "class field",
+    `
 class X {                                  // 000
   x = function() { }                       // 050
 };                                         // 100
 let x = new X();                           // 150
 x.x();                                     // 200
-`,
-[{"start":0,"end":249,"count":1},
- {"start":0,"end":101,"count":1},
- {"start":56,"end":70,"count":1}]
-);
-
-TestCoverage(
-"non contiguous class field",
-`
+    `,
+    [ {"start":0,"end":249,"count":1},
+      {"start":0,"end":101,"count":1},
+      {"start":56,"end":70,"count":1} ]
+  );
+
+  await TestCoverage(
+    "non contiguous class field",
+    `
 class X {                                  // 000
   x = function() { }                       // 050
   foo() { }                                // 100
@@ -66,17 +69,17 @@
 let x = new X();                           // 250
 x.x();                                     // 300
 x.y();                                     // 350
-`,
-[{"start":0,"end":399,"count":1},
- {"start":0,"end":201,"count":1},
- {"start":56,"end":70,"count":1},
- {"start":102,"end":111,"count":0},
- {"start":156,"end":169,"count":1}]
-);
-
-TestCoverage(
-"non contiguous class field thats called",
-`
+    `,
+    [ {"start":0,"end":399,"count":1},
+      {"start":0,"end":201,"count":1},
+      {"start":56,"end":70,"count":1},
+      {"start":102,"end":111,"count":0},
+      {"start":156,"end":169,"count":1} ]
+  );
+
+  await TestCoverage(
+    "non contiguous class field thats called",
+    `
 class X {                                  // 000
   x = function() { }                       // 050
   foo() { }                                // 100
@@ -86,89 +89,91 @@
 x.x();                                     // 300
 x.y();                                     // 350
 x.foo();                                   // 400
-`,
-[{"start":0,"end":449,"count":1},
- {"start":0,"end":201,"count":1},
- {"start":56,"end":70,"count":1},
- {"start":102,"end":111,"count":1},
- {"start":156,"end":169,"count":1}]
-);
-
-TestCoverage(
-"class with initializer iife",
-`
+    `,
+    [ {"start":0,"end":449,"count":1},
+      {"start":0,"end":201,"count":1},
+      {"start":56,"end":70,"count":1},
+      {"start":102,"end":111,"count":1},
+      {"start":156,"end":169,"count":1} ]
+  );
+
+  await TestCoverage(
+    "class with initializer iife",
+    `
 class X {                                  // 000
   x = (function() { })()                   // 050
 };                                         // 100
 let x = new X();                           // 150
-`,
-[{"start":0,"end":199,"count":1},
- {"start":0,"end":101,"count":1},
- {"start":57,"end":71,"count":1}]
-);
-
-TestCoverage(
-"class with computed field",
-`
+    `,
+    [ {"start":0,"end":199,"count":1},
+      {"start":0,"end":101,"count":1},
+      {"start":57,"end":71,"count":1} ]
+  );
+
+  await TestCoverage(
+    "class with computed field",
+    `
 function f() {};                           // 000
 class X {                                  // 050
   [f()] = (function() { })()               // 100
 };                                         // 150
 let x = new X();                           // 200
-`,
-[{"start":0,"end":249,"count":1},
- {"start":0,"end":15,"count":1},
- {"start":50,"end":151,"count":1},
- {"start":111,"end":125,"count":1}]
-);
-
-TestCoverage(
-"static class field that's not called",
-`
+    `,
+    [ {"start":0,"end":249,"count":1},
+      {"start":0,"end":15,"count":1},
+      {"start":50,"end":151,"count":1},
+      {"start":111,"end":125,"count":1} ]
+  );
+
+  await TestCoverage(
+    "static class field that's not called",
+    `
 class X {                                  // 000
   static x = function() { }                // 050
 };                                         // 100
-`,
-[{"start":0,"end":149,"count":1},
- {"start":52,"end":77,"count":1},
- {"start":63,"end":77,"count":0}]
-);
-
-TestCoverage(
-"static class field",
-`
+    `,
+    [ {"start":0,"end":149,"count":1},
+      {"start":52,"end":77,"count":1},
+      {"start":63,"end":77,"count":0} ]
+  );
+
+  await TestCoverage(
+    "static class field",
+    `
 class X {                                  // 000
   static x = function() { }                // 050
 };                                         // 100
 X.x();                                     // 150
-`,
-[{"start":0,"end":199,"count":1},
- {"start":52,"end":77,"count":1},
- {"start":63,"end":77,"count":1}]
-);
-
-TestCoverage(
-"static class field with iife",
-`
+    `,
+    [ {"start":0,"end":199,"count":1},
+      {"start":52,"end":77,"count":1},
+      {"start":63,"end":77,"count":1} ]
+  );
+
+  await TestCoverage(
+    "static class field with iife",
+    `
 class X {                                  // 000
   static x = (function() { })()            // 050
 };                                         // 100
-`,
-[{"start":0,"end":149,"count":1},
- {"start":52,"end":81,"count":1},
- {"start":64,"end":78,"count":1}]
-);
-
-TestCoverage(
-"computed static class field",
-`
+    `,
+    [ {"start":0,"end":149,"count":1},
+      {"start":52,"end":81,"count":1},
+      {"start":64,"end":78,"count":1} ]
+  );
+
+  await TestCoverage(
+    "computed static class field",
+    `
 function f() {}                            // 000
 class X {                                  // 050
   static [f()] = (function() { })()        // 100
 };                                         // 150
-`,
-[{"start":0,"end":199,"count":1},
- {"start":0,"end":15,"count":1},
- {"start":102,"end":135,"count":1},
- {"start":118,"end":132,"count":1}]
-);
+    `,
+    [ {"start":0,"end":199,"count":1},
+      {"start":0,"end":15,"count":1},
+      {"start":102,"end":135,"count":1},
+      {"start":118,"end":132,"count":1} ]
+  );
+
+})();
diff -r -u --color up/v8/test/mjsunit/code-coverage-precise.js nw/v8/test/mjsunit/code-coverage-precise.js
--- up/v8/test/mjsunit/code-coverage-precise.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/code-coverage-precise.js	2023-01-19 16:46:36.796442745 -0500
@@ -4,63 +4,70 @@
 
 // Flags: --allow-natives-syntax --no-always-turbofan --no-stress-flush-code
 // Flags: --no-stress-incremental-marking
+// Flags: --expose-gc
 // Files: test/mjsunit/code-coverage-utils.js
 
 // Test precise code coverage.
 
-// Without precise coverage enabled, we lose coverage data to the GC.
-TestCoverage(
-"call an IIFE",
-`
+(async function () {
+
+  // Without precise coverage enabled, we lose coverage data to the GC.
+  await TestCoverage(
+    "call an IIFE",
+    `
 (function f() {})();
-`,
-undefined  // The IIFE has been garbage-collected.
-);
-
-TestCoverage(
-"call locally allocated function",
-`
+    `,
+    undefined  // The IIFE has been garbage-collected.
+  );
+
+  await TestCoverage(
+    "call locally allocated function",
+    `
 for (var i = 0; i < 10; i++) {
   let f = () => 1;
   i += f();
 }
-`,
-undefined
-);
-
-// This does not happen with precise coverage enabled.
-%DebugTogglePreciseCoverage(true);
-
-TestCoverage(
-"call an IIFE",
-`
+    `,
+    undefined
+  );
+
+  // This does not happen with precise coverage enabled.
+  %DebugTogglePreciseCoverage(true);
+
+  await TestCoverage(
+    "call an IIFE",
+    `
 (function f() {})();
-`,
-[{"start":0,"end":20,"count":1},{"start":1,"end":16,"count":1}]
-);
-
-TestCoverage(
-"call locally allocated function",
-`
+    `,
+    [ {"start":0,"end":20,"count":1},
+      {"start":1,"end":16,"count":1} ]
+  );
+
+  await TestCoverage(
+    "call locally allocated function",
+    `
 for (var i = 0; i < 10; i++) {
   let f = () => 1;
   i += f();
 }
-`,
-[{"start":0,"end":63,"count":1},{"start":41,"end":48,"count":5}]
-);
-
-TestCoverage(
-"https://crbug.com/927464",
-`
+    `,
+    [ {"start":0,"end":63,"count":1},
+      {"start":41,"end":48,"count":5} ]
+  );
+
+  await TestCoverage(
+    "https://crbug.com/927464",
+    `
 !function f() {                           // 0000
   function unused() { nop(); }            // 0100
   nop();                                  // 0150
 }();                                      // 0200
-`,
-[{"start":0,"end":199,"count":1},
- {"start":1,"end":151,"count":1},
- {"start":52,"end":80,"count":0}]
-);
+    `,
+    [ {"start":0,"end":199,"count":1},
+      {"start":1,"end":151,"count":1},
+      {"start":52,"end":80,"count":0} ]
+  );
+
+  %DebugTogglePreciseCoverage(false);
 
-%DebugTogglePreciseCoverage(false);
+})();
diff -r -u --color up/v8/test/mjsunit/code-coverage-utils.js nw/v8/test/mjsunit/code-coverage-utils.js
--- up/v8/test/mjsunit/code-coverage-utils.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/code-coverage-utils.js	2023-01-19 16:46:36.796442745 -0500
@@ -3,6 +3,7 @@
 // found in the LICENSE file.
 
 // Flags: --allow-natives-syntax
+// Flags: --expose-gc
 
 let TestCoverage;
 let TestCoverageNoGC;
@@ -18,11 +19,14 @@
     return undefined;
   };
 
-  function TestCoverageInternal(
+  async function TestCoverageInternal(
       name, source, expectation, collect_garbage, prettyPrintResults) {
     source = source.trim();
     eval(source);
-    if (collect_garbage) %CollectGarbage("collect dead objects");
+    // We need to invoke GC asynchronously, so that it doesn't need to scan
+    // the stack. Otherwise, some objects may not be reclaimed because of
+    // conservative stack scanning and the tests may fail.
+    if (collect_garbage) await gc({ type: 'major', execution: 'async' });
     var covfefe = GetCoverage(source);
     var stringified_result = JSON.stringify(covfefe);
     var stringified_expectation = JSON.stringify(expectation);
@@ -46,12 +50,14 @@
     assertEquals(stringified_expectation, stringified_result, name + " failed");
   };
 
-  TestCoverage = function(name, source, expectation, prettyPrintResults) {
-    TestCoverageInternal(name, source, expectation, true, prettyPrintResults);
+  TestCoverage = async function(name, source, expectation, prettyPrintResults) {
+    return TestCoverageInternal(name, source, expectation, true,
+                                prettyPrintResults);
   };
 
   TestCoverageNoGC = function(name, source, expectation, prettyPrintResults) {
-    TestCoverageInternal(name, source, expectation, false, prettyPrintResults);
+    return TestCoverageInternal(name, source, expectation, false,
+                                prettyPrintResults);
   };
 
   nop = function() {};
diff -r -u --color up/v8/test/mjsunit/compiler/array-multiple-receiver-maps.js nw/v8/test/mjsunit/compiler/array-multiple-receiver-maps.js
--- up/v8/test/mjsunit/compiler/array-multiple-receiver-maps.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/compiler/array-multiple-receiver-maps.js	2023-01-19 16:46:36.796442745 -0500
@@ -35,7 +35,7 @@
       for (let a of t3) {
         message += " for args " + JSON.stringify(a) + " should have been optimized";
         f(a.arr, () => a.el);
-        assertOptimized(f, undefined, message);
+        assertOptimized(f, message);
       }
     } else {
       // Trigger deopt, causing no-speculation bit to be set.
@@ -46,18 +46,18 @@
       message_unoptimized = message + " should have been unoptimized"
       message_optimized = message + " should have been optimized"
       f(a1.darr, () => a1.del);
-      assertUnoptimized(f, undefined, message_unoptimized);
+      assertUnoptimized(f, message_unoptimized);
       if (speculationCheck) {
         %PrepareFunctionForOptimization(f);
         %OptimizeFunctionOnNextCall(f);
         f(a2.darr, () => a2.del);
-        assertUnoptimized(f, undefined, message_unoptimized);
+        assertUnoptimized(f, message_unoptimized);
       }
       %PrepareFunctionForOptimization(f);
       %OptimizeFunctionOnNextCall(f);
       // No speculation should protect against further deopts.
       f(a3.darr, () => a3.del);
-      assertOptimized(f, undefined,  message_optimized);
+      assertOptimized(f,  message_optimized);
     }
   }
 
diff -r -u --color up/v8/test/mjsunit/compiler/bigint-asintn.js nw/v8/test/mjsunit/compiler/bigint-asintn.js
--- up/v8/test/mjsunit/compiler/bigint-asintn.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/compiler/bigint-asintn.js	2023-01-19 16:46:36.796442745 -0500
@@ -27,6 +27,8 @@
   // The next time the function is optimized, speculation should be disabled
   // so the builtin call is kept, which won't deoptimize again.
   %PrepareFunctionForOptimization(f);
+  assertEquals(-1n, f(7n));
+  assertEquals(1n, f(9n));
   %OptimizeFunctionOnNextCall(f);
 }
 assertEquals(-1n, f(7n));
diff -r -u --color up/v8/test/mjsunit/compiler/bigint-asuintn.js nw/v8/test/mjsunit/compiler/bigint-asuintn.js
--- up/v8/test/mjsunit/compiler/bigint-asuintn.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/compiler/bigint-asuintn.js	2023-01-19 16:46:36.796442745 -0500
@@ -27,6 +27,8 @@
   // The next time the function is optimized, speculation should be disabled
   // so the builtin call is kept, which won't deoptimize again.
   %PrepareFunctionForOptimization(f);
+  assertEquals(7n, f(7n));
+  assertEquals(1n, f(9n));
   %OptimizeFunctionOnNextCall(f);
 }
 assertEquals(7n, f(7n));
Only in nw/v8/test/mjsunit/compiler: bigint-modulus.js
diff -r -u --color up/v8/test/mjsunit/compiler/bigint64-add-no-deopt-loop.js nw/v8/test/mjsunit/compiler/bigint64-add-no-deopt-loop.js
--- up/v8/test/mjsunit/compiler/bigint64-add-no-deopt-loop.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/compiler/bigint64-add-no-deopt-loop.js	2023-01-19 16:46:36.796442745 -0500
@@ -4,7 +4,7 @@
 
 // Flags: --allow-natives-syntax --turbofan --no-always-turbofan
 
-(function OptimizeAndTest() {
+(function OptimizeAndTestNegativeLimit() {
   function f(x, y) {
     return x + y;
   }
@@ -12,21 +12,54 @@
   assertEquals(1n, f(0n, 1n));
   assertEquals(5n, f(2n, 3n));
   %OptimizeFunctionOnNextCall(f);
-  assertEquals(9n, f(4n, 5n));
-  assertOptimized(f);
-  // CheckBigInt64 should trigger deopt.
   assertEquals(-(2n ** 63n), f(-(2n ** 63n), 0n));
+  assertOptimized(f);
+  // Re-prepare the function before the first deopt to ensure type feedback is
+  // not cleared by an umtimely gc.
+  %PrepareFunctionForOptimization(f);
+  assertOptimized(f);
+  // CheckBigInt64 should trigger deopt on INT_MIN - 1.
+  assertEquals(-(2n ** 63n) - 1n, f(-(2n ** 63n) - 1n, 0n));
   if (%Is64Bit()) {
     assertUnoptimized(f);
 
-    %PrepareFunctionForOptimization(f);
     assertEquals(1n, f(0n, 1n));
     assertEquals(5n, f(2n, 3n));
     %OptimizeFunctionOnNextCall(f);
-    assertEquals(9n, f(4n, 5n));
+    assertEquals(-(2n ** 63n), f(-(2n ** 63n), 0n));
     assertOptimized(f);
     // Ensure there is no deopt loop.
-    assertEquals(-(2n ** 63n), f(-(2n ** 63n), 0n));
+    assertEquals(-(2n ** 63n) - 1n, f(-(2n ** 63n) - 1n, 0n));
+    assertOptimized(f);
+  }
+})();
+
+(function OptimizeAndTestPositiveLimit() {
+  function f(x, y) {
+    return x + y;
+  }
+  %PrepareFunctionForOptimization(f);
+  assertEquals(1n, f(0n, 1n));
+  assertEquals(5n, f(2n, 3n));
+  %OptimizeFunctionOnNextCall(f);
+  assertEquals(2n ** 63n - 1n, f(2n ** 63n - 1n, 0n));
+  assertOptimized(f);
+  // Re-prepare the function before the first deopt to ensure type feedback is
+  // not cleared by an untimely gc.
+  %PrepareFunctionForOptimization(f);
+  assertOptimized(f);
+  // CheckBigInt64 should trigger deopt on INT_MAX + 1.
+  assertEquals(2n ** 63n, f(2n ** 63n, 0n));
+  if (%Is64Bit()) {
+    assertUnoptimized(f);
+
+    assertEquals(1n, f(0n, 1n));
+    assertEquals(5n, f(2n, 3n));
+    %OptimizeFunctionOnNextCall(f);
+    assertEquals(2n ** 63n - 1n, f(2n ** 63n - 1n, 0n));
+    assertOptimized(f);
+    // Ensure there is no deopt loop.
+    assertEquals(2n ** 63n, f(2n ** 63n, 0n));
     assertOptimized(f);
   }
 })();
@@ -43,12 +76,15 @@
   assertOptimized(f);
   assertEquals(-(2n ** 63n), f(-(2n ** 62n), -(2n ** 62n)));
   assertOptimized(f);
-  // CheckedBigInt64Add will trigger deopt due to overflow.
+  // Re-prepare the function before the first deopt to ensure type feedback is
+  // not cleared by an umtimely gc.
+  %PrepareFunctionForOptimization(f);
+  assertOptimized(f);
+  // CheckedInt64Add will trigger deopt due to overflow.
   assertEquals(-(2n ** 63n) - 1n, f(-(2n ** 62n + 1n), -(2n ** 62n)));
   if (%Is64Bit()) {
     assertUnoptimized(f);
 
-    %PrepareFunctionForOptimization(f);
     assertEquals(1n, f(0n, 1n));
     assertEquals(5n, f(2n, 3n));
     %OptimizeFunctionOnNextCall(f);
Only in nw/v8/test/mjsunit/compiler: bigint64-div-no-deopt-loop.js
Only in nw/v8/test/mjsunit/compiler: bigint64-mod-no-deopt-loop.js
Only in nw/v8/test/mjsunit/compiler: bigint64-mul-no-deopt-loop.js
Only in nw/v8/test/mjsunit/compiler: bigint64-sub-no-deopt-loop.js
diff -r -u --color up/v8/test/mjsunit/compiler/call-with-arraylike-or-spread.js nw/v8/test/mjsunit/compiler/call-with-arraylike-or-spread.js
--- up/v8/test/mjsunit/compiler/call-with-arraylike-or-spread.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/compiler/call-with-arraylike-or-spread.js	2023-01-19 16:46:36.796442745 -0500
@@ -220,6 +220,62 @@
   assertOptimized(foo);
 })();
 
+// Test with FixedDoubleArray and Math.min/max.
+(function () {
+  "use strict";
+  function arrayMin(val) {
+    return Math.min.apply(Math, val);
+  }
+  function arrayMax(val) {
+    return Math.max.apply(Math, val);
+  }
+
+  %PrepareFunctionForOptimization(arrayMin);
+  %PrepareFunctionForOptimization(arrayMin);
+  assertEquals(11.03, arrayMin([11.03, 16.11, 26.06]));
+
+  %PrepareFunctionForOptimization(arrayMax);
+  %PrepareFunctionForOptimization(arrayMax);
+  assertEquals(26.06, arrayMax([11.03, 16.11, 26.06]));
+  %OptimizeFunctionOnNextCall(arrayMin);
+  %OptimizeFunctionOnNextCall(arrayMax);
+
+  assertEquals(11.03, arrayMin([11.03, 16.11, 26.06]));
+  assertEquals(26.06, arrayMax([11.03, 16.11, 26.06]));
+
+  assertOptimized(arrayMin);
+  assertOptimized(arrayMax);
+
+})();
+
+// Test with holey double array and Math.min/max.
+(function () {
+  "use strict";
+  function arrayMin(val) {
+    return Math.min.apply(Math, val);
+  }
+  function arrayMax(val) {
+    return Math.max.apply(Math, val);
+  }
+
+  %PrepareFunctionForOptimization(arrayMin);
+  %PrepareFunctionForOptimization(arrayMin);
+  assertEquals(NaN, arrayMin([11.03, 16.11, , 26.06]));
+
+  %PrepareFunctionForOptimization(arrayMax);
+  %PrepareFunctionForOptimization(arrayMax);
+  assertEquals(NaN, arrayMax([11.03, 16.11, , 26.06]));
+  %OptimizeFunctionOnNextCall(arrayMin);
+  %OptimizeFunctionOnNextCall(arrayMax);
+
+  assertEquals(NaN, arrayMin([11.03, 16.11, , 26.06]));
+  assertEquals(NaN, arrayMax([11.03, 16.11, , 26.06]));
+
+  assertOptimized(arrayMin);
+  assertOptimized(arrayMax);
+
+})();
+
 // Test Reflect.apply().
 (function () {
   "use strict";
diff -r -u --color up/v8/test/mjsunit/compiler/fast-api-annotations.js nw/v8/test/mjsunit/compiler/fast-api-annotations.js
--- up/v8/test/mjsunit/compiler/fast-api-annotations.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/compiler/fast-api-annotations.js	2023-01-19 16:46:36.807276075 -0500
@@ -5,7 +5,6 @@
 // These tests exercise WebIDL annotations support in the fast API.
 
 // Flags: --turbo-fast-api-calls --expose-fast-api --allow-natives-syntax --turbofan
-// TODO(mslekova): Implement support for TryTruncateFloat64ToInt32.
 // Flags: --no-turboshaft
 // --always-turbofan is disabled because we rely on particular feedback for
 // optimizing to the fastest path.
@@ -40,11 +39,6 @@
 %OptimizeFunctionOnNextCall(add_all_annotate_enforce_range);
 assertEquals(limits_result, add_all_annotate_enforce_range(limits_params));
 
-const min_int32 = -(2 ** 31);
-const max_int32 = 2 ** 31 - 1;
-const min_uint32 = 0;
-const max_uint32 = 2 ** 32 - 1;
-
 // ----------- enforce_range_compare -----------
 // `enforce_range_compare` has the following signature:
 // bool enforce_range_compare(bool /*in_range*/,
@@ -131,105 +125,3 @@
 assertThrows(() => compare_u64(false, -1.5));
 assertThrows(() => compare_u64(false, Number.MIN_SAFE_INTEGER));
 assertThrows(() => compare_u64(false, 2 ** 64 + 3.15));
-
-// ----------- clamp_compare -----------
-// `clamp_compare` has the following signature:
-// void clamp_compare(bool /*in_range*/,
-//   double, integer_type)
-// where integer_type = {int32_t, uint32_t, int64_t, uint64_t}
-
-// ----------- i32 -----------
-function is_in_range_i32(in_range, arg, expected) {
-  let result = fast_c_api.clamp_compare_i32(in_range, arg, arg);
-
-  assertEquals(expected, result);
-}
-
-%PrepareFunctionForOptimization(is_in_range_i32);
-is_in_range_i32(true, 123, 123);
-%OptimizeFunctionOnNextCall(is_in_range_i32);
-is_in_range_i32(true, 123, 123);
-is_in_range_i32(true, -0.5, 0);
-is_in_range_i32(true, 0.5, 0);
-is_in_range_i32(true, 1.5, 2);
-is_in_range_i32(true, min_int32, min_int32);
-is_in_range_i32(true, max_int32, max_int32);
-// Slow path doesn't perform clamping.
-if (isOptimized(is_in_range_i32)) {
-  is_in_range_i32(false, -(2 ** 32), min_int32);
-  is_in_range_i32(false, -(2 ** 32 + 1), min_int32);
-  is_in_range_i32(false, 2 ** 32, max_int32);
-  is_in_range_i32(false, 2 ** 32 + 3.15, max_int32);
-  is_in_range_i32(false, Number.MIN_SAFE_INTEGER, min_int32);
-  is_in_range_i32(false, Number.MAX_SAFE_INTEGER, max_int32);
-}
-
-// ----------- u32 -----------
-function is_in_range_u32(in_range, arg, expected) {
-  let result = fast_c_api.clamp_compare_u32(in_range, arg, arg);
-
-  assertEquals(expected, result);
-}
-
-%PrepareFunctionForOptimization(is_in_range_u32);
-is_in_range_u32(true, 123, 123);
-%OptimizeFunctionOnNextCall(is_in_range_u32);
-is_in_range_u32(true, 123, 123);
-is_in_range_u32(true, 0, 0);
-is_in_range_u32(true, -0.5, 0);
-is_in_range_u32(true, 0.5, 0);
-is_in_range_u32(true, 2 ** 32 - 1, max_uint32);
-is_in_range_u32(false, -(2 ** 31), min_uint32);
-is_in_range_u32(false, 2 ** 32, max_uint32);
-is_in_range_u32(false, -1, min_uint32);
-is_in_range_u32(false, -1.5, min_uint32);
-is_in_range_u32(false, Number.MIN_SAFE_INTEGER, min_uint32);
-is_in_range_u32(false, Number.MAX_SAFE_INTEGER, max_uint32);
-
-// ----------- i64 -----------
-function is_in_range_i64(in_range, arg, expected) {
-  let result = fast_c_api.clamp_compare_i64(in_range, arg, arg);
-  assertEquals(expected, result);
-}
-
-%PrepareFunctionForOptimization(is_in_range_i64);
-is_in_range_i64(true, 123, 123);
-%OptimizeFunctionOnNextCall(is_in_range_i64);
-is_in_range_i64(true, 123, 123);
-is_in_range_i64(true, -0.5, 0);
-is_in_range_i64(true, 0.5, 0);
-is_in_range_i64(true, 1.5, 2);
-is_in_range_i64(true, Number.MIN_SAFE_INTEGER, Number.MIN_SAFE_INTEGER);
-is_in_range_i64(true, Number.MAX_SAFE_INTEGER, Number.MAX_SAFE_INTEGER);
-is_in_range_i64(false, -(2 ** 63), Number.MIN_SAFE_INTEGER);
-is_in_range_i64(false, 2 ** 63 - 1024, Number.MAX_SAFE_INTEGER);
-is_in_range_i64(false, 2 ** 63, Number.MAX_SAFE_INTEGER);
-is_in_range_i64(false, -(2 ** 64), Number.MIN_SAFE_INTEGER);
-is_in_range_i64(false, -(2 ** 64 + 1), Number.MIN_SAFE_INTEGER);
-is_in_range_i64(false, 2 ** 64, Number.MAX_SAFE_INTEGER);
-is_in_range_i64(false, 2 ** 64 + 3.15, Number.MAX_SAFE_INTEGER);
-
-// ----------- u64 -----------
-function is_in_range_u64(in_range, arg, expected) {
-  let result = fast_c_api.clamp_compare_u64(in_range, arg, arg);
-  assertEquals(expected, result);
-}
-
-%PrepareFunctionForOptimization(is_in_range_u64);
-is_in_range_u64(true, 123, 123);
-%OptimizeFunctionOnNextCall(is_in_range_u64);
-is_in_range_u64(true, 123, 123);
-is_in_range_u64(true, 0, 0);
-is_in_range_u64(true, -0.5, 0);
-is_in_range_u64(true, 0.5, 0);
-is_in_range_u64(true, 2 ** 32 - 1, 2 ** 32 - 1);
-is_in_range_u64(true, Number.MAX_SAFE_INTEGER, Number.MAX_SAFE_INTEGER);
-is_in_range_u64(false, Number.MIN_SAFE_INTEGER, 0);
-is_in_range_u64(false, -1, 0);
-is_in_range_u64(false, -1.5, 0);
-is_in_range_u64(false, 2 ** 64, Number.MAX_SAFE_INTEGER);
-is_in_range_u64(false, 2 ** 64 + 3.15, Number.MAX_SAFE_INTEGER);
-
-// ---------- invalid arguments for clamp_compare ---------
-fast_c_api.clamp_compare_i32(true);
-fast_c_api.clamp_compare_i32(true, 753801, -2147483650);
Only in nw/v8/test/mjsunit/compiler: fast-api-calls-8args.js
Only in nw/v8/test/mjsunit/compiler: fast-api-clamp-annotations.js
diff -r -u --color up/v8/test/mjsunit/compiler/regress-905555-2.js nw/v8/test/mjsunit/compiler/regress-905555-2.js
--- up/v8/test/mjsunit/compiler/regress-905555-2.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/compiler/regress-905555-2.js	2023-01-19 16:46:36.839776066 -0500
@@ -22,6 +22,6 @@
 %FinalizeOptimization();
 
 // boom should be deoptimized because the global property cell has changed.
-assertUnoptimized(boom, "sync");
+assertUnoptimized(boom);
 
 assertThrows(boom);
diff -r -u --color up/v8/test/mjsunit/compiler/stress-deopt-count-1.js nw/v8/test/mjsunit/compiler/stress-deopt-count-1.js
--- up/v8/test/mjsunit/compiler/stress-deopt-count-1.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/compiler/stress-deopt-count-1.js	2023-01-19 16:46:36.839776066 -0500
@@ -15,7 +15,7 @@
 %OptimizeFunctionOnNextCall(f);
 
 f(1);
-assertOptimized(f, undefined, undefined, false);
+assertOptimized(f, undefined, false);
 
 f(1);
-assertOptimized(f, undefined, undefined, false);
+assertOptimized(f, undefined, false);
diff -r -u --color up/v8/test/mjsunit/compiler/stress-deopt-count-2.js nw/v8/test/mjsunit/compiler/stress-deopt-count-2.js
--- up/v8/test/mjsunit/compiler/stress-deopt-count-2.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/compiler/stress-deopt-count-2.js	2023-01-19 16:46:36.839776066 -0500
@@ -18,33 +18,33 @@
 // stress_deopt_count == 6
 
 f(1);
-assertOptimized(f, undefined, undefined, false);
+assertOptimized(f, undefined, false);
 
 // stress_deopt_count == 4
 
 f(1);
-assertOptimized(f, undefined, undefined, false);
+assertOptimized(f, undefined, false);
 
 // stress_deopt_count == 2
 
 f(1);
 // deopt & counter reset
-assertUnoptimized(f, undefined, undefined, false);
+assertUnoptimized(f, undefined, false);
 
 // stress_deopt_count == 6
 
 %PrepareFunctionForOptimization(f);
 %OptimizeFunctionOnNextCall(f);
 f(1);
-assertOptimized(f, undefined, undefined, false);
+assertOptimized(f, undefined, false);
 
 // stress_deopt_count == 4
 
 f(1);
-assertOptimized(f, undefined, undefined, false);
+assertOptimized(f, undefined, false);
 
 // stress_deopt_count == 2
 
 f(1);
 // deopt & counter reset
-assertUnoptimized(f, undefined, undefined, false);
+assertUnoptimized(f, undefined, false);
diff -r -u --color up/v8/test/mjsunit/compiler/typedarray-resizablearraybuffer.js nw/v8/test/mjsunit/compiler/typedarray-resizablearraybuffer.js
--- up/v8/test/mjsunit/compiler/typedarray-resizablearraybuffer.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/compiler/typedarray-resizablearraybuffer.js	2023-01-19 16:46:36.839776066 -0500
@@ -3,25 +3,823 @@
 // found in the LICENSE file.
 
 // Flags: --harmony-rab-gsab --allow-natives-syntax --turbofan
+// Flags: --no-always-turbofan --turbo-rab-gsab
 
 "use strict";
 
 d8.file.execute('test/mjsunit/typedarray-helpers.js');
 
-(function TypedArrayLength() {
-  for(let ctor of ctors) {
-    // We have to make sure that we construct a new string for each case to
-    // prevent the compiled function from being reused with spoiled feedback.
-    const test = new Function('\
-      const rab = CreateResizableArrayBuffer(16, 40); \
-      const ta = new ' + ctor.name + '(rab); \
-      rab.resize(32); \
-      return ta.length;');
-
-    %PrepareFunctionForOptimization(test);
-    assertEquals(32 / ctor.BYTES_PER_ELEMENT, test(ctor));
-    assertEquals(32 / ctor.BYTES_PER_ELEMENT, test(ctor));
-    %OptimizeFunctionOnNextCall(test);
-    assertEquals(32 / ctor.BYTES_PER_ELEMENT, test(ctor));
+const is_little_endian = (() => {
+  var buffer = new ArrayBuffer(4);
+  const HEAP32 = new Int32Array(buffer);
+  const HEAPU8 = new Uint8Array(buffer);
+  HEAP32[0] = 255;
+  return (HEAPU8[0] === 255 && HEAPU8[3] === 0);
+})();
+
+function FillBuffer(buffer) {
+  const view = new Uint8Array(buffer);
+  for (let i = 0; i < view.length; ++i) {
+    view[i] = i;
+  }
+}
+%NeverOptimizeFunction(FillBuffer);
+
+function asU16(index) {
+  const start = index * 2;
+  if (is_little_endian) {
+    return (start + 1) * 256 + start;
+  } else {
+    return start * 256 + start + 1;
+  }
+}
+%NeverOptimizeFunction(asU16);
+
+function asU32(index) {
+  const start = index * 4;
+  if (is_little_endian) {
+    return (((start + 3) * 256 + start + 2) * 256 + start + 1) * 256 + start;
+  } else {
+    return ((((start * 256) + start + 1) * 256) + start + 2) * 256 + start + 3;
+  }
+}
+%NeverOptimizeFunction(asU32);
+
+function asF32(index) {
+  const start = index * 4;
+  const ab = new ArrayBuffer(4);
+  const ta = new Uint8Array(ab);
+  for (let i = 0; i < 4; ++i) ta[i] = start + i;
+  return new Float32Array(ab)[0];
+}
+%NeverOptimizeFunction(asF32);
+
+function asF64(index) {
+  const start = index * 8;
+  const ab = new ArrayBuffer(8);
+  const ta = new Uint8Array(ab);
+  for (let i = 0; i < 8; ++i) ta[i] = start + i;
+  return new Float64Array(ab)[0];
+}
+%NeverOptimizeFunction(asF64);
+
+function asB64(index) {
+  const start = index * 8;
+  let result = 0n;
+  if (is_little_endian) {
+    for (let i = 0; i < 8; ++i) {
+      result = result << 8n;
+      result += BigInt(start + 7 - i);
+    }
+  } else {
+    for (let i = 0; i < 8; ++i) {
+      result = result << 8n;
+      result += BigInt(start + i);
+    }
+  }
+  return result;
+}
+%NeverOptimizeFunction(asB64);
+
+function CreateBuffer(shared, len, max_len) {
+  return shared ? new SharedArrayBuffer(len, {maxByteLength: max_len}) :
+                  new ArrayBuffer(len, {maxByteLength: max_len});
+}
+%NeverOptimizeFunction(CreateBuffer);
+
+function MakeResize(target, shared, offset, fixed_len) {
+  const bpe = target.name === 'DataView' ? 1 : target.BYTES_PER_ELEMENT;
+  function RoundDownToElementSize(blen) {
+    return Math.floor(blen / bpe) * bpe;
+  }
+  if (!shared) {
+    if (fixed_len === undefined) {
+      return (b, len) => {
+        b.resize(len);
+        const blen = Math.max(0, len - offset);
+        return RoundDownToElementSize(blen);
+      };
+    } else {
+      const fixed_blen = fixed_len * bpe;
+      return (b, len) => {
+        b.resize(len);
+        const blen = fixed_blen <= (len - offset) ? fixed_blen : 0;
+        return RoundDownToElementSize(blen);
+      }
+    }
+  } else {
+    if (fixed_len === undefined) {
+      return (b, len) => {
+        let blen = 0;
+        if (len > b.byteLength) {
+          b.grow(len);
+          blen = Math.max(0, len - offset);
+        } else {
+          blen = b.byteLength - offset;
+        }
+        return RoundDownToElementSize(blen);
+      };
+    } else {
+      return (b, len) => {
+        if (len > b.byteLength) {
+          b.grow(len);
+        }
+        return fixed_len * bpe;
+      };
+    }
+  }
+}
+%NeverOptimizeFunction(MakeResize);
+
+function MakeElement(target, offset) {
+  const o = offset / target.BYTES_PER_ELEMENT;
+  if (target.name === 'Int8Array') {
+    return (index) => {
+      return o + index;
+    };
+  } else if (target.name === 'Uint32Array') {
+    return (index) => {
+      return asU32(o + index);
+    };
+  } else if (target.name === 'Float64Array') {
+    return (index) => {
+      return asF64(o + index);
+    };
+  } else if (target.name === 'BigInt64Array') {
+    return (index) => {
+      return asB64(o + index);
+    };
+  } else {
+    console.log(`unimplemented: MakeElement(${target.name})`);
+    return () => undefined;
+  }
+}
+%NeverOptimizeFunction(MakeElement);
+
+function MakeCheckBuffer(target, offset) {
+  return (ab, up_to) => {
+    const view = new Uint8Array(ab);
+    for (let i = 0; i < offset; ++i) {
+      assertEquals(0, view[i]);
+    }
+    for (let i = 0; i < (up_to * target.BYTES_PER_ELEMENT) + 1; ++i) {
+      // Use PrintBuffer(ab) for debugging.
+      assertEquals(offset + i, view[offset + i]);
+    }
+  }
+}
+%NeverOptimizeFunction(MakeCheckBuffer);
+
+function ClearBuffer(ab) {
+  for (let i = 0; i < ab.byteLength; ++i) ab[i] = 0;
+}
+%NeverOptimizeFunction(ClearBuffer);
+
+// Use this for debugging these tests.
+function PrintBuffer(buffer) {
+  const view = new Uint8Array(buffer);
+  for (let i = 0; i < 32; ++i) {
+    console.log(`[${i}]: ${view[i]}`)
   }
+}
+%NeverOptimizeFunction(PrintBuffer);
+
+(function() {
+for (let shared of [false, true]) {
+  for (let length_tracking of [false, true]) {
+    for (let with_offset of [false, true]) {
+      for (let target
+               of [Int8Array, Uint32Array, Float64Array, BigInt64Array]) {
+        const test_case = `Testing: Length_${shared ? 'GSAB' : 'RAB'}_${
+            length_tracking ? 'LengthTracking' : 'FixedLength'}${
+            with_offset ? 'WithOffset' : ''}_${target.name}`;
+        // console.log(test_case);
+
+        const byte_length_code = 'return ta.byteLength; // ' + test_case;
+        const ByteLength = new Function('ta', byte_length_code);
+        const length_code = 'return ta.length; // ' + test_case;
+        const Length = new Function('ta', length_code);
+        const offset = with_offset ? 8 : 0;
+
+        let blen = 16 - offset;
+        const fixed_len =
+            length_tracking ? undefined : (blen / target.BYTES_PER_ELEMENT);
+        const ab = CreateBuffer(shared, 16, 40);
+        const ta = new target(ab, offset, fixed_len);
+        const Resize = MakeResize(target, shared, offset, fixed_len);
+
+        assertUnoptimized(ByteLength);
+        assertUnoptimized(Length);
+        %PrepareFunctionForOptimization(ByteLength);
+        %PrepareFunctionForOptimization(Length);
+        assertEquals(blen, ByteLength(ta));
+        assertEquals(blen, ByteLength(ta));
+        assertEquals(Math.floor(blen / target.BYTES_PER_ELEMENT), Length(ta));
+        assertEquals(Math.floor(blen / target.BYTES_PER_ELEMENT), Length(ta));
+        %OptimizeFunctionOnNextCall(ByteLength);
+        %OptimizeFunctionOnNextCall(Length);
+        assertEquals(blen, ByteLength(ta));
+        assertEquals(Math.floor(blen / target.BYTES_PER_ELEMENT), Length(ta));
+        blen = Resize(ab, 32);
+        assertEquals(blen, ByteLength(ta));
+        assertEquals(Math.floor(blen / target.BYTES_PER_ELEMENT), Length(ta));
+        blen = Resize(ab, 9);
+        assertEquals(blen, ByteLength(ta));
+        assertEquals(Math.floor(blen / target.BYTES_PER_ELEMENT), Length(ta));
+        assertOptimized(ByteLength);
+        assertOptimized(Length);
+        blen = Resize(ab, 24);
+        assertEquals(blen, ByteLength(ta));
+        assertEquals(Math.floor(blen / target.BYTES_PER_ELEMENT), Length(ta));
+        assertOptimized(ByteLength);
+        assertOptimized(Length);
+
+        if (!shared) {
+          %ArrayBufferDetach(ab);
+          assertEquals(0, ByteLength(ta));
+          assertEquals(0, Length(ta));
+          assertOptimized(Length);
+        }
+      }
+    }
+  }
+}
+})();
+
+(function() {
+for (let shared of [false, true]) {
+  for (let length_tracking of [false, true]) {
+    for (let with_offset of [false, true]) {
+      for (let target
+               of [Int8Array, Uint32Array, Float64Array, BigInt64Array]) {
+        const test_case = `Testing: Read_${shared ? 'GSAB' : 'RAB'}_${
+            length_tracking ? 'LengthTracking' : 'FixedLength'}${
+            with_offset ? 'WithOffset' : ''}_${target.name}`;
+        // console.log(test_case);
+
+        const read_code = 'return ta[index]; // ' + test_case;
+        const Read = new Function('ta', 'index', read_code);
+        const offset = with_offset ? 8 : 0;
+
+        let blen = 16 - offset;
+        let len = Math.floor(blen / target.BYTES_PER_ELEMENT);
+        const fixed_len = length_tracking ? undefined : len;
+        const ab = CreateBuffer(shared, 16, 40);
+        const ta = new target(ab, offset, fixed_len);
+        const Resize = MakeResize(target, shared, offset, fixed_len);
+        const Element = MakeElement(target, offset);
+        FillBuffer(ab);
+
+        assertUnoptimized(Read);
+        %PrepareFunctionForOptimization(Read);
+        for (let i = 0; i < len * 2; ++i)
+          assertEquals(i < len ? Element(i) : undefined, Read(ta, i));
+        %OptimizeFunctionOnNextCall(Read);
+        for (let i = 0; i < len * 2; ++i)
+          assertEquals(i < len ? Element(i) : undefined, Read(ta, i));
+        assertOptimized(Read);
+        blen = Resize(ab, 32);
+        FillBuffer(ab);
+        len = Math.floor(blen / target.BYTES_PER_ELEMENT);
+        for (let i = 0; i < len * 2; ++i)
+          assertEquals(i < len ? Element(i) : undefined, Read(ta, i));
+        assertOptimized(Read);
+        blen = Resize(ab, 9);
+        FillBuffer(ab);
+        len = Math.floor(blen / target.BYTES_PER_ELEMENT);
+        for (let i = 0; i < len * 2; ++i)
+          assertEquals(i < len ? Element(i) : undefined, Read(ta, i));
+        assertOptimized(Read);
+        blen = Resize(ab, 0);
+        len = Math.floor(blen / target.BYTES_PER_ELEMENT);
+        for (let i = 0; i < len * 2; ++i)
+          assertEquals(i < len ? Element(i) : undefined, Read(ta, i));
+        assertOptimized(Read);
+        blen = Resize(ab, 24);
+        FillBuffer(ab);
+        len = Math.floor(blen / target.BYTES_PER_ELEMENT);
+        for (let i = 0; i < len * 2; ++i)
+          assertEquals(i < len ? Element(i) : undefined, Read(ta, i));
+        assertOptimized(Read);
+
+        if (!shared) {
+          %ArrayBufferDetach(ab);
+          assertEquals(undefined, Read(ta, 0));
+          //                        assertOptimized(Read);
+        }
+      }
+    }
+  }
+}
+})();
+
+(function() {
+for (let shared of [false, true]) {
+  for (let length_tracking of [false, true]) {
+    for (let with_offset of [false, true]) {
+      for (let target
+               of [Int8Array, Uint32Array, Float64Array, BigInt64Array]) {
+        const test_case = `Testing: Write_${shared ? 'GSAB' : 'RAB'}_${
+            length_tracking ? 'LengthTracking' : 'FixedLength'}${
+            with_offset ? 'WithOffset' : ''}_${target.name}`;
+        // console.log(test_case);
+
+        const write_code = 'ta[index] = value; // ' + test_case;
+        const Write = new Function('ta', 'index', 'value', write_code);
+        const offset = with_offset ? 8 : 0;
+
+        let blen = 16 - offset;
+        let len = Math.floor(blen / target.BYTES_PER_ELEMENT);
+        const fixed_len = length_tracking ? undefined : len;
+        const ab = CreateBuffer(shared, 16, 40);
+        const ta = new target(ab, offset, fixed_len);
+        const Resize = MakeResize(target, shared, offset, fixed_len);
+        const Element = MakeElement(target, offset);
+        const CheckBuffer = MakeCheckBuffer(target, offset);
+        ClearBuffer(ab);
+
+        assertUnoptimized(Write);
+        %PrepareFunctionForOptimization(Write);
+        for (let i = 0; i < len; ++i) {
+          Write(ta, i, Element(i));
+          CheckBuffer(ab, i);
+        }
+        ClearBuffer(ab);
+        %OptimizeFunctionOnNextCall(Write);
+        for (let i = 0; i < len; ++i) {
+          Write(ta, i, Element(i));
+          CheckBuffer(ab, i);
+        }
+        assertOptimized(Write);
+        blen = Resize(ab, 32);
+        ClearBuffer(ab);
+        len = Math.floor(blen / target.BYTES_PER_ELEMENT);
+        for (let i = 0; i < len; ++i) {
+          Write(ta, i, Element(i));
+          CheckBuffer(ab, i);
+        }
+        assertOptimized(Write);
+        blen = Resize(ab, 9);
+        ClearBuffer(ab);
+        len = Math.floor(blen / target.BYTES_PER_ELEMENT);
+        for (let i = 0; i < len; ++i) {
+          Write(ta, i, Element(i));
+          CheckBuffer(ab, i);
+        }
+        assertOptimized(Write);
+        blen = Resize(ab, 24);
+        ClearBuffer(ab);
+        len = Math.floor(blen / target.BYTES_PER_ELEMENT);
+        for (let i = 0; i < len; ++i) {
+          Write(ta, i, Element(i));
+          CheckBuffer(ab, i);
+        }
+        assertOptimized(Write);
+      }
+    }
+  }
+}
+})();
+
+(function() {
+for (let shared of [false, true]) {
+  for (let length_tracking of [false, true]) {
+    for (let with_offset of [false, true]) {
+      const test_case = `Testing: ByteLength_${shared ? 'GSAB' : 'RAB'}_${
+          length_tracking ?
+              'LengthTracking' :
+              'FixedLength'}${with_offset ? 'WithOffset' : ''}_DataView`;
+      // console.log(test_case);
+
+      const byte_length_code = 'return dv.byteLength; // ' + test_case;
+      const ByteLength = new Function('dv', byte_length_code);
+      const offset = with_offset ? 8 : 0;
+
+      let blen = 16 - offset;
+      const fixed_blen = length_tracking ? undefined : blen;
+      const ab = CreateBuffer(shared, 16, 40);
+      const dv = new DataView(ab, offset, fixed_blen);
+      const Resize = MakeResize(DataView, shared, offset, fixed_blen);
+
+      assertUnoptimized(ByteLength);
+      %PrepareFunctionForOptimization(ByteLength);
+      assertEquals(blen, ByteLength(dv));
+      assertEquals(blen, ByteLength(dv));
+      %OptimizeFunctionOnNextCall(ByteLength);
+      assertEquals(blen, ByteLength(dv));
+      assertOptimized(ByteLength);
+      blen = Resize(ab, 32);
+      assertEquals(blen, ByteLength(dv));
+      assertOptimized(ByteLength);
+      blen = Resize(ab, 9);
+      if (length_tracking || shared) {
+        assertEquals(blen, ByteLength(dv));
+      } else {
+        // For fixed length rabs, Resize(ab, 9) will put the ArrayBuffer in
+        // detached state, for which DataView.prototype.byteLength has to throw.
+        assertThrows(() => { ByteLength(dv); }, TypeError);
+      }
+      assertOptimized(ByteLength);
+      blen = Resize(ab, 24);
+      assertEquals(blen, ByteLength(dv));
+      assertOptimized(ByteLength);
+
+      if (!shared) {
+        %ArrayBufferDetach(ab);
+        assertThrows(() => { ByteLength(dv); }, TypeError);
+        assertOptimized(ByteLength);
+      }
+    }
+  }
+}
+})();
+
+(function() {
+function ByteLength_RAB_LengthTrackingWithOffset_DataView(dv) {
+  return dv.byteLength;
+}
+const ByteLength = ByteLength_RAB_LengthTrackingWithOffset_DataView;
+
+const rab = CreateResizableArrayBuffer(16, 40);
+const dv = new DataView(rab, 7);
+
+%PrepareFunctionForOptimization(ByteLength);
+assertEquals(9, ByteLength(dv));
+assertEquals(9, ByteLength(dv));
+%OptimizeFunctionOnNextCall(ByteLength);
+assertEquals(9, ByteLength(dv));
+assertOptimized(ByteLength);
+})();
+
+(function() {
+function Read_TA_RAB_LengthTracking_Mixed(ta, index) {
+  return ta[index];
+}
+const Get = Read_TA_RAB_LengthTracking_Mixed;
+
+const ab = new ArrayBuffer(16);
+FillBuffer(ab);
+const rab = CreateResizableArrayBuffer(16, 40);
+FillBuffer(rab);
+let ta_int8 = new Int8Array(ab);
+let ta_uint16 = new Uint16Array(rab);
+let ta_float32 = new Float32Array(ab);
+let ta_float64 = new Float64Array(rab);
+
+// Train with feedback for all elements kinds.
+%PrepareFunctionForOptimization(Get);
+assertEquals(0, Get(ta_int8, 0));
+assertEquals(3, Get(ta_int8, 3));
+assertEquals(15, Get(ta_int8, 15));
+assertEquals(undefined, Get(ta_int8, 16));
+assertEquals(undefined, Get(ta_int8, 32));
+assertEquals(asU16(0), Get(ta_uint16, 0));
+assertEquals(asU16(3), Get(ta_uint16, 3));
+assertEquals(asU16(7), Get(ta_uint16, 7));
+assertEquals(undefined, Get(ta_uint16, 8));
+assertEquals(undefined, Get(ta_uint16, 12));
+assertEquals(asF32(0), Get(ta_float32, 0));
+assertEquals(asF32(3), Get(ta_float32, 3));
+assertEquals(undefined, Get(ta_float32, 4));
+assertEquals(undefined, Get(ta_float32, 12));
+assertEquals(asF64(0), Get(ta_float64, 0));
+assertEquals(asF64(1), Get(ta_float64, 1));
+assertEquals(undefined, Get(ta_float64, 2));
+assertEquals(undefined, Get(ta_float64, 12));
+%OptimizeFunctionOnNextCall(Get);
+assertEquals(0, Get(ta_int8, 0));
+assertEquals(3, Get(ta_int8, 3));
+assertEquals(15, Get(ta_int8, 15));
+assertEquals(undefined, Get(ta_int8, 16));
+assertEquals(undefined, Get(ta_int8, 32));
+assertEquals(asU16(0), Get(ta_uint16, 0));
+assertEquals(asU16(3), Get(ta_uint16, 3));
+assertEquals(asU16(7), Get(ta_uint16, 7));
+assertEquals(undefined, Get(ta_uint16, 8));
+assertEquals(undefined, Get(ta_uint16, 12));
+assertEquals(asF32(0), Get(ta_float32, 0));
+assertEquals(asF32(3), Get(ta_float32, 3));
+assertEquals(undefined, Get(ta_float32, 4));
+assertEquals(undefined, Get(ta_float32, 12));
+assertEquals(asF64(0), Get(ta_float64, 0));
+assertEquals(asF64(1), Get(ta_float64, 1));
+assertEquals(undefined, Get(ta_float64, 2));
+assertEquals(undefined, Get(ta_float64, 12));
+assertOptimized(Get);
+rab.resize(32);
+FillBuffer(rab);
+assertEquals(0, Get(ta_int8, 0));
+assertEquals(3, Get(ta_int8, 3));
+assertEquals(15, Get(ta_int8, 15));
+assertEquals(undefined, Get(ta_int8, 16));
+assertEquals(undefined, Get(ta_int8, 32));
+assertEquals(asU16(0), Get(ta_uint16, 0));
+assertEquals(asU16(3), Get(ta_uint16, 3));
+assertEquals(asU16(15), Get(ta_uint16, 15));
+assertEquals(undefined, Get(ta_uint16, 16));
+assertEquals(undefined, Get(ta_uint16, 40));
+assertEquals(asF32(0), Get(ta_float32, 0));
+assertEquals(asF32(3), Get(ta_float32, 3));
+assertEquals(undefined, Get(ta_float32, 4));
+assertEquals(undefined, Get(ta_float32, 12));
+assertEquals(asF64(0), Get(ta_float64, 0));
+assertEquals(asF64(1), Get(ta_float64, 1));
+assertEquals(asF64(3), Get(ta_float64, 3));
+assertEquals(undefined, Get(ta_float64, 4));
+assertEquals(undefined, Get(ta_float64, 12));
+assertOptimized(Get);
+rab.resize(9);
+assertEquals(0, Get(ta_int8, 0));
+assertEquals(3, Get(ta_int8, 3));
+assertEquals(15, Get(ta_int8, 15));
+assertEquals(undefined, Get(ta_int8, 16));
+assertEquals(undefined, Get(ta_int8, 32));
+assertEquals(asU16(0), Get(ta_uint16, 0));
+assertEquals(asU16(3), Get(ta_uint16, 3));
+assertEquals(undefined, Get(ta_uint16, 4));
+assertEquals(undefined, Get(ta_uint16, 12));
+assertEquals(asF32(0), Get(ta_float32, 0));
+assertEquals(asF32(3), Get(ta_float32, 3));
+assertEquals(undefined, Get(ta_float32, 4));
+assertEquals(undefined, Get(ta_float32, 12));
+assertEquals(asF64(0), Get(ta_float64, 0));
+assertEquals(undefined, Get(ta_float64, 1));
+assertEquals(undefined, Get(ta_float64, 12));
+assertOptimized(Get);
+
+// Call with a different map to trigger deoptimization. We use this
+// to verify that we have actually specialized on the above maps only.
+let ta_uint8 = new Uint8Array(rab);
+assertEquals(7, Get(ta_uint8, 7));
+assertUnoptimized(Get);
+}());
+
+(function() {
+function Read_TA_RAB_LengthTracking_Mixed(ta, index) {
+  return ta[index];
+}
+const Get = Read_TA_RAB_LengthTracking_Mixed;
+
+const ab = new ArrayBuffer(16);
+FillBuffer(ab);
+const rab = CreateResizableArrayBuffer(16, 40);
+FillBuffer(rab);
+let ta_int8 = new Int8Array(ab);
+let ta_uint16 = new Uint16Array(rab);
+let ta_float32 = new Float32Array(ab);
+let ta_float64 = new Float64Array(rab);
+
+// Train with feedback for all elements kinds.
+%PrepareFunctionForOptimization(Get);
+assertEquals(0, Get(ta_int8, 0));
+assertEquals(3, Get(ta_int8, 3));
+assertEquals(15, Get(ta_int8, 15));
+assertEquals(undefined, Get(ta_int8, 16));
+assertEquals(undefined, Get(ta_int8, 32));
+assertEquals(asU16(0), Get(ta_uint16, 0));
+assertEquals(asU16(3), Get(ta_uint16, 3));
+assertEquals(asU16(7), Get(ta_uint16, 7));
+assertEquals(undefined, Get(ta_uint16, 8));
+assertEquals(undefined, Get(ta_uint16, 12));
+assertEquals(asF32(0), Get(ta_float32, 0));
+assertEquals(asF32(3), Get(ta_float32, 3));
+assertEquals(undefined, Get(ta_float32, 4));
+assertEquals(undefined, Get(ta_float32, 12));
+assertEquals(asF64(0), Get(ta_float64, 0));
+assertEquals(asF64(1), Get(ta_float64, 1));
+assertEquals(undefined, Get(ta_float64, 2));
+assertEquals(undefined, Get(ta_float64, 12));
+%OptimizeFunctionOnNextCall(Get);
+assertEquals(0, Get(ta_int8, 0));
+assertEquals(3, Get(ta_int8, 3));
+assertEquals(15, Get(ta_int8, 15));
+assertEquals(undefined, Get(ta_int8, 16));
+assertEquals(undefined, Get(ta_int8, 32));
+assertEquals(asU16(0), Get(ta_uint16, 0));
+assertEquals(asU16(3), Get(ta_uint16, 3));
+assertEquals(asU16(7), Get(ta_uint16, 7));
+assertEquals(undefined, Get(ta_uint16, 8));
+assertEquals(undefined, Get(ta_uint16, 12));
+assertEquals(asF32(0), Get(ta_float32, 0));
+assertEquals(asF32(3), Get(ta_float32, 3));
+assertEquals(undefined, Get(ta_float32, 4));
+assertEquals(undefined, Get(ta_float32, 12));
+assertEquals(asF64(0), Get(ta_float64, 0));
+assertEquals(asF64(1), Get(ta_float64, 1));
+assertEquals(undefined, Get(ta_float64, 2));
+assertEquals(undefined, Get(ta_float64, 12));
+assertOptimized(Get);
+rab.resize(32);
+FillBuffer(rab);
+assertEquals(0, Get(ta_int8, 0));
+assertEquals(3, Get(ta_int8, 3));
+assertEquals(15, Get(ta_int8, 15));
+assertEquals(undefined, Get(ta_int8, 16));
+assertEquals(undefined, Get(ta_int8, 32));
+assertEquals(asU16(0), Get(ta_uint16, 0));
+assertEquals(asU16(3), Get(ta_uint16, 3));
+assertEquals(asU16(15), Get(ta_uint16, 15));
+assertEquals(undefined, Get(ta_uint16, 16));
+assertEquals(undefined, Get(ta_uint16, 40));
+assertEquals(asF32(0), Get(ta_float32, 0));
+assertEquals(asF32(3), Get(ta_float32, 3));
+assertEquals(undefined, Get(ta_float32, 4));
+assertEquals(undefined, Get(ta_float32, 12));
+assertEquals(asF64(0), Get(ta_float64, 0));
+assertEquals(asF64(1), Get(ta_float64, 1));
+assertEquals(asF64(3), Get(ta_float64, 3));
+assertEquals(undefined, Get(ta_float64, 4));
+assertEquals(undefined, Get(ta_float64, 12));
+assertOptimized(Get);
+rab.resize(9);
+assertEquals(0, Get(ta_int8, 0));
+assertEquals(3, Get(ta_int8, 3));
+assertEquals(15, Get(ta_int8, 15));
+assertEquals(undefined, Get(ta_int8, 16));
+assertEquals(undefined, Get(ta_int8, 32));
+assertEquals(asU16(0), Get(ta_uint16, 0));
+assertEquals(asU16(3), Get(ta_uint16, 3));
+assertEquals(undefined, Get(ta_uint16, 4));
+assertEquals(undefined, Get(ta_uint16, 12));
+assertEquals(asF32(0), Get(ta_float32, 0));
+assertEquals(asF32(3), Get(ta_float32, 3));
+assertEquals(undefined, Get(ta_float32, 4));
+assertEquals(undefined, Get(ta_float32, 12));
+assertEquals(asF64(0), Get(ta_float64, 0));
+assertEquals(undefined, Get(ta_float64, 1));
+assertEquals(undefined, Get(ta_float64, 12));
+assertOptimized(Get);
+
+// Call with a different map to trigger deoptimization. We use this
+// to verify that we have actually specialized on the above maps only.
+let ta_uint8 = new Uint8Array(rab);
+Get(7, Get(ta_uint8, 7));
+assertUnoptimized(Get);
+}());
+
+(function() {
+function Length_TA_RAB_LengthTracking_Mixed(ta) {
+  return ta.length;
+}
+let Length = Length_TA_RAB_LengthTracking_Mixed;
+
+const ab = new ArrayBuffer(32);
+const rab = CreateResizableArrayBuffer(16, 40);
+let ta_int8 = new Int8Array(ab);
+let ta_uint16 = new Uint16Array(rab);
+let ta_float32 = new Float32Array(ab);
+let ta_bigint64 = new BigInt64Array(rab);
+
+// Train with feedback for all elements kinds.
+%PrepareFunctionForOptimization(Length);
+assertEquals(32, Length(ta_int8));
+assertEquals(8, Length(ta_uint16));
+assertEquals(8, Length(ta_float32));
+assertEquals(2, Length(ta_bigint64));
+%OptimizeFunctionOnNextCall(Length);
+assertEquals(32, Length(ta_int8));
+assertEquals(8, Length(ta_uint16));
+assertEquals(8, Length(ta_float32));
+assertEquals(2, Length(ta_bigint64));
+assertOptimized(Length);
+}());
+
+(function() {
+function Length_RAB_GSAB_LengthTrackingWithOffset_Mixed(ta) {
+  return ta.length;
+}
+const Length = Length_RAB_GSAB_LengthTrackingWithOffset_Mixed;
+
+const rab = CreateResizableArrayBuffer(16, 40);
+let ta_int8 = new Int8Array(rab);
+let ta_float64 = new Float64Array(rab);
+
+// Train with feedback for Int8Array and Float64Array.
+%PrepareFunctionForOptimization(Length);
+assertEquals(16, Length(ta_int8));
+assertEquals(2, Length(ta_float64));
+%OptimizeFunctionOnNextCall(Length);
+assertEquals(16, Length(ta_int8));
+assertEquals(2, Length(ta_float64));
+assertOptimized(Length);
+
+let ta_uint32 = new Uint32Array(rab);
+let ta_bigint64 = new BigInt64Array(rab);
+// Calling with Uint32Array will deopt because of the map check on length.
+assertEquals(4, Length(ta_uint32));
+assertUnoptimized(Length);
+%PrepareFunctionForOptimization(Length);
+assertEquals(2, Length(ta_bigint64));
+// Recompile with additional feedback for Uint32Array and BigInt64Array.
+%OptimizeFunctionOnNextCall(Length);
+assertEquals(2, Length(ta_bigint64));
+assertOptimized(Length);
+
+// Length handles all four TypedArrays without deopting.
+assertEquals(16, Length(ta_int8));
+assertEquals(2, Length(ta_float64));
+assertEquals(4, Length(ta_uint32));
+assertEquals(2, Length(ta_bigint64));
+assertOptimized(Length);
+
+// Length handles corresponding gsab-backed TypedArrays without deopting.
+const gsab = CreateGrowableSharedArrayBuffer(16, 40);
+let ta2_uint32 = new Uint32Array(gsab, 8);
+let ta2_float64 = new Float64Array(gsab, 8);
+let ta2_bigint64 = new BigInt64Array(gsab, 8);
+let ta2_int8 = new Int8Array(gsab, 8);
+assertEquals(8, Length(ta2_int8));
+assertEquals(1, Length(ta2_float64));
+assertEquals(2, Length(ta2_uint32));
+assertEquals(1, Length(ta2_bigint64));
+assertOptimized(Length);
+
+// Test Length after rab has been resized to a smaller size.
+rab.resize(5);
+assertEquals(5, Length(ta_int8));
+assertEquals(0, Length(ta_float64));
+assertEquals(1, Length(ta_uint32));
+assertEquals(0, Length(ta_bigint64));
+assertOptimized(Length);
+
+// Test Length after rab has been resized to a larger size.
+rab.resize(40);
+assertEquals(40, Length(ta_int8));
+assertEquals(5, Length(ta_float64));
+assertEquals(10, Length(ta_uint32));
+assertEquals(5, Length(ta_bigint64));
+assertOptimized(Length);
+
+// Test Length after gsab has been grown to a larger size.
+gsab.grow(25);
+assertEquals(17, Length(ta2_int8));
+assertEquals(2, Length(ta2_float64));
+assertEquals(4, Length(ta2_uint32));
+assertEquals(2, Length(ta2_bigint64));
+assertOptimized(Length);
+})();
+
+(function() {
+function Length_AB_RAB_GSAB_LengthTrackingWithOffset_Mixed(ta) {
+  return ta.length;
+}
+const Length = Length_AB_RAB_GSAB_LengthTrackingWithOffset_Mixed;
+
+let ab = new ArrayBuffer(32);
+let rab = CreateResizableArrayBuffer(16, 40);
+let gsab = CreateGrowableSharedArrayBuffer(16, 40);
+
+let ta_ab_int32 = new Int32Array(ab, 8, 3);
+let ta_rab_int32 = new Int32Array(rab, 4);
+let ta_gsab_float64 = new Float64Array(gsab);
+let ta_gsab_bigint64 = new BigInt64Array(gsab, 0, 2);
+
+// Optimize Length with polymorphic feedback.
+%PrepareFunctionForOptimization(Length);
+assertEquals(3, Length(ta_ab_int32));
+assertEquals(3, Length(ta_rab_int32));
+assertEquals(2, Length(ta_gsab_float64));
+assertEquals(2, Length(ta_gsab_bigint64));
+%OptimizeFunctionOnNextCall(Length);
+assertEquals(3, Length(ta_ab_int32));
+assertEquals(3, Length(ta_rab_int32));
+assertEquals(2, Length(ta_gsab_float64));
+assertEquals(2, Length(ta_gsab_bigint64));
+assertOptimized(Length);
+
+// Test resizing and growing the underlying rab/gsab buffers.
+rab.resize(8);
+gsab.grow(36);
+assertEquals(3, Length(ta_ab_int32));
+assertEquals(1, Length(ta_rab_int32));
+assertEquals(4, Length(ta_gsab_float64));
+assertEquals(2, Length(ta_gsab_bigint64));
+assertOptimized(Length);
+
+// Construct additional TypedArrays with the same ElementsKind.
+let ta2_ab_bigint64 = new BigInt64Array(ab, 0, 1);
+let ta2_gsab_int32 = new Int32Array(gsab, 16);
+let ta2_rab_float64 = new Float64Array(rab, 8);
+let ta2_rab_int32 = new Int32Array(rab, 0, 1);
+assertEquals(1, Length(ta2_ab_bigint64));
+assertEquals(5, Length(ta2_gsab_int32));
+assertEquals(0, Length(ta2_rab_float64));
+assertEquals(1, Length(ta2_rab_int32));
+assertOptimized(Length);
+})();
+
+(function() {
+function ByteOffset(ta) {
+  return ta.byteOffset;
+}
+
+const rab = CreateResizableArrayBuffer(16, 40);
+const ta = new Int32Array(rab, 4);
+
+%PrepareFunctionForOptimization(ByteOffset);
+assertEquals(4, ByteOffset(ta));
+assertEquals(4, ByteOffset(ta));
+%OptimizeFunctionOnNextCall(ByteOffset);
+assertEquals(4, ByteOffset(ta));
+assertOptimized(ByteOffset);
 })();
diff -r -u --color up/v8/test/mjsunit/concurrent-initial-prototype-change-1.js nw/v8/test/mjsunit/concurrent-initial-prototype-change-1.js
--- up/v8/test/mjsunit/concurrent-initial-prototype-change-1.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/concurrent-initial-prototype-change-1.js	2023-01-19 16:46:36.839776066 -0500
@@ -52,6 +52,6 @@
 // Sync with background thread to conclude optimization, which bails out
 // due to map dependency.
 %FinalizeOptimization();
-assertUnoptimized(f1, "sync");
+assertUnoptimized(f1);
 // Clear type info for stress runs.
 %ClearFunctionFeedback(f1);
diff -r -u --color up/v8/test/mjsunit/es6/array-iterator-turbo.js nw/v8/test/mjsunit/es6/array-iterator-turbo.js
--- up/v8/test/mjsunit/es6/array-iterator-turbo.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/es6/array-iterator-turbo.js	2023-01-19 16:46:36.850609399 -0500
@@ -107,13 +107,13 @@
       %OptimizeFunctionOnNextCall(fn);
       fn(array);
 
-      assertOptimized(fn, '', key);
+      assertOptimized(fn, key);
       assertEquals(expected, fn(array), key);
-      assertOptimized(fn, '', key);
+      assertOptimized(fn, key);
 
       // Check no deopt when another array with the same map is used
       assertTrue(%HaveSameMap(array, array2), key);
-      assertOptimized(fn, '', key);
+      assertOptimized(fn, key);
       assertEquals(expected2, fn(array2), key);
 
       // CheckMaps bailout
@@ -121,7 +121,7 @@
           [1, 2, 3], 2, { enumerable: false, configurable: false,
                           get() { return 7; } });
       fn(newArray);
-      assertUnoptimized(fn, '', key);
+      assertUnoptimized(fn, key);
     }
   },
 
@@ -210,12 +210,12 @@
       %OptimizeFunctionOnNextCall(sum);
       assertEquals(expected, sum(array), key);
 
-      assertOptimized(sum, '', key);
+      assertOptimized(sum, key);
 
       // Not deoptimized when called on typed array of same type / map
       assertTrue(%HaveSameMap(array, array2));
       assertEquals(expected2, sum(array2), key);
-      assertOptimized(sum, '', key);
+      assertOptimized(sum, key);
 
       // Throw when detached
       let clone = new array.constructor(array);
diff -r -u --color up/v8/test/mjsunit/es6/collections.js nw/v8/test/mjsunit/es6/collections.js
--- up/v8/test/mjsunit/es6/collections.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/es6/collections.js	2023-01-19 16:46:36.850609399 -0500
@@ -77,7 +77,6 @@
   assertThrows(function () { m.set(null, 0) }, TypeError);
   assertThrows(function () { m.set(0, 0) }, TypeError);
   assertThrows(function () { m.set('a-key', 0) }, TypeError);
-  assertThrows(function () { m.set(Symbol(), 0) }, TypeError);
 }
 TestInvalidCalls(new WeakMap);
 
diff -r -u --color up/v8/test/mjsunit/es6/templates.js nw/v8/test/mjsunit/es6/templates.js
--- up/v8/test/mjsunit/es6/templates.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/es6/templates.js	2023-01-19 16:46:36.872276059 -0500
@@ -1,6 +1,8 @@
 // Copyright 2014 the V8 project authors. All rights reserved.
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
+//
+// Flags: --expose-gc
 
 var num = 5;
 var str = "str";
@@ -865,3 +867,33 @@
 assertNotSame(templates[2], templates[4]);
 assertNotSame(templates[3], templates[5]);
 assertSame(templates[4], templates[5]);
+
+// Template objects should be kept alive even if only held weakly, and should
+// preserve values when used as weak keys.
+let weak_templates = new WeakMap();
+let weak_tagged_value_id = 0;
+function weakTag(callSite) {
+  if (weak_templates.has(callSite)) {
+    return weak_templates.get(callSite);
+  }
+  const value = {id: weak_tagged_value_id++};
+  weak_templates.set(callSite, value);
+  return value;
+}
+
+(function () {
+  function valueForTag1(x) {
+    return weakTag`Hello${x}world`;
+  }
+  function valueForTag2(x) {
+    return weakTag`Hello${x}world`;
+  }
+  let valueForTag1_1 = valueForTag1(1);
+  gc();
+  gc();
+  gc();
+  let valueForTag1_2 = valueForTag1(2);
+  let valueForTag2_0 = valueForTag2(0);
+  assertSame(valueForTag1_1, valueForTag1_2);
+  assertNotSame(valueForTag1_1, valueForTag2_0);
+})();
diff -r -u --color up/v8/test/mjsunit/es6/unicode-regexp-ignore-case.js nw/v8/test/mjsunit/es6/unicode-regexp-ignore-case.js
--- up/v8/test/mjsunit/es6/unicode-regexp-ignore-case.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/es6/unicode-regexp-ignore-case.js	2023-01-19 16:46:36.872276059 -0500
@@ -60,3 +60,12 @@
 assertEquals(["s"], /^\u017F/ui.exec("s\u1234"));
 assertEquals(["as"], /^a[\u017F]/ui.exec("as"));
 assertEquals(["as"], /^a[\u017F]/ui.exec("as\u1234"));
+
+// Non-simple mappings created by UnicodeSet::closeOver() requiring special
+// treatment.
+assertFalse(/[\u0390]/ui.test("\u1fd3"));
+assertFalse(/[\u1fd3]/ui.test("\u0390"));
+assertFalse(/[\u03b0]/ui.test("\u1fe3"));
+assertFalse(/[\u1fe3]/ui.test("\u03b0"));
+assertFalse(/[\ufb05]/ui.test("\ufb06"));
+assertFalse(/[\ufb06]/ui.test("\ufb05"));
Only in nw/v8/test/mjsunit/harmony/regress: regress-crbug-1381656.js
diff -r -u --color up/v8/test/mjsunit/harmony/weakrefs/basics.js nw/v8/test/mjsunit/harmony/weakrefs/basics.js
--- up/v8/test/mjsunit/harmony/weakrefs/basics.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/harmony/weakrefs/basics.js	2023-01-19 16:46:36.915609381 -0500
@@ -51,7 +51,6 @@
   assertThrows(() => fg.register(1, "holdings"), TypeError, message);
   assertThrows(() => fg.register(false, "holdings"), TypeError, message);
   assertThrows(() => fg.register("foo", "holdings"), TypeError, message);
-  assertThrows(() => fg.register(Symbol(), "holdings"), TypeError, message);
   assertThrows(() => fg.register(null, "holdings"), TypeError, message);
   assertThrows(() => fg.register(undefined, "holdings"), TypeError, message);
 })();
@@ -97,7 +96,6 @@
   assertThrows(() => fg.unregister(1), TypeError);
   assertThrows(() => fg.unregister(1n), TypeError);
   assertThrows(() => fg.unregister('one'), TypeError);
-  assertThrows(() => fg.unregister(Symbol()), TypeError);
   assertThrows(() => fg.unregister(true), TypeError);
   assertThrows(() => fg.unregister(false), TypeError);
   assertThrows(() => fg.unregister(undefined), TypeError);
@@ -121,7 +119,6 @@
   assertThrows(() => new WeakRef(1), TypeError, message);
   assertThrows(() => new WeakRef(false), TypeError, message);
   assertThrows(() => new WeakRef("foo"), TypeError, message);
-  assertThrows(() => new WeakRef(Symbol()), TypeError, message);
   assertThrows(() => new WeakRef(null), TypeError, message);
   assertThrows(() => new WeakRef(undefined), TypeError, message);
 })();
diff -r -u --color up/v8/test/mjsunit/interrupt-budget-override.js nw/v8/test/mjsunit/interrupt-budget-override.js
--- up/v8/test/mjsunit/interrupt-budget-override.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/interrupt-budget-override.js	2023-01-19 16:46:36.926442711 -0500
@@ -12,7 +12,7 @@
   return s;
 }
 
-%PrepareFunctionForOptimization(f, "allow heuristic optimization");
+%EnsureFeedbackVectorForFunction(f);
 f();
 f();
 f();
Only in nw/v8/test/mjsunit/maglev: int32_constants_in_phi.js
diff -r -u --color up/v8/test/mjsunit/maglev/regress/regress-1364074.js nw/v8/test/mjsunit/maglev/regress/regress-1364074.js
--- up/v8/test/mjsunit/maglev/regress/regress-1364074.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/maglev/regress/regress-1364074.js	2023-01-19 16:46:36.937276041 -0500
@@ -11,7 +11,7 @@
       super();
   }
 };
-for (let i = 0; i < 100; i++) {
+for (let i = 0; i < 10; i++) {
     Class = class extends Class {
       constructor() {
         try {
Only in nw/v8/test/mjsunit/maglev/regress: regress-1381663.js
Only in nw/v8/test/mjsunit/maglev/regress: regress-6373.js
Only in nw/v8/test/mjsunit/maglev: string-at.js
Only in nw/v8/test/mjsunit/maglev: throw-in-cstr.js
Only in nw/v8/test/mjsunit/maglev: unstable-map-transition.js
diff -r -u --color up/v8/test/mjsunit/mjsunit.js nw/v8/test/mjsunit/mjsunit.js
--- up/v8/test/mjsunit/mjsunit.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/mjsunit.js	2023-01-19 16:46:36.948109370 -0500
@@ -196,6 +196,7 @@
   kBaseline: 1 << 15,
   kTopmostFrameIsInterpreted: 1 << 16,
   kTopmostFrameIsBaseline: 1 << 17,
+  kIsLazy: 1 << 18,
 };
 
 // Returns true if --lite-mode is on and we can't ever turn on optimization.
@@ -207,6 +208,9 @@
 // Returns true if --always-turbofan mode is on.
 var isAlwaysOptimize;
 
+// Returns true if given function in lazily compiled.
+var isLazy;
+
 // Returns true if given function in interpreted.
 var isInterpreted;
 
@@ -526,7 +530,7 @@
   };
 
   function executeCode(code) {
-    if (typeof code === 'function')  return code();
+    if (typeof code === 'function') return code();
     if (typeof code === 'string') return eval(code);
     failWithMessage(
         'Given code is neither function nor string, but ' + (typeof code) +
@@ -730,6 +734,7 @@
   assertUnoptimized = function assertUnoptimized(
       fun, name_opt, skip_if_maybe_deopted = true) {
     var opt_status = OptimizationStatus(fun);
+    name_opt = name_opt ?? fun.name;
     // Tests that use assertUnoptimized() do not make sense if --always-turbofan
     // option is provided. Such tests must add --no-always-turbofan to flags comment.
     assertFalse((opt_status & V8OptimizationStatus.kAlwaysOptimize) !== 0,
@@ -749,6 +754,7 @@
   assertOptimized = function assertOptimized(
       fun, name_opt, skip_if_maybe_deopted = true) {
     var opt_status = OptimizationStatus(fun);
+    name_opt = name_opt ?? fun.name;
     // Tests that use assertOptimized() do not make sense for Lite mode where
     // optimization is always disabled, explicitly exit the test with a warning.
     if (opt_status & V8OptimizationStatus.kLiteMode) {
@@ -789,6 +795,13 @@
     return (opt_status & V8OptimizationStatus.kAlwaysOptimize) !== 0;
   }
 
+  isLazy = function isLazy(fun) {
+    var opt_status = OptimizationStatus(fun, '');
+    assertTrue((opt_status & V8OptimizationStatus.kIsFunction) !== 0,
+               "not a function");
+    return (opt_status & V8OptimizationStatus.kIsLazy) !== 0;
+  }
+
   isInterpreted = function isInterpreted(fun) {
     var opt_status = OptimizationStatus(fun, "");
     assertTrue((opt_status & V8OptimizationStatus.kIsFunction) !== 0,
diff -r -u --color up/v8/test/mjsunit/mjsunit.status nw/v8/test/mjsunit/mjsunit.status
--- up/v8/test/mjsunit/mjsunit.status	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/mjsunit.status	2023-01-19 16:46:36.948109370 -0500
@@ -364,6 +364,9 @@
 
   # Flaky tests due to GC interferring with optimization.
   'compiler/regress-crbug-1323114': [SKIP],
+
+  # https://crbug.com/v8/13461
+  'maglev/inner-function': [SKIP],
 }],  # 'gc_stress'
 
 ##############################################################################
@@ -899,6 +902,14 @@
 }],  # 'system == macos'
 
 ##############################################################################
+['system == macos and arch == arm64', {
+  # BUG(v8:13171): The following tests a function that shouldn't be optimized
+  # on M1 hardware, unless a proper fix for the stack corruption is
+  # implemented (see linked issue).
+  'compiler/fast-api-calls-8args': [FAIL],
+}],  # 'system == macos  and arch == arm64'
+
+##############################################################################
 ['system == windows', {
   # Too slow with turbo fan.
   'math-floor-of-div': [PASS, ['mode == debug', SKIP]],
@@ -1082,6 +1093,9 @@
 
   # BUG(v8:13331) Skipped until issue is fixed to reduce noise on alerts.
   'harmony/regress/regress-crbug-1367133': [SKIP],
+
+  # BUG(v8:13379) maglev-inlining flag isn't stable enough for fuzzing.
+  'maglev/eager-deopt-in-inline': [SKIP],
 }], # gc_fuzzer or deopt_fuzzer or interrupt_fuzzer
 
 ##############################################################################
@@ -1571,6 +1585,14 @@
 }], # single_generation
 
 ################################################################################
+['conservative_stack_scanning', {
+  # TODO(v8:13257): Conservative stack scanning is not currently compatible
+  # with stack switching.
+  'wasm/stack-switching': [SKIP],
+  'wasm/stack-switching-export': [SKIP],
+}], # conservative_stack_scanning
+
+################################################################################
 ['third_party_heap', {
   # Requires local heaps
   'const-field-tracking': [SKIP],
Only in nw/v8/test/mjsunit/regress: regress-1364429.js
diff -r -u --color up/v8/test/mjsunit/regress/regress-1371935.js nw/v8/test/mjsunit/regress/regress-1371935.js
--- up/v8/test/mjsunit/regress/regress-1371935.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/regress-1371935.js	2023-01-19 16:46:37.013109354 -0500
@@ -8,7 +8,7 @@
   // CheckBigInt64 is required if the type of input is UnsignedBigInt64
   // because its value can be out of the range of SignedBigInt64.
   let t = BigInt.asUintN(64, a + b);
-  // The addition is speculated as CheckedBigInt64Add and triggers the deopt
+  // The addition is speculated as CheckedInt64Add and triggers the deopt
   // for the large value coming in through <t>.
   return t + c;
 }
Only in nw/v8/test/mjsunit/regress: regress-1376784.js
Only in nw/v8/test/mjsunit/regress: regress-1378439.js
Only in nw/v8/test/mjsunit/regress: regress-1379738.js
Only in nw/v8/test/mjsunit/regress: regress-1380337.js
Only in nw/v8/test/mjsunit/regress: regress-1380398.js
Only in nw/v8/test/mjsunit/regress: regress-crbug-1374746.js
Only in nw/v8/test/mjsunit/regress: regress-crbug-1374995.js
Only in nw/v8/test/mjsunit/regress: regress-crbug-1375073.js
Only in nw/v8/test/mjsunit/regress: regress-crbug-1377840.js
Only in nw/v8/test/mjsunit/regress: regress-crbug-1381064.js
Only in nw/v8/test/mjsunit/regress: regress-crbug-1394973.js
Only in nw/v8/test/mjsunit/regress: regress-v8-13190.js
Only in nw/v8/test/mjsunit/regress: regress-v8-13445.js
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-1010272.js nw/v8/test/mjsunit/regress/wasm/regress-1010272.js
--- up/v8/test/mjsunit/regress/wasm/regress-1010272.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-1010272.js	2023-01-19 16:46:37.186442640 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --wasm-grow-shared-memory --experimental-wasm-threads
+// Flags: --wasm-grow-shared-memory
 
 const kNumWorkers = 100;
 const kNumMessages = 50;
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-10309.js nw/v8/test/mjsunit/regress/wasm/regress-10309.js
--- up/v8/test/mjsunit/regress/wasm/regress-10309.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-10309.js	2023-01-19 16:46:37.186442640 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-simd
-
 let registry = {};
 
 function module(bytes, valid = true) {
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-1054466.js nw/v8/test/mjsunit/regress/wasm/regress-1054466.js
--- up/v8/test/mjsunit/regress/wasm/regress-1054466.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-1054466.js	2023-01-19 16:46:37.186442640 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-simd
-
 d8.file.execute('test/mjsunit/wasm/wasm-module-builder.js');
 
 const builder = new WasmModuleBuilder();
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-1065599.js nw/v8/test/mjsunit/regress/wasm/regress-1065599.js
--- up/v8/test/mjsunit/regress/wasm/regress-1065599.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-1065599.js	2023-01-19 16:46:37.186442640 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-simd
-
 d8.file.execute('test/mjsunit/wasm/wasm-module-builder.js');
 
 const builder = new WasmModuleBuilder();
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-1067621.js nw/v8/test/mjsunit/regress/wasm/regress-1067621.js
--- up/v8/test/mjsunit/regress/wasm/regress-1067621.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-1067621.js	2023-01-19 16:46:37.186442640 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-threads
-
 d8.file.execute('test/mjsunit/wasm/wasm-module-builder.js');
 
 const kNumberOfWorker = 4;
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-1070078.js nw/v8/test/mjsunit/regress/wasm/regress-1070078.js
--- up/v8/test/mjsunit/regress/wasm/regress-1070078.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-1070078.js	2023-01-19 16:46:37.186442640 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-simd
-
 d8.file.execute('test/mjsunit/wasm/wasm-module-builder.js');
 
 const builder = new WasmModuleBuilder();
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-1080902.js nw/v8/test/mjsunit/regress/wasm/regress-1080902.js
--- up/v8/test/mjsunit/regress/wasm/regress-1080902.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-1080902.js	2023-01-19 16:46:37.186442640 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-threads
-
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 
 let memory = new WebAssembly.Memory({
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-1081030.js nw/v8/test/mjsunit/regress/wasm/regress-1081030.js
--- up/v8/test/mjsunit/regress/wasm/regress-1081030.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-1081030.js	2023-01-19 16:46:37.186442640 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-simd
-
 d8.file.execute('test/mjsunit/wasm/wasm-module-builder.js');
 
 const builder = new WasmModuleBuilder();
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-10831.js nw/v8/test/mjsunit/regress/wasm/regress-10831.js
--- up/v8/test/mjsunit/regress/wasm/regress-10831.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-10831.js	2023-01-19 16:46:37.186442640 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-simd
-
 d8.file.execute('test/mjsunit/wasm/wasm-module-builder.js');
 
 // This test is shrunk from a test case provided at https://crbug.com/v8/10831.
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-1111522.js nw/v8/test/mjsunit/regress/wasm/regress-1111522.js
--- up/v8/test/mjsunit/regress/wasm/regress-1111522.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-1111522.js	2023-01-19 16:46:37.186442640 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-simd
-
 d8.file.execute('test/mjsunit/wasm/wasm-module-builder.js');
 
 // Regression test to exercise Liftoff's i64x2.shr_s codegen, which back up rcx
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-1112124.js nw/v8/test/mjsunit/regress/wasm/regress-1112124.js
--- up/v8/test/mjsunit/regress/wasm/regress-1112124.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-1112124.js	2023-01-19 16:46:37.186442640 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-simd
-
 d8.file.execute('test/mjsunit/wasm/wasm-module-builder.js');
 
 const builder = new WasmModuleBuilder();
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-1116019.js nw/v8/test/mjsunit/regress/wasm/regress-1116019.js
--- up/v8/test/mjsunit/regress/wasm/regress-1116019.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-1116019.js	2023-01-19 16:46:37.186442640 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-simd
-
 d8.file.execute('test/mjsunit/wasm/wasm-module-builder.js');
 
 const builder = new WasmModuleBuilder();
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-1124885.js nw/v8/test/mjsunit/regress/wasm/regress-1124885.js
--- up/v8/test/mjsunit/regress/wasm/regress-1124885.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-1124885.js	2023-01-19 16:46:37.186442640 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-simd
-
 // This exercises an bug in scalar-lowering for load transforms. In
 // particular, if the index input to v128.load32_splat was a extract_lane, the
 // input wasn't correctly lowered. This caused the extract_lane node to stick
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-1125951.js nw/v8/test/mjsunit/regress/wasm/regress-1125951.js
--- up/v8/test/mjsunit/regress/wasm/regress-1125951.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-1125951.js	2023-01-19 16:46:37.186442640 -0500
@@ -3,7 +3,6 @@
 // found in the LICENSE file.
 
 // Flags: --expose-wasm --liftoff --no-wasm-tier-up --print-code --wasm-staging
-// Flags: --experimental-wasm-threads
 
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-1137608.js nw/v8/test/mjsunit/regress/wasm/regress-1137608.js
--- up/v8/test/mjsunit/regress/wasm/regress-1137608.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-1137608.js	2023-01-19 16:46:37.186442640 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 //
-// Flags: --no-liftoff --experimental-wasm-return-call --experimental-wasm-threads
+// Flags: --no-liftoff --experimental-wasm-return-call
 
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-11472.js nw/v8/test/mjsunit/regress/wasm/regress-11472.js
--- up/v8/test/mjsunit/regress/wasm/regress-11472.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-11472.js	2023-01-19 16:46:37.186442640 -0500
@@ -1,8 +1,6 @@
 // Copyright 2021 the V8 project authors. All rights reserved.
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
-//
-// Flags: --experimental-wasm-eh
 
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-1153442.js nw/v8/test/mjsunit/regress/wasm/regress-1153442.js
--- up/v8/test/mjsunit/regress/wasm/regress-1153442.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-1153442.js	2023-01-19 16:46:37.186442640 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-threads
-
 d8.file.execute('test/mjsunit/wasm/wasm-module-builder.js');
 
 const builder = new WasmModuleBuilder();
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-1161555.js nw/v8/test/mjsunit/regress/wasm/regress-1161555.js
--- up/v8/test/mjsunit/regress/wasm/regress-1161555.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-1161555.js	2023-01-19 16:46:37.197275970 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-simd --wasm-lazy-compilation
+// Flags: --wasm-lazy-compilation
 
 // Test case copied from clusterfuzz, this exercises a bug in WasmCompileLazy
 // where we are not correctly pushing the full 128-bits of a SIMD register.
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-1188825.js nw/v8/test/mjsunit/regress/wasm/regress-1188825.js
--- up/v8/test/mjsunit/regress/wasm/regress-1188825.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-1188825.js	2023-01-19 16:46:37.197275970 -0500
@@ -2,7 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-eh
 d8.file.execute('test/mjsunit/wasm/wasm-module-builder.js')
 let obj = {};
 let proxy = new Proxy(obj, {});
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-1188975.js nw/v8/test/mjsunit/regress/wasm/regress-1188975.js
--- up/v8/test/mjsunit/regress/wasm/regress-1188975.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-1188975.js	2023-01-19 16:46:37.197275970 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-eh
-
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 
 (function Regress1188975() {
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-1189454.js nw/v8/test/mjsunit/regress/wasm/regress-1189454.js
--- up/v8/test/mjsunit/regress/wasm/regress-1189454.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-1189454.js	2023-01-19 16:46:37.197275970 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-gc --experimental-wasm-threads
+// Flags: --experimental-wasm-gc
 
 // During Turbofan optimizations, when a TrapIf/Unless node is found to always
 // trap, its uses need to be marked as dead. However, in the case that one of
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-12789.js nw/v8/test/mjsunit/regress/wasm/regress-12789.js
--- up/v8/test/mjsunit/regress/wasm/regress-12789.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-12789.js	2023-01-19 16:46:37.197275970 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-gc --wasm-gc-js-interop
+// Flags: --experimental-wasm-gc
 
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-12874.js nw/v8/test/mjsunit/regress/wasm/regress-12874.js
--- up/v8/test/mjsunit/regress/wasm/regress-12874.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-12874.js	2023-01-19 16:46:37.197275970 -0500
@@ -9,12 +9,12 @@
 
 var builder = new WasmModuleBuilder();
 
-var sig_index = builder.addType({params: [kWasmDataRef], results: [kWasmI32]});
+var sig_index = builder.addType({params: [kWasmStructRef], results: [kWasmI32]});
 
 var sub1 = builder.addStruct([makeField(kWasmI32, true)]);
 var sub2 = builder.addStruct([makeField(kWasmI32, false)]);
 
-builder.addFunction('producer', makeSig([], [kWasmDataRef]))
+builder.addFunction('producer', makeSig([], [kWasmStructRef]))
   .addBody([
     kExprI32Const, 10,
     kGCPrefix, kExprStructNew, sub1])
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-13061.js nw/v8/test/mjsunit/regress/wasm/regress-13061.js
--- up/v8/test/mjsunit/regress/wasm/regress-13061.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-13061.js	2023-01-19 16:46:37.197275970 -0500
@@ -10,7 +10,7 @@
 
 builder.addFunction('repro', kSig_v_v)
   .exportFunc()
-  .addLocals(wasmRefNullType(kWasmDataRef), 1)
+  .addLocals(wasmRefNullType(kWasmStructRef), 1)
   .addBody([
     kExprI32Const, 0,
     kExprIf, kWasmVoid,
Only in up/v8/test/mjsunit/regress/wasm: regress-13123.js
Only in nw/v8/test/mjsunit/regress/wasm: regress-1374535.js
Only in nw/v8/test/mjsunit/regress/wasm: regress-1379364.js
Only in nw/v8/test/mjsunit/regress/wasm: regress-1380498.js
Only in nw/v8/test/mjsunit/regress/wasm: regress-1380646.js
Only in up/v8/test/mjsunit/regress/wasm: regress-763697.js
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-8094.js nw/v8/test/mjsunit/regress/wasm/regress-8094.js
--- up/v8/test/mjsunit/regress/wasm/regress-8094.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-8094.js	2023-01-19 16:46:37.208109300 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --expose-wasm --experimental-wasm-eh
-
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 
 // Instantiate a throwing module.
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-8095.js nw/v8/test/mjsunit/regress/wasm/regress-8095.js
--- up/v8/test/mjsunit/regress/wasm/regress-8095.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-8095.js	2023-01-19 16:46:37.208109300 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --expose-wasm --experimental-wasm-eh
-
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 
 // Prepare a special error object to throw.
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-8533.js nw/v8/test/mjsunit/regress/wasm/regress-8533.js
--- up/v8/test/mjsunit/regress/wasm/regress-8533.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-8533.js	2023-01-19 16:46:37.208109300 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --allow-natives-syntax --experimental-wasm-threads
+// Flags: --allow-natives-syntax
 
 d8.file.execute('test/mjsunit/wasm/wasm-module-builder.js');
 
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-853453.js nw/v8/test/mjsunit/regress/wasm/regress-853453.js
--- up/v8/test/mjsunit/regress/wasm/regress-853453.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-853453.js	2023-01-19 16:46:37.208109300 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-threads
-
 assertThrows(() => new WebAssembly.Module(
     new Uint8Array([
       0x00, 0x61, 0x73, 0x6d,     // wasm magic
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-8846.js nw/v8/test/mjsunit/regress/wasm/regress-8846.js
--- up/v8/test/mjsunit/regress/wasm/regress-8846.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-8846.js	2023-01-19 16:46:37.208109300 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-eh --wasm-test-streaming
+// Flags: --wasm-test-streaming
 
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-8896.js nw/v8/test/mjsunit/regress/wasm/regress-8896.js
--- up/v8/test/mjsunit/regress/wasm/regress-8896.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-8896.js	2023-01-19 16:46:37.208109300 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-eh --allow-natives-syntax
+// Flags: --allow-natives-syntax
 // Force TurboFan code for serialization.
 // Flags: --no-liftoff --no-wasm-lazy-compilation
 
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-9425.js nw/v8/test/mjsunit/regress/wasm/regress-9425.js
--- up/v8/test/mjsunit/regress/wasm/regress-9425.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-9425.js	2023-01-19 16:46:37.208109300 -0500
@@ -1,7 +1,6 @@
 // Copyright 2019 the V8 project authors. All rights reserved.
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
-// Flags: --experimental-wasm-threads
 
 d8.file.execute('test/mjsunit/wasm/wasm-module-builder.js');
 
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-9447.js nw/v8/test/mjsunit/regress/wasm/regress-9447.js
--- up/v8/test/mjsunit/regress/wasm/regress-9447.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-9447.js	2023-01-19 16:46:37.208109300 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-simd
-
 d8.file.execute('test/mjsunit/wasm/wasm-module-builder.js');
 
 let kSig_s_v = makeSig([], [kWasmS128]);
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress-9832.js nw/v8/test/mjsunit/regress/wasm/regress-9832.js
--- up/v8/test/mjsunit/regress/wasm/regress-9832.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress-9832.js	2023-01-19 16:46:37.218942630 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-eh
-
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 
 (function TestRegress9832() {
diff -r -u --color up/v8/test/mjsunit/regress/wasm/regress1192313.js nw/v8/test/mjsunit/regress/wasm/regress1192313.js
--- up/v8/test/mjsunit/regress/wasm/regress1192313.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/regress1192313.js	2023-01-19 16:46:37.218942630 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-eh --experimental-wasm-threads
-
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 
 (function Regress1192313() {
diff -r -u --color up/v8/test/mjsunit/regress/wasm/typecheck-null-undefined.js nw/v8/test/mjsunit/regress/wasm/typecheck-null-undefined.js
--- up/v8/test/mjsunit/regress/wasm/typecheck-null-undefined.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/regress/wasm/typecheck-null-undefined.js	2023-01-19 16:46:37.218942630 -0500
@@ -9,7 +9,7 @@
 var builder = new WasmModuleBuilder();
 
 var sig_index = builder.addType(
-    {params: [wasmRefType(kWasmDataRef)], results: []});
+    {params: [wasmRefType(kWasmStructRef)], results: []});
 
 builder.addFunction('main', sig_index).addBody([]).exportFunc();
 
Only in nw/v8/test/mjsunit: regress-1358505.js
Only in nw/v8/test/mjsunit: regress-crbug-1374042.js
Only in nw/v8/test/mjsunit/shared-memory: shared-struct-property-storage.js
diff -r -u --color up/v8/test/mjsunit/shared-memory/shared-struct-surface.js nw/v8/test/mjsunit/shared-memory/shared-struct-surface.js
--- up/v8/test/mjsunit/shared-memory/shared-struct-surface.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/shared-memory/shared-struct-surface.js	2023-01-19 16:46:37.218942630 -0500
@@ -49,11 +49,13 @@
 })();
 
 (function TestTooManyFields() {
-  let field_names = [];
+  let fieldNames = [];
   for (let i = 0; i < 1000; i++) {
-    field_names.push('field' + i);
+    fieldNames.push('field' + i);
   }
-  assertThrows(() => { new SharedStructType(field_names); });
+  assertThrows(() => {
+    new SharedStructType(fieldNames);
+  });
 })();
 
 (function TestOwnPropertyEnumeration() {
@@ -76,3 +78,19 @@
   assertEquals(1, entries.length);
   assertArrayEquals(['field', 42], entries[0]);
 })();
+
+(function TestForIn() {
+  let fieldNames = [];
+  for (let i = 0; i < 512; i++) {
+    fieldNames.push('field' + i);
+  }
+  let S2 = new SharedStructType(fieldNames);
+  let s = new S2();
+  let propNames = [];
+  for (let prop in s) propNames.push(prop);
+  assertArrayEquals(propNames, fieldNames);
+})();
+
+(function TestDuplicateFieldNames() {
+  assertThrows(() => new SharedStructType(['same', 'same']));
+})();
Only in up/v8/test/mjsunit/shared-memory: shared-struct-without-map-space.js
diff -r -u --color up/v8/test/mjsunit/temporal/calendar-constructor.js nw/v8/test/mjsunit/temporal/calendar-constructor.js
--- up/v8/test/mjsunit/temporal/calendar-constructor.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/temporal/calendar-constructor.js	2023-01-19 16:46:37.218942630 -0500
@@ -10,7 +10,6 @@
 
 assertThrows(() => new Temporal.Calendar(), RangeError);
 
-// Wrong case
-assertThrows(() => new Temporal.Calendar("ISO8601"), RangeError);
+assertEquals("iso8601", (new Temporal.Calendar("IsO8601")).id)
 
 assertEquals("iso8601", (new Temporal.Calendar("iso8601")).id)
diff -r -u --color up/v8/test/mjsunit/typedarray-growablesharedarraybuffer.js nw/v8/test/mjsunit/typedarray-growablesharedarraybuffer.js
--- up/v8/test/mjsunit/typedarray-growablesharedarraybuffer.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/typedarray-growablesharedarraybuffer.js	2023-01-19 16:46:37.240609293 -0500
@@ -2,8 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --harmony-rab-gsab --allow-natives-syntax
-// Flags: --harmony-relative-indexing-methods --harmony-array-find-last
+// Flags: --harmony-rab-gsab --allow-natives-syntax --harmony-array-find-last
 
 "use strict";
 
@@ -142,6 +141,25 @@
     assertEquals([3, 4, 5, 6],
                  ToNumbers(new targetCtor(lengthTrackingWithOffset)));
   });
+
+  AllBigIntUnmatchedCtorCombinations((targetCtor, sourceCtor) => {
+    const gsab = CreateGrowableSharedArrayBuffer(
+        4 * sourceCtor.BYTES_PER_ELEMENT,
+        8 * sourceCtor.BYTES_PER_ELEMENT);
+    const fixedLength = new sourceCtor(gsab, 0, 4);
+    const fixedLengthWithOffset = new sourceCtor(
+        gsab, 2 * sourceCtor.BYTES_PER_ELEMENT, 2);
+    const lengthTracking = new sourceCtor(gsab, 0);
+    const lengthTrackingWithOffset = new sourceCtor(
+        gsab, 2 * sourceCtor.BYTES_PER_ELEMENT);
+
+    assertThrows(() => { new targetCtor(fixedLength); }, TypeError);
+    assertThrows(() => { new targetCtor(fixedLengthWithOffset); }, TypeError);
+    assertThrows(() => { new targetCtor(lengthTracking); }, TypeError);
+    assertThrows(() => { new targetCtor(lengthTrackingWithOffset); },
+                 TypeError);
+  });
+
 })();
 
 (function ConstructFromTypedArraySpeciesConstructorNotCalled() {
@@ -3907,3 +3925,72 @@
                  ToNumbers(func.apply(null, lengthTrackingWithOffset)));
   }
 })();
+
+(function TypedArrayFrom() {
+  AllBigIntMatchedCtorCombinations((targetCtor, sourceCtor) => {
+    const gsab = CreateGrowableSharedArrayBuffer(
+        4 * sourceCtor.BYTES_PER_ELEMENT,
+        8 * sourceCtor.BYTES_PER_ELEMENT);
+    const fixedLength = new sourceCtor(gsab, 0, 4);
+    const fixedLengthWithOffset = new sourceCtor(
+        gsab, 2 * sourceCtor.BYTES_PER_ELEMENT, 2);
+    const lengthTracking = new sourceCtor(gsab, 0);
+    const lengthTrackingWithOffset = new sourceCtor(
+        gsab, 2 * sourceCtor.BYTES_PER_ELEMENT);
+
+    // Write some data into the array.
+    const taFull = new sourceCtor(gsab);
+    for (let i = 0; i < 4; ++i) {
+      WriteToTypedArray(taFull, i, i + 1);
+    }
+
+    // Orig. array: [1, 2, 3, 4]
+    //              [1, 2, 3, 4] << fixedLength
+    //                    [3, 4] << fixedLengthWithOffset
+    //              [1, 2, 3, 4, ...] << lengthTracking
+    //                    [3, 4, ...] << lengthTrackingWithOffset
+
+    assertEquals([1, 2, 3, 4], ToNumbers(targetCtor.from(fixedLength)));
+    assertEquals([3, 4], ToNumbers(targetCtor.from(fixedLengthWithOffset)));
+    assertEquals([1, 2, 3, 4], ToNumbers(targetCtor.from(lengthTracking)));
+    assertEquals([3, 4], ToNumbers(targetCtor.from(lengthTrackingWithOffset)));
+
+    // Grow.
+    gsab.grow(6 * sourceCtor.BYTES_PER_ELEMENT);
+
+    for (let i = 0; i < 6; ++i) {
+      WriteToTypedArray(taFull, i, i + 1);
+    }
+
+    // Orig. array: [1, 2, 3, 4, 5, 6]
+    //              [1, 2, 3, 4] << fixedLength
+    //                    [3, 4] << fixedLengthWithOffset
+    //              [1, 2, 3, 4, 5, 6, ...] << lengthTracking
+    //                    [3, 4, 5, 6, ...] << lengthTrackingWithOffset
+
+    assertEquals([1, 2, 3, 4], ToNumbers(targetCtor.from(fixedLength)));
+    assertEquals([3, 4], ToNumbers(targetCtor.from(fixedLengthWithOffset)));
+    assertEquals([1, 2, 3, 4, 5, 6],
+                 ToNumbers(targetCtor.from(lengthTracking)));
+    assertEquals([3, 4, 5, 6],
+                 ToNumbers(targetCtor.from(lengthTrackingWithOffset)));
+  });
+
+  AllBigIntUnmatchedCtorCombinations((targetCtor, sourceCtor) => {
+    const gsab = CreateGrowableSharedArrayBuffer(
+        4 * sourceCtor.BYTES_PER_ELEMENT,
+        8 * sourceCtor.BYTES_PER_ELEMENT);
+    const fixedLength = new sourceCtor(gsab, 0, 4);
+    const fixedLengthWithOffset = new sourceCtor(
+        gsab, 2 * sourceCtor.BYTES_PER_ELEMENT, 2);
+    const lengthTracking = new sourceCtor(gsab, 0);
+    const lengthTrackingWithOffset = new sourceCtor(
+        gsab, 2 * sourceCtor.BYTES_PER_ELEMENT);
+
+    assertThrows(() => { targetCtor.from(fixedLength); }, TypeError);
+    assertThrows(() => { targetCtor.from(fixedLengthWithOffset); }, TypeError);
+    assertThrows(() => { targetCtor.from(lengthTracking); }, TypeError);
+    assertThrows(() => { targetCtor.from(lengthTrackingWithOffset); },
+                 TypeError);
+  });
+})();
diff -r -u --color up/v8/test/mjsunit/typedarray-helpers.js nw/v8/test/mjsunit/typedarray-helpers.js
--- up/v8/test/mjsunit/typedarray-helpers.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/typedarray-helpers.js	2023-01-19 16:46:37.240609293 -0500
@@ -91,6 +91,18 @@
   }
 }
 
+function AllBigIntUnmatchedCtorCombinations(test) {
+  for (let targetCtor of ctors) {
+    for (let sourceCtor of ctors) {
+      if (IsBigIntTypedArray(new targetCtor()) ==
+          IsBigIntTypedArray(new sourceCtor())) {
+        continue;
+      }
+      test(targetCtor, sourceCtor);
+    }
+  }
+}
+
 function ReadDataFromBuffer(ab, ctor) {
   let result = [];
   const ta = new ctor(ab, 0, ab.byteLength / ctor.BYTES_PER_ELEMENT);
diff -r -u --color up/v8/test/mjsunit/typedarray-resizablearraybuffer-detach.js nw/v8/test/mjsunit/typedarray-resizablearraybuffer-detach.js
--- up/v8/test/mjsunit/typedarray-resizablearraybuffer-detach.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/typedarray-resizablearraybuffer-detach.js	2023-01-19 16:46:37.240609293 -0500
@@ -1763,3 +1763,45 @@
 }
 AtParameterConversionDetaches(TypedArrayAtHelper);
 AtParameterConversionDetaches(ArrayAtHelper);
+
+(function TypedArrayFrom() {
+  AllBigIntMatchedCtorCombinations((targetCtor, sourceCtor) => {
+    const rab = CreateResizableArrayBuffer(
+        4 * sourceCtor.BYTES_PER_ELEMENT,
+        8 * sourceCtor.BYTES_PER_ELEMENT);
+    const fixedLength = new sourceCtor(rab, 0, 4);
+    const fixedLengthWithOffset = new sourceCtor(
+        rab, 2 * sourceCtor.BYTES_PER_ELEMENT, 2);
+    const lengthTracking = new sourceCtor(rab, 0);
+    const lengthTrackingWithOffset = new sourceCtor(
+        rab, 2 * sourceCtor.BYTES_PER_ELEMENT);
+
+    %ArrayBufferDetach(rab);
+
+    assertThrows(() => { targetCtor.from(fixedLength); }, TypeError);
+    assertThrows(() => { targetCtor.from(fixedLengthWithOffset); }, TypeError);
+    assertThrows(() => { targetCtor.from(lengthTracking); }, TypeError);
+    assertThrows(() => { targetCtor.from(lengthTrackingWithOffset); },
+                 TypeError);
+  });
+
+  AllBigIntUnmatchedCtorCombinations((targetCtor, sourceCtor) => {
+    const rab = CreateResizableArrayBuffer(
+        4 * sourceCtor.BYTES_PER_ELEMENT,
+        8 * sourceCtor.BYTES_PER_ELEMENT);
+    const fixedLength = new sourceCtor(rab, 0, 4);
+    const fixedLengthWithOffset = new sourceCtor(
+        rab, 2 * sourceCtor.BYTES_PER_ELEMENT, 2);
+    const lengthTracking = new sourceCtor(rab, 0);
+    const lengthTrackingWithOffset = new sourceCtor(
+        rab, 2 * sourceCtor.BYTES_PER_ELEMENT);
+
+    %ArrayBufferDetach(rab);
+
+    assertThrows(() => { targetCtor.from(fixedLength); }, TypeError);
+    assertThrows(() => { targetCtor.from(fixedLengthWithOffset); }, TypeError);
+    assertThrows(() => { targetCtor.from(lengthTracking); }, TypeError);
+    assertThrows(() => { targetCtor.from(lengthTrackingWithOffset); },
+                 TypeError);
+  });
+})();
diff -r -u --color up/v8/test/mjsunit/typedarray-resizablearraybuffer.js nw/v8/test/mjsunit/typedarray-resizablearraybuffer.js
--- up/v8/test/mjsunit/typedarray-resizablearraybuffer.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/typedarray-resizablearraybuffer.js	2023-01-19 16:46:37.240609293 -0500
@@ -172,6 +172,24 @@
     assertEquals([3, 4, 5, 6],
                  ToNumbers(new targetCtor(lengthTrackingWithOffset)));
   });
+
+  AllBigIntUnmatchedCtorCombinations((targetCtor, sourceCtor) => {
+    const rab = CreateResizableArrayBuffer(
+        4 * sourceCtor.BYTES_PER_ELEMENT,
+        8 * sourceCtor.BYTES_PER_ELEMENT);
+    const fixedLength = new sourceCtor(rab, 0, 4);
+    const fixedLengthWithOffset = new sourceCtor(
+        rab, 2 * sourceCtor.BYTES_PER_ELEMENT, 2);
+    const lengthTracking = new sourceCtor(rab, 0);
+    const lengthTrackingWithOffset = new sourceCtor(
+        rab, 2 * sourceCtor.BYTES_PER_ELEMENT);
+
+    assertThrows(() => { new targetCtor(fixedLength); }, TypeError);
+    assertThrows(() => { new targetCtor(fixedLengthWithOffset); }, TypeError);
+    assertThrows(() => { new targetCtor(lengthTracking); }, TypeError);
+    assertThrows(() => { new targetCtor(lengthTrackingWithOffset); },
+                 TypeError);
+  });
 })();
 
 (function TypedArrayLengthWhenResizedOutOfBounds1() {
@@ -7646,3 +7664,102 @@
                  ToNumbers(func.apply(null, lengthTrackingWithOffset)));
   }
 })();
+
+(function TypedArrayFrom() {
+  AllBigIntMatchedCtorCombinations((targetCtor, sourceCtor) => {
+    const rab = CreateResizableArrayBuffer(
+        4 * sourceCtor.BYTES_PER_ELEMENT,
+        8 * sourceCtor.BYTES_PER_ELEMENT);
+    const fixedLength = new sourceCtor(rab, 0, 4);
+    const fixedLengthWithOffset = new sourceCtor(
+        rab, 2 * sourceCtor.BYTES_PER_ELEMENT, 2);
+    const lengthTracking = new sourceCtor(rab, 0);
+    const lengthTrackingWithOffset = new sourceCtor(
+        rab, 2 * sourceCtor.BYTES_PER_ELEMENT);
+
+    // Write some data into the array.
+    const taFull = new sourceCtor(rab);
+    for (let i = 0; i < 4; ++i) {
+      WriteToTypedArray(taFull, i, i + 1);
+    }
+
+    // Orig. array: [1, 2, 3, 4]
+    //              [1, 2, 3, 4] << fixedLength
+    //                    [3, 4] << fixedLengthWithOffset
+    //              [1, 2, 3, 4, ...] << lengthTracking
+    //                    [3, 4, ...] << lengthTrackingWithOffset
+
+    assertEquals([1, 2, 3, 4], ToNumbers(targetCtor.from(fixedLength)));
+    assertEquals([3, 4], ToNumbers(targetCtor.from(fixedLengthWithOffset)));
+    assertEquals([1, 2, 3, 4], ToNumbers(targetCtor.from(lengthTracking)));
+    assertEquals([3, 4], ToNumbers(targetCtor.from(lengthTrackingWithOffset)));
+
+    // Shrink so that fixed length TAs go out of bounds.
+    rab.resize(3 * sourceCtor.BYTES_PER_ELEMENT);
+
+    // Orig. array: [1, 2, 3]
+    //              [1, 2, 3, ...] << lengthTracking
+    //                    [3, ...] << lengthTrackingWithOffset
+
+    assertThrows(() => { targetCtor.from(fixedLength); }, TypeError);
+    assertThrows(() => { targetCtor.from(fixedLengthWithOffset); }, TypeError);
+    assertEquals([1, 2, 3], ToNumbers(targetCtor.from(lengthTracking)));
+    assertEquals([3], ToNumbers(targetCtor.from(lengthTrackingWithOffset)));
+
+    // Shrink so that the TAs with offset go out of bounds.
+    rab.resize(1 * sourceCtor.BYTES_PER_ELEMENT);
+
+    assertThrows(() => { targetCtor.from(fixedLength); }, TypeError);
+    assertThrows(() => { targetCtor.from(fixedLengthWithOffset); }, TypeError);
+    assertEquals([1], ToNumbers(targetCtor.from(lengthTracking)));
+    assertThrows(() => { targetCtor.from(lengthTrackingWithOffset); },
+                 TypeError);
+
+    // Shrink to zero.
+    rab.resize(0);
+
+    assertThrows(() => { targetCtor.from(fixedLength); }, TypeError);
+    assertThrows(() => { targetCtor.from(fixedLengthWithOffset); }, TypeError);
+    assertEquals([], ToNumbers(targetCtor.from(lengthTracking)));
+    assertThrows(() => { targetCtor.from(lengthTrackingWithOffset); },
+                 TypeError);
+
+    // Grow so that all TAs are back in-bounds.
+    rab.resize(6 * sourceCtor.BYTES_PER_ELEMENT);
+
+    for (let i = 0; i < 6; ++i) {
+      WriteToTypedArray(taFull, i, i + 1);
+    }
+
+    // Orig. array: [1, 2, 3, 4, 5, 6]
+    //              [1, 2, 3, 4] << fixedLength
+    //                    [3, 4] << fixedLengthWithOffset
+    //              [1, 2, 3, 4, 5, 6, ...] << lengthTracking
+    //                    [3, 4, 5, 6, ...] << lengthTrackingWithOffset
+
+    assertEquals([1, 2, 3, 4], ToNumbers(targetCtor.from(fixedLength)));
+    assertEquals([3, 4], ToNumbers(targetCtor.from(fixedLengthWithOffset)));
+    assertEquals([1, 2, 3, 4, 5, 6],
+                 ToNumbers(targetCtor.from(lengthTracking)));
+    assertEquals([3, 4, 5, 6],
+                 ToNumbers(targetCtor.from(lengthTrackingWithOffset)));
+  });
+
+  AllBigIntUnmatchedCtorCombinations((targetCtor, sourceCtor) => {
+    const rab = CreateResizableArrayBuffer(
+        4 * sourceCtor.BYTES_PER_ELEMENT,
+        8 * sourceCtor.BYTES_PER_ELEMENT);
+    const fixedLength = new sourceCtor(rab, 0, 4);
+    const fixedLengthWithOffset = new sourceCtor(
+        rab, 2 * sourceCtor.BYTES_PER_ELEMENT, 2);
+    const lengthTracking = new sourceCtor(rab, 0);
+    const lengthTrackingWithOffset = new sourceCtor(
+        rab, 2 * sourceCtor.BYTES_PER_ELEMENT);
+
+    assertThrows(() => { targetCtor.from(fixedLength); }, TypeError);
+    assertThrows(() => { targetCtor.from(fixedLengthWithOffset); }, TypeError);
+    assertThrows(() => { targetCtor.from(lengthTracking); }, TypeError);
+    assertThrows(() => { targetCtor.from(lengthTrackingWithOffset); },
+                 TypeError);
+  });
+})();
diff -r -u --color up/v8/test/mjsunit/wasm/array-init-from-segment.js nw/v8/test/mjsunit/wasm/array-init-from-segment.js
--- up/v8/test/mjsunit/wasm/array-init-from-segment.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/array-init-from-segment.js	2023-01-19 16:46:37.251442623 -0500
@@ -123,7 +123,7 @@
     .addBody([
       kExprLocalGet, 0,  // offset in table
       kExprTableGet, table,
-      kGCPrefix, kExprRefAsData,
+      kGCPrefix, kExprRefAsArray,
       kGCPrefix, kExprRefCast, array_type_index,
       kExprLocalGet, 1,  // index in the array
       kGCPrefix, kExprArrayGet, array_type_index,
diff -r -u --color up/v8/test/mjsunit/wasm/atomics-non-shared.js nw/v8/test/mjsunit/wasm/atomics-non-shared.js
--- up/v8/test/mjsunit/wasm/atomics-non-shared.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/atomics-non-shared.js	2023-01-19 16:46:37.262275953 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-threads
-
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 
 // TODO(gdeepti): If non-shared atomics are moving forward, ensure that
diff -r -u --color up/v8/test/mjsunit/wasm/atomics-stress.js nw/v8/test/mjsunit/wasm/atomics-stress.js
--- up/v8/test/mjsunit/wasm/atomics-stress.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/atomics-stress.js	2023-01-19 16:46:37.262275953 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-threads
-
 // This test might time out if the search space for a sequential
 // interleaving becomes to large. However, it should never fail.
 // Note that results of this test are flaky by design. While the test is
diff -r -u --color up/v8/test/mjsunit/wasm/atomics.js nw/v8/test/mjsunit/wasm/atomics.js
--- up/v8/test/mjsunit/wasm/atomics.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/atomics.js	2023-01-19 16:46:37.262275953 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-threads
-
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 
 const kMemtypeSize32 = 4;
@@ -470,3 +468,16 @@
   CmpExchgLoop(kExprI64AtomicCompareExchange16U, 1);
   CmpExchgLoop(kExprI64AtomicCompareExchange8U, 0);
 })();
+
+(function TestIllegalAtomicOp() {
+  // Regression test for https://crbug.com/1381330.
+  print(arguments.callee.name);
+  let builder = new WasmModuleBuilder();
+  builder.addFunction('main', kSig_v_v).addBody([
+    kAtomicPrefix, 0x90, 0x0f
+  ]);
+  assertEquals(false, WebAssembly.validate(builder.toBuffer()));
+  assertThrows(
+      () => builder.toModule(), WebAssembly.CompileError,
+      /invalid atomic opcode: 0xfe790/);
+})();
diff -r -u --color up/v8/test/mjsunit/wasm/atomics64-stress.js nw/v8/test/mjsunit/wasm/atomics64-stress.js
--- up/v8/test/mjsunit/wasm/atomics64-stress.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/atomics64-stress.js	2023-01-19 16:46:37.262275953 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-threads
-
 // This test might time out if the search space for a sequential
 // interleaving becomes to large. However, it should never fail.
 // Note that results of this test are flaky by design. While the test is
diff -r -u --color up/v8/test/mjsunit/wasm/bulk-memory.js nw/v8/test/mjsunit/wasm/bulk-memory.js
--- up/v8/test/mjsunit/wasm/bulk-memory.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/bulk-memory.js	2023-01-19 16:46:37.262275953 -0500
@@ -5,6 +5,7 @@
 d8.file.execute('test/mjsunit/wasm/wasm-module-builder.js');
 
 (function TestPassiveDataSegment() {
+  print(arguments.callee.name);
   const builder = new WasmModuleBuilder();
   builder.addMemory(1, 1, false);
   builder.addPassiveDataSegment([0, 1, 2]);
@@ -15,6 +16,7 @@
 })();
 
 (function TestPassiveElementSegment() {
+  print(arguments.callee.name);
   const builder = new WasmModuleBuilder();
   builder.addFunction('f', kSig_v_v).addBody([]);
   builder.setTableBounds(1, 1);
@@ -43,6 +45,7 @@
 }
 
 (function TestMemoryInitOutOfBoundsGrow() {
+  print(arguments.callee.name);
   const mem = new WebAssembly.Memory({initial: 1});
   // Create a data segment that has a length of kPageSize.
   const memoryInit = getMemoryInit(mem, new Array(kPageSize));
@@ -58,6 +61,7 @@
 })();
 
 (function TestMemoryInitOnActiveSegment() {
+  print(arguments.callee.name);
   const builder = new WasmModuleBuilder();
   builder.addMemory(1);
   builder.addPassiveDataSegment([1, 2, 3]);
@@ -86,6 +90,7 @@
 })();
 
 (function TestDataDropOnActiveSegment() {
+  print(arguments.callee.name);
   const builder = new WasmModuleBuilder();
   builder.addMemory(1);
   builder.addPassiveDataSegment([1, 2, 3]);
@@ -115,6 +120,7 @@
 }
 
 (function TestMemoryCopyOutOfBoundsGrow() {
+  print(arguments.callee.name);
   const mem = new WebAssembly.Memory({initial: 1});
   const memoryCopy = getMemoryCopy(mem);
 
@@ -141,6 +147,7 @@
 }
 
 (function TestMemoryFillOutOfBoundsGrow() {
+  print(arguments.callee.name);
   const mem = new WebAssembly.Memory({initial: 1});
   const memoryFill = getMemoryFill(mem);
   const v = 123;
@@ -156,6 +163,7 @@
 })();
 
 (function TestElemDropActive() {
+  print(arguments.callee.name);
   const builder = new WasmModuleBuilder();
   builder.setTableBounds(5, 5);
   builder.addActiveElementSegment(0, wasmI32Const(0), [0, 0, 0]);
@@ -173,6 +181,7 @@
 })();
 
 (function TestLazyDataSegmentBoundsCheck() {
+  print(arguments.callee.name);
   const memory = new WebAssembly.Memory({initial: 1});
   const view = new Uint8Array(memory.buffer);
   const builder = new WasmModuleBuilder();
@@ -192,6 +201,7 @@
 })();
 
 (function TestLazyDataAndElementSegments() {
+  print(arguments.callee.name);
   const table = new WebAssembly.Table({initial: 1, element: 'anyfunc'});
   const memory = new WebAssembly.Memory({initial: 1});
   const view = new Uint8Array(memory.buffer);
@@ -218,6 +228,7 @@
 })();
 
 (function TestPassiveDataSegmentNoMemory() {
+  print(arguments.callee.name);
   const builder = new WasmModuleBuilder();
   builder.addPassiveDataSegment([0, 1, 2]);
 
@@ -226,6 +237,7 @@
 })();
 
 (function TestPassiveElementSegmentNoMemory() {
+  print(arguments.callee.name);
   const builder = new WasmModuleBuilder();
   builder.addFunction('f', kSig_v_v).addBody([]);
   builder.addPassiveElementSegment([0, 0, 0]);
@@ -233,3 +245,14 @@
   // Should not throw.
   builder.instantiate();
 })();
+
+(function TestIllegalNumericOpcode() {
+  // Regression test for https://crbug.com/1382816.
+  print(arguments.callee.name);
+  let builder = new WasmModuleBuilder();
+  builder.addFunction('main', kSig_v_v).addBody([kNumericPrefix, 0x90, 0x0f]);
+  assertEquals(false, WebAssembly.validate(builder.toBuffer()));
+  assertThrows(
+      () => builder.toModule(), WebAssembly.CompileError,
+      /invalid numeric opcode: 0xfc790/);
+})();
diff -r -u --color up/v8/test/mjsunit/wasm/compare-exchange-stress.js nw/v8/test/mjsunit/wasm/compare-exchange-stress.js
--- up/v8/test/mjsunit/wasm/compare-exchange-stress.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/compare-exchange-stress.js	2023-01-19 16:46:37.262275953 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-threads
-
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 
 const kSequenceLength = 8192;
diff -r -u --color up/v8/test/mjsunit/wasm/compare-exchange64-stress.js nw/v8/test/mjsunit/wasm/compare-exchange64-stress.js
--- up/v8/test/mjsunit/wasm/compare-exchange64-stress.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/compare-exchange64-stress.js	2023-01-19 16:46:37.262275953 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-threads
-
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 
 const kSequenceLength = 8192;
diff -r -u --color up/v8/test/mjsunit/wasm/exceptions-api.js nw/v8/test/mjsunit/wasm/exceptions-api.js
--- up/v8/test/mjsunit/wasm/exceptions-api.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/exceptions-api.js	2023-01-19 16:46:37.294775945 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-eh
-
 load("test/mjsunit/wasm/wasm-module-builder.js");
 
 (function TestImport() {
diff -r -u --color up/v8/test/mjsunit/wasm/exceptions-export.js nw/v8/test/mjsunit/wasm/exceptions-export.js
--- up/v8/test/mjsunit/wasm/exceptions-export.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/exceptions-export.js	2023-01-19 16:46:37.294775945 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --expose-wasm --experimental-wasm-eh
-
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 
 (function TestExportMultiple() {
diff -r -u --color up/v8/test/mjsunit/wasm/exceptions-externref.js nw/v8/test/mjsunit/wasm/exceptions-externref.js
--- up/v8/test/mjsunit/wasm/exceptions-externref.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/exceptions-externref.js	2023-01-19 16:46:37.294775945 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-eh --allow-natives-syntax
+// Flags: --allow-natives-syntax
 
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 d8.file.execute("test/mjsunit/wasm/exceptions-utils.js");
diff -r -u --color up/v8/test/mjsunit/wasm/exceptions-import.js nw/v8/test/mjsunit/wasm/exceptions-import.js
--- up/v8/test/mjsunit/wasm/exceptions-import.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/exceptions-import.js	2023-01-19 16:46:37.294775945 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --expose-wasm --experimental-wasm-eh
-
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 
 // Helper function to return a new exported exception with the {kSig_v_v} type
diff -r -u --color up/v8/test/mjsunit/wasm/exceptions-rethrow.js nw/v8/test/mjsunit/wasm/exceptions-rethrow.js
--- up/v8/test/mjsunit/wasm/exceptions-rethrow.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/exceptions-rethrow.js	2023-01-19 16:46:37.294775945 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --expose-wasm --experimental-wasm-eh --allow-natives-syntax
+// Flags: --allow-natives-syntax
 
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 d8.file.execute("test/mjsunit/wasm/exceptions-utils.js");
diff -r -u --color up/v8/test/mjsunit/wasm/exceptions-shared.js nw/v8/test/mjsunit/wasm/exceptions-shared.js
--- up/v8/test/mjsunit/wasm/exceptions-shared.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/exceptions-shared.js	2023-01-19 16:46:37.294775945 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --expose-wasm --experimental-wasm-eh
-
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 
 // Helper function to return a new exported exception with the {kSig_v_v} type
diff -r -u --color up/v8/test/mjsunit/wasm/exceptions-simd.js nw/v8/test/mjsunit/wasm/exceptions-simd.js
--- up/v8/test/mjsunit/wasm/exceptions-simd.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/exceptions-simd.js	2023-01-19 16:46:37.294775945 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --expose-wasm --experimental-wasm-eh --experimental-wasm-simd --allow-natives-syntax
+// Flags: --allow-natives-syntax
 
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 d8.file.execute("test/mjsunit/wasm/exceptions-utils.js");
diff -r -u --color up/v8/test/mjsunit/wasm/exceptions-type-reflection.js nw/v8/test/mjsunit/wasm/exceptions-type-reflection.js
--- up/v8/test/mjsunit/wasm/exceptions-type-reflection.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/exceptions-type-reflection.js	2023-01-19 16:46:37.294775945 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-eh --experimental-wasm-type-reflection
+// Flags: --experimental-wasm-type-reflection
 
 load("test/mjsunit/wasm/wasm-module-builder.js");
 
diff -r -u --color up/v8/test/mjsunit/wasm/exceptions.js nw/v8/test/mjsunit/wasm/exceptions.js
--- up/v8/test/mjsunit/wasm/exceptions.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/exceptions.js	2023-01-19 16:46:37.294775945 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --expose-wasm --experimental-wasm-eh --allow-natives-syntax
+// Flags: --allow-natives-syntax
 
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 d8.file.execute("test/mjsunit/wasm/exceptions-utils.js");
diff -r -u --color up/v8/test/mjsunit/wasm/futex.js nw/v8/test/mjsunit/wasm/futex.js
--- up/v8/test/mjsunit/wasm/futex.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/futex.js	2023-01-19 16:46:37.294775945 -0500
@@ -3,7 +3,6 @@
 // found in the LICENSE file.
 
 // Flags: --allow-natives-syntax --harmony-sharedarraybuffer
-// Flags: --experimental-wasm-threads
 
 'use strict';
 
diff -r -u --color up/v8/test/mjsunit/wasm/gc-casts-from-any.js nw/v8/test/mjsunit/wasm/gc-casts-from-any.js
--- up/v8/test/mjsunit/wasm/gc-casts-from-any.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/gc-casts-from-any.js	2023-01-19 16:46:37.294775945 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-gc
+// Flags: --experimental-wasm-gc --no-wasm-gc-structref-as-dataref
 
 d8.file.execute('test/mjsunit/wasm/wasm-module-builder.js');
 
@@ -43,7 +43,7 @@
     ["Array", array],
     ["I31", kI31RefCode],
     ["AnyArray", kArrayRefCode],
-    ["Data", kDataRefCode],
+    ["Struct", kStructRefCode],
     ["Eq", kEqRefCode],
     // 'ref.test any' is semantically the same as '!ref.is_null' here.
     ["Any", kAnyRefCode],
@@ -58,6 +58,24 @@
       kGCPrefix, kExprExternInternalize,
       kGCPrefix, kExprRefTestNull, typeCode,
     ]).exportFunc();
+
+    builder.addFunction(`refCast${typeName}`,
+                        makeSig([kWasmExternRef], [kWasmExternRef]))
+    .addBody([
+      kExprLocalGet, 0,
+      kGCPrefix, kExprExternInternalize,
+      kGCPrefix, kExprRefCast, typeCode,
+      kGCPrefix, kExprExternExternalize,
+    ]).exportFunc();
+
+    builder.addFunction(`refCastNull${typeName}`,
+                        makeSig([kWasmExternRef], [kWasmExternRef]))
+    .addBody([
+      kExprLocalGet, 0,
+      kGCPrefix, kExprExternInternalize,
+      kGCPrefix, kExprRefCastNull, typeCode,
+      kGCPrefix, kExprExternExternalize,
+    ]).exportFunc();
   });
 
   var instance = builder.instantiate();
@@ -108,14 +126,14 @@
   assertEquals([0, 0], wasm.refTestAnyArray(1));
   assertEquals([0, 0], wasm.refTestAnyArray({'JavaScript': 'Object'}));
 
-  assertEquals([0, 1], wasm.refTestData(null));
-  assertEquals([0, 0], wasm.refTestData(undefined));
-  assertEquals([1, 1], wasm.refTestData(wasm.createStructSuper()));
-  assertEquals([1, 1], wasm.refTestData(wasm.createStructSub()));
-  assertEquals([1, 1], wasm.refTestData(wasm.createArray()));
-  assertEquals([0, 0], wasm.refTestData(wasm.createFuncRef()));
-  assertEquals([0, 0], wasm.refTestData(1));
-  assertEquals([0, 0], wasm.refTestData({'JavaScript': 'Object'}));
+  assertEquals([0, 1], wasm.refTestStruct(null));
+  assertEquals([0, 0], wasm.refTestStruct(undefined));
+  assertEquals([1, 1], wasm.refTestStruct(wasm.createStructSuper()));
+  assertEquals([1, 1], wasm.refTestStruct(wasm.createStructSub()));
+  assertEquals([0, 0], wasm.refTestStruct(wasm.createArray()));
+  assertEquals([0, 0], wasm.refTestStruct(wasm.createFuncRef()));
+  assertEquals([0, 0], wasm.refTestStruct(1));
+  assertEquals([0, 0], wasm.refTestStruct({'JavaScript': 'Object'}));
 
   assertEquals([0, 1], wasm.refTestEq(null));
   assertEquals([0, 0], wasm.refTestEq(undefined));
@@ -134,4 +152,156 @@
   assertEquals([1, 1], wasm.refTestAny(wasm.createFuncRef()));
   assertEquals([1, 1], wasm.refTestAny(1)); // ref.i31
   assertEquals([1, 1], wasm.refTestAny({'JavaScript': 'Object'}));
+
+  // ref.cast
+  let structSuperObj = wasm.createStructSuper();
+  let structSubObj = wasm.createStructSub();
+  let arrayObj = wasm.createArray();
+  let jsObj = {'JavaScript': 'Object'};
+  let funcObj = wasm.createFuncRef();
+
+  assertTraps(kTrapIllegalCast, () => wasm.refCastStructSuper(null));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastStructSuper(undefined));
+  assertSame(structSuperObj, wasm.refCastStructSuper(structSuperObj));
+  assertSame(structSubObj, wasm.refCastStructSuper(structSubObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastStructSuper(arrayObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastStructSuper(funcObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastStructSuper(1));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastStructSuper(jsObj));
+
+  assertTraps(kTrapIllegalCast, () => wasm.refCastStructSub(null));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastStructSub(undefined));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastStructSub(structSuperObj));
+  assertSame(structSubObj, wasm.refCastStructSub(structSubObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastStructSub(arrayObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastStructSub(funcObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastStructSub(1));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastStructSub(jsObj));
+
+  assertTraps(kTrapIllegalCast, () => wasm.refCastArray(null));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastArray(undefined));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastArray(structSuperObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastArray(structSubObj));
+  assertSame(arrayObj, wasm.refCastArray(arrayObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastArray(funcObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastArray(1));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastArray(jsObj));
+
+  assertTraps(kTrapIllegalCast, () => wasm.refCastI31(null));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastI31(undefined));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastI31(structSuperObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastI31(structSubObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastI31(arrayObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastI31(funcObj));
+  assertEquals(1, wasm.refCastI31(1));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastI31(jsObj));
+
+  assertTraps(kTrapIllegalCast, () => wasm.refCastAnyArray(null));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastAnyArray(undefined));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastAnyArray(structSuperObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastAnyArray(structSubObj));
+  assertSame(arrayObj, wasm.refCastAnyArray(arrayObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastAnyArray(funcObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastAnyArray(1));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastAnyArray(jsObj));
+
+  assertTraps(kTrapIllegalCast, () => wasm.refCastStruct(null));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastStruct(undefined));
+  assertSame(structSuperObj, wasm.refCastStruct(structSuperObj));
+  assertSame(structSubObj, wasm.refCastStruct(structSubObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastStruct(arrayObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastStruct(funcObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastStruct(1));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastStruct(jsObj));
+
+  assertTraps(kTrapIllegalCast, () => wasm.refCastEq(null));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastEq(undefined));
+  assertSame(structSuperObj, wasm.refCastEq(structSuperObj));
+  assertSame(structSubObj, wasm.refCastEq(structSubObj));
+  assertSame(arrayObj, wasm.refCastEq(arrayObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastEq(funcObj));
+  assertEquals(1, wasm.refCastEq(1));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastEq(jsObj));
+
+  assertTraps(kTrapIllegalCast, () => wasm.refCastAny(null));
+  assertSame(undefined, wasm.refCastAny(undefined));
+  assertSame(structSuperObj, wasm.refCastAny(structSuperObj));
+  assertSame(structSubObj, wasm.refCastAny(structSubObj));
+  assertSame(arrayObj, wasm.refCastAny(arrayObj));
+  assertSame(funcObj, wasm.refCastAny(funcObj));
+  assertEquals(1, wasm.refCastAny(1));
+  assertSame(jsObj, wasm.refCastAny(jsObj));
+
+  // ref.cast null
+  assertSame(null, wasm.refCastNullStructSuper(null));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullStructSuper(undefined));
+  assertSame(structSuperObj, wasm.refCastNullStructSuper(structSuperObj));
+  assertSame(structSubObj, wasm.refCastNullStructSuper(structSubObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullStructSuper(arrayObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullStructSuper(funcObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullStructSuper(1));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullStructSuper(jsObj));
+
+  assertSame(null, wasm.refCastNullStructSub(null));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullStructSub(undefined));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullStructSub(structSuperObj));
+  assertSame(structSubObj, wasm.refCastNullStructSub(structSubObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullStructSub(arrayObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullStructSub(funcObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullStructSub(1));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullStructSub(jsObj));
+
+  assertSame(null, wasm.refCastNullArray(null));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullArray(undefined));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullArray(structSuperObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullArray(structSubObj));
+  assertSame(arrayObj, wasm.refCastNullArray(arrayObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullArray(funcObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullArray(1));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullArray(jsObj));
+
+  assertSame(null, wasm.refCastNullI31(null));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullI31(undefined));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullI31(structSuperObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullI31(structSubObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullI31(arrayObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullI31(funcObj));
+  assertEquals(1, wasm.refCastNullI31(1));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullI31(jsObj));
+
+  assertSame(null, wasm.refCastNullAnyArray(null));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullAnyArray(undefined));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullAnyArray(structSuperObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullAnyArray(structSubObj));
+  assertSame(arrayObj, wasm.refCastNullAnyArray(arrayObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullAnyArray(funcObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullAnyArray(1));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullAnyArray(jsObj));
+
+  assertSame(null, wasm.refCastNullStruct(null));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullStruct(undefined));
+  assertSame(structSuperObj, wasm.refCastNullStruct(structSuperObj));
+  assertSame(structSubObj, wasm.refCastNullStruct(structSubObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullStruct(arrayObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullStruct(funcObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullStruct(1));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullStruct(jsObj));
+
+  assertSame(null, wasm.refCastNullEq(null));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullEq(undefined));
+  assertSame(structSuperObj, wasm.refCastNullEq(structSuperObj));
+  assertSame(structSubObj, wasm.refCastNullEq(structSubObj));
+  assertSame(arrayObj, wasm.refCastNullEq(arrayObj));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullEq(funcObj));
+  assertEquals(1, wasm.refCastNullEq(1));
+  assertTraps(kTrapIllegalCast, () => wasm.refCastNullEq(jsObj));
+
+  assertSame(null, wasm.refCastNullAny(null));
+  assertSame(undefined, wasm.refCastNullAny(undefined));
+  assertSame(structSuperObj, wasm.refCastNullAny(structSuperObj));
+  assertSame(structSubObj, wasm.refCastNullAny(structSubObj));
+  assertSame(arrayObj, wasm.refCastNullAny(arrayObj));
+  assertSame(funcObj, wasm.refCastNullAny(funcObj));
+  assertEquals(1, wasm.refCastNullAny(1));
+  assertSame(jsObj, wasm.refCastNullAny(jsObj));
 })();
diff -r -u --color up/v8/test/mjsunit/wasm/gc-casts-invalid.js nw/v8/test/mjsunit/wasm/gc-casts-invalid.js
--- up/v8/test/mjsunit/wasm/gc-casts-invalid.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/gc-casts-invalid.js	2023-01-19 16:46:37.294775945 -0500
@@ -25,6 +25,8 @@
   let casts = [
     kExprRefTest,
     kExprRefTestNull,
+    kExprRefCast,
+    kExprRefCastNull,
   ];
 
   for (let [source_type, target_type_imm] of types) {
@@ -33,10 +35,11 @@
       assertEquals(struct, builder.addStruct([makeField(kWasmI32, true)]));
       assertEquals(array, builder.addArray(kWasmI32));
       assertEquals(sig, builder.addType(makeSig([kWasmI32], [])));
-      builder.addFunction('refTest', makeSig([kWasmI32], [source_type]))
+      builder.addFunction('refTest', makeSig([source_type], []))
       .addBody([
         kExprLocalGet, 0,
         kGCPrefix, cast, target_type_imm,
+        kExprDrop,
       ]);
 
       assertThrows(() => builder.instantiate(),
diff -r -u --color up/v8/test/mjsunit/wasm/gc-casts-subtypes.js nw/v8/test/mjsunit/wasm/gc-casts-subtypes.js
--- up/v8/test/mjsunit/wasm/gc-casts-subtypes.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/gc-casts-subtypes.js	2023-01-19 16:46:37.294775945 -0500
@@ -3,11 +3,12 @@
 // found in the LICENSE file.
 
 // Flags: --experimental-wasm-gc --experimental-wasm-type-reflection
+// Flags: --no-wasm-gc-structref-as-dataref
 
 d8.file.execute('test/mjsunit/wasm/wasm-module-builder.js');
 
-// Test casting null from one type to another using ref.test.
-(function RefTestNull() {
+// Test casting null from one type to another using ref.test & ref.cast.
+(function RefCastFromNull() {
   print(arguments.callee.name);
   let builder = new WasmModuleBuilder();
   let structSuper = builder.addStruct([makeField(kWasmI32, true)]);
@@ -36,13 +37,19 @@
   ];
 
   for (let [sourceType, targetType, testName] of tests) {
-    builder.addFunction('testNull' + testName,
-                      makeSig([], [kWasmI32]))
+    builder.addFunction('testNull' + testName, makeSig([], [kWasmI32]))
     .addLocals(wasmRefNullType(sourceType), 1)
     .addBody([
       kExprLocalGet, 0,
       kGCPrefix, kExprRefTest, targetType & kLeb128Mask,
     ]).exportFunc();
+    builder.addFunction('castNull' + testName, makeSig([], []))
+    .addLocals(wasmRefNullType(sourceType), 1)
+    .addBody([
+      kExprLocalGet, 0,
+      kGCPrefix, kExprRefCast, targetType & kLeb128Mask,
+      kExprDrop,
+    ]).exportFunc();
   }
 
   let instance = builder.instantiate();
@@ -50,6 +57,7 @@
 
   for (let [sourceType, targetType, testName] of tests) {
     assertEquals(0, wasm['testNull' + testName]());
+    assertTraps(kTrapIllegalCast, wasm['castNull' + testName]);
   }
 })();
 
@@ -80,12 +88,56 @@
   assertEquals([1, 0, 1, 1], wasm.testFromFuncRef(wasm.fctSub));
 })();
 
+(function RefCastFuncRef() {
+  print(arguments.callee.name);
+  let builder = new WasmModuleBuilder();
+  let sigSuper = builder.addType(makeSig([kWasmI32], []));
+  let sigSub = builder.addType(makeSig([kWasmI32], []), sigSuper);
+
+  builder.addFunction('fctSuper', sigSuper).addBody([]).exportFunc();
+  builder.addFunction('fctSub', sigSub).addBody([]).exportFunc();
+  builder.addFunction('castToFuncRef', makeSig([kWasmFuncRef], [kWasmFuncRef]))
+    .addBody([kExprLocalGet, 0, kGCPrefix, kExprRefCast, kFuncRefCode])
+    .exportFunc();
+  builder.addFunction('castToNullFuncRef',
+                      makeSig([kWasmFuncRef], [kWasmFuncRef]))
+    .addBody([kExprLocalGet, 0, kGCPrefix, kExprRefCast, kNullFuncRefCode])
+    .exportFunc();
+  builder.addFunction('castToSuper', makeSig([kWasmFuncRef], [kWasmFuncRef]))
+    .addBody([kExprLocalGet, 0, kGCPrefix, kExprRefCast, sigSuper])
+    .exportFunc();
+  builder.addFunction('castToSub', makeSig([kWasmFuncRef], [kWasmFuncRef]))
+    .addBody([kExprLocalGet, 0, kGCPrefix, kExprRefCast, sigSub])
+    .exportFunc();
+
+  let instance = builder.instantiate();
+  let wasm = instance.exports;
+  let jsFct = new WebAssembly.Function(
+      {parameters:['i32', 'i32'], results: ['i32']},
+      function mul(a, b) { return a * b; });
+  assertSame(jsFct, wasm.castToFuncRef(jsFct));
+  assertSame(wasm.fctSuper, wasm.castToFuncRef(wasm.fctSuper));
+  assertSame(wasm.fctSub, wasm.castToFuncRef(wasm.fctSub));
+
+  assertTraps(kTrapIllegalCast, () => wasm.castToNullFuncRef(jsFct));
+  assertTraps(kTrapIllegalCast, () => wasm.castToNullFuncRef(wasm.fctSuper));
+  assertTraps(kTrapIllegalCast, () => wasm.castToNullFuncRef(wasm.fctSub));
+
+  assertTraps(kTrapIllegalCast, () => wasm.castToSuper(jsFct));
+  assertSame(wasm.fctSuper, wasm.castToSuper(wasm.fctSuper));
+  assertSame(wasm.fctSub, wasm.castToSuper(wasm.fctSub));
+
+  assertTraps(kTrapIllegalCast, () => wasm.castToSub(jsFct));
+  assertTraps(kTrapIllegalCast, () => wasm.castToSub(wasm.fctSuper));
+  assertSame(wasm.fctSub, wasm.castToSub(wasm.fctSub));
+})();
+
 (function RefTestExternRef() {
   print(arguments.callee.name);
   let builder = new WasmModuleBuilder();
 
   builder.addFunction('testExternRef',
-      makeSig([kWasmExternRef], [kWasmI32, kWasmI32,]))
+      makeSig([kWasmExternRef], [kWasmI32, kWasmI32]))
     .addBody([
       kExprLocalGet, 0, kGCPrefix, kExprRefTest, kExternRefCode,
       kExprLocalGet, 0, kGCPrefix, kExprRefTest, kNullExternRefCode,
@@ -100,6 +152,36 @@
   assertEquals([1, 0], wasm.testExternRef(wasm.testExternRef));
 })();
 
+(function RefCastExternRef() {
+  print(arguments.callee.name);
+  let builder = new WasmModuleBuilder();
+
+  builder.addFunction('castToExternRef',
+      makeSig([kWasmExternRef], [kWasmExternRef]))
+    .addBody([kExprLocalGet, 0, kGCPrefix, kExprRefCast, kExternRefCode])
+    .exportFunc();
+  builder.addFunction('castToNullExternRef',
+    makeSig([kWasmExternRef], [kWasmExternRef]))
+  .addBody([kExprLocalGet, 0, kGCPrefix, kExprRefCast, kNullExternRefCode])
+  .exportFunc();
+
+  let instance = builder.instantiate();
+  let wasm = instance.exports;
+  assertTraps(kTrapIllegalCast, () => wasm.castToExternRef(null));
+  assertEquals(undefined, wasm.castToExternRef(undefined));
+  assertEquals(1, wasm.castToExternRef(1));
+  let obj = {};
+  assertSame(obj, wasm.castToExternRef(obj));
+  assertSame(wasm.castToExternRef, wasm.castToExternRef(wasm.castToExternRef));
+
+  assertTraps(kTrapIllegalCast, () => wasm.castToNullExternRef(null));
+  assertTraps(kTrapIllegalCast, () => wasm.castToNullExternRef(undefined));
+  assertTraps(kTrapIllegalCast, () => wasm.castToNullExternRef(1));
+  assertTraps(kTrapIllegalCast, () => wasm.castToNullExternRef(obj));
+  assertTraps(kTrapIllegalCast,
+              () => wasm.castToNullExternRef(wasm.castToExternRef));
+})();
+
 (function RefTestAnyRefHierarchy() {
   print(arguments.callee.name);
   let builder = new WasmModuleBuilder();
@@ -110,7 +192,7 @@
   let types = {
     any: kWasmAnyRef,
     eq: kWasmEqRef,
-    data: kWasmDataRef,
+    struct: kWasmStructRef,
     anyArray: kWasmArrayRef,
     array: wasmRefNullType(array),
     structSuper: wasmRefNullType(structSuper),
@@ -136,8 +218,9 @@
       source: 'any',
       values: ['nullref', 'i31ref', 'structSuper', 'structSub', 'array'],
       targets: {
+        any: ['i31ref', 'structSuper', 'structSub', 'array'],
         eq: ['i31ref', 'structSuper', 'structSub', 'array'],
-        data: ['structSuper', 'structSub', 'array'],
+        struct: ['structSuper', 'structSub'],
         anyArray: ['array'],
         array: ['array'],
         structSuper: ['structSuper', 'structSub'],
@@ -149,7 +232,7 @@
       values: ['nullref', 'i31ref', 'structSuper', 'structSub', 'array'],
       targets: {
         eq: ['i31ref', 'structSuper', 'structSub', 'array'],
-        data: ['structSuper', 'structSub', 'array'],
+        struct: ['structSuper', 'structSub'],
         anyArray: ['array'],
         array: ['array'],
         structSuper: ['structSuper', 'structSub'],
@@ -157,11 +240,11 @@
       }
     },
     {
-      source: 'data',
-      values: ['nullref', 'structSuper', 'structSub', 'array'],
+      source: 'struct',
+      values: ['nullref', 'structSuper', 'structSub'],
       targets: {
-        eq: ['structSuper', 'structSub', 'array'],
-        data: ['structSuper', 'structSub', 'array'],
+        eq: ['structSuper', 'structSub'],
+        struct: ['structSuper', 'structSub'],
         anyArray: ['array'],
         array: ['array'],
         structSuper: ['structSuper', 'structSub'],
@@ -173,7 +256,7 @@
       values: ['nullref', 'array'],
       targets: {
         eq: ['array'],
-        data: ['array'],
+        struct: [],
         anyArray: ['array'],
         array: ['array'],
         structSuper: [],
@@ -185,7 +268,7 @@
       values: ['nullref', 'structSuper', 'structSub'],
       targets: {
         eq: ['structSuper', 'structSub'],
-        data: ['structSuper', 'structSub'],
+        struct: ['structSuper', 'structSub'],
         anyArray: [],
         array: [],
         structSuper: ['structSuper', 'structSub'],
@@ -225,6 +308,24 @@
         kExprCallRef, creatorType,
         kGCPrefix, kExprRefTestNull, heapType,
       ]).exportFunc();
+
+      builder.addFunction(`cast_${test.source}_to_${target}`,
+                          makeSig([wasmRefType(creatorType)], [kWasmI32]))
+      .addBody([
+        kExprLocalGet, 0,
+        kExprCallRef, creatorType,
+        kGCPrefix, kExprRefCast, heapType,
+        kExprRefIsNull, // We can't expose the cast object to JS in most cases.
+      ]).exportFunc();
+
+      builder.addFunction(`cast_null_${test.source}_to_${target}`,
+                          makeSig([wasmRefType(creatorType)], [kWasmI32]))
+      .addBody([
+        kExprLocalGet, 0,
+        kExprCallRef, creatorType,
+        kGCPrefix, kExprRefCastNull, heapType,
+        kExprRefIsNull, // We can't expose the cast object to JS in most cases.
+      ]).exportFunc();
     }
   }
 
@@ -242,6 +343,21 @@
         res = wasm[`test_null_${test.source}_to_${target}`](create_value);
         assertEquals(
             (validValues.includes(value) || value == "nullref") ? 1 : 0, res);
+
+        print(`Test ref.cast: ${test.source}(${value}) -> ${target}`);
+        let cast = wasm[`cast_${test.source}_to_${target}`];
+        if (validValues.includes(value)) {
+          assertEquals(0, cast(create_value));
+        } else {
+          assertTraps(kTrapIllegalCast, () => cast(create_value));
+        }
+        let castNull = wasm[`cast_null_${test.source}_to_${target}`];
+        if (validValues.includes(value) || value == "nullref") {
+          let expected = value == "nullref" ? 1 : 0;
+          assertEquals(expected, castNull(create_value));
+        } else {
+          assertTraps(kTrapIllegalCast, () => castNull(create_value));
+        }
       }
     }
   }
diff -r -u --color up/v8/test/mjsunit/wasm/gc-experimental-string-conversions.js nw/v8/test/mjsunit/wasm/gc-experimental-string-conversions.js
--- up/v8/test/mjsunit/wasm/gc-experimental-string-conversions.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/gc-experimental-string-conversions.js	2023-01-19 16:46:37.294775945 -0500
@@ -20,7 +20,7 @@
 
 builder.addFunction('getChar', makeSig([kWasmArrayRef, kWasmI32], [kWasmI32]))
     .addBody([
-      kExprLocalGet, 0, kGCPrefix, kExprRefAsData, kGCPrefix,
+      kExprLocalGet, 0, kGCPrefix, kExprRefAsArray, kGCPrefix,
       kExprRefCast, i16Array, kExprLocalGet, 1, kGCPrefix, kExprArrayGetS,
       i16Array
     ])
diff -r -u --color up/v8/test/mjsunit/wasm/gc-experiments.js nw/v8/test/mjsunit/wasm/gc-experiments.js
--- up/v8/test/mjsunit/wasm/gc-experiments.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/gc-experiments.js	2023-01-19 16:46:37.294775945 -0500
@@ -11,7 +11,7 @@
   let struct = builder.addStruct([makeField(kWasmI32, true)]);
 
   builder.addFunction("main", kSig_i_i)
-    .addLocals(wasmRefNullType(kWasmDataRef), 1)
+    .addLocals(wasmRefNullType(kWasmStructRef), 1)
     .addBody([
       kExprLocalGet, 0,
       kGCPrefix, kExprStructNew, struct,
diff -r -u --color up/v8/test/mjsunit/wasm/gc-js-interop-async.js nw/v8/test/mjsunit/wasm/gc-js-interop-async.js
--- up/v8/test/mjsunit/wasm/gc-js-interop-async.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/gc-js-interop-async.js	2023-01-19 16:46:37.294775945 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-gc --wasm-gc-js-interop --allow-natives-syntax
+// Flags: --experimental-wasm-gc --allow-natives-syntax
 
 d8.file.execute('test/mjsunit/wasm/gc-js-interop-helpers.js');
 
diff -r -u --color up/v8/test/mjsunit/wasm/gc-js-interop-collections.js nw/v8/test/mjsunit/wasm/gc-js-interop-collections.js
--- up/v8/test/mjsunit/wasm/gc-js-interop-collections.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/gc-js-interop-collections.js	2023-01-19 16:46:37.294775945 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-gc --wasm-gc-js-interop --allow-natives-syntax
+// Flags: --experimental-wasm-gc --allow-natives-syntax
 
 d8.file.execute('test/mjsunit/wasm/gc-js-interop-helpers.js');
 
diff -r -u --color up/v8/test/mjsunit/wasm/gc-js-interop-global-constructors.js nw/v8/test/mjsunit/wasm/gc-js-interop-global-constructors.js
--- up/v8/test/mjsunit/wasm/gc-js-interop-global-constructors.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/gc-js-interop-global-constructors.js	2023-01-19 16:46:37.294775945 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-gc --wasm-gc-js-interop --allow-natives-syntax
+// Flags: --experimental-wasm-gc --allow-natives-syntax
 
 d8.file.execute('test/mjsunit/wasm/gc-js-interop-helpers.js');
 
diff -r -u --color up/v8/test/mjsunit/wasm/gc-js-interop-import.mjs nw/v8/test/mjsunit/wasm/gc-js-interop-import.mjs
--- up/v8/test/mjsunit/wasm/gc-js-interop-import.mjs	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/gc-js-interop-import.mjs	2023-01-19 16:46:37.294775945 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-gc --wasm-gc-js-interop --allow-natives-syntax
+// Flags: --experimental-wasm-gc --allow-natives-syntax
 
 import {struct, array} from 'gc-js-interop-export.mjs';
 
@@ -16,7 +16,7 @@
     .addBody([
       kExprLocalGet, 0,                           // --
       kGCPrefix, kExprExternInternalize,          // --
-      kGCPrefix, kExprRefAsData,                  // --
+      kGCPrefix, kExprRefAsStruct,                // --
       kGCPrefix, kExprRefCast, struct_type,       // --
       kGCPrefix, kExprStructGet, struct_type, 0,  // --
     ]);
diff -r -u --color up/v8/test/mjsunit/wasm/gc-js-interop-numeric.js nw/v8/test/mjsunit/wasm/gc-js-interop-numeric.js
--- up/v8/test/mjsunit/wasm/gc-js-interop-numeric.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/gc-js-interop-numeric.js	2023-01-19 16:46:37.294775945 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-gc --wasm-gc-js-interop --allow-natives-syntax
+// Flags: --experimental-wasm-gc --allow-natives-syntax
 
 d8.file.execute('test/mjsunit/wasm/gc-js-interop-helpers.js');
 
diff -r -u --color up/v8/test/mjsunit/wasm/gc-js-interop-objects.js nw/v8/test/mjsunit/wasm/gc-js-interop-objects.js
--- up/v8/test/mjsunit/wasm/gc-js-interop-objects.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/gc-js-interop-objects.js	2023-01-19 16:46:37.294775945 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-gc --wasm-gc-js-interop --allow-natives-syntax
+// Flags: --experimental-wasm-gc --allow-natives-syntax
 
 d8.file.execute('test/mjsunit/wasm/gc-js-interop-helpers.js');
 
@@ -41,9 +41,7 @@
   repeated(() => assertEquals(true, Object.isFrozen(wasm_obj)));
   repeated(() => assertEquals(false, Object.isExtensible(wasm_obj)));
   repeated(() => assertEquals('object', typeof wasm_obj));
-  repeated(
-      () => assertEquals(
-          '[object Object]', Object.prototype.toString.call(wasm_obj)));
+  testThrowsRepeated(() => Object.prototype.toString.call(wasm_obj), TypeError);
 
   repeated(() => {
     let tgt = {};
diff -r -u --color up/v8/test/mjsunit/wasm/gc-js-interop-wasm.js nw/v8/test/mjsunit/wasm/gc-js-interop-wasm.js
--- up/v8/test/mjsunit/wasm/gc-js-interop-wasm.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/gc-js-interop-wasm.js	2023-01-19 16:46:37.294775945 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-gc --wasm-gc-js-interop --wasm-test-streaming
+// Flags: --experimental-wasm-gc --wasm-test-streaming
 // Flags: --allow-natives-syntax
 
 d8.file.execute('test/mjsunit/wasm/gc-js-interop-helpers.js');
@@ -70,7 +70,7 @@
   testThrowsRepeated(
       () => new WebAssembly.Tag({parameters: [wasm_obj]}), TypeError);
 
-  let tag = new WebAssembly.Tag({parameters: ['dataref']});
+  let tag = new WebAssembly.Tag({parameters: ['structref']});
   testThrowsRepeated(() => new WebAssembly.Exception(wasm_obj), TypeError);
   testThrowsRepeated(() => new WebAssembly.Exception(tag, wasm_obj), TypeError);
   repeated(() => new WebAssembly.Exception(tag, [wasm_obj]));
diff -r -u --color up/v8/test/mjsunit/wasm/gc-js-interop.js nw/v8/test/mjsunit/wasm/gc-js-interop.js
--- up/v8/test/mjsunit/wasm/gc-js-interop.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/gc-js-interop.js	2023-01-19 16:46:37.294775945 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-gc --wasm-gc-js-interop --allow-natives-syntax
+// Flags: --experimental-wasm-gc --allow-natives-syntax
 
 d8.file.execute('test/mjsunit/wasm/gc-js-interop-helpers.js');
 
diff -r -u --color up/v8/test/mjsunit/wasm/gc-optimizations.js nw/v8/test/mjsunit/wasm/gc-optimizations.js
--- up/v8/test/mjsunit/wasm/gc-optimizations.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/gc-optimizations.js	2023-01-19 16:46:37.294775945 -0500
@@ -468,8 +468,9 @@
     .addBody([
       // Cast from struct_a to struct_b via common base type struct_super.
       kExprLocalGet, 0,
-      kGCPrefix, kExprRefCast, struct_super,
-      kGCPrefix, kExprRefCast, struct_b, // annotated as 'ref null none'
+      // TODO(7748): Replace cast op with "ref.cast null".
+      kGCPrefix, kExprRefCastDeprecated, struct_super,
+      kGCPrefix, kExprRefCastDeprecated, struct_b, // annotated as 'ref null none'
       kExprRefIsNull,
     ]);
 
@@ -514,7 +515,8 @@
       // local.get 0 is known to be null until end of block.
       kExprLocalGet, 0,
       // This cast is a no-op and shold be optimized away.
-      kGCPrefix, kExprRefCast, struct_b,
+      // TODO(7748): Replace with "ref.cast null".
+      kGCPrefix, kExprRefCastDeprecated, struct_b,
       kExprEnd,
       kExprRefIsNull,
     ]);
@@ -528,3 +530,65 @@
   let instance = builder.instantiate({});
   assertEquals(1, instance.exports.main());
 })();
+
+(function AssertNullAfterCastIncompatibleTypes() {
+  print(arguments.callee.name);
+  let builder = new WasmModuleBuilder();
+  let struct_super = builder.addStruct([makeField(kWasmI32, true)]);
+  let struct_b = builder.addStruct([makeField(kWasmI32, true)], struct_super);
+  let struct_a = builder.addStruct(
+    [makeField(kWasmI32, true), makeField(kWasmI32, true)], struct_super);
+  let callee_sig = makeSig([wasmRefNullType(struct_super)], [kWasmI32]);
+
+  builder.addFunction("mkStruct", makeSig([], [kWasmExternRef]))
+    .addBody([kGCPrefix, kExprStructNewDefault, struct_a,
+              kGCPrefix, kExprExternExternalize])
+    .exportFunc();
+
+  let callee = builder.addFunction("callee", callee_sig)
+    .addBody([
+       kExprLocalGet, 0, kGCPrefix, kExprRefCast, struct_b,
+       kExprRefAsNonNull,
+       kGCPrefix, kExprStructGet, struct_b, 0]);
+
+  builder.addFunction("main", makeSig([kWasmExternRef], [kWasmI32]))
+    .addBody([kExprLocalGet, 0, kGCPrefix, kExprExternInternalize,
+              kGCPrefix, kExprRefAsStruct,
+              kGCPrefix, kExprRefCast, struct_a,
+              kExprCallFunction, callee.index])
+    .exportFunc();
+
+  let instance = builder.instantiate({});
+  assertTraps(kTrapIllegalCast,
+              () => instance.exports.main(instance.exports.mkStruct()));
+})();
+
+(function StructGetMultipleNullChecks() {
+  print(arguments.callee.name);
+  let builder = new WasmModuleBuilder();
+  let struct = builder.addStruct([makeField(kWasmI32, true),
+                                  makeField(kWasmI32, true)]);
+
+  builder.addFunction("main",
+                      makeSig([kWasmI32, wasmRefNullType(struct)], [kWasmI32]))
+    .addBody([
+      kExprLocalGet, 0,
+      kExprIf, kWasmI32,
+        kExprLocalGet, 1,
+        kGCPrefix, kExprStructGet, struct, 0,
+        kExprLocalGet, 1,
+        // The null check should be removed for this struct.
+        kGCPrefix, kExprStructGet, struct, 1,
+        kExprI32Add,
+      kExprElse,
+        kExprLocalGet, 1,
+        kGCPrefix, kExprStructGet, struct, 0,
+      kExprEnd,
+      kExprLocalGet, 1,
+      // The null check here could be removed if we compute type intersections.
+      kGCPrefix, kExprStructGet, struct, 1,
+      kExprI32Mul])
+    .exportFunc();
+
+  builder.instantiate({});
+})();
diff -r -u --color up/v8/test/mjsunit/wasm/inlining.js nw/v8/test/mjsunit/wasm/inlining.js
--- up/v8/test/mjsunit/wasm/inlining.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/inlining.js	2023-01-19 16:46:37.294775945 -0500
@@ -443,3 +443,29 @@
   // {factorial} should not be fully inlined in the trace.
   assertEquals(120, instance.exports.main(5));
 })();
+
+// When inlining a function with a tail call into a regular call, the tail call
+// has to be transformed into a call. That new call node (or its projections)
+// has to be typed.
+(function CallFromTailCallMustBeTyped() {
+  print(arguments.callee.name);
+
+  let builder = new WasmModuleBuilder();
+
+  let tail_call = builder
+    .addFunction("tail_call", makeSig([], [kWasmFuncRef]))
+    .addBody([kExprReturnCall, 0]);
+
+  let tail_call_multi = builder
+    .addFunction("tail_call", makeSig([], [kWasmFuncRef, kWasmFuncRef]))
+    .addBody([kExprReturnCall, 1]);
+
+  builder
+    .addFunction("main", makeSig([], [wasmRefType(kWasmFuncRef), kWasmFuncRef,
+                                      wasmRefType(kWasmFuncRef)]))
+    .addBody([
+      kExprCallFunction, tail_call.index, kExprRefAsNonNull,
+      kExprCallFunction, tail_call_multi.index, kExprRefAsNonNull])
+
+  builder.instantiate({});
+})();
Only in nw/v8/test/mjsunit/wasm: lazy-feedback-vector-allocation.js
diff -r -u --color up/v8/test/mjsunit/wasm/liftoff-simd-params.js nw/v8/test/mjsunit/wasm/liftoff-simd-params.js
--- up/v8/test/mjsunit/wasm/liftoff-simd-params.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/liftoff-simd-params.js	2023-01-19 16:46:37.305609275 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-simd
-
 d8.file.execute('test/mjsunit/wasm/wasm-module-builder.js');
 
 // This test case tries to exercise SIMD stack to stack movements by creating
diff -r -u --color up/v8/test/mjsunit/wasm/loop-unrolling.js nw/v8/test/mjsunit/wasm/loop-unrolling.js
--- up/v8/test/mjsunit/wasm/loop-unrolling.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/loop-unrolling.js	2023-01-19 16:46:37.305609275 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-typed-funcref --experimental-wasm-eh
+// Flags: --experimental-wasm-typed-funcref
 // Flags: --experimental-wasm-return-call --no-liftoff
 // Needed for exceptions-utils.js.
 // Flags: --allow-natives-syntax
diff -r -u --color up/v8/test/mjsunit/wasm/memory64.js nw/v8/test/mjsunit/wasm/memory64.js
--- up/v8/test/mjsunit/wasm/memory64.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/memory64.js	2023-01-19 16:46:37.305609275 -0500
@@ -278,3 +278,96 @@
   assertTraps(kTrapMemOutOfBounds, () => fill(1n << 62n, 0, 1n));
   assertTraps(kTrapMemOutOfBounds, () => fill(1n << 63n, 0, 1n));
 })();
+
+(function TestMemory64SharedBasic() {
+  print(arguments.callee.name);
+  let builder = new WasmModuleBuilder();
+  builder.addMemory64(1, 10, true, true);
+  builder.addFunction('load', makeSig([kWasmI64], [kWasmI32]))
+      .addBody([
+        kExprLocalGet, 0,       // local.get 0
+        kExprI32LoadMem, 0, 0,  // i32.load_mem align=1 offset=0
+      ])
+      .exportFunc();
+  let instance = builder.instantiate();
+
+  assertTrue(instance.exports.memory instanceof WebAssembly.Memory);
+  assertTrue(instance.exports.memory.buffer instanceof SharedArrayBuffer);
+  assertEquals(0, instance.exports.load(0n));
+})();
+
+(function TestMemory64SharedBetweenWorkers() {
+  print(arguments.callee.name);
+  // Generate a shared memory64 by instantiating an module that exports one.
+  // TODO(clemensb): Use the proper API once that's decided.
+  let shared_mem64 = (function() {
+    let builder = new WasmModuleBuilder();
+    builder.addMemory64(1, 10, true, true);
+    return builder.instantiate().exports.memory;
+  })();
+
+  let builder = new WasmModuleBuilder();
+  builder.addImportedMemory('imp', 'mem', 1, 10, true, true);
+
+  builder.addFunction('grow', makeSig([kWasmI64], [kWasmI64]))
+      .addBody([
+        kExprLocalGet, 0,    // local.get 0
+        kExprMemoryGrow, 0,  // memory.grow 0
+      ])
+      .exportFunc();
+  builder.addFunction('load', makeSig([kWasmI64], [kWasmI32]))
+      .addBody([
+        kExprLocalGet, 0,       // local.get 0
+        kExprI32LoadMem, 0, 0,  // i32.load_mem align=1 offset=0
+      ])
+      .exportFunc();
+  builder.addFunction('store', makeSig([kWasmI64, kWasmI32], []))
+      .addBody([
+        kExprLocalGet, 0,        // local.get 0
+        kExprLocalGet, 1,        // local.get 1
+        kExprI32StoreMem, 0, 0,  // i32.store_mem align=1 offset=0
+      ])
+      .exportFunc();
+
+  let module = builder.toModule();
+  let instance = new WebAssembly.Instance(module, {imp: {mem: shared_mem64}});
+
+  assertEquals(1n, instance.exports.grow(2n));
+  assertEquals(3n, instance.exports.grow(1n));
+  const kOffset1 = 47n;
+  const kOffset2 = 128n;
+  const kValue = 21;
+  assertEquals(0, instance.exports.load(kOffset1));
+  instance.exports.store(kOffset1, kValue);
+  assertEquals(kValue, instance.exports.load(kOffset1));
+  let worker = new Worker(function() {
+    onmessage = function([mem, module]) {
+      function workerAssert(condition, message) {
+        if (!condition) postMessage(`Check failed: ${message}`);
+      }
+
+      function workerAssertEquals(expected, actual, message) {
+        if (expected != actual)
+          postMessage(`Check failed (${message}): ${expected} != ${actual}`);
+      }
+
+      const kOffset1 = 47n;
+      const kOffset2 = 128n;
+      const kValue = 21;
+      workerAssert(mem instanceof WebAssembly.Memory, 'Wasm memory');
+      workerAssert(mem.buffer instanceof SharedArrayBuffer);
+      workerAssertEquals(4, mem.grow(1), 'grow');
+      let instance = new WebAssembly.Instance(module, {imp: {mem: mem}});
+      let exports = instance.exports;
+      workerAssertEquals(kValue, exports.load(kOffset1), 'load 1');
+      workerAssertEquals(0, exports.load(kOffset2), 'load 2');
+      exports.store(kOffset2, kValue);
+      workerAssertEquals(kValue, exports.load(kOffset2), 'load 3');
+      postMessage('OK');
+    }
+  }, {type: 'function'});
+  worker.postMessage([shared_mem64, module]);
+  assertEquals('OK', worker.getMessage());
+  assertEquals(kValue, instance.exports.load(kOffset2));
+  assertEquals(5n, instance.exports.grow(1n));
+})();
diff -r -u --color up/v8/test/mjsunit/wasm/multi-value-simd.js nw/v8/test/mjsunit/wasm/multi-value-simd.js
--- up/v8/test/mjsunit/wasm/multi-value-simd.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/multi-value-simd.js	2023-01-19 16:46:37.305609275 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-simd
-
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 
 (function MultiReturnS128Test() {
diff -r -u --color up/v8/test/mjsunit/wasm/reference-globals-import.js nw/v8/test/mjsunit/wasm/reference-globals-import.js
--- up/v8/test/mjsunit/wasm/reference-globals-import.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/reference-globals-import.js	2023-01-19 16:46:37.305609275 -0500
@@ -3,6 +3,7 @@
 // found in the LICENSE file.
 
 // Flags: --experimental-wasm-gc --experimental-wasm-stringref
+// Flags: --no-wasm-gc-structref-as-dataref
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 
 // Test type checks when creating a global with a value imported from a global
@@ -301,7 +302,7 @@
   builder.addFunction("get_struct_val", makeSig([], [kWasmI32]))
   .addBody([
     kExprGlobalGet, 0,
-    kGCPrefix, kExprRefAsData,
+    kGCPrefix, kExprRefAsStruct,
     kGCPrefix, kExprRefCast, struct_type,
     kGCPrefix, kExprStructGet, struct_type, 0,
   ])
@@ -309,7 +310,7 @@
   builder.addFunction("get_array_val", makeSig([], [kWasmI32]))
   .addBody([
     kExprGlobalGet, 0,
-    kGCPrefix, kExprRefAsData,
+    kGCPrefix, kExprRefAsArray,
     kGCPrefix, kExprRefCast, array_type,
     kExprI32Const, 0,
     kGCPrefix, kExprArrayGet, array_type,
@@ -369,7 +370,7 @@
   builder.addFunction("get_struct_val", makeSig([], [kWasmI32]))
   .addBody([
     kExprGlobalGet, 0,
-    kGCPrefix, kExprRefAsData,
+    kGCPrefix, kExprRefAsStruct,
     kGCPrefix, kExprRefCast, struct_type,
     kGCPrefix, kExprStructGet, struct_type, 0,
   ])
@@ -377,7 +378,7 @@
   builder.addFunction("get_array_val", makeSig([], [kWasmI32]))
   .addBody([
     kExprGlobalGet, 0,
-    kGCPrefix, kExprRefAsData,
+    kGCPrefix, kExprRefAsArray,
     kGCPrefix, kExprRefCast, array_type,
     kExprI32Const, 0,
     kGCPrefix, kExprArrayGet, array_type,
@@ -414,34 +415,25 @@
   assertThrows(() => eqref_global.value = "string", TypeError);
 })();
 
-(function TestDataRefGlobalFromJS() {
+(function TestStructRefGlobalFromJS() {
   print(arguments.callee.name);
-  let dataref_global = new WebAssembly.Global(
-      { value: "dataref", mutable: true }, null);
-  assertNull(dataref_global.value);
+  let structref_global = new WebAssembly.Global(
+      { value: "structref", mutable: true }, null);
+  assertNull(structref_global.value);
 
   let builder = new WasmModuleBuilder();
-  builder.addImportedGlobal("imports", "dataref_global", kWasmDataRef, true);
+  builder.addImportedGlobal("imports", "structref_global", kWasmStructRef, true);
   let struct_type = builder.addStruct([makeField(kWasmI32, false)]);
   let array_type = builder.addArray(kWasmI32);
 
   builder.addFunction("get_struct_val", makeSig([], [kWasmI32]))
   .addBody([
     kExprGlobalGet, 0,
-    kGCPrefix, kExprRefAsData,
+    kGCPrefix, kExprRefAsStruct,
     kGCPrefix, kExprRefCast, struct_type,
     kGCPrefix, kExprStructGet, struct_type, 0,
   ])
   .exportFunc();
-  builder.addFunction("get_array_val", makeSig([], [kWasmI32]))
-  .addBody([
-    kExprGlobalGet, 0,
-    kGCPrefix, kExprRefAsData,
-    kGCPrefix, kExprRefCast, array_type,
-    kExprI32Const, 0,
-    kGCPrefix, kExprArrayGet, array_type,
-  ])
-  .exportFunc();
   builder.addFunction("create_struct", makeSig([kWasmI32], [kWasmExternRef]))
   .addBody([
     kExprLocalGet, 0,
@@ -455,18 +447,17 @@
     kGCPrefix, kExprExternExternalize])
   .exportFunc();
 
-  let instance = builder.instantiate({imports : {dataref_global}});
+  let instance = builder.instantiate({imports : {structref_global}});
   let wasm = instance.exports;
 
-  dataref_global.value = wasm.create_struct(42);
+  structref_global.value = wasm.create_struct(42);
   assertEquals(42, wasm.get_struct_val());
-  dataref_global.value = wasm.create_array(43);
-  assertEquals(43, wasm.get_array_val());
-  dataref_global.value = null;
-  assertEquals(null, dataref_global.value);
+  structref_global.value = null;
+  assertEquals(null, structref_global.value);
 
-  assertThrows(() => dataref_global.value = undefined, TypeError);
-  assertThrows(() => dataref_global.value = "string", TypeError);
+  assertThrows(() => structref_global.value = undefined, TypeError);
+  assertThrows(() => structref_global.value = "string", TypeError);
+  assertThrows(() => structref_global.value = wasm.create_array(1), TypeError);
 })();
 
 (function TestArrayRefGlobalFromJS() {
@@ -483,7 +474,7 @@
   builder.addFunction("get_array_val", makeSig([], [kWasmI32]))
   .addBody([
     kExprGlobalGet, 0,
-    kGCPrefix, kExprRefAsData,
+    kGCPrefix, kExprRefAsArray,
     kGCPrefix, kExprRefCast, array_type,
     kExprI32Const, 0,
     kGCPrefix, kExprArrayGet, array_type,
diff -r -u --color up/v8/test/mjsunit/wasm/reference-globals.js nw/v8/test/mjsunit/wasm/reference-globals.js
--- up/v8/test/mjsunit/wasm/reference-globals.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/reference-globals.js	2023-01-19 16:46:37.305609275 -0500
@@ -162,7 +162,7 @@
       kGCPrefix, kExprStructGet, composite_struct_index, 0])
     .exportFunc();
 
-  builder.addFunction("field_2_default", makeSig([], [kWasmDataRef]))
+  builder.addFunction("field_2_default", makeSig([], [kWasmStructRef]))
     .addBody([
       kExprGlobalGet, global_default.index,
       kGCPrefix, kExprStructGet, composite_struct_index, 1])
@@ -245,7 +245,7 @@
       kGCPrefix, kExprStructGet, struct_index, 0])
     .exportFunc();
 
-  builder.addFunction("element1", makeSig([], [kWasmDataRef]))
+  builder.addFunction("element1", makeSig([], [kWasmStructRef]))
     .addBody([
       kExprGlobalGet, global.index,
       kExprI32Const, 1,
diff -r -u --color up/v8/test/mjsunit/wasm/reference-table-js-interop.js nw/v8/test/mjsunit/wasm/reference-table-js-interop.js
--- up/v8/test/mjsunit/wasm/reference-table-js-interop.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/reference-table-js-interop.js	2023-01-19 16:46:37.305609275 -0500
@@ -2,14 +2,15 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-gc --experimental-wasm-stringref --wasm-gc-js-interop
+// Flags: --experimental-wasm-gc --experimental-wasm-stringref
+// Flags: --no-wasm-gc-structref-as-dataref
 
 d8.file.execute('test/mjsunit/wasm/wasm-module-builder.js');
 
 let tableTypes = {
   "anyref": kWasmAnyRef,
   "eqref": kWasmEqRef,
-  "dataref": kWasmDataRef,
+  "structref": kWasmStructRef,
   "arrayref": kWasmArrayRef,
 };
 
@@ -74,7 +75,7 @@
   builder.addFunction("tableGetStructVal", getValSig)
     .addBody([
       kExprLocalGet, 0, kExprTableGet, 0,
-      kGCPrefix, kExprRefAsData,
+      kGCPrefix, kExprRefAsStruct,
       kGCPrefix, kExprRefCast, struct,
       kGCPrefix, kExprStructGet, struct, 0,
     ])
@@ -82,7 +83,7 @@
   builder.addFunction("tableGetArrayVal", getValSig)
     .addBody([
       kExprLocalGet, 0, kExprTableGet, 0,
-      kGCPrefix, kExprRefAsData,
+      kGCPrefix, kExprRefAsArray,
       kGCPrefix, kExprRefCast, array,
       kExprI32Const, 0,
       kGCPrefix, kExprArrayGet, array,
@@ -106,21 +107,10 @@
     ])
     .exportFunc();
 
-  let blockSig = builder.addType(makeSig([kWasmAnyRef], [kWasmEqRef]));
-  let castExternToEqRef = [
-    kGCPrefix, kExprExternInternalize,
-    kExprBlock, blockSig,
-      kGCPrefix, kExprBrOnI31, 0,
-      kGCPrefix, kExprBrOnData, 0,
-      // non-data, non-i31
-      kExprUnreachable, // conversion failure
-    kExprEnd,
-  ];
-
   builder.addFunction("createNull", creatorSig)
     .addBody([kExprRefNull, kNullRefCode])
     .exportFunc();
-  let i31Sig = typeName != "dataref" && typeName != "arrayref"
+  let i31Sig = typeName != "structref" && typeName != "arrayref"
                ? creatorSig : creatorAnySig;
   builder.addFunction("createI31", i31Sig)
     .addBody([kExprI32Const, 12, kGCPrefix, kExprI31New])
@@ -129,7 +119,8 @@
   builder.addFunction("createStruct", structSig)
     .addBody([kExprI32Const, 12, kGCPrefix, kExprStructNew, struct])
     .exportFunc();
-  builder.addFunction("createArray", creatorSig)
+  let arraySig = typeName != "structref" ? creatorSig : creatorAnySig;
+  builder.addFunction("createArray", arraySig)
     .addBody([
       kExprI32Const, 12,
       kGCPrefix, kExprArrayNewFixed, array, 1
@@ -159,7 +150,7 @@
   assertEquals(null, wasm.tableGet(1));
   assertEquals(null, table.get(1));
   // Set i31.
-  if (typeName != "dataref" && typeName != "arrayref") {
+  if (typeName != "structref" && typeName != "arrayref") {
     table.set(2, wasm.exported(wasm.createI31));
     assertSame(table.get(2), wasm.tableGet(2));
     wasm.tableSet(3, wasm.createI31);
@@ -177,13 +168,15 @@
     assertNotSame(table.get(4), table.get(5));
   }
   // Set array.
-  table.set(6, wasm.exported(wasm.createArray));
-  assertSame(table.get(6), wasm.tableGet(6));
-  assertEquals(12, wasm.tableGetArrayVal(6));
-  wasm.tableSet(7, wasm.createArray);
-  assertSame(table.get(7), wasm.tableGet(7));
-  assertEquals(12, wasm.tableGetArrayVal(7));
-  assertNotSame(table.get(6), table.get(7));
+  if (typeName != "structref") {
+    table.set(6, wasm.exported(wasm.createArray));
+    assertSame(table.get(6), wasm.tableGet(6));
+    assertEquals(12, wasm.tableGetArrayVal(6));
+    wasm.tableSet(7, wasm.createArray);
+    assertSame(table.get(7), wasm.tableGet(7));
+    assertEquals(12, wasm.tableGetArrayVal(7));
+    assertNotSame(table.get(6), table.get(7));
+  }
 
   // Set stringref.
   if (typeName == "anyref") {
@@ -217,7 +210,7 @@
   let invalidValues = {
     "anyref": [],
     "eqref": [],
-    "dataref": ["I31"],
+    "structref": ["I31", "Array"],
     "arrayref": ["I31", "Struct"],
   };
   for (let invalidType of invalidValues[typeName]) {
diff -r -u --color up/v8/test/mjsunit/wasm/reference-tables.js nw/v8/test/mjsunit/wasm/reference-tables.js
--- up/v8/test/mjsunit/wasm/reference-tables.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/reference-tables.js	2023-01-19 16:46:37.305609275 -0500
@@ -186,7 +186,7 @@
   builder.addFunction("struct_getter", kSig_i_v)
     .addBody([
       kExprI32Const, 2, kExprTableGet, 0,
-      kGCPrefix, kExprRefAsData, kGCPrefix, kExprRefCast, struct_type,
+      kGCPrefix, kExprRefAsStruct, kGCPrefix, kExprRefCast, struct_type,
       kGCPrefix, kExprStructGet, struct_type, 0])
     .exportFunc();
 
@@ -250,7 +250,7 @@
   builder.addFunction("struct_getter", kSig_i_i)
     .addBody([
       kExprLocalGet, 0, kExprTableGet, 0,
-      kGCPrefix, kExprRefAsData, kGCPrefix, kExprRefCast, struct_type,
+      kGCPrefix, kExprRefAsStruct, kGCPrefix, kExprRefCast, struct_type,
       kGCPrefix, kExprStructGet, struct_type, 0])
     .exportFunc();
 
diff -r -u --color up/v8/test/mjsunit/wasm/runtime-type-canonicalization.js nw/v8/test/mjsunit/wasm/runtime-type-canonicalization.js
--- up/v8/test/mjsunit/wasm/runtime-type-canonicalization.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/runtime-type-canonicalization.js	2023-01-19 16:46:37.305609275 -0500
@@ -14,16 +14,16 @@
 let distinct_struct_index = builder.addStruct([makeField(kWasmI64, true)]);
 
 let struct_init = builder.addFunction("struct_init",
-                                      makeSig([], [kWasmDataRef]))
+                                      makeSig([], [kWasmStructRef]))
     .addBody([kGCPrefix, kExprStructNewDefault, struct_index])
     .exportFunc();
 let test_pass = builder.addFunction("test_pass",
-                                    makeSig([kWasmDataRef], [kWasmI32]))
+                                    makeSig([kWasmStructRef], [kWasmI32]))
     .addBody([kExprLocalGet, 0,
               kGCPrefix, kExprRefTestDeprecated, identical_struct_index])
     .exportFunc();
 let test_fail = builder.addFunction("test_fail",
-                                    makeSig([kWasmDataRef], [kWasmI32]))
+                                    makeSig([kWasmStructRef], [kWasmI32]))
     .addBody([kExprLocalGet, 0,
               kGCPrefix, kExprRefTestDeprecated, distinct_struct_index])
     .exportFunc();
diff -r -u --color up/v8/test/mjsunit/wasm/shared-memory-worker-explicit-gc-stress.js nw/v8/test/mjsunit/wasm/shared-memory-worker-explicit-gc-stress.js
--- up/v8/test/mjsunit/wasm/shared-memory-worker-explicit-gc-stress.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/shared-memory-worker-explicit-gc-stress.js	2023-01-19 16:46:37.305609275 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-threads --expose-gc
+// Flags: --expose-gc
 
 d8.file.execute("test/mjsunit/worker-ping-test.js");
 
diff -r -u --color up/v8/test/mjsunit/wasm/shared-memory-worker-gc-stress.js nw/v8/test/mjsunit/wasm/shared-memory-worker-gc-stress.js
--- up/v8/test/mjsunit/wasm/shared-memory-worker-gc-stress.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/shared-memory-worker-gc-stress.js	2023-01-19 16:46:37.305609275 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-threads
-
 d8.file.execute("test/mjsunit/worker-ping-test.js");
 
 let kDisabledAbort = false; // TODO(9380): enable abort for this test
diff -r -u --color up/v8/test/mjsunit/wasm/shared-memory-worker-gc.js nw/v8/test/mjsunit/wasm/shared-memory-worker-gc.js
--- up/v8/test/mjsunit/wasm/shared-memory-worker-gc.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/shared-memory-worker-gc.js	2023-01-19 16:46:37.305609275 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-threads --expose-gc
+// Flags: --expose-gc
 
 const kNumMessages = 1000;
 
diff -r -u --color up/v8/test/mjsunit/wasm/shared-memory-worker-simple-gc.js nw/v8/test/mjsunit/wasm/shared-memory-worker-simple-gc.js
--- up/v8/test/mjsunit/wasm/shared-memory-worker-simple-gc.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/shared-memory-worker-simple-gc.js	2023-01-19 16:46:37.305609275 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-threads --expose-gc
+// Flags: --expose-gc
 
 const kNumIterations = 10;
 
diff -r -u --color up/v8/test/mjsunit/wasm/shared-memory-worker-stress.js nw/v8/test/mjsunit/wasm/shared-memory-worker-stress.js
--- up/v8/test/mjsunit/wasm/shared-memory-worker-stress.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/shared-memory-worker-stress.js	2023-01-19 16:46:37.305609275 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-threads
-
 d8.file.execute("test/mjsunit/worker-ping-test.js");
 
 // TODO(v8:9380): increase {numThings} and {numWorkers} when stress-opt mode
diff -r -u --color up/v8/test/mjsunit/wasm/shared-memory.js nw/v8/test/mjsunit/wasm/shared-memory.js
--- up/v8/test/mjsunit/wasm/shared-memory.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/shared-memory.js	2023-01-19 16:46:37.305609275 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-threads
-
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 
 function assertMemoryIsValid(memory, shared) {
diff -r -u --color up/v8/test/mjsunit/wasm/simd-call.js nw/v8/test/mjsunit/wasm/simd-call.js
--- up/v8/test/mjsunit/wasm/simd-call.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/simd-call.js	2023-01-19 16:46:37.305609275 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-simd
-
 d8.file.execute('test/mjsunit/wasm/wasm-module-builder.js');
 
 // Tests function calls containing s128 parameters. It also checks that
diff -r -u --color up/v8/test/mjsunit/wasm/simd-errors.js nw/v8/test/mjsunit/wasm/simd-errors.js
--- up/v8/test/mjsunit/wasm/simd-errors.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/simd-errors.js	2023-01-19 16:46:37.305609275 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-simd
-
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 
 (function TestS128InSignatureThrows() {
diff -r -u --color up/v8/test/mjsunit/wasm/simd-globals.js nw/v8/test/mjsunit/wasm/simd-globals.js
--- up/v8/test/mjsunit/wasm/simd-globals.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/simd-globals.js	2023-01-19 16:46:37.305609275 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-simd
-
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 
 // Test for S128 global with initialization.
diff -r -u --color up/v8/test/mjsunit/wasm/stringrefs-invalid.js nw/v8/test/mjsunit/wasm/stringrefs-invalid.js
--- up/v8/test/mjsunit/wasm/stringrefs-invalid.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/stringrefs-invalid.js	2023-01-19 16:46:37.305609275 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --expose-wasm --experimental-wasm-eh --experimental-wasm-gc
+// Flags: --experimental-wasm-gc
 
 d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
 
diff -r -u --color up/v8/test/mjsunit/wasm/test-wasm-module-builder.js nw/v8/test/mjsunit/wasm/test-wasm-module-builder.js
--- up/v8/test/mjsunit/wasm/test-wasm-module-builder.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/test-wasm-module-builder.js	2023-01-19 16:46:37.305609275 -0500
@@ -188,7 +188,7 @@
   print(arguments.callee.name);
   // These are all positive type indices (e.g. kI31RefCode and not kWasmI31Ref)
   // and should be treated as such.
-  let indices = [kI31RefCode, kDataRefCode, 200, 400];
+  let indices = [kI31RefCode, kStructRefCode, 200, 400];
   let kMaxIndex = 400;
   let builder = new WasmModuleBuilder();
   for (let i = 0; i <= kMaxIndex; i++) {
diff -r -u --color up/v8/test/mjsunit/wasm/wasm-gc-externalize-internalize.js nw/v8/test/mjsunit/wasm/wasm-gc-externalize-internalize.js
--- up/v8/test/mjsunit/wasm/wasm-gc-externalize-internalize.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/wasm-gc-externalize-internalize.js	2023-01-19 16:46:37.316442605 -0500
@@ -43,7 +43,7 @@
       kExprBlock, kWasmVoid,
         kExprLocalGet, 0,
         kExprBrOnNull, 0,
-        kGCPrefix, kExprRefAsData,
+        kGCPrefix, kExprRefAsStruct,
         kGCPrefix, kExprRefCast, struct,
         kGCPrefix, kExprStructGet, struct, 0, // value
         kExprI32Const, 0, // isNull
@@ -64,7 +64,7 @@
         kExprLocalGet, 0,
         kGCPrefix, kExprExternInternalize,
         kExprBrOnNull, 0,
-        kGCPrefix, kExprRefAsData,
+        kGCPrefix, kExprRefAsStruct,
         kGCPrefix, kExprRefCast, struct,
         kGCPrefix, kExprStructGet, struct, 0, // value
         kExprI32Const, 0, // isNull
diff -r -u --color up/v8/test/mjsunit/wasm/wasm-gc-js-roundtrip.js nw/v8/test/mjsunit/wasm/wasm-gc-js-roundtrip.js
--- up/v8/test/mjsunit/wasm/wasm-gc-js-roundtrip.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/wasm-gc-js-roundtrip.js	2023-01-19 16:46:37.316442605 -0500
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-gc
+// Flags: --experimental-wasm-gc --no-wasm-gc-structref-as-dataref
 
 d8.file.execute('test/mjsunit/wasm/wasm-module-builder.js');
 
@@ -16,11 +16,11 @@
                  .addBody([kExprLocalGet, 0, kExprI32Const, 1, kExprI32Add])
                  .exportAs('inc');
 
-  builder.addFunction('struct_producer', makeSig([], [kWasmDataRef]))
+  builder.addFunction('struct_producer', makeSig([], [kWasmStructRef]))
       .addBody([kGCPrefix, kExprStructNewDefault, struct])
       .exportFunc();
 
-  builder.addFunction('array_producer', makeSig([], [kWasmDataRef]))
+  builder.addFunction('array_producer', makeSig([], [kWasmArrayRef]))
       .addBody([
         kExprI32Const, 10,
         kGCPrefix, kExprArrayNewDefault, array
@@ -36,12 +36,11 @@
       .exportFunc();
 
   let test_types = {
-    struct: kWasmDataRef,
-    array: kWasmDataRef,
+    struct: kWasmStructRef,
+    array: kWasmArrayRef,
     raw_struct: struct,
     raw_array: array,
     typed_func: sig,
-    data: kWasmDataRef,
     eq: kWasmEqRef,
     func: kWasmFuncRef,
     any: kWasmAnyRef,
@@ -67,15 +66,18 @@
 // Wasm-exposed null is the same as JS null.
 assertEquals(instance.exports.struct_null(), null);
 
-// We can roundtrip a struct as dataref.
-instance.exports.data_id(instance.exports.struct_producer());
-// We can roundtrip an array as dataref.
-instance.exports.data_id(instance.exports.array_producer());
-// We can roundtrip null as dataref.
-instance.exports.data_id(instance.exports.data_null());
-// We cannot roundtrip an i31 as dataref.
+// We can roundtrip a struct as structref.
+instance.exports.struct_id(instance.exports.struct_producer());
+// We cannot roundtrip an array as structref.
 assertThrows(
-    () => instance.exports.data_id(instance.exports.i31_as_eq_producer()),
+    () => instance.exports.struct_id(instance.exports.array_producer()),
+    TypeError,
+    'type incompatibility when transforming from/to JS');
+// We can roundtrip null as structref.
+instance.exports.struct_id(instance.exports.struct_null());
+// We cannot roundtrip an i31 as structref.
+assertThrows(
+    () => instance.exports.struct_id(instance.exports.i31_as_eq_producer()),
     TypeError,
     'type incompatibility when transforming from/to JS');
 
@@ -86,7 +88,7 @@
 // We can roundtrip an i31 as eqref.
 instance.exports.eq_id(instance.exports.i31_as_eq_producer());
 // We can roundtrip any null as eqref.
-instance.exports.eq_id(instance.exports.data_null());
+instance.exports.eq_id(instance.exports.struct_null());
 instance.exports.eq_id(instance.exports.eq_null());
 instance.exports.eq_id(instance.exports.func_null());
 // We cannot roundtrip a func as eqref.
diff -r -u --color up/v8/test/mjsunit/wasm/wasm-module-builder.js nw/v8/test/mjsunit/wasm/wasm-module-builder.js
--- up/v8/test/mjsunit/wasm/wasm-module-builder.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/wasm-module-builder.js	2023-01-19 16:46:37.316442605 -0500
@@ -90,6 +90,8 @@
 let kLimitsSharedWithMaximum = 0x03;
 let kLimitsMemory64NoMaximum = 0x04;
 let kLimitsMemory64WithMaximum = 0x05;
+let kLimitsMemory64SharedNoMaximum = 0x06;
+let kLimitsMemory64SharedWithMaximum = 0x07;
 
 // Segment flags
 let kActiveNoIndex = 0;
@@ -125,7 +127,7 @@
 let kWasmI31Ref = -0x16;
 let kWasmNullExternRef = -0x17;
 let kWasmNullFuncRef = -0x18;
-let kWasmDataRef = -0x19;
+let kWasmStructRef = -0x19;
 let kWasmArrayRef = -0x1a;
 let kWasmNullRef = -0x1b;
 let kWasmStringRef = -0x1c;
@@ -143,7 +145,7 @@
 let kI31RefCode = kWasmI31Ref & kLeb128Mask;
 let kNullExternRefCode = kWasmNullExternRef & kLeb128Mask;
 let kNullFuncRefCode = kWasmNullFuncRef & kLeb128Mask;
-let kDataRefCode = kWasmDataRef & kLeb128Mask;
+let kStructRefCode = kWasmStructRef & kLeb128Mask;
 let kArrayRefCode = kWasmArrayRef & kLeb128Mask;
 let kNullRefCode = kWasmNullRef & kLeb128Mask;
 let kStringRefCode = kWasmStringRef & kLeb128Mask;
@@ -270,7 +272,7 @@
   'CallIndirect': 0x11,
   'ReturnCall': 0x12,
   'ReturnCallIndirect': 0x13,
-  'CallRef': 0x17,  // TODO(7748): Temporary. Switch back to 0x14.
+  'CallRef': 0x14,
   'ReturnCallRef': 0x15,
   'Delegate': 0x18,
   'Drop': 0x1a,
@@ -510,20 +512,22 @@
 let kExprRefTest = 0x40;
 let kExprRefTestNull = 0x48;
 let kExprRefTestDeprecated = 0x44;
-let kExprRefCast = 0x45;
+let kExprRefCast = 0x41;
+let kExprRefCastNull = 0x49;
+let kExprRefCastDeprecated = 0x45;
 let kExprBrOnCast = 0x46;
 let kExprBrOnCastFail = 0x47;
 let kExprRefCastNop = 0x4c;
 let kExprRefIsData = 0x51;
 let kExprRefIsI31 = 0x52;
 let kExprRefIsArray = 0x53;
-let kExprRefAsData = 0x59;
+let kExprRefAsStruct = 0x59;
 let kExprRefAsI31 = 0x5a;
 let kExprRefAsArray = 0x5b;
-let kExprBrOnData = 0x61;
+let kExprBrOnStruct = 0x61;
 let kExprBrOnI31 = 0x62;
 let kExprBrOnArray = 0x66;
-let kExprBrOnNonData = 0x64;
+let kExprBrOnNonStruct = 0x64;
 let kExprBrOnNonI31 = 0x65;
 let kExprBrOnNonArray = 0x67;
 let kExprExternInternalize = 0x70;
@@ -1298,12 +1302,12 @@
     return this;
   }
 
-  addMemory64(min, max, exported) {
+  addMemory64(min, max, exported, shared) {
     this.memory = {
       min: min,
       max: max,
       exported: exported,
-      shared: false,
+      shared: shared || false,
       is_memory64: true
     };
     return this;
@@ -1456,14 +1460,15 @@
     return this.num_imported_globals++;
   }
 
-  addImportedMemory(module, name, initial = 0, maximum, shared) {
+  addImportedMemory(module, name, initial = 0, maximum, shared, is_memory64) {
     let o = {
       module: module,
       name: name,
       kind: kExternalMemory,
       initial: initial,
       maximum: maximum,
-      shared: shared
+      shared: !!shared,
+      is_memory64: !!is_memory64
     };
     this.imports.push(o);
     return this;
@@ -1670,7 +1675,7 @@
       });
     }
 
-    // Add imports section
+    // Add imports section.
     if (wasm.imports.length > 0) {
       if (debug) print('emitting imports @ ' + binary.length);
       binary.emit_section(kImportSectionCode, section => {
@@ -1685,15 +1690,16 @@
             section.emit_type(imp.type);
             section.emit_u8(imp.mutable);
           } else if (imp.kind == kExternalMemory) {
-            var has_max = (typeof imp.maximum) != 'undefined';
-            var is_shared = (typeof imp.shared) != 'undefined';
-            if (is_shared) {
-              section.emit_u8(has_max ? 3 : 2);  // flags
-            } else {
-              section.emit_u8(has_max ? 1 : 0);  // flags
-            }
-            section.emit_u32v(imp.initial);               // initial
-            if (has_max) section.emit_u32v(imp.maximum);  // maximum
+            const has_max = imp.maximum !== undefined;
+            const is_shared = !!imp.shared;
+            const is_memory64 = !!imp.is_memory64;
+            let limits_byte =
+                (is_memory64 ? 4 : 0) | (is_shared ? 2 : 0) | (has_max ? 1 : 0);
+            section.emit_u8(limits_byte);
+            let emit = val =>
+                is_memory64 ? section.emit_u64v(val) : section.emit_u32v(val);
+            emit(imp.initial);
+            if (has_max) emit(imp.maximum);
           } else if (imp.kind == kExternalTable) {
             section.emit_type(imp.type);
             var has_max = (typeof imp.maximum) != 'undefined';
@@ -1751,23 +1757,15 @@
       binary.emit_section(kMemorySectionCode, section => {
         section.emit_u8(1);  // one memory entry
         const has_max = wasm.memory.max !== undefined;
-        if (wasm.memory.is_memory64) {
-          if (wasm.memory.shared) {
-            throw new Error('sharing memory64 is not supported (yet)');
-          }
-          section.emit_u8(
-              has_max ? kLimitsMemory64WithMaximum : kLimitsMemory64NoMaximum);
-          section.emit_u64v(wasm.memory.min);
-          if (has_max) section.emit_u64v(wasm.memory.max);
-        } else {
-          section.emit_u8(
-              wasm.memory.shared ?
-                  (has_max ? kLimitsSharedWithMaximum :
-                             kLimitsSharedNoMaximum) :
-                  (has_max ? kLimitsWithMaximum : kLimitsNoMaximum));
-          section.emit_u32v(wasm.memory.min);
-          if (has_max) section.emit_u32v(wasm.memory.max);
-        }
+        const is_shared = !!wasm.memory.shared;
+        const is_memory64 = !!wasm.memory.is_memory64;
+        let limits_byte =
+            (is_memory64 ? 4 : 0) | (is_shared ? 2 : 0) | (has_max ? 1 : 0);
+        section.emit_u8(limits_byte);
+        let emit = val =>
+            is_memory64 ? section.emit_u64v(val) : section.emit_u32v(val);
+        emit(wasm.memory.min);
+        if (has_max) emit(wasm.memory.max);
       });
     }
 
diff -r -u --color up/v8/test/mjsunit/wasm/worker-memory.js nw/v8/test/mjsunit/wasm/worker-memory.js
--- up/v8/test/mjsunit/wasm/worker-memory.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mjsunit/wasm/worker-memory.js	2023-01-19 16:46:37.316442605 -0500
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-// Flags: --experimental-wasm-threads
-
 (function TestPostMessageUnsharedMemory() {
   let worker = new Worker('', {type: 'string'});
   let memory = new WebAssembly.Memory({initial: 1, maximum: 2});
diff -r -u --color up/v8/test/mkgrokdump/mkgrokdump.cc nw/v8/test/mkgrokdump/mkgrokdump.cc
--- up/v8/test/mkgrokdump/mkgrokdump.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/mkgrokdump/mkgrokdump.cc	2023-01-19 16:46:37.316442605 -0500
@@ -127,7 +127,7 @@
   {
     Isolate::Scope scope(isolate);
     i::Heap* heap = reinterpret_cast<i::Isolate*>(isolate)->heap();
-    i::SafepointScope safepoint_scope(heap);
+    i::IsolateSafepointScope safepoint_scope(heap);
     i::ReadOnlyHeap* read_only_heap =
         reinterpret_cast<i::Isolate*>(isolate)->read_only_heap();
     i::PrintF(out, "%s", kHeader);
@@ -149,12 +149,11 @@
                      object);
       }
 
-      i::PagedSpace* space_for_maps = heap->space_for_maps();
-      i::PagedSpaceObjectIterator iterator(heap, space_for_maps);
+      i::PagedSpaceObjectIterator iterator(heap, heap->old_space());
       for (i::HeapObject object = iterator.Next(); !object.is_null();
            object = iterator.Next()) {
         if (!object.IsMap()) continue;
-        DumpKnownMap(out, heap, space_for_maps->name(), object);
+        DumpKnownMap(out, heap, heap->old_space()->name(), object);
       }
       i::PrintF(out, "}\n");
     }
@@ -176,8 +175,7 @@
       for (i::PagedSpace* s = spit.Next(); s != nullptr; s = spit.Next()) {
         i::PagedSpaceObjectIterator it(heap, s);
         // Code objects are generally platform-dependent.
-        if (s->identity() == i::CODE_SPACE || s->identity() == i::MAP_SPACE)
-          continue;
+        if (s->identity() == i::CODE_SPACE) continue;
         const char* sname = s->name();
         for (i::HeapObject o = it.Next(); !o.is_null(); o = it.Next()) {
           DumpKnownObject(out, heap, sname, o);
diff -r -u --color up/v8/test/test262/test262.status nw/v8/test/test262/test262.status
--- up/v8/test/test262/test262.status	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/test262/test262.status	2023-01-19 16:46:37.327275935 -0500
@@ -66,9 +66,6 @@
   'language/expressions/prefix-increment/S11.4.4_A5_*': [FAIL],
   'language/statements/variable/binding-resolution': [FAIL],
 
-  # https://code.google.com/p/v8/issues/detail?id=10958
-  'language/module-code/eval-gtbndng-indirect-faux-assertion': [FAIL],
-
   # https://bugs.chromium.org/p/v8/issues/detail?id=4951
   'language/expressions/assignment/destructuring/iterator-destructuring-property-reference-target-evaluation-order': [FAIL],
   'language/expressions/assignment/destructuring/keyed-destructuring-property-reference-target-evaluation-order': [FAIL],
@@ -410,11 +407,7 @@
 
   'staging/Temporal/Duration/old/round': [FAIL],
   'staging/Temporal/Duration/old/total': [FAIL],
-  'staging/Temporal/ZonedDateTime/old/construction-and-properties': [FAIL],
   'staging/Temporal/ZonedDateTime/old/dst-math': [FAIL],
-  'staging/Temporal/ZonedDateTime/old/equals': [FAIL],
-  'staging/Temporal/ZonedDateTime/old/toPlainMonthDay': [FAIL],
-  'staging/Temporal/ZonedDateTime/old/toPlainYearMonth': [FAIL],
 
   # UBSan complain about static_cast<int32_t> from double in AddISODate()
   'built-ins/Temporal/Calendar/prototype/dateAdd/argument-duration-years-and-months-number-max-value': [SKIP],
@@ -434,62 +427,7 @@
   'staging/Intl402/Temporal/old/yearmonth-toLocaleString': [FAIL],
   'staging/Intl402/Temporal/old/zoneddatetime-toLocaleString': [FAIL],
 
-  'built-ins/Temporal/Calendar/from/calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/Calendar/prototype/dateAdd/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/Calendar/prototype/day/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/Calendar/prototype/dayOfWeek/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/Calendar/prototype/dayOfYear/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/Calendar/prototype/daysInMonth/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/Calendar/prototype/daysInWeek/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/Calendar/prototype/daysInYear/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/Calendar/prototype/inLeapYear/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/Calendar/prototype/month/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/Calendar/prototype/monthCode/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/Calendar/prototype/monthsInYear/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/Calendar/prototype/weekOfYear/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/Calendar/prototype/year/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/Instant/prototype/toZonedDateTime/calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/Now/plainDate/calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/Now/plainDateTime/calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/Now/zonedDateTime/calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/PlainDate/calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/PlainDate/from/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/PlainDate/prototype/equals/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/PlainDate/prototype/since/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/PlainDate/prototype/until/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/PlainDate/prototype/withCalendar/calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/PlainDateTime/calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/PlainDateTime/from/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/PlainDateTime/prototype/equals/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/PlainDateTime/prototype/since/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/PlainDateTime/prototype/until/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/PlainDateTime/prototype/withCalendar/calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/PlainDateTime/prototype/withPlainDate/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/PlainMonthDay/calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/PlainMonthDay/from/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/PlainMonthDay/prototype/equals/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/PlainTime/prototype/toPlainDateTime/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/PlainTime/prototype/toZonedDateTime/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/PlainYearMonth/calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/PlainYearMonth/from/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/PlainYearMonth/prototype/equals/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/PlainYearMonth/prototype/since/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/PlainYearMonth/prototype/until/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/TimeZone/prototype/getInstantFor/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/TimeZone/prototype/getPlainDateTimeFor/calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/TimeZone/prototype/getPossibleInstantsFor/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/ZonedDateTime/calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/ZonedDateTime/from/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/ZonedDateTime/prototype/equals/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/ZonedDateTime/prototype/since/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/ZonedDateTime/prototype/until/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/ZonedDateTime/prototype/withCalendar/calendar-case-insensitive': [FAIL],
-  'built-ins/Temporal/ZonedDateTime/prototype/withPlainDate/argument-propertybag-calendar-case-insensitive': [FAIL],
   # intl402
-  'intl402/Temporal/Calendar/calendar-case-insensitive': [FAIL],
-  'intl402/Temporal/Calendar/from/calendar-case-insensitive': [FAIL],
-  'intl402/Temporal/Calendar/prototype/era/argument-propertybag-calendar-case-insensitive': [FAIL],
-  'intl402/Temporal/Calendar/prototype/eraYear/argument-propertybag-calendar-case-insensitive': [FAIL],
   'intl402/Temporal/TimeZone/from/timezone-case-insensitive': [FAIL],
 
   'built-ins/Temporal/Duration/compare/precision-exact-mathematical-values-1': [FAIL],
@@ -521,7 +459,408 @@
   'built-ins/Temporal/ZonedDateTime/prototype/until/roundingmode-ceil': [FAIL],
   'built-ins/Temporal/ZonedDateTime/prototype/until/roundingmode-expand': [FAIL],
   'built-ins/Temporal/ZonedDateTime/prototype/until/roundingmode-floor': [FAIL],
-  'intl402/Temporal/ZonedDateTime/prototype/withCalendar/calendar-case-insensitive': [FAIL],
+
+  'built-ins/Temporal/Duration/compare/order-of-operations': [FAIL],
+  'built-ins/Temporal/Duration/prototype/round/order-of-operations': [FAIL],
+  'built-ins/Temporal/Duration/prototype/total/order-of-operations': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/since/order-of-operations': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/until/order-of-operations': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/since/order-of-operations': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/until/order-of-operations': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/since/order-of-operations': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/until/order-of-operations': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/since/order-of-operations': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/until/order-of-operations': [FAIL],
+
+  'built-ins/Temporal/Duration/prototype/round/nanoseconds-to-days-precision-exact-mathematical-values-1': [FAIL],
+  'built-ins/Temporal/Duration/prototype/round/nanoseconds-to-days-precision-exact-mathematical-values-2': [FAIL],
+
+  'built-ins/Temporal/PlainDate/prototype/dayOfWeek/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/dayOfYear/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/daysInMonth/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/daysInWeek/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/daysInYear/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/inLeapYear/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/monthsInYear/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/weekOfYear/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/dayOfWeek/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/dayOfYear/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/daysInMonth/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/daysInWeek/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/daysInYear/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/inLeapYear/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/monthsInYear/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/weekOfYear/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/daysInMonth/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/daysInYear/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/inLeapYear/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/monthsInYear/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/dayOfWeek/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/dayOfYear/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/daysInMonth/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/daysInWeek/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/daysInYear/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/hoursInDay/precision-exact-mathematical-values': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/inLeapYear/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/monthsInYear/validate-calendar-value': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/weekOfYear/validate-calendar-value': [FAIL],
+
+  'staging/Intl402/Temporal/old/duration-arithmetic-dst': [FAIL],
+
+  'built-ins/Temporal/Calendar/from/calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/dateAdd/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/dateAdd/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/dateAdd/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/dateAdd/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/dateAdd/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/dateUntil/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/dateUntil/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/dateUntil/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/dateUntil/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/dateUntil/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/day/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/day/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/day/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/day/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/day/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/dayOfWeek/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/dayOfWeek/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/dayOfWeek/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/dayOfWeek/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/dayOfWeek/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/dayOfYear/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/dayOfYear/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/dayOfYear/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/dayOfYear/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/dayOfYear/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/daysInMonth/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/daysInMonth/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/daysInMonth/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/daysInMonth/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/daysInMonth/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/daysInWeek/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/daysInWeek/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/daysInWeek/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/daysInWeek/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/daysInWeek/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/daysInYear/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/daysInYear/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/daysInYear/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/daysInYear/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/daysInYear/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/inLeapYear/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/inLeapYear/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/inLeapYear/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/inLeapYear/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/inLeapYear/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/mergeFields/non-string-properties': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/mergeFields/order-of-operations': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/month/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/month/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/month/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/month/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/month/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/monthCode/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/monthCode/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/monthCode/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/monthCode/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/monthCode/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/monthsInYear/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/monthsInYear/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/monthsInYear/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/monthsInYear/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/monthsInYear/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/weekOfYear/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/weekOfYear/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/weekOfYear/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/weekOfYear/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/weekOfYear/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/year/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/year/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/year/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/year/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/Calendar/prototype/year/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/Duration/compare/relativeto-propertybag-timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'built-ins/Temporal/Duration/prototype/add/relativeto-propertybag-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/Duration/prototype/add/relativeto-propertybag-timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'built-ins/Temporal/Duration/prototype/add/relativeto-string-datetime': [FAIL],
+  'built-ins/Temporal/Duration/prototype/round/relativeto-propertybag-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/Duration/prototype/round/relativeto-propertybag-timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'built-ins/Temporal/Duration/prototype/round/relativeto-string-datetime': [FAIL],
+  'built-ins/Temporal/Duration/prototype/subtract/relativeto-propertybag-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/Duration/prototype/subtract/relativeto-propertybag-timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'built-ins/Temporal/Duration/prototype/subtract/relativeto-string-datetime': [FAIL],
+  'built-ins/Temporal/Duration/prototype/total/relativeto-propertybag-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/Duration/prototype/total/relativeto-propertybag-timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'built-ins/Temporal/Duration/prototype/total/relativeto-string-datetime': [FAIL],
+  'built-ins/Temporal/Instant/compare/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/Instant/compare/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/Instant/compare/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/Instant/from/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/Instant/from/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/Instant/from/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/Instant/prototype/equals/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/Instant/prototype/equals/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/Instant/prototype/equals/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/Instant/prototype/since/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/Instant/prototype/since/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/Instant/prototype/since/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/Instant/prototype/toString/timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'built-ins/Temporal/Instant/prototype/toZonedDateTime/calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/Instant/prototype/toZonedDateTimeISO/timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'built-ins/Temporal/Instant/prototype/toZonedDateTime/timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'built-ins/Temporal/Instant/prototype/until/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/Instant/prototype/until/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/Instant/prototype/until/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/Now/plainDate/calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/Now/plainDateISO/timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'built-ins/Temporal/Now/plainDateTime/calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/Now/plainDateTimeISO/timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'built-ins/Temporal/Now/plainDateTime/timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'built-ins/Temporal/Now/plainDate/timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'built-ins/Temporal/Now/plainTimeISO/timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'built-ins/Temporal/Now/zonedDateTime/calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/Now/zonedDateTimeISO/timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'built-ins/Temporal/Now/zonedDateTime/timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'built-ins/Temporal/PlainDate/calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainDate/compare/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainDate/compare/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainDate/compare/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainDate/compare/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainDate/compare/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainDate/from/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainDate/from/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainDate/from/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainDate/from/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainDate/from/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/equals/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/equals/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/equals/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/equals/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/equals/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/since/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/since/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/since/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/since/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/since/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/toPlainDateTime/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/toPlainDateTime/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/toPlainDateTime/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/toPlainMonthDay/calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/toPlainYearMonth/calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/toString/calendarname-critical': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/toString/calendar-tostring': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/toZonedDateTime/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/toZonedDateTime/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/toZonedDateTime/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/toZonedDateTime/timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/until/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/until/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/until/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/until/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/until/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/withCalendar/calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainDate/prototype/with/calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainDateTime/calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainDateTime/compare/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainDateTime/compare/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainDateTime/compare/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainDateTime/compare/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainDateTime/compare/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainDateTime/from/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainDateTime/from/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainDateTime/from/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainDateTime/from/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainDateTime/from/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/equals/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/equals/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/equals/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/equals/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/equals/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/since/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/since/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/since/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/since/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/since/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/toPlainMonthDay/calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/toPlainYearMonth/calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/toString/calendarname-critical': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/toString/calendar-tostring': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/toZonedDateTime/timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/until/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/until/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/until/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/until/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/until/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/withCalendar/calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/with/calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/withPlainDate/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/withPlainDate/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/withPlainDate/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/withPlainDate/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/withPlainDate/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/withPlainTime/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/withPlainTime/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainDateTime/prototype/withPlainTime/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainMonthDay/calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainMonthDay/from/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainMonthDay/from/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainMonthDay/from/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainMonthDay/from/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainMonthDay/from/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainMonthDay/prototype/equals/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainMonthDay/prototype/equals/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainMonthDay/prototype/equals/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainMonthDay/prototype/equals/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainMonthDay/prototype/equals/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainMonthDay/prototype/toPlainDate/calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainMonthDay/prototype/toString/calendarname-critical': [FAIL],
+  'built-ins/Temporal/PlainMonthDay/prototype/toString/calendar-tostring': [FAIL],
+  'built-ins/Temporal/PlainMonthDay/prototype/with/calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainTime/compare/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainTime/compare/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainTime/compare/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainTime/from/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainTime/from/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainTime/from/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainTime/prototype/equals/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainTime/prototype/equals/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainTime/prototype/equals/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainTime/prototype/since/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainTime/prototype/since/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainTime/prototype/since/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainTime/prototype/toPlainDateTime/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainTime/prototype/toPlainDateTime/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainTime/prototype/toPlainDateTime/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainTime/prototype/toPlainDateTime/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainTime/prototype/toPlainDateTime/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainTime/prototype/toZonedDateTime/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainTime/prototype/toZonedDateTime/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainTime/prototype/toZonedDateTime/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainTime/prototype/toZonedDateTime/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainTime/prototype/toZonedDateTime/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainTime/prototype/toZonedDateTime/timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'built-ins/Temporal/PlainTime/prototype/until/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainTime/prototype/until/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainTime/prototype/until/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/compare/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/compare/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/compare/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/compare/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/compare/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/from/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/from/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/from/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/from/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/from/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/add/calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/equals/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/equals/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/equals/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/equals/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/equals/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/since/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/since/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/since/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/since/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/since/calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/subtract/calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/toPlainDate/calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/toString/calendarname-critical': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/toString/calendar-tostring': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/until/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/until/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/until/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/until/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/until/calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/PlainYearMonth/prototype/with/calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/TimeZone/from/timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getInstantFor/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getInstantFor/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getInstantFor/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getInstantFor/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getInstantFor/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getNextTransition/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getNextTransition/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getNextTransition/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getOffsetNanosecondsFor/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getOffsetNanosecondsFor/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getOffsetNanosecondsFor/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getOffsetStringFor/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getOffsetStringFor/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getOffsetStringFor/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getPlainDateTimeFor/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getPlainDateTimeFor/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getPlainDateTimeFor/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getPlainDateTimeFor/calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getPossibleInstantsFor/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getPossibleInstantsFor/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getPossibleInstantsFor/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getPossibleInstantsFor/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getPossibleInstantsFor/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getPreviousTransition/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getPreviousTransition/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/TimeZone/prototype/getPreviousTransition/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/compare/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/compare/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/compare/argument-propertybag-timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/compare/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/compare/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/compare/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/from/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/from/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/from/argument-propertybag-timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/from/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/from/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/from/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/from/offset-overrides-critical-flag': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/equals/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/equals/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/equals/argument-propertybag-timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/equals/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/equals/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/equals/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/since/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/since/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/since/argument-propertybag-timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/since/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/since/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/since/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/toPlainMonthDay/calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/toPlainYearMonth/calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/toString/calendarname-critical': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/toString/calendar-tostring': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/toString/timezonename-critical': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/until/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/until/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/until/argument-propertybag-timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/until/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/until/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/until/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/withCalendar/calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/with/calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/withPlainDate/argument-calendar-fields-undefined': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/withPlainDate/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/withPlainDate/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/withPlainDate/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/withPlainDate/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/withPlainTime/argument-string-calendar-annotation': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/withPlainTime/argument-string-time-zone-annotation': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/withPlainTime/argument-string-unknown-annotation': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/prototype/withTimeZone/timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'built-ins/Temporal/ZonedDateTime/timezone-instance-does-not-get-timeZone-property': [FAIL],
+  'intl402/Temporal/Calendar/prototype/era/argument-calendar-fields-undefined': [FAIL],
+  'intl402/Temporal/Calendar/prototype/era/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'intl402/Temporal/Calendar/prototype/era/argument-string-calendar-annotation': [FAIL],
+  'intl402/Temporal/Calendar/prototype/era/argument-string-time-zone-annotation': [FAIL],
+  'intl402/Temporal/Calendar/prototype/era/argument-string-unknown-annotation': [FAIL],
+  'intl402/Temporal/Calendar/prototype/eraYear/argument-calendar-fields-undefined': [FAIL],
+  'intl402/Temporal/Calendar/prototype/eraYear/argument-propertybag-calendar-instance-does-not-get-calendar-property': [FAIL],
+  'intl402/Temporal/Calendar/prototype/eraYear/argument-string-calendar-annotation': [FAIL],
+  'intl402/Temporal/Calendar/prototype/eraYear/argument-string-time-zone-annotation': [FAIL],
+  'intl402/Temporal/Calendar/prototype/eraYear/argument-string-unknown-annotation': [FAIL],
 
   # https://bugs.chromium.org/p/v8/issues/detail?id=13342
   'built-ins/RegExp/property-escapes/generated/Alphabetic': [FAIL],
@@ -624,7 +963,6 @@
   'language/statements/class/decorator/syntax/valid/decorator-parenthesized-expr-identifier-reference-yield': [FAIL],
   'language/statements/class/elements/syntax/valid/grammar-field-accessor': [FAIL],
 
-
   # https://bugs.chromium.org/p/v8/issues/detail?id=11989
   'built-ins/ShadowRealm/prototype/evaluate/globalthis-ordinary-object': [FAIL],
   'built-ins/ShadowRealm/prototype/importValue/throws-if-exportname-not-string': [FAIL],
@@ -790,42 +1128,9 @@
 
   # Temporal staging test which use timeZone other than "UTC" or
   # calendar other than "iso8601" which are not supported in no i18n mode.
-  'staging/Temporal/Duration/old/add': [FAIL],
-  'staging/Temporal/Duration/old/subtract': [FAIL],
-  'staging/Temporal/Instant/old/toZonedDateTime': [FAIL],
-  'staging/Temporal/Instant/old/toZonedDateTimeISO': [FAIL],
-  'staging/Temporal/Regex/old/timezone': [FAIL],
-  'staging/Temporal/TimeZone/old/dst-change': [FAIL],
   'staging/Temporal/TimeZone/old/getInstantFor': [FAIL],
-  'staging/Temporal/TimeZone/old/getInstantFor-disambiguation': [FAIL],
   'staging/Temporal/TimeZone/old/getNextTransition': [FAIL],
-  'staging/Temporal/TimeZone/old/getPossibleInstantsFor': [FAIL],
-  'staging/Temporal/TimeZone/old/getPreviousTransition': [FAIL],
-  'staging/Temporal/TimeZone/old/timezone-america-la': [FAIL],
-  'staging/Temporal/UserTimezone/old/subminute-offset': [FAIL],
-  'staging/Temporal/UserTimezone/old/trivial-protocol': [FAIL],
-  'staging/Temporal/UserTimezone/old/trivial-subclass': [FAIL],
-  'staging/Temporal/ZonedDateTime/old/add': [FAIL],
-  'staging/Temporal/ZonedDateTime/old/compare': [FAIL],
-  'staging/Temporal/ZonedDateTime/old/date-time-hours-overflow': [FAIL],
-  'staging/Temporal/ZonedDateTime/old/dst-properties': [FAIL],
-  'staging/Temporal/ZonedDateTime/old/order-of-operations': [FAIL],
-  'staging/Temporal/ZonedDateTime/old/property-bags': [FAIL],
-  'staging/Temporal/ZonedDateTime/old/reversibility-of-differences': [FAIL],
-  'staging/Temporal/ZonedDateTime/old/round': [FAIL],
-  'staging/Temporal/ZonedDateTime/old/since': [FAIL],
-  'staging/Temporal/ZonedDateTime/old/string-parsing': [FAIL],
-  'staging/Temporal/ZonedDateTime/old/subtract': [FAIL],
-  'staging/Temporal/ZonedDateTime/old/toInstant': [FAIL],
-  'staging/Temporal/ZonedDateTime/old/toPlainDate': [FAIL],
-  'staging/Temporal/ZonedDateTime/old/toPlainTime': [FAIL],
-  'staging/Temporal/ZonedDateTime/old/toString': [FAIL],
-  'staging/Temporal/ZonedDateTime/old/until': [FAIL],
-  'staging/Temporal/ZonedDateTime/old/with': [FAIL],
-  'staging/Temporal/ZonedDateTime/old/withCalendar': [FAIL],
-  'staging/Temporal/ZonedDateTime/old/withPlainDate': [FAIL],
-  'staging/Temporal/ZonedDateTime/old/withPlainTime': [FAIL],
-  'staging/Temporal/ZonedDateTime/old/withTimezone': [FAIL],
+  'staging/Temporal/ZonedDateTime/old/construction-and-properties': [FAIL],
 }],  # no_i18n == True
 
 ['arch == arm or arch == arm64 or arch == mips64 or arch == mips64el', {
diff -r -u --color up/v8/test/test262/testcfg.py nw/v8/test/test262/testcfg.py
--- up/v8/test/test262/testcfg.py	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/test262/testcfg.py	2023-01-19 16:46:37.327275935 -0500
@@ -43,7 +43,6 @@
 FEATURE_FLAGS = {
     'Intl.NumberFormat-v3': '--harmony-intl-number-format-v3',
     'Intl.DurationFormat': '--harmony-intl-duration-format',
-    'Symbol.prototype.description': '--harmony-symbol-description',
     'FinalizationRegistry': '--harmony-weak-refs-with-cleanup-some',
     'WeakRef': '--harmony-weak-refs-with-cleanup-some',
     'host-gc-required': '--expose-gc-as=v8GC',
@@ -55,6 +54,8 @@
     'ShadowRealm': '--harmony-shadow-realm',
     'regexp-v-flag': '--harmony-regexp-unicode-sets',
     'array-grouping': '--harmony-array-grouping',
+    'change-array-by-copy': '--harmony-change-array-by-copy',
+    'symbols-as-weakmap-keys': '--harmony-symbol-as-weakmap-key',
 }
 
 SKIPPED_FEATURES = set([])
@@ -195,18 +196,18 @@
     return tokens[:2] == ["built-ins", "Atomics"]
 
   def _get_files_params(self):
-    return (
-        list(self.suite.harness) +
-        ([os.path.join(self.suite.root, "harness-agent.js")]
-         if self.__needs_harness_agent() else []) +
-        ([os.path.join(self.suite.root, "harness-ishtmldda.js")]
-         if "IsHTMLDDA" in self.test_record.get("features", []) else []) +
-        ([os.path.join(self.suite.root, "harness-adapt-donotevaluate.js")]
-         if self.fail_phase_only and not self._fail_phase_reverse else []) +
-        self._get_includes() +
-        (["--module"] if "module" in self.test_record else []) +
-        [self._get_source_path()]
-    )
+    harness_args = []
+    if "raw" not in self.test_record.get("flags", []):
+      harness_args = list(self.suite.harness)
+    return (harness_args + ([os.path.join(self.suite.root, "harness-agent.js")]
+                            if self.__needs_harness_agent() else []) +
+            ([os.path.join(self.suite.root, "harness-ishtmldda.js")]
+             if "IsHTMLDDA" in self.test_record.get("features", []) else []) +
+            ([os.path.join(self.suite.root, "harness-adapt-donotevaluate.js")]
+             if self.fail_phase_only and not self._fail_phase_reverse else []) +
+            self._get_includes() +
+            (["--module"] if "module" in self.test_record else []) +
+            [self._get_source_path()])
 
   def _get_suite_flags(self):
     return (
diff -r -u --color up/v8/test/unittests/BUILD.gn nw/v8/test/unittests/BUILD.gn
--- up/v8/test/unittests/BUILD.gn	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/BUILD.gn	2023-01-19 16:46:37.327275935 -0500
@@ -366,6 +366,7 @@
     "compiler/simplified-operator-unittest.cc",
     "compiler/sloppy-equality-unittest.cc",
     "compiler/state-values-utils-unittest.cc",
+    "compiler/turboshaft/snapshot-table-unittest.cc",
     "compiler/typed-optimization-unittest.cc",
     "compiler/typer-unittest.cc",
     "compiler/types-unittest.cc",
@@ -566,6 +567,13 @@
       "wasm/subtyping-unittest.cc",
       "wasm/wasm-code-manager-unittest.cc",
       "wasm/wasm-compiler-unittest.cc",
+      "wasm/wasm-disassembler-unittest-mvp.wasm.inc",
+      "wasm/wasm-disassembler-unittest-mvp.wat.inc",
+      "wasm/wasm-disassembler-unittest-names.wasm.inc",
+      "wasm/wasm-disassembler-unittest-names.wat.inc",
+      "wasm/wasm-disassembler-unittest-simd.wasm.inc",
+      "wasm/wasm-disassembler-unittest-simd.wat.inc",
+      "wasm/wasm-disassembler-unittest.cc",
       "wasm/wasm-macro-gen-unittest.cc",
       "wasm/wasm-module-builder-unittest.cc",
       "wasm/wasm-module-sourcemap-unittest.cc",
diff -r -u --color up/v8/test/unittests/api/api-wasm-unittest.cc nw/v8/test/unittests/api/api-wasm-unittest.cc
--- up/v8/test/unittests/api/api-wasm-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/api/api-wasm-unittest.cc	2023-01-19 16:46:37.327275935 -0500
@@ -156,70 +156,4 @@
                     Promise::kPending);
 }
 
-namespace {
-
-bool wasm_simd_enabled_value = false;
-bool wasm_exceptions_enabled_value = false;
-
-bool MockWasmSimdEnabledCallback(Local<Context>) {
-  return wasm_simd_enabled_value;
-}
-
-bool MockWasmExceptionsEnabledCallback(Local<Context>) {
-  return wasm_exceptions_enabled_value;
-}
-
-}  // namespace
-
-TEST_F(ApiWasmTest, TestSetWasmSimdEnabledCallback) {
-  Local<Context> context = Context::New(isolate());
-  i::Handle<i::Context> i_context = Utils::OpenHandle(*context);
-
-  // {Isolate::IsWasmSimdEnabled} calls the callback set by the embedder if
-  // such a callback exists. Otherwise it returns
-  // {v8_flags.experimental_wasm_simd}. First we test that the flag is returned
-  // correctly if no callback is set. Then we test that the flag is ignored if
-  // the callback is set.
-
-  i::v8_flags.experimental_wasm_simd = false;
-  CHECK(!i_isolate()->IsWasmSimdEnabled(i_context));
-
-  i::v8_flags.experimental_wasm_simd = true;
-  CHECK(i_isolate()->IsWasmSimdEnabled(i_context));
-
-  isolate()->SetWasmSimdEnabledCallback(MockWasmSimdEnabledCallback);
-  wasm_simd_enabled_value = false;
-  CHECK(!i_isolate()->IsWasmSimdEnabled(i_context));
-
-  wasm_simd_enabled_value = true;
-  i::v8_flags.experimental_wasm_simd = false;
-  CHECK(i_isolate()->IsWasmSimdEnabled(i_context));
-}
-
-TEST_F(ApiWasmTest, TestSetWasmExceptionsEnabledCallback) {
-  Local<Context> context = Context::New(isolate());
-  i::Handle<i::Context> i_context = Utils::OpenHandle(*context);
-
-  // {Isolate::AreWasmExceptionsEnabled} calls the callback set by the embedder
-  // if such a callback exists. Otherwise it returns
-  // {v8_flags.experimental_wasm_eh}. First we test that the flag is returned
-  // correctly if no callback is set. Then we test that the flag is ignored if
-  // the callback is set.
-
-  i::v8_flags.experimental_wasm_eh = false;
-  CHECK(!i_isolate()->AreWasmExceptionsEnabled(i_context));
-
-  i::v8_flags.experimental_wasm_eh = true;
-  CHECK(i_isolate()->AreWasmExceptionsEnabled(i_context));
-
-  isolate()->SetWasmExceptionsEnabledCallback(
-      MockWasmExceptionsEnabledCallback);
-  wasm_exceptions_enabled_value = false;
-  CHECK(!i_isolate()->AreWasmExceptionsEnabled(i_context));
-
-  wasm_exceptions_enabled_value = true;
-  i::v8_flags.experimental_wasm_eh = false;
-  CHECK(i_isolate()->AreWasmExceptionsEnabled(i_context));
-}
-
 }  // namespace v8
diff -r -u --color up/v8/test/unittests/api/deserialize-unittest.cc nw/v8/test/unittests/api/deserialize-unittest.cc
--- up/v8/test/unittests/api/deserialize-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/api/deserialize-unittest.cc	2023-01-19 16:46:37.327275935 -0500
@@ -9,6 +9,7 @@
 #include "include/v8-platform.h"
 #include "include/v8-primitive.h"
 #include "include/v8-script.h"
+#include "src/codegen/compilation-cache.h"
 #include "test/unittests/test-utils.h"
 #include "testing/gtest/include/gtest/gtest.h"
 
@@ -357,6 +358,9 @@
         }
       }
     }
+
+    i::ScanStackModeScopeForTesting no_stack_scanning(
+        i_isolate->heap(), i::Heap::ScanStackMode::kNone);
     i_isolate->heap()->CollectAllGarbage(i::Heap::kNoGCFlags,
                                          i::GarbageCollectionReason::kTesting);
 
@@ -505,6 +509,8 @@
     // At this point, the original_objects array might still have pointers to
     // some old discarded content, such as UncompiledData from flushed
     // functions. GC again to clear it all out.
+    i::ScanStackModeScopeForTesting no_stack_scanning(
+        i_isolate->heap(), i::Heap::ScanStackMode::kNone);
     i_isolate->heap()->CollectAllGarbage(i::Heap::kNoGCFlags,
                                          i::GarbageCollectionReason::kTesting);
 
@@ -715,4 +721,102 @@
            GetSharedFunctionInfo(original_script));
 }
 
+TEST_F(MergeDeserializedCodeTest, MergeThatCompilesLazyFunction) {
+  i::v8_flags.merge_background_deserialized_script_with_compilation_cache =
+      true;
+  std::unique_ptr<v8::ScriptCompiler::CachedData> cached_data;
+  IsolateAndContextScope scope(this);
+  i::Isolate* i_isolate = reinterpret_cast<i::Isolate*>(isolate());
+  ScriptOrigin default_origin(isolate(), NewString(""));
+
+  constexpr char kSourceCode[] =
+      "var f = function () {var s = f.toString(); f = null; return s;};";
+  constexpr uint8_t kFunctionText[] =
+      "function () {var s = f.toString(); f = null; return s;}";
+
+  // Compile the script for the first time to produce code cache data.
+  {
+    v8::HandleScope handle_scope(isolate());
+    Local<Script> script =
+        Script::Compile(context(), NewString(kSourceCode), &default_origin)
+            .ToLocalChecked();
+    CHECK(!script->Run(context()).IsEmpty());
+
+    // Cause the function to become compiled before creating the code cache.
+    Local<String> expected =
+        String::NewFromOneByte(isolate(), kFunctionText).ToLocalChecked();
+    Local<Value> actual = RunGlobalFunc("f");
+    CHECK(expected->StrictEquals(actual));
+
+    cached_data.reset(
+        ScriptCompiler::CreateCodeCache(script->GetUnboundScript()));
+  }
+
+  i_isolate->compilation_cache()->Clear();
+
+  // Compile the script for the second time, but don't run the function 'f'.
+  {
+    v8::HandleScope handle_scope(isolate());
+    Local<Script> script =
+        Script::Compile(context(), NewString(kSourceCode), &default_origin)
+            .ToLocalChecked();
+    CHECK(!script->Run(context()).IsEmpty());
+
+    // Age the top-level bytecode so that the Isolate compilation cache will
+    // contain only the Script.
+    i::BytecodeArray bytecode =
+        GetSharedFunctionInfo(script).GetBytecodeArray(i_isolate);
+    const int kAgingThreshold = 6;
+    for (int j = 0; j < kAgingThreshold; ++j) {
+      bytecode.MakeOlder();
+    }
+  }
+
+  i_isolate->heap()->CollectAllGarbage(i::Heap::kNoGCFlags,
+                                       i::GarbageCollectionReason::kTesting);
+
+  // A second round of GC is necessary in case incremental marking had already
+  // started before the bytecode was aged.
+  i_isolate->heap()->CollectAllGarbage(i::Heap::kNoGCFlags,
+                                       i::GarbageCollectionReason::kTesting);
+
+  DeserializeThread deserialize_thread(ScriptCompiler::StartConsumingCodeCache(
+      isolate(), std::make_unique<ScriptCompiler::CachedData>(
+                     cached_data->data, cached_data->length,
+                     ScriptCompiler::CachedData::BufferNotOwned)));
+  CHECK(deserialize_thread.Start());
+  deserialize_thread.Join();
+
+  std::unique_ptr<ScriptCompiler::ConsumeCodeCacheTask> task =
+      deserialize_thread.TakeTask();
+
+  // At this point, the cached script's function 'f' is not compiled, but the
+  // matching function in the deserialized graph is compiled, so a background
+  // merge is recommended.
+  task->SourceTextAvailable(isolate(), NewString(kSourceCode), default_origin);
+
+  CHECK(task->ShouldMergeWithExistingScript());
+
+  MergeThread merge_thread(task.get());
+  CHECK(merge_thread.Start());
+  merge_thread.Join();
+
+  // Complete compilation on the main thread. This step installs compiled data
+  // for the function 'f'.
+  ScriptCompiler::Source source(NewString(kSourceCode), default_origin,
+                                cached_data.release(), task.release());
+  Local<Script> script =
+      ScriptCompiler::Compile(context(), &source,
+                              ScriptCompiler::kConsumeCodeCache)
+          .ToLocalChecked();
+  CHECK(!script->Run(context()).IsEmpty());
+
+  // Ensure that we can get the string representation of 'f', which requires the
+  // ScopeInfo to be set correctly.
+  Local<String> expected =
+      String::NewFromOneByte(isolate(), kFunctionText).ToLocalChecked();
+  Local<Value> actual = RunGlobalFunc("f");
+  CHECK(expected->StrictEquals(actual));
+}
+
 }  // namespace v8
diff -r -u --color up/v8/test/unittests/api/isolate-unittest.cc nw/v8/test/unittests/api/isolate-unittest.cc
--- up/v8/test/unittests/api/isolate-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/api/isolate-unittest.cc	2023-01-19 16:46:37.327275935 -0500
@@ -145,17 +145,14 @@
   i::Isolate* i_isolate = reinterpret_cast<internal::Isolate*>(isolate());
   i::Heap* heap = i_isolate->heap();
 
-  size_t expected_keys_count = 4;
+  size_t expected_keys_count = 5;
   EXPECT_EQ(crash_keys.count(v8::CrashKeyId::kIsolateAddress), 1u);
   EXPECT_EQ(crash_keys.count(v8::CrashKeyId::kReadonlySpaceFirstPageAddress),
             1u);
+  EXPECT_EQ(crash_keys.count(v8::CrashKeyId::kOldSpaceFirstPageAddress), 1u);
   EXPECT_EQ(crash_keys.count(v8::CrashKeyId::kSnapshotChecksumCalculated), 1u);
   EXPECT_EQ(crash_keys.count(v8::CrashKeyId::kSnapshotChecksumExpected), 1u);
 
-  if (heap->map_space()) {
-    ++expected_keys_count;
-    EXPECT_EQ(crash_keys.count(v8::CrashKeyId::kMapSpaceFirstPageAddress), 1u);
-  }
   if (heap->code_range_base()) {
     ++expected_keys_count;
     EXPECT_EQ(crash_keys.count(v8::CrashKeyId::kCodeRangeBaseAddress), 1u);
Only in nw/v8/test/unittests/compiler: turboshaft
diff -r -u --color up/v8/test/unittests/debug/debug-property-iterator-unittest.cc nw/v8/test/unittests/debug/debug-property-iterator-unittest.cc
--- up/v8/test/unittests/debug/debug-property-iterator-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/debug/debug-property-iterator-unittest.cc	2023-01-19 16:46:37.359775927 -0500
@@ -148,6 +148,40 @@
   }
 }
 
+#if V8_CAN_CREATE_SHARED_HEAP_BOOL
+
+using SharedObjectDebugPropertyIteratorTest = TestJSSharedMemoryWithContext;
+
+TEST_F(SharedObjectDebugPropertyIteratorTest, SharedStruct) {
+  TryCatch try_catch(isolate());
+
+  const char source_text[] =
+      "let S = new SharedStructType(['field', 'another_field']);"
+      "new S();";
+
+  auto shared_struct =
+      RunJS(context(), source_text)->ToObject(context()).ToLocalChecked();
+  auto iterator = PropertyIterator::Create(context(), shared_struct);
+
+  ASSERT_NE(iterator, nullptr);
+  ASSERT_FALSE(iterator->Done());
+  EXPECT_TRUE(iterator->is_own());
+  char name_buffer[64];
+  iterator->name().As<v8::String>()->WriteUtf8(isolate(), name_buffer);
+  EXPECT_EQ("field", std::string(name_buffer));
+  ASSERT_TRUE(iterator->Advance().FromMaybe(false));
+
+  ASSERT_FALSE(iterator->Done());
+  EXPECT_TRUE(iterator->is_own());
+  iterator->name().As<v8::String>()->WriteUtf8(isolate(), name_buffer);
+  EXPECT_EQ("another_field", std::string(name_buffer));
+  ASSERT_TRUE(iterator->Advance().FromMaybe(false));
+
+  ASSERT_FALSE(iterator->Done());
+}
+
+#endif  // V8_CAN_CREATE_SHARED_HEAP_BOOL
+
 }  // namespace
 }  // namespace debug
 }  // namespace v8
diff -r -u --color up/v8/test/unittests/deoptimizer/deoptimization-unittest.cc nw/v8/test/unittests/deoptimizer/deoptimization-unittest.cc
--- up/v8/test/unittests/deoptimizer/deoptimization-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/deoptimizer/deoptimization-unittest.cc	2023-01-19 16:46:37.359775927 -0500
@@ -109,6 +109,20 @@
   bool turbo_inlining_;
 };
 
+namespace {
+void CheckJsInt32(int expected, const char* variable_name,
+                  v8::Local<v8::Context> context) {
+  v8::Local<v8::String> str =
+      v8::String::NewFromUtf8(context->GetIsolate(), variable_name)
+          .ToLocalChecked();
+  CHECK_EQ(expected, context->Global()
+                         ->Get(context, str)
+                         .ToLocalChecked()
+                         ->Int32Value(context)
+                         .FromJust());
+}
+}  // namespace
+
 TEST_F(DeoptimizationTest, DeoptimizeSimple) {
   ManualGCScope manual_gc_scope(i_isolate());
   v8::HandleScope scope(isolate());
@@ -124,13 +138,8 @@
         "f();");
   }
   CollectAllGarbage();
+  CheckJsInt32(1, "count", context());
 
-  CHECK_EQ(1, context()
-                  ->Global()
-                  ->Get(context(), NewString("count"))
-                  .ToLocalChecked()
-                  ->Int32Value(context())
-                  .FromJust());
   CHECK(!GetJSFunction("f")->HasAttachedOptimizedCode());
   CHECK_EQ(0, Deoptimizer::GetDeoptimizedCodeCount(i_isolate()));
 
@@ -146,12 +155,7 @@
   }
   CollectAllGarbage();
 
-  CHECK_EQ(1, context()
-                  ->Global()
-                  ->Get(context(), NewString("count"))
-                  .ToLocalChecked()
-                  ->Int32Value(context())
-                  .FromJust());
+  CheckJsInt32(1, "count", context());
   CHECK(!GetJSFunction("f")->HasAttachedOptimizedCode());
   CHECK_EQ(0, Deoptimizer::GetDeoptimizedCodeCount(i_isolate()));
 }
@@ -172,12 +176,7 @@
   }
   CollectAllGarbage();
 
-  CHECK_EQ(1, context()
-                  ->Global()
-                  ->Get(context(), NewString("count"))
-                  .ToLocalChecked()
-                  ->Int32Value(context())
-                  .FromJust());
+  CheckJsInt32(1, "count", context());
   CHECK(!GetJSFunction("f")->HasAttachedOptimizedCode());
   CHECK_EQ(0, Deoptimizer::GetDeoptimizedCodeCount(i_isolate()));
 
@@ -194,12 +193,7 @@
   }
   CollectAllGarbage();
 
-  CHECK_EQ(1, context()
-                  ->Global()
-                  ->Get(context(), NewString("count"))
-                  .ToLocalChecked()
-                  ->Int32Value(context())
-                  .FromJust());
+  CheckJsInt32(1, "count", context());
   CHECK(!GetJSFunction("f")->HasAttachedOptimizedCode());
   CHECK_EQ(0, Deoptimizer::GetDeoptimizedCodeCount(i_isolate()));
 }
@@ -221,18 +215,8 @@
         "result = f(1, 2, 3);");
     CollectAllGarbage();
 
-    CHECK_EQ(1, context()
-                    ->Global()
-                    ->Get(context(), NewString("count"))
-                    .ToLocalChecked()
-                    ->Int32Value(context())
-                    .FromJust());
-    CHECK_EQ(6, context()
-                    ->Global()
-                    ->Get(context(), NewString("result"))
-                    .ToLocalChecked()
-                    ->Int32Value(context())
-                    .FromJust());
+    CheckJsInt32(1, "count", context());
+    CheckJsInt32(6, "result", context());
     CHECK(!GetJSFunction("f")->HasAttachedOptimizedCode());
     CHECK_EQ(0, Deoptimizer::GetDeoptimizedCodeCount(i_isolate()));
   }
@@ -255,18 +239,8 @@
   }
   CollectAllGarbage();
 
-  CHECK_EQ(1, context()
-                  ->Global()
-                  ->Get(context(), NewString("count"))
-                  .ToLocalChecked()
-                  ->Int32Value(context())
-                  .FromJust());
-  CHECK_EQ(11, context()
-                   ->Global()
-                   ->Get(context(), NewString("calls"))
-                   .ToLocalChecked()
-                   ->Int32Value(context())
-                   .FromJust());
+  CheckJsInt32(1, "count", context());
+  CheckJsInt32(11, "calls", context());
   CHECK_EQ(0, Deoptimizer::GetDeoptimizedCodeCount(i_isolate()));
 
   v8::Local<v8::Function> fun = v8::Local<v8::Function>::Cast(
@@ -296,18 +270,8 @@
   }
   CollectAllGarbage();
 
-  CHECK_EQ(1, context()
-                  ->Global()
-                  ->Get(context(), NewString("count"))
-                  .ToLocalChecked()
-                  ->Int32Value(context())
-                  .FromJust());
-  CHECK_EQ(14, context()
-                   ->Global()
-                   ->Get(context(), NewString("result"))
-                   .ToLocalChecked()
-                   ->Int32Value(context())
-                   .FromJust());
+  CheckJsInt32(1, "count", context());
+  CheckJsInt32(14, "result", context());
   CHECK_EQ(0, Deoptimizer::GetDeoptimizedCodeCount(i_isolate()));
 }
 
@@ -326,12 +290,7 @@
   }
   CollectAllGarbage();
 
-  CHECK_EQ(1, context()
-                  ->Global()
-                  ->Get(context(), NewString("count"))
-                  .ToLocalChecked()
-                  ->Int32Value(context())
-                  .FromJust());
+  CheckJsInt32(1, "count", context());
   CHECK(context()
             ->Global()
             ->Get(context(), NewString("result"))
@@ -352,18 +311,8 @@
   }
   CollectAllGarbage();
 
-  CHECK_EQ(1, context()
-                  ->Global()
-                  ->Get(context(), NewString("count"))
-                  .ToLocalChecked()
-                  ->Int32Value(context())
-                  .FromJust());
-  CHECK_EQ(3, context()
-                  ->Global()
-                  ->Get(context(), NewString("result"))
-                  .ToLocalChecked()
-                  ->Int32Value(context())
-                  .FromJust());
+  CheckJsInt32(1, "count", context());
+  CheckJsInt32(3, "result", context());
   CHECK_EQ(0, Deoptimizer::GetDeoptimizedCodeCount(i_isolate()));
 }
 
@@ -390,18 +339,8 @@
   }
   CollectAllGarbage();
 
-  CHECK_EQ(1, context()
-                  ->Global()
-                  ->Get(context(), NewString("count"))
-                  .ToLocalChecked()
-                  ->Int32Value(context())
-                  .FromJust());
-  CHECK_EQ(14, context()
-                   ->Global()
-                   ->Get(context(), NewString("result"))
-                   .ToLocalChecked()
-                   ->Int32Value(context())
-                   .FromJust());
+  CheckJsInt32(1, "count", context());
+  CheckJsInt32(14, "result", context());
   CHECK_EQ(0, Deoptimizer::GetDeoptimizedCodeCount(i_isolate()));
 }
 
@@ -494,12 +433,7 @@
   CollectAllGarbage();
 
   CHECK(!GetJSFunction("f")->HasAttachedOptimizedCode());
-  CHECK_EQ(1, context()
-                  ->Global()
-                  ->Get(context(), NewString("count"))
-                  .ToLocalChecked()
-                  ->Int32Value(context())
-                  .FromJust());
+  CheckJsInt32(1, "count", context());
   v8::Local<v8::Value> result =
       context()->Global()->Get(context(), NewString("result")).ToLocalChecked();
   CHECK(result->IsString());
@@ -515,18 +449,8 @@
 
   TestDeoptimizeBinaryOp("+");
 
-  CHECK_EQ(1, context()
-                  ->Global()
-                  ->Get(context(), NewString("count"))
-                  .ToLocalChecked()
-                  ->Int32Value(context())
-                  .FromJust());
-  CHECK_EQ(15, context()
-                   ->Global()
-                   ->Get(context(), NewString("result"))
-                   .ToLocalChecked()
-                   ->Int32Value(context())
-                   .FromJust());
+  CheckJsInt32(1, "count", context());
+  CheckJsInt32(15, "result", context());
   CHECK_EQ(0, Deoptimizer::GetDeoptimizedCodeCount(i_isolate()));
 }
 
@@ -537,18 +461,8 @@
 
   TestDeoptimizeBinaryOp("-");
 
-  CHECK_EQ(1, context()
-                  ->Global()
-                  ->Get(context(), NewString("count"))
-                  .ToLocalChecked()
-                  ->Int32Value(context())
-                  .FromJust());
-  CHECK_EQ(-1, context()
-                   ->Global()
-                   ->Get(context(), NewString("result"))
-                   .ToLocalChecked()
-                   ->Int32Value(context())
-                   .FromJust());
+  CheckJsInt32(1, "count", context());
+  CheckJsInt32(-1, "result", context());
   CHECK_EQ(0, Deoptimizer::GetDeoptimizedCodeCount(i_isolate()));
 }
 
@@ -560,18 +474,8 @@
 
   TestDeoptimizeBinaryOp("*");
 
-  CHECK_EQ(1, context()
-                  ->Global()
-                  ->Get(context(), NewString("count"))
-                  .ToLocalChecked()
-                  ->Int32Value(context())
-                  .FromJust());
-  CHECK_EQ(56, context()
-                   ->Global()
-                   ->Get(context(), NewString("result"))
-                   .ToLocalChecked()
-                   ->Int32Value(context())
-                   .FromJust());
+  CheckJsInt32(1, "count", context());
+  CheckJsInt32(56, "result", context());
   CHECK_EQ(0, Deoptimizer::GetDeoptimizedCodeCount(i_isolate()));
 }
 
@@ -582,18 +486,8 @@
 
   TestDeoptimizeBinaryOp("/");
 
-  CHECK_EQ(1, context()
-                  ->Global()
-                  ->Get(context(), NewString("count"))
-                  .ToLocalChecked()
-                  ->Int32Value(context())
-                  .FromJust());
-  CHECK_EQ(0, context()
-                  ->Global()
-                  ->Get(context(), NewString("result"))
-                  .ToLocalChecked()
-                  ->Int32Value(context())
-                  .FromJust());
+  CheckJsInt32(1, "count", context());
+  CheckJsInt32(0, "result", context());
   CHECK_EQ(0, Deoptimizer::GetDeoptimizedCodeCount(i_isolate()));
 }
 
@@ -604,18 +498,8 @@
 
   TestDeoptimizeBinaryOp("%");
 
-  CHECK_EQ(1, context()
-                  ->Global()
-                  ->Get(context(), NewString("count"))
-                  .ToLocalChecked()
-                  ->Int32Value(context())
-                  .FromJust());
-  CHECK_EQ(7, context()
-                  ->Global()
-                  ->Get(context(), NewString("result"))
-                  .ToLocalChecked()
-                  ->Int32Value(context())
-                  .FromJust());
+  CheckJsInt32(1, "count", context());
+  CheckJsInt32(7, "result", context());
   CHECK_EQ(0, Deoptimizer::GetDeoptimizedCodeCount(i_isolate()));
 }
 
@@ -659,17 +543,13 @@
   CollectAllGarbage();
 
   CHECK(!GetJSFunction("f")->HasAttachedOptimizedCode());
-  CHECK_EQ(1, context()
-                  ->Global()
-                  ->Get(context(), NewString("count"))
-                  .ToLocalChecked()
-                  ->Int32Value(context())
-                  .FromJust());
-  CHECK_EQ(true, context()
-                     ->Global()
-                     ->Get(context(), NewString("result"))
-                     .ToLocalChecked()
-                     ->BooleanValue(isolate()));
+  CheckJsInt32(1, "count", context());
+  CheckJsInt32(1, "result", context());
+  CHECK(context()
+            ->Global()
+            ->Get(context(), NewString("result"))
+            .ToLocalChecked()
+            ->IsTrue());
   CHECK_EQ(0, Deoptimizer::GetDeoptimizedCodeCount(i_isolate()));
 }
 
@@ -751,18 +631,8 @@
   CHECK(!GetJSFunction("g1")->HasAttachedOptimizedCode());
   CHECK(!GetJSFunction("f2")->HasAttachedOptimizedCode());
   CHECK(!GetJSFunction("g2")->HasAttachedOptimizedCode());
-  CHECK_EQ(4, context()
-                  ->Global()
-                  ->Get(context(), NewString("count"))
-                  .ToLocalChecked()
-                  ->Int32Value(context())
-                  .FromJust());
-  CHECK_EQ(13, context()
-                   ->Global()
-                   ->Get(context(), NewString("result"))
-                   .ToLocalChecked()
-                   ->Int32Value(context())
-                   .FromJust());
+  CheckJsInt32(4, "count", context());
+  CheckJsInt32(13, "result", context());
 }
 
 TEST_F(DeoptimizationDisableConcurrentRecompilationTest,
@@ -844,18 +714,8 @@
   CHECK(!GetJSFunction("g1")->HasAttachedOptimizedCode());
   CHECK(!GetJSFunction("f2")->HasAttachedOptimizedCode());
   CHECK(!GetJSFunction("g2")->HasAttachedOptimizedCode());
-  CHECK_EQ(1, context()
-                  ->Global()
-                  ->Get(context(), NewString("count"))
-                  .ToLocalChecked()
-                  ->Int32Value(context())
-                  .FromJust());
-  CHECK_EQ(13, context()
-                   ->Global()
-                   ->Get(context(), NewString("result"))
-                   .ToLocalChecked()
-                   ->Int32Value(context())
-                   .FromJust());
+  CheckJsInt32(1, "count", context());
+  CheckJsInt32(13, "result", context());
 }
 
 }  // namespace internal
diff -r -u --color up/v8/test/unittests/execution/microtask-queue-unittest.cc nw/v8/test/unittests/execution/microtask-queue-unittest.cc
--- up/v8/test/unittests/execution/microtask-queue-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/execution/microtask-queue-unittest.cc	2023-01-19 16:46:37.359775927 -0500
@@ -146,9 +146,10 @@
   bool ran = false;
   EXPECT_EQ(0, microtask_queue()->capacity());
   EXPECT_EQ(0, microtask_queue()->size());
-  microtask_queue()->EnqueueMicrotask(*NewMicrotask([&ran] {
+  microtask_queue()->EnqueueMicrotask(*NewMicrotask([this, &ran] {
     EXPECT_FALSE(ran);
     ran = true;
+    EXPECT_TRUE(microtask_queue()->HasMicrotasksSuppressions());
   }));
   EXPECT_EQ(MicrotaskQueue::kMinimumCapacity, microtask_queue()->capacity());
   EXPECT_EQ(1, microtask_queue()->size());
diff -r -u --color up/v8/test/unittests/heap/conservative-stack-visitor-unittest.cc nw/v8/test/unittests/heap/conservative-stack-visitor-unittest.cc
--- up/v8/test/unittests/heap/conservative-stack-visitor-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/heap/conservative-stack-visitor-unittest.cc	2023-01-19 16:46:37.543942542 -0500
@@ -79,7 +79,7 @@
   auto recorder = std::make_unique<RecordingVisitor>(isolate());
 
   // Ensure the heap is iterable before CSS.
-  SafepointScope safepoint_scope(heap());
+  IsolateSafepointScope safepoint_scope(heap());
   heap()->MakeHeapIterable();
 
   {
@@ -100,7 +100,7 @@
   auto recorder = std::make_unique<RecordingVisitor>(isolate());
 
   // Ensure the heap is iterable before CSS.
-  SafepointScope safepoint_scope(heap());
+  IsolateSafepointScope safepoint_scope(heap());
   heap()->MakeHeapIterable();
 
   {
@@ -121,7 +121,7 @@
   auto recorder = std::make_unique<RecordingVisitor>(isolate());
 
   // Ensure the heap is iterable before CSS.
-  SafepointScope safepoint_scope(heap());
+  IsolateSafepointScope safepoint_scope(heap());
   heap()->MakeHeapIterable();
 
   {
@@ -144,7 +144,7 @@
   auto recorder = std::make_unique<RecordingVisitor>(isolate());
 
   // Ensure the heap is iterable before CSS.
-  SafepointScope safepoint_scope(heap());
+  IsolateSafepointScope safepoint_scope(heap());
   heap()->MakeHeapIterable();
 
   {
@@ -165,7 +165,7 @@
   auto recorder = std::make_unique<RecordingVisitor>(isolate());
 
   // Ensure the heap is iterable before CSS.
-  SafepointScope safepoint_scope(heap());
+  IsolateSafepointScope safepoint_scope(heap());
   heap()->MakeHeapIterable();
 
   {
@@ -186,7 +186,7 @@
   auto recorder = std::make_unique<RecordingVisitor>(isolate());
 
   // Ensure the heap is iterable before CSS.
-  SafepointScope safepoint_scope(heap());
+  IsolateSafepointScope safepoint_scope(heap());
   heap()->MakeHeapIterable();
 
   {
@@ -207,7 +207,7 @@
   auto recorder = std::make_unique<RecordingVisitor>(isolate());
 
   // Ensure the heap is iterable before CSS.
-  SafepointScope safepoint_scope(heap());
+  IsolateSafepointScope safepoint_scope(heap());
   heap()->MakeHeapIterable();
 
   {
diff -r -u --color up/v8/test/unittests/heap/cppgc/minor-gc-unittest.cc nw/v8/test/unittests/heap/cppgc/minor-gc-unittest.cc
--- up/v8/test/unittests/heap/cppgc/minor-gc-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/heap/cppgc/minor-gc-unittest.cc	2023-01-19 16:46:37.543942542 -0500
@@ -151,13 +151,28 @@
     Heap::From(GetHeap())->CollectGarbage(GCConfig::MinorPreciseAtomicConfig());
   }
 
+  void CollectMinorWithStack() {
+    Heap::From(GetHeap())->CollectGarbage(
+        GCConfig::MinorConservativeAtomicConfig());
+  }
+
   void CollectMajor() {
     Heap::From(GetHeap())->CollectGarbage(GCConfig::PreciseAtomicConfig());
   }
 
+  void CollectMajorWithStack() {
+    Heap::From(GetHeap())->CollectGarbage(GCConfig::ConservativeAtomicConfig());
+  }
+
   const auto& RememberedSourceObjects() const {
     return Heap::From(GetHeap())->remembered_set().remembered_source_objects_;
   }
+
+  const auto& RememberedInConstructionObjects() const {
+    return Heap::From(GetHeap())
+        ->remembered_set()
+        .remembered_in_construction_objects_.previous;
+  }
 };
 
 template <typename SmallOrLarge>
@@ -170,10 +185,33 @@
 TYPED_TEST_SUITE(MinorGCTestForType, ObjectTypes);
 
 namespace {
-template <typename... Args>
-void RunMinorGCAndExpectObjectsPromoted(MinorGCTest& test, Args*... args) {
+
+enum class GCType {
+  kMinor,
+  kMajor,
+};
+
+enum class StackType {
+  kWithout,
+  kWith,
+};
+
+template <GCType gc_type, StackType stack_type, typename... Args>
+void RunGCAndExpectObjectsPromoted(MinorGCTest& test, Args*... args) {
   EXPECT_TRUE((IsHeapObjectYoung(args) && ...));
-  test.CollectMinor();
+  if constexpr (gc_type == GCType::kMajor) {
+    if constexpr (stack_type == StackType::kWithout) {
+      test.CollectMajor();
+    } else {
+      test.CollectMajorWithStack();
+    }
+  } else {
+    if constexpr (stack_type == StackType::kWithout) {
+      test.CollectMinor();
+    } else {
+      test.CollectMinorWithStack();
+    }
+  }
   EXPECT_TRUE((IsHeapObjectOld(args) && ...));
 }
 
@@ -595,7 +633,7 @@
                       &HeapObjectHeader::FromObject(array->objects)));
   }
 
-  RunMinorGCAndExpectObjectsPromoted(
+  RunGCAndExpectObjectsPromoted<GCType::kMinor, StackType::kWithout>(
       *this, array->objects[2].ref.Get(), array->objects[2].inner.ref.Get(),
       array->objects[3].ref.Get(), array->objects[3].inner.ref.Get());
 
@@ -692,6 +730,152 @@
   ExpectPageOld(*BasePage::FromPayload(p3.Get()));
 }
 
+namespace {
+
+template <GCType type>
+struct GCOnConstruction {
+  explicit GCOnConstruction(MinorGCTest& test, size_t depth) {
+    if constexpr (type == GCType::kMajor) {
+      test.CollectMajorWithStack();
+    } else {
+      test.CollectMinorWithStack();
+    }
+    EXPECT_EQ(depth, test.RememberedInConstructionObjects().size());
+  }
+};
+
+template <GCType type>
+struct InConstructionWithYoungRef
+    : GarbageCollected<InConstructionWithYoungRef<type>> {
+  using ValueType = SimpleGCed<64>;
+
+  explicit InConstructionWithYoungRef(MinorGCTest& test)
+      : call_gc(test, 1u),
+        m(MakeGarbageCollected<ValueType>(test.GetAllocationHandle())) {}
+
+  void Trace(Visitor* v) const { v->Trace(m); }
+
+  GCOnConstruction<type> call_gc;
+  Member<ValueType> m;
+};
+
+}  // namespace
+
+TEST_F(MinorGCTest, RevisitInConstructionObjectsMinorMinorWithStack) {
+  static constexpr auto kFirstGCType = GCType::kMinor;
+
+  auto* gced = MakeGarbageCollected<InConstructionWithYoungRef<kFirstGCType>>(
+      GetAllocationHandle(), *this);
+
+  RunGCAndExpectObjectsPromoted<GCType::kMinor, StackType::kWith>(
+      *this, gced->m.Get());
+
+  EXPECT_EQ(0u, RememberedInConstructionObjects().size());
+}
+
+TEST_F(MinorGCTest, RevisitInConstructionObjectsMinorMinorWithoutStack) {
+  static constexpr auto kFirstGCType = GCType::kMinor;
+
+  Persistent<InConstructionWithYoungRef<kFirstGCType>> gced =
+      MakeGarbageCollected<InConstructionWithYoungRef<kFirstGCType>>(
+          GetAllocationHandle(), *this);
+
+  RunGCAndExpectObjectsPromoted<GCType::kMinor, StackType::kWithout>(
+      *this, gced->m.Get());
+
+  EXPECT_EQ(0u, RememberedInConstructionObjects().size());
+}
+
+TEST_F(MinorGCTest, RevisitInConstructionObjectsMajorMinorWithStack) {
+  static constexpr auto kFirstGCType = GCType::kMajor;
+
+  auto* gced = MakeGarbageCollected<InConstructionWithYoungRef<kFirstGCType>>(
+      GetAllocationHandle(), *this);
+
+  RunGCAndExpectObjectsPromoted<GCType::kMinor, StackType::kWith>(
+      *this, gced->m.Get());
+
+  EXPECT_EQ(0u, RememberedInConstructionObjects().size());
+}
+
+TEST_F(MinorGCTest, RevisitInConstructionObjectsMajorMinorWithoutStack) {
+  static constexpr auto kFirstGCType = GCType::kMajor;
+
+  Persistent<InConstructionWithYoungRef<kFirstGCType>> gced =
+      MakeGarbageCollected<InConstructionWithYoungRef<kFirstGCType>>(
+          GetAllocationHandle(), *this);
+
+  RunGCAndExpectObjectsPromoted<GCType::kMinor, StackType::kWithout>(
+      *this, gced->m.Get());
+
+  EXPECT_EQ(0u, RememberedInConstructionObjects().size());
+}
+
+TEST_F(MinorGCTest, PreviousInConstructionObjectsAreDroppedAfterFullGC) {
+  MakeGarbageCollected<InConstructionWithYoungRef<GCType::kMinor>>(
+      GetAllocationHandle(), *this);
+
+  EXPECT_EQ(1u, RememberedInConstructionObjects().size());
+
+  CollectMajor();
+
+  EXPECT_EQ(0u, RememberedInConstructionObjects().size());
+}
+
+namespace {
+
+template <GCType type>
+struct NestedInConstructionWithYoungRef
+    : GarbageCollected<NestedInConstructionWithYoungRef<type>> {
+  using ValueType = SimpleGCed<64>;
+
+  NestedInConstructionWithYoungRef(MinorGCTest& test, size_t depth)
+      : NestedInConstructionWithYoungRef(test, 1, depth) {}
+
+  NestedInConstructionWithYoungRef(MinorGCTest& test, size_t current_depth,
+                                   size_t max_depth)
+      : current_depth(current_depth),
+        max_depth(max_depth),
+        next(current_depth != max_depth
+                 ? MakeGarbageCollected<NestedInConstructionWithYoungRef<type>>(
+                       test.GetAllocationHandle(), test, current_depth + 1,
+                       max_depth)
+                 : nullptr),
+        call_gc(test, current_depth),
+        m(MakeGarbageCollected<ValueType>(test.GetAllocationHandle())) {}
+
+  void Trace(Visitor* v) const {
+    v->Trace(next);
+    v->Trace(m);
+  }
+
+  size_t current_depth = 0;
+  size_t max_depth = 0;
+
+  Member<NestedInConstructionWithYoungRef<type>> next;
+  GCOnConstruction<type> call_gc;
+  Member<ValueType> m;
+};
+
+}  // namespace
+
+TEST_F(MinorGCTest, RevisitNestedInConstructionObjects) {
+  static constexpr auto kFirstGCType = GCType::kMinor;
+
+  Persistent<NestedInConstructionWithYoungRef<kFirstGCType>> gced =
+      MakeGarbageCollected<NestedInConstructionWithYoungRef<kFirstGCType>>(
+          GetAllocationHandle(), *this, 10);
+
+  CollectMinor();
+
+  for (auto* p = gced.Get(); p; p = p->next.Get()) {
+    EXPECT_TRUE(IsHeapObjectOld(p));
+    EXPECT_TRUE(IsHeapObjectOld(p->m));
+  }
+
+  EXPECT_EQ(0u, RememberedInConstructionObjects().size());
+}
+
 }  // namespace internal
 }  // namespace cppgc
 
diff -r -u --color up/v8/test/unittests/heap/cppgc-js/unified-heap-unittest.cc nw/v8/test/unittests/heap/cppgc-js/unified-heap-unittest.cc
--- up/v8/test/unittests/heap/cppgc-js/unified-heap-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/heap/cppgc-js/unified-heap-unittest.cc	2023-01-19 16:46:37.543942542 -0500
@@ -359,6 +359,8 @@
   InConstructionObjectReferringToGlobalHandle(Heap* heap,
                                               v8::Local<v8::Object> wrapper)
       : wrapper_(reinterpret_cast<v8::Isolate*>(heap->isolate()), wrapper) {
+    ScanStackModeScopeForTesting no_stack_scanning(heap,
+                                                   Heap::ScanStackMode::kNone);
     heap->CollectGarbage(OLD_SPACE, GarbageCollectionReason::kTesting);
     heap->CollectGarbage(OLD_SPACE, GarbageCollectionReason::kTesting);
   }
diff -r -u --color up/v8/test/unittests/heap/embedder-tracing-unittest.cc nw/v8/test/unittests/heap/embedder-tracing-unittest.cc
--- up/v8/test/unittests/heap/embedder-tracing-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/heap/embedder-tracing-unittest.cc	2023-01-19 16:46:37.543942542 -0500
@@ -466,7 +466,7 @@
 
   i::IncrementalMarking* marking = heap->incremental_marking();
   {
-    SafepointScope scope(heap);
+    IsolateSafepointScope scope(heap);
     heap->tracer()->StartCycle(
         GarbageCollector::MARK_COMPACTOR, GarbageCollectionReason::kTesting,
         "collector cctest", GCTracer::MarkingType::kIncremental);
@@ -545,9 +545,9 @@
 TEST_F(EmbedderTracingTest, TracedReferenceCopyReferences) {
   ManualGCScope manual_gc(i_isolate());
   v8::HandleScope outer_scope(v8_isolate());
-  i::GlobalHandles* global_handles = i_isolate()->global_handles();
+  auto* traced_handles = i_isolate()->traced_handles();
 
-  const size_t initial_count = global_handles->handles_count();
+  const size_t initial_count = traced_handles->used_node_count();
   auto handle1 = std::make_unique<v8::TracedReference<v8::Value>>();
   {
     v8::HandleScope scope(v8_isolate());
@@ -556,7 +556,7 @@
   auto handle2 = std::make_unique<v8::TracedReference<v8::Value>>(*handle1);
   auto handle3 = std::make_unique<v8::TracedReference<v8::Value>>();
   *handle3 = *handle2;
-  EXPECT_EQ(initial_count + 3, global_handles->handles_count());
+  EXPECT_EQ(initial_count + 3, traced_handles->used_node_count());
   EXPECT_FALSE(handle1->IsEmpty());
   EXPECT_EQ(*handle1, *handle2);
   EXPECT_EQ(*handle2, *handle3);
@@ -574,7 +574,7 @@
             EmbedderHeapTracer::EmbedderStackState::kNoHeapPointers);
     FullGC();
   }
-  EXPECT_EQ(initial_count, global_handles->handles_count());
+  EXPECT_EQ(initial_count, traced_handles->used_node_count());
 }
 
 TEST_F(EmbedderTracingTest, TracedReferenceToUnmodifiedJSObjectDiesOnFullGC) {
@@ -681,12 +681,12 @@
   auto dead = std::make_unique<v8::TracedReference<v8::Value>>();
   live->Reset(v8_isolate(), v8::Undefined(v8_isolate()));
   dead->Reset(v8_isolate(), v8::Undefined(v8_isolate()));
-  i::GlobalHandles* global_handles = i_isolate()->global_handles();
+  auto* traced_handles = i_isolate()->traced_handles();
   {
     TestEmbedderHeapTracer tracer;
     heap::TemporaryEmbedderHeapTracerScope tracer_scope(v8_isolate(), &tracer);
     tracer.AddReferenceForTracing(live.get());
-    const size_t initial_count = global_handles->handles_count();
+    const size_t initial_count = traced_handles->used_node_count();
     {
       // Conservative scanning may find stale pointers to on-stack handles.
       // Disable scanning, assuming the slots are overwritten.
@@ -698,7 +698,7 @@
               EmbedderHeapTracer::EmbedderStackState::kNoHeapPointers);
       FullGC();
     }
-    const size_t final_count = global_handles->handles_count();
+    const size_t final_count = traced_handles->used_node_count();
     // Handles are not black allocated, so `dead` is immediately reclaimed.
     EXPECT_EQ(initial_count, final_count + 1);
   }
@@ -712,12 +712,12 @@
   v8::HandleScope scope(v8_isolate());
   auto ref = std::make_unique<v8::TracedReference<v8::Value>>();
   ref->Reset(v8_isolate(), v8::Undefined(v8_isolate()));
-  i::GlobalHandles* global_handles = i_isolate()->global_handles();
-  const size_t initial_count = global_handles->handles_count();
+  auto* traced_handles = i_isolate()->traced_handles();
+  const size_t initial_count = traced_handles->used_node_count();
   // We need two GCs because handles are black allocated.
   FullGC();
   FullGC();
-  const size_t final_count = global_handles->handles_count();
+  const size_t final_count = traced_handles->used_node_count();
   EXPECT_EQ(initial_count, final_count + 1);
 }
 
@@ -792,9 +792,9 @@
   heap::TemporaryEmbedderHeapTracerScope tracer_scope(v8_isolate(), &tracer);
   tracer.SetStackStart(
       static_cast<void*>(base::Stack::GetCurrentFrameAddress()));
-  i::GlobalHandles* global_handles = i_isolate()->global_handles();
+  auto* traced_handles = i_isolate()->traced_handles();
 
-  const size_t initial_count = global_handles->handles_count();
+  const size_t initial_count = traced_handles->used_node_count();
   char* memory = new char[sizeof(v8::TracedReference<v8::Value>)];
   auto* traced = new (memory) v8::TracedReference<v8::Value>();
   {
@@ -804,10 +804,10 @@
     EXPECT_TRUE(traced->IsEmpty());
     *traced = v8::TracedReference<v8::Value>(v8_isolate(), object);
     EXPECT_FALSE(traced->IsEmpty());
-    EXPECT_EQ(initial_count + 1, global_handles->handles_count());
+    EXPECT_EQ(initial_count + 1, traced_handles->used_node_count());
   }
   traced->~TracedReference<v8::Value>();
-  EXPECT_EQ(initial_count + 1, global_handles->handles_count());
+  EXPECT_EQ(initial_count + 1, traced_handles->used_node_count());
   {
     // Conservative scanning may find stale pointers to on-stack handles.
     // Disable scanning, assuming the slots are overwritten.
@@ -819,7 +819,7 @@
             EmbedderHeapTracer::EmbedderStackState::kNoHeapPointers);
     FullGC();
   }
-  EXPECT_EQ(initial_count, global_handles->handles_count());
+  EXPECT_EQ(initial_count, traced_handles->used_node_count());
   delete[] memory;
 }
 
@@ -902,22 +902,22 @@
   constexpr uint16_t kClassIdToOptimize = 23;
   EmbedderHeapTracerNoDestructorNonTracingClearing tracer(kClassIdToOptimize);
   heap::TemporaryEmbedderHeapTracerScope tracer_scope(v8_isolate(), &tracer);
-  i::GlobalHandles* global_handles = i_isolate()->global_handles();
+  auto* traced_handles = i_isolate()->traced_handles();
 
-  const size_t initial_count = global_handles->handles_count();
+  const size_t initial_count = traced_handles->used_node_count();
   auto* optimized_handle = new v8::TracedReference<v8::Value>();
   auto* non_optimized_handle = new v8::TracedReference<v8::Value>();
   SetupOptimizedAndNonOptimizedHandle(v8_isolate(), kClassIdToOptimize,
                                       optimized_handle, non_optimized_handle);
-  EXPECT_EQ(initial_count + 2, global_handles->handles_count());
+  EXPECT_EQ(initial_count + 2, traced_handles->used_node_count());
   YoungGC();
-  EXPECT_EQ(initial_count + 1, global_handles->handles_count());
+  EXPECT_EQ(initial_count + 1, traced_handles->used_node_count());
   EXPECT_TRUE(optimized_handle->IsEmpty());
   delete optimized_handle;
   EXPECT_FALSE(non_optimized_handle->IsEmpty());
   non_optimized_handle->Reset();
   delete non_optimized_handle;
-  EXPECT_EQ(initial_count, global_handles->handles_count());
+  EXPECT_EQ(initial_count, traced_handles->used_node_count());
 }
 
 namespace {
diff -r -u --color up/v8/test/unittests/heap/gc-tracer-unittest.cc nw/v8/test/unittests/heap/gc-tracer-unittest.cc
--- up/v8/test/unittests/heap/gc-tracer-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/heap/gc-tracer-unittest.cc	2023-01-19 16:46:37.543942542 -0500
@@ -103,10 +103,16 @@
   tracer->StopAtomicPause();
   tracer->StopObservablePause();
   tracer->UpdateStatistics(collector);
-  if (Heap::IsYoungGenerationCollector(collector)) {
-    tracer->StopYoungCycleIfNeeded();
-  } else {
-    tracer->NotifySweepingCompleted();
+  switch (collector) {
+    case GarbageCollector::SCAVENGER:
+      tracer->StopYoungCycleIfNeeded();
+      break;
+    case GarbageCollector::MINOR_MARK_COMPACTOR:
+      tracer->NotifyYoungSweepingCompleted();
+      break;
+    case GarbageCollector::MARK_COMPACTOR:
+      tracer->NotifyFullSweepingCompleted();
+      break;
   }
 }
 
diff -r -u --color up/v8/test/unittests/heap/global-handles-unittest.cc nw/v8/test/unittests/heap/global-handles-unittest.cc
--- up/v8/test/unittests/heap/global-handles-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/heap/global-handles-unittest.cc	2023-01-19 16:46:37.543942542 -0500
@@ -606,15 +606,15 @@
   v8::HandleScope scope(isolate);
 
   v8::TracedReference<v8::Object>* handle = new TracedReference<v8::Object>();
-  CHECK_EQ(i_isolate()->global_handles()->TotalSize(), 0);
-  CHECK_EQ(i_isolate()->global_handles()->UsedSize(), 0);
+  CHECK_EQ(i_isolate()->traced_handles()->total_size_bytes(), 0);
+  CHECK_EQ(i_isolate()->traced_handles()->used_size_bytes(), 0);
   ConstructJSObject(isolate, handle);
-  CHECK_GT(i_isolate()->global_handles()->TotalSize(), 0);
-  CHECK_GT(i_isolate()->global_handles()->UsedSize(), 0);
+  CHECK_GT(i_isolate()->traced_handles()->total_size_bytes(), 0);
+  CHECK_GT(i_isolate()->traced_handles()->used_size_bytes(), 0);
   delete handle;
   CollectAllGarbage();
-  CHECK_GT(i_isolate()->global_handles()->TotalSize(), 0);
-  CHECK_EQ(i_isolate()->global_handles()->UsedSize(), 0);
+  CHECK_GT(i_isolate()->traced_handles()->total_size_bytes(), 0);
+  CHECK_EQ(i_isolate()->traced_handles()->used_size_bytes(), 0);
 }
 
 }  // namespace internal
diff -r -u --color up/v8/test/unittests/heap/heap-unittest.cc nw/v8/test/unittests/heap/heap-unittest.cc
--- up/v8/test/unittests/heap/heap-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/heap/heap-unittest.cc	2023-01-19 16:46:37.543942542 -0500
@@ -162,7 +162,7 @@
   base::AddressRegion heap_reservation(cage_base, size_t{4} * GB);
   base::AddressRegion code_reservation(code_cage_base, size_t{4} * GB);
 
-  SafepointScope scope(i_isolate()->heap());
+  IsolateSafepointScope scope(i_isolate()->heap());
   OldGenerationMemoryChunkIterator iter(i_isolate()->heap());
   for (;;) {
     MemoryChunk* chunk = iter.next();
@@ -208,7 +208,7 @@
   paged_new_space->FinishShrinking();
   tracer->StopAtomicPause();
   tracer->StopObservablePause();
-  tracer->NotifySweepingCompleted();
+  tracer->NotifyFullSweepingCompleted();
 }
 }  // namespace
 
@@ -373,9 +373,10 @@
     CHECK(!new_space->IsAtMaximumCapacity());
     // Fill current pages to force MinorMC to promote them.
     SimulateFullSpace(new_space, &handles);
-    SafepointScope scope(heap);
+    IsolateSafepointScope scope(heap);
     // New empty pages should remain in new space.
     new_space->Grow();
+    CHECK(new_space->EnsureCurrentCapacity());
   } else {
     CollectGarbage(i::NEW_SPACE);
   }
@@ -422,7 +423,7 @@
   // 5. Start incremental marking.
   i::IncrementalMarking* marking = heap->incremental_marking();
   if (marking->IsStopped()) {
-    SafepointScope scope(heap);
+    IsolateSafepointScope scope(heap);
     heap->tracer()->StartCycle(
         GarbageCollector::MARK_COMPACTOR, GarbageCollectionReason::kTesting,
         "collector cctest", GCTracer::MarkingType::kIncremental);
diff -r -u --color up/v8/test/unittests/heap/heap-utils.cc nw/v8/test/unittests/heap/heap-utils.cc
--- up/v8/test/unittests/heap/heap-utils.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/heap/heap-utils.cc	2023-01-19 16:46:37.543942542 -0500
@@ -8,6 +8,7 @@
 
 #include "src/common/globals.h"
 #include "src/flags/flags.h"
+#include "src/heap/gc-tracer-inl.h"
 #include "src/heap/incremental-marking.h"
 #include "src/heap/mark-compact.h"
 #include "src/heap/new-spaces.h"
@@ -24,7 +25,7 @@
   i::IncrementalMarking* marking = heap->incremental_marking();
 
   if (heap->sweeping_in_progress()) {
-    SafepointScope scope(heap);
+    IsolateSafepointScope scope(heap);
     heap->EnsureSweepingCompleted(
         Heap::SweepingForcedFinalizationMode::kV8Only);
   }
@@ -50,12 +51,20 @@
 
 void FillPageInPagedSpace(Page* page,
                           std::vector<Handle<FixedArray>>* out_handles) {
+  Heap* heap = page->heap();
   DCHECK(page->SweepingDone());
   PagedSpaceBase* paged_space = static_cast<PagedSpaceBase*>(page->owner());
   // Make sure the LAB is empty to guarantee that all free space is accounted
   // for in the freelist.
   DCHECK_EQ(paged_space->limit(), paged_space->top());
 
+  PauseAllocationObserversScope no_observers_scope(heap);
+
+  CollectionEpoch full_epoch =
+      heap->tracer()->CurrentEpoch(GCTracer::Scope::ScopeId::MARK_COMPACTOR);
+  CollectionEpoch young_epoch = heap->tracer()->CurrentEpoch(
+      GCTracer::Scope::ScopeId::MINOR_MARK_COMPACTOR);
+
   for (Page* p : *paged_space) {
     if (p != page) paged_space->UnlinkFreeListCategories(p);
   }
@@ -72,56 +81,67 @@
       [&available_sizes](FreeListCategory* category) {
         category->IterateNodesForTesting([&available_sizes](FreeSpace node) {
           int node_size = node.Size();
-          DCHECK_LT(0, FixedArrayLenFromSize(node_size));
-          available_sizes.push_back(node_size);
+          if (node_size >= kMaxRegularHeapObjectSize) {
+            available_sizes.push_back(node_size);
+          }
         });
       });
 
-  Isolate* isolate = page->heap()->isolate();
+  Isolate* isolate = heap->isolate();
 
   // Allocate as many max size arrays as possible, while making sure not to
   // leave behind a block too small to fit a FixedArray.
   const int max_array_length = FixedArrayLenFromSize(kMaxRegularHeapObjectSize);
   for (size_t i = 0; i < available_sizes.size(); ++i) {
     int available_size = available_sizes[i];
-    while (available_size >
-           kMaxRegularHeapObjectSize + FixedArray::kHeaderSize) {
+    while (available_size > kMaxRegularHeapObjectSize) {
       Handle<FixedArray> fixed_array = isolate->factory()->NewFixedArray(
           max_array_length, AllocationType::kYoung);
       if (out_handles) out_handles->push_back(fixed_array);
       available_size -= kMaxRegularHeapObjectSize;
     }
-    if (available_size > kMaxRegularHeapObjectSize) {
-      // Allocate less than kMaxRegularHeapObjectSize to ensure remaining space
-      // can be used to allcoate another FixedArray.
-      int array_size = kMaxRegularHeapObjectSize - FixedArray::kHeaderSize;
+  }
+
+  paged_space->FreeLinearAllocationArea();
+
+  // Allocate FixedArrays in remaining free list blocks, from largest
+  // category to smallest.
+  std::vector<std::vector<int>> remaining_sizes;
+  page->ForAllFreeListCategories(
+      [&remaining_sizes](FreeListCategory* category) {
+        remaining_sizes.push_back({});
+        std::vector<int>& sizes_in_category =
+            remaining_sizes[remaining_sizes.size() - 1];
+        category->IterateNodesForTesting([&sizes_in_category](FreeSpace node) {
+          int node_size = node.Size();
+          DCHECK_LT(0, FixedArrayLenFromSize(node_size));
+          sizes_in_category.push_back(node_size);
+        });
+      });
+  for (auto it = remaining_sizes.rbegin(); it != remaining_sizes.rend(); ++it) {
+    std::vector<int> sizes_in_category = *it;
+    for (int size : sizes_in_category) {
+      DCHECK_LE(size, kMaxRegularHeapObjectSize);
+      int array_length = FixedArrayLenFromSize(size);
+      DCHECK_LT(0, array_length);
       Handle<FixedArray> fixed_array = isolate->factory()->NewFixedArray(
-          FixedArrayLenFromSize(array_size), AllocationType::kYoung);
+          array_length, AllocationType::kYoung);
       if (out_handles) out_handles->push_back(fixed_array);
-      available_size -= array_size;
     }
-    DCHECK_LE(available_size, kMaxRegularHeapObjectSize);
-    DCHECK_LT(0, FixedArrayLenFromSize(available_size));
-    available_sizes[i] = available_size;
   }
 
-  // Allocate FixedArrays in remaining free list blocks, from largest to
-  // smallest.
-  std::sort(available_sizes.begin(), available_sizes.end(),
-            [](size_t a, size_t b) { return a > b; });
-  for (size_t i = 0; i < available_sizes.size(); ++i) {
-    int available_size = available_sizes[i];
-    DCHECK_LE(available_size, kMaxRegularHeapObjectSize);
-    int array_length = FixedArrayLenFromSize(available_size);
-    DCHECK_LT(0, array_length);
-    Handle<FixedArray> fixed_array =
-        isolate->factory()->NewFixedArray(array_length, AllocationType::kYoung);
-    if (out_handles) out_handles->push_back(fixed_array);
-  }
+  DCHECK_EQ(0, page->AvailableInFreeList());
+  DCHECK_EQ(0, page->AvailableInFreeListFromAllocatedBytes());
 
   for (Page* p : *paged_space) {
     if (p != page) paged_space->RelinkFreeListCategories(p);
   }
+
+  // Allocations in this method should not require a GC.
+  CHECK_EQ(full_epoch, heap->tracer()->CurrentEpoch(
+                           GCTracer::Scope::ScopeId::MARK_COMPACTOR));
+  CHECK_EQ(young_epoch, heap->tracer()->CurrentEpoch(
+                            GCTracer::Scope::ScopeId::MINOR_MARK_COMPACTOR));
 }
 
 }  // namespace
@@ -133,6 +153,8 @@
   // v8_flags.stress_concurrent_allocation = false;
   // Background thread allocating concurrently interferes with this function.
   CHECK(!v8_flags.stress_concurrent_allocation);
+  space->heap()->EnsureSweepingCompleted(
+      Heap::SweepingForcedFinalizationMode::kV8Only);
   space->FreeLinearAllocationArea();
   if (v8_flags.minor_mc) {
     for (Page* page : *space) {
@@ -237,6 +259,8 @@
                               std::vector<Handle<FixedArray>>* out_handles) {
   if (space->top() == kNullAddress) return;
   Page* page = Page::FromAllocationAreaAddress(space->top());
+  space->heap()->EnsureSweepingCompleted(
+      Heap::SweepingForcedFinalizationMode::kV8Only);
   space->FreeLinearAllocationArea();
   FillPageInPagedSpace(page, out_handles);
 }
diff -r -u --color up/v8/test/unittests/heap/heap-utils.h nw/v8/test/unittests/heap/heap-utils.h
--- up/v8/test/unittests/heap/heap-utils.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/heap/heap-utils.h	2023-01-19 16:46:37.543942542 -0500
@@ -32,20 +32,24 @@
   WithHeapInternals(const WithHeapInternals&) = delete;
   WithHeapInternals& operator=(const WithHeapInternals&) = delete;
 
-  void CollectGarbage(i::AllocationSpace space) {
-    heap()->CollectGarbage(space, i::GarbageCollectionReason::kTesting);
+  void CollectGarbage(AllocationSpace space) {
+    ScanStackModeScopeForTesting scope(heap(), Heap::ScanStackMode::kNone);
+    heap()->CollectGarbage(space, GarbageCollectionReason::kTesting);
   }
 
   void FullGC() {
-    heap()->CollectGarbage(OLD_SPACE, i::GarbageCollectionReason::kTesting);
+    ScanStackModeScopeForTesting scope(heap(), Heap::ScanStackMode::kNone);
+    heap()->CollectGarbage(OLD_SPACE, GarbageCollectionReason::kTesting);
   }
 
   void YoungGC() {
-    heap()->CollectGarbage(NEW_SPACE, i::GarbageCollectionReason::kTesting);
+    ScanStackModeScopeForTesting scope(heap(), Heap::ScanStackMode::kNone);
+    heap()->CollectGarbage(NEW_SPACE, GarbageCollectionReason::kTesting);
   }
 
   void CollectAllAvailableGarbage() {
-    heap()->CollectAllAvailableGarbage(i::GarbageCollectionReason::kTesting);
+    ScanStackModeScopeForTesting scope(heap(), Heap::ScanStackMode::kNone);
+    heap()->CollectAllAvailableGarbage(GarbageCollectionReason::kTesting);
   }
 
   Heap* heap() const { return this->i_isolate()->heap(); }
@@ -65,10 +69,11 @@
   }
 
   void GrowNewSpace() {
-    SafepointScope scope(heap());
+    IsolateSafepointScope scope(heap());
     if (!heap()->new_space()->IsAtMaximumCapacity()) {
       heap()->new_space()->Grow();
     }
+    CHECK(heap()->new_space()->EnsureCurrentCapacity());
   }
 
   void SealCurrentObjects() {
@@ -86,10 +91,11 @@
     }
   }
 
-  void GcAndSweep(i::AllocationSpace space) {
+  void GcAndSweep(AllocationSpace space) {
+    ScanStackModeScopeForTesting scope(heap(), Heap::ScanStackMode::kNone);
     heap()->CollectGarbage(space, GarbageCollectionReason::kTesting);
     if (heap()->sweeping_in_progress()) {
-      SafepointScope scope(heap());
+      IsolateSafepointScope scope(heap());
       heap()->EnsureSweepingCompleted(
           Heap::SweepingForcedFinalizationMode::kV8Only);
     }
@@ -128,19 +134,22 @@
     WithContextMixin<                    //
         TestWithHeapInternals>;
 
-inline void CollectGarbage(i::AllocationSpace space, v8::Isolate* isolate) {
-  reinterpret_cast<i::Isolate*>(isolate)->heap()->CollectGarbage(
-      space, i::GarbageCollectionReason::kTesting);
+inline void CollectGarbage(AllocationSpace space, v8::Isolate* isolate) {
+  Heap* heap = reinterpret_cast<i::Isolate*>(isolate)->heap();
+  ScanStackModeScopeForTesting scope(heap, Heap::ScanStackMode::kNone);
+  heap->CollectGarbage(space, GarbageCollectionReason::kTesting);
 }
 
 inline void FullGC(v8::Isolate* isolate) {
-  reinterpret_cast<i::Isolate*>(isolate)->heap()->CollectAllGarbage(
-      i::Heap::kNoGCFlags, i::GarbageCollectionReason::kTesting);
+  Heap* heap = reinterpret_cast<i::Isolate*>(isolate)->heap();
+  ScanStackModeScopeForTesting scope(heap, Heap::ScanStackMode::kNone);
+  heap->CollectAllGarbage(Heap::kNoGCFlags, GarbageCollectionReason::kTesting);
 }
 
 inline void YoungGC(v8::Isolate* isolate) {
-  reinterpret_cast<i::Isolate*>(isolate)->heap()->CollectGarbage(
-      i::NEW_SPACE, i::GarbageCollectionReason::kTesting);
+  Heap* heap = reinterpret_cast<i::Isolate*>(isolate)->heap();
+  ScanStackModeScopeForTesting scope(heap, Heap::ScanStackMode::kNone);
+  heap->CollectGarbage(NEW_SPACE, GarbageCollectionReason::kTesting);
 }
 
 template <typename GlobalOrPersistent>
@@ -148,7 +157,7 @@
   CHECK(!v8_flags.single_generation);
   v8::HandleScope scope(isolate);
   auto tmp = global.Get(isolate);
-  return i::Heap::InYoungGeneration(*v8::Utils::OpenHandle(*tmp));
+  return Heap::InYoungGeneration(*v8::Utils::OpenHandle(*tmp));
 }
 
 bool IsNewObjectInCorrectGeneration(HeapObject object);
diff -r -u --color up/v8/test/unittests/heap/local-heap-unittest.cc nw/v8/test/unittests/heap/local-heap-unittest.cc
--- up/v8/test/unittests/heap/local-heap-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/heap/local-heap-unittest.cc	2023-01-19 16:46:37.554775871 -0500
@@ -176,8 +176,7 @@
   epilogue[2].WaitUntilStarted();
   {
     UnparkedScope scope(&lh);
-    heap->PreciseCollectAllGarbage(Heap::kNoGCFlags,
-                                   GarbageCollectionReason::kTesting);
+    PreciseCollectAllGarbage(i_isolate());
   }
   epilogue[1].RequestStop();
   epilogue[2].RequestStop();
diff -r -u --color up/v8/test/unittests/heap/marking-inner-pointer-resolution-unittest.cc nw/v8/test/unittests/heap/marking-inner-pointer-resolution-unittest.cc
--- up/v8/test/unittests/heap/marking-inner-pointer-resolution-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/heap/marking-inner-pointer-resolution-unittest.cc	2023-01-19 16:46:37.554775871 -0500
@@ -680,7 +680,7 @@
     // Start incremental marking and mark the third object.
     i::IncrementalMarking* marking = heap()->incremental_marking();
     if (marking->IsStopped()) {
-      SafepointScope scope(heap());
+      IsolateSafepointScope scope(heap());
       heap()->tracer()->StartCycle(
           GarbageCollector::MARK_COMPACTOR, GarbageCollectionReason::kTesting,
           "unit test", GCTracer::MarkingType::kIncremental);
diff -r -u --color up/v8/test/unittests/heap/safepoint-unittest.cc nw/v8/test/unittests/heap/safepoint-unittest.cc
--- up/v8/test/unittests/heap/safepoint-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/heap/safepoint-unittest.cc	2023-01-19 16:46:37.554775871 -0500
@@ -21,7 +21,7 @@
   Heap* heap = i_isolate()->heap();
   bool run = false;
   {
-    SafepointScope scope(heap);
+    IsolateSafepointScope scope(heap);
     run = true;
   }
   CHECK(run);
@@ -68,7 +68,7 @@
     }
 
     {
-      SafepointScope scope(heap);
+      IsolateSafepointScope scope(heap);
       safepoints++;
     }
     mutex.Unlock();
@@ -124,7 +124,7 @@
     }
 
     for (int i = 0; i < kSafepoints; i++) {
-      SafepointScope scope(heap);
+      IsolateSafepointScope scope(heap);
       safepoint_count++;
     }
 
diff -r -u --color up/v8/test/unittests/heap/unmapper-unittest.cc nw/v8/test/unittests/heap/unmapper-unittest.cc
--- up/v8/test/unittests/heap/unmapper-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/heap/unmapper-unittest.cc	2023-01-19 16:46:37.554775871 -0500
@@ -314,8 +314,6 @@
              SetPlatformPageAllocatorForTesting(tracking_page_allocator_));
     old_sweeping_flag_ = i::v8_flags.concurrent_sweeping;
     i::v8_flags.concurrent_sweeping = false;
-    old_minor_sweeping_flag_ = i::v8_flags.concurrent_minor_mc_sweeping;
-    i::v8_flags.concurrent_minor_mc_sweeping = false;
 #ifdef V8_COMPRESS_POINTERS_IN_SHARED_CAGE
     // Reinitialize the process-wide pointer cage so it can pick up the
     // TrackingPageAllocator.
@@ -342,7 +340,6 @@
     GetProcessWideSandbox()->TearDown();
 #endif
     i::v8_flags.concurrent_sweeping = old_sweeping_flag_;
-    i::v8_flags.concurrent_minor_mc_sweeping = old_minor_sweeping_flag_;
     CHECK(tracking_page_allocator_->IsEmpty());
 
     // Restore the original v8::PageAllocator and delete the tracking one.
@@ -364,14 +361,12 @@
   static TrackingPageAllocator* tracking_page_allocator_;
   static v8::PageAllocator* old_page_allocator_;
   static bool old_sweeping_flag_;
-  static bool old_minor_sweeping_flag_;
 };
 
 TrackingPageAllocator* SequentialUnmapperTest::tracking_page_allocator_ =
     nullptr;
 v8::PageAllocator* SequentialUnmapperTest::old_page_allocator_ = nullptr;
 bool SequentialUnmapperTest::old_sweeping_flag_;
-bool SequentialUnmapperTest::old_minor_sweeping_flag_;
 
 template <typename TMixin>
 SequentialUnmapperTestMixin<TMixin>::SequentialUnmapperTestMixin() {
diff -r -u --color up/v8/test/unittests/inspector/inspector-unittest.cc nw/v8/test/unittests/inspector/inspector-unittest.cc
--- up/v8/test/unittests/inspector/inspector-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/inspector/inspector-unittest.cc	2023-01-19 16:46:37.554775871 -0500
@@ -259,7 +259,6 @@
 }
 
 TEST_F(InspectorTest, ApiCreatedTasksAreCleanedUp) {
-  i::v8_flags.experimental_async_stack_tagging_api = true;
   v8::Isolate* isolate = v8_isolate();
   v8::HandleScope handle_scope(isolate);
 
diff -r -u --color up/v8/test/unittests/interpreter/bytecode_expectations/ClassAndSuperClass.golden nw/v8/test/unittests/interpreter/bytecode_expectations/ClassAndSuperClass.golden
--- up/v8/test/unittests/interpreter/bytecode_expectations/ClassAndSuperClass.golden	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/interpreter/bytecode_expectations/ClassAndSuperClass.golden	2023-01-19 16:46:37.554775871 -0500
@@ -87,17 +87,21 @@
     test = new B().constructor;
   })();
 "
-frame size: 5
+frame size: 8
 parameter count: 1
-bytecode array length: 39
+bytecode array length: 52
 bytecodes: [
                 B(Mov), R(closure), R(1),
   /*  118 S> */ B(LdaSmi), I8(1),
                 B(Star4),
-                B(Ldar), R(1),
-  /*  118 E> */ B(GetSuperConstructor), R(3),
+  /*  118 E> */ B(FindNonDefaultConstructorOrConstruct), R(1), R(0), R(6),
+                B(Ldar), R(6),
+                B(Mov), R(1), R(2),
+                B(Mov), R(0), R(5),
+                B(Mov), R(7), R(3),
+                B(JumpIfTrue), U8(12),
                 B(ThrowIfNotSuperConstructor), R(3),
-                B(Ldar), R(0),
+                B(Ldar), R(5),
   /*  118 E> */ B(Construct), R(3), R(4), U8(1), U8(0),
                 B(Star3),
                 B(Ldar), R(this),
@@ -130,15 +134,19 @@
     test = new B().constructor;
   })();
 "
-frame size: 4
+frame size: 7
 parameter count: 1
-bytecode array length: 36
+bytecode array length: 49
 bytecodes: [
                 B(Mov), R(closure), R(1),
-  /*  117 S> */ B(Ldar), R(1),
-  /*  117 E> */ B(GetSuperConstructor), R(3),
+  /*  117 S> */ B(FindNonDefaultConstructorOrConstruct), R(1), R(0), R(5),
+                B(Ldar), R(5),
+                B(Mov), R(1), R(2),
+                B(Mov), R(0), R(4),
+                B(Mov), R(6), R(3),
+                B(JumpIfTrue), U8(12),
                 B(ThrowIfNotSuperConstructor), R(3),
-                B(Ldar), R(0),
+                B(Ldar), R(4),
   /*  117 E> */ B(Construct), R(3), R(0), U8(0), U8(0),
                 B(Star3),
                 B(Ldar), R(this),
diff -r -u --color up/v8/test/unittests/interpreter/bytecode_expectations/PrivateAccessorAccess.golden nw/v8/test/unittests/interpreter/bytecode_expectations/PrivateAccessorAccess.golden
--- up/v8/test/unittests/interpreter/bytecode_expectations/PrivateAccessorAccess.golden	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/interpreter/bytecode_expectations/PrivateAccessorAccess.golden	2023-01-19 16:46:37.565609204 -0500
@@ -83,7 +83,7 @@
   /*   48 E> */ B(DefineKeyedOwnProperty), R(this), R(0), U8(0),
   /*   53 S> */ B(LdaImmutableCurrentContextSlot), U8(3),
   /*   58 E> */ B(GetKeyedProperty), R(this), U8(2),
-                B(Wide), B(LdaSmi), I16(305),
+                B(Wide), B(LdaSmi), I16(306),
                 B(Star2),
                 B(LdaConstant), U8(0),
                 B(Star3),
@@ -115,7 +115,7 @@
   /*   41 E> */ B(DefineKeyedOwnProperty), R(this), R(0), U8(0),
   /*   46 S> */ B(LdaImmutableCurrentContextSlot), U8(3),
   /*   51 E> */ B(GetKeyedProperty), R(this), U8(2),
-                B(Wide), B(LdaSmi), I16(304),
+                B(Wide), B(LdaSmi), I16(305),
                 B(Star2),
                 B(LdaConstant), U8(0),
                 B(Star3),
@@ -149,7 +149,7 @@
                 B(Star2),
                 B(LdaImmutableCurrentContextSlot), U8(3),
   /*   58 E> */ B(GetKeyedProperty), R(this), U8(2),
-                B(Wide), B(LdaSmi), I16(305),
+                B(Wide), B(LdaSmi), I16(306),
                 B(Star3),
                 B(LdaConstant), U8(0),
                 B(Star4),
@@ -181,7 +181,7 @@
   /*   41 E> */ B(DefineKeyedOwnProperty), R(this), R(0), U8(0),
   /*   46 S> */ B(LdaImmutableCurrentContextSlot), U8(3),
   /*   51 E> */ B(GetKeyedProperty), R(this), U8(2),
-                B(Wide), B(LdaSmi), I16(304),
+                B(Wide), B(LdaSmi), I16(305),
                 B(Star2),
                 B(LdaConstant), U8(0),
                 B(Star3),
diff -r -u --color up/v8/test/unittests/interpreter/bytecode_expectations/PrivateMethodAccess.golden nw/v8/test/unittests/interpreter/bytecode_expectations/PrivateMethodAccess.golden
--- up/v8/test/unittests/interpreter/bytecode_expectations/PrivateMethodAccess.golden	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/interpreter/bytecode_expectations/PrivateMethodAccess.golden	2023-01-19 16:46:37.565609204 -0500
@@ -58,7 +58,7 @@
                 B(Star2),
                 B(LdaImmutableCurrentContextSlot), U8(3),
   /*   54 E> */ B(GetKeyedProperty), R(this), U8(2),
-                B(Wide), B(LdaSmi), I16(303),
+                B(Wide), B(LdaSmi), I16(304),
                 B(Star3),
                 B(LdaConstant), U8(0),
                 B(Star4),
@@ -91,7 +91,7 @@
   /*   44 E> */ B(DefineKeyedOwnProperty), R(this), R(0), U8(0),
   /*   49 S> */ B(LdaImmutableCurrentContextSlot), U8(3),
   /*   54 E> */ B(GetKeyedProperty), R(this), U8(2),
-                B(Wide), B(LdaSmi), I16(303),
+                B(Wide), B(LdaSmi), I16(304),
                 B(Star2),
                 B(LdaConstant), U8(0),
                 B(Star3),
diff -r -u --color up/v8/test/unittests/interpreter/bytecode_expectations/StaticPrivateMethodAccess.golden nw/v8/test/unittests/interpreter/bytecode_expectations/StaticPrivateMethodAccess.golden
--- up/v8/test/unittests/interpreter/bytecode_expectations/StaticPrivateMethodAccess.golden	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/interpreter/bytecode_expectations/StaticPrivateMethodAccess.golden	2023-01-19 16:46:37.565609204 -0500
@@ -24,7 +24,7 @@
                 B(TestReferenceEqual), R(this),
                 B(Mov), R(this), R(1),
                 B(JumpIfTrue), U8(16),
-                B(Wide), B(LdaSmi), I16(297),
+                B(Wide), B(LdaSmi), I16(298),
                 B(Star2),
                 B(LdaConstant), U8(0),
                 B(Star3),
@@ -61,13 +61,13 @@
                 B(TestReferenceEqual), R(this),
                 B(Mov), R(this), R(0),
                 B(JumpIfTrue), U8(16),
-                B(Wide), B(LdaSmi), I16(297),
+                B(Wide), B(LdaSmi), I16(298),
                 B(Star2),
                 B(LdaConstant), U8(0),
                 B(Star3),
   /*   61 E> */ B(CallRuntime), U16(Runtime::kNewTypeError), R(2), U8(2),
                 B(Throw),
-                B(Wide), B(LdaSmi), I16(303),
+                B(Wide), B(LdaSmi), I16(304),
                 B(Star2),
                 B(LdaConstant), U8(1),
                 B(Star3),
@@ -99,13 +99,13 @@
                 B(TestReferenceEqual), R(this),
                 B(Mov), R(this), R(0),
                 B(JumpIfTrue), U8(16),
-                B(Wide), B(LdaSmi), I16(297),
+                B(Wide), B(LdaSmi), I16(298),
                 B(Star1),
                 B(LdaConstant), U8(0),
                 B(Star2),
   /*   61 E> */ B(CallRuntime), U16(Runtime::kNewTypeError), R(1), U8(2),
                 B(Throw),
-                B(Wide), B(LdaSmi), I16(303),
+                B(Wide), B(LdaSmi), I16(304),
                 B(Star1),
                 B(LdaConstant), U8(1),
                 B(Star2),
@@ -145,7 +145,7 @@
                 B(TestReferenceEqual), R(this),
                 B(Mov), R(this), R(0),
                 B(JumpIfTrue), U8(16),
-                B(Wide), B(LdaSmi), I16(297),
+                B(Wide), B(LdaSmi), I16(298),
                 B(Star2),
                 B(LdaConstant), U8(0),
                 B(Star3),
@@ -167,7 +167,7 @@
                 B(TestReferenceEqual), R(this),
                 B(Mov), R(this), R(0),
                 B(JumpIfTrue), U8(16),
-                B(Wide), B(LdaSmi), I16(297),
+                B(Wide), B(LdaSmi), I16(298),
                 B(Star3),
                 B(LdaConstant), U8(0),
                 B(Star4),
@@ -182,7 +182,7 @@
                 B(TestReferenceEqual), R(this),
                 B(Mov), R(this), R(0),
                 B(JumpIfTrue), U8(16),
-                B(Wide), B(LdaSmi), I16(297),
+                B(Wide), B(LdaSmi), I16(298),
                 B(Star2),
                 B(LdaConstant), U8(0),
                 B(Star3),
@@ -216,13 +216,13 @@
                 B(TestReferenceEqual), R(this),
                 B(Mov), R(this), R(0),
                 B(JumpIfTrue), U8(16),
-                B(Wide), B(LdaSmi), I16(297),
+                B(Wide), B(LdaSmi), I16(298),
                 B(Star1),
                 B(LdaConstant), U8(0),
                 B(Star2),
   /*   65 E> */ B(CallRuntime), U16(Runtime::kNewTypeError), R(1), U8(2),
                 B(Throw),
-                B(Wide), B(LdaSmi), I16(305),
+                B(Wide), B(LdaSmi), I16(306),
                 B(Star1),
                 B(LdaConstant), U8(1),
                 B(Star2),
@@ -253,13 +253,13 @@
                 B(TestReferenceEqual), R(this),
                 B(Mov), R(this), R(0),
                 B(JumpIfTrue), U8(16),
-                B(Wide), B(LdaSmi), I16(297),
+                B(Wide), B(LdaSmi), I16(298),
                 B(Star1),
                 B(LdaConstant), U8(0),
                 B(Star2),
   /*   58 E> */ B(CallRuntime), U16(Runtime::kNewTypeError), R(1), U8(2),
                 B(Throw),
-                B(Wide), B(LdaSmi), I16(304),
+                B(Wide), B(LdaSmi), I16(305),
                 B(Star1),
                 B(LdaConstant), U8(1),
                 B(Star2),
@@ -292,13 +292,13 @@
                 B(TestReferenceEqual), R(this),
                 B(Mov), R(this), R(0),
                 B(JumpIfTrue), U8(16),
-                B(Wide), B(LdaSmi), I16(297),
+                B(Wide), B(LdaSmi), I16(298),
                 B(Star2),
                 B(LdaConstant), U8(0),
                 B(Star3),
   /*   65 E> */ B(CallRuntime), U16(Runtime::kNewTypeError), R(2), U8(2),
                 B(Throw),
-                B(Wide), B(LdaSmi), I16(305),
+                B(Wide), B(LdaSmi), I16(306),
                 B(Star2),
                 B(LdaConstant), U8(1),
                 B(Star3),
@@ -327,7 +327,7 @@
 bytecodes: [
   /*   46 S> */ B(LdaImmutableCurrentContextSlot), U8(3),
   /*   51 E> */ B(GetKeyedProperty), R(this), U8(0),
-                B(Wide), B(LdaSmi), I16(304),
+                B(Wide), B(LdaSmi), I16(305),
                 B(Star1),
                 B(LdaConstant), U8(0),
                 B(Star2),
diff -r -u --color up/v8/test/unittests/interpreter/bytecode_expectations/SuperCallAndSpread.golden nw/v8/test/unittests/interpreter/bytecode_expectations/SuperCallAndSpread.golden
--- up/v8/test/unittests/interpreter/bytecode_expectations/SuperCallAndSpread.golden	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/interpreter/bytecode_expectations/SuperCallAndSpread.golden	2023-01-19 16:46:37.565609204 -0500
@@ -17,18 +17,25 @@
     test = new B(1, 2, 3).constructor;
   })();
 "
-frame size: 5
+frame size: 9
 parameter count: 1
-bytecode array length: 19
+bytecode array length: 38
 bytecodes: [
   /*   93 E> */ B(CreateRestParameter),
                 B(Star2),
                 B(Mov), R(closure), R(1),
-  /*   93 S> */ B(Ldar), R(1),
-  /*   93 E> */ B(GetSuperConstructor), R(4),
+  /*   93 S> */ B(FindNonDefaultConstructorOrConstruct), R(1), R(0), R(7),
+                B(Mov), R(2), R(5),
+                B(Ldar), R(7),
+                B(Mov), R(1), R(3),
+                B(Mov), R(0), R(6),
+                B(Mov), R(8), R(4),
+                B(JumpIfTrue), U8(12),
                 B(ThrowIfNotSuperConstructor), R(4),
-                B(Ldar), R(0),
-  /*   93 E> */ B(ConstructWithSpread), R(4), R(2), U8(1), U8(0),
+                B(Ldar), R(6),
+  /*   93 E> */ B(ConstructWithSpread), R(4), R(5), U8(1), U8(0),
+                B(Star4),
+                B(Ldar), R(4),
   /*   93 S> */ B(Return),
 ]
 constant pool: [
@@ -49,9 +56,9 @@
     test = new B(1, 2, 3).constructor;
   })();
 "
-frame size: 8
+frame size: 11
 parameter count: 1
-bytecode array length: 38
+bytecode array length: 51
 bytecodes: [
   /*  128 E> */ B(CreateRestParameter),
                 B(Star3),
@@ -59,11 +66,15 @@
                 B(Mov), R(3), R(2),
   /*  140 S> */ B(LdaSmi), I8(1),
                 B(Star6),
-                B(Ldar), R(closure),
-  /*  140 E> */ B(GetSuperConstructor), R(5),
-                B(ThrowIfNotSuperConstructor), R(5),
-                B(Ldar), R(0),
+  /*  140 E> */ B(FindNonDefaultConstructorOrConstruct), R(closure), R(0), R(9),
                 B(Mov), R(3), R(7),
+                B(Ldar), R(9),
+                B(Mov), R(1), R(4),
+                B(Mov), R(0), R(8),
+                B(Mov), R(10), R(5),
+                B(JumpIfTrue), U8(12),
+                B(ThrowIfNotSuperConstructor), R(5),
+                B(Ldar), R(8),
   /*  140 E> */ B(ConstructWithSpread), R(5), R(6), U8(2), U8(0),
                 B(Star5),
                 B(Ldar), R(this),
@@ -93,7 +104,7 @@
 "
 frame size: 11
 parameter count: 1
-bytecode array length: 94
+bytecode array length: 101
 bytecodes: [
   /*  128 E> */ B(CreateRestParameter),
                 B(Star3),
@@ -122,10 +133,12 @@
                 B(JumpLoop), U8(31), I8(0), U8(18),
                 B(LdaSmi), I8(1),
                 B(StaInArrayLiteral), R(6), R(7), U8(12),
-                B(Ldar), R(4),
-  /*  140 E> */ B(GetSuperConstructor), R(5),
-                B(ThrowIfNotSuperConstructor), R(5),
+  /*  140 E> */ B(FindNonDefaultConstructorOrConstruct), R(4), R(0), R(8),
+                B(Ldar), R(8),
                 B(Mov), R(0), R(7),
+                B(Mov), R(9), R(5),
+                B(JumpIfTrue), U8(9),
+                B(ThrowIfNotSuperConstructor), R(5),
                 B(CallJSRuntime), U8(%reflect_construct), R(5), U8(3),
                 B(Star5),
                 B(Ldar), R(this),
diff -r -u --color up/v8/test/unittests/objects/swiss-hash-table-helpers-unittest.cc nw/v8/test/unittests/objects/swiss-hash-table-helpers-unittest.cc
--- up/v8/test/unittests/objects/swiss-hash-table-helpers-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/objects/swiss-hash-table-helpers-unittest.cc	2023-01-19 16:46:37.608942524 -0500
@@ -68,42 +68,6 @@
   }
 }
 
-TYPED_TEST(SwissTableGroupTest, MatchEmptyOrDeleted) {
-  if (TypeParam::kWidth == 16) {
-    ctrl_t group[] = {kEmpty, 1, kDeleted, 3, kEmpty, 5, kSentinel, 7,
-                      7,      5, 3,        1, 1,      1, 1,         1};
-    EXPECT_THAT(TypeParam{group}.MatchEmptyOrDeleted(), ElementsAre(0, 2, 4));
-  } else if (TypeParam::kWidth == 8) {
-    ctrl_t group[] = {kEmpty, 1, 2, kDeleted, 2, 1, kSentinel, 1};
-    EXPECT_THAT(TypeParam{group}.MatchEmptyOrDeleted(), ElementsAre(0, 3));
-  } else {
-    FAIL() << "No test coverage for kWidth==" << TypeParam::kWidth;
-  }
-}
-
-TYPED_TEST(SwissTableGroupTest, CountLeadingEmptyOrDeleted) {
-  const std::vector<ctrl_t> empty_examples = {kEmpty, kDeleted};
-  const std::vector<ctrl_t> full_examples = {0, 1, 2, 3, 5, 9, 127, kSentinel};
-
-  for (ctrl_t empty : empty_examples) {
-    std::vector<ctrl_t> e(TypeParam::kWidth, empty);
-    EXPECT_EQ(TypeParam::kWidth,
-              TypeParam{e.data()}.CountLeadingEmptyOrDeleted());
-    for (ctrl_t full : full_examples) {
-      for (size_t i = 0; i != TypeParam::kWidth; ++i) {
-        std::vector<ctrl_t> f(TypeParam::kWidth, empty);
-        f[i] = full;
-        EXPECT_EQ(i, TypeParam{f.data()}.CountLeadingEmptyOrDeleted());
-      }
-      std::vector<ctrl_t> f(TypeParam::kWidth, empty);
-      f[TypeParam::kWidth * 2 / 3] = full;
-      f[TypeParam::kWidth / 2] = full;
-      EXPECT_EQ(TypeParam::kWidth / 2,
-                TypeParam{f.data()}.CountLeadingEmptyOrDeleted());
-    }
-  }
-}
-
 }  // namespace swiss_table
 }  // namespace internal
 }  // namespace v8
diff -r -u --color up/v8/test/unittests/objects/value-serializer-unittest.cc nw/v8/test/unittests/objects/value-serializer-unittest.cc
--- up/v8/test/unittests/objects/value-serializer-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/objects/value-serializer-unittest.cc	2023-01-19 16:46:37.608942524 -0500
@@ -2608,9 +2608,6 @@
 #if V8_ENABLE_WEBASSEMBLY
 TEST_F(ValueSerializerTestWithSharedArrayBufferClone,
        RoundTripWebAssemblyMemory) {
-  bool flag_was_enabled = i::v8_flags.experimental_wasm_threads;
-  i::v8_flags.experimental_wasm_threads = true;
-
   std::vector<uint8_t> data = {0x00, 0x01, 0x80, 0xFF};
   data.resize(65536);
   InitializeData(data, true);
@@ -2635,8 +2632,6 @@
   ExpectScriptTrue("result.buffer.byteLength === 65536");
   ExpectScriptTrue(
       "new Uint8Array(result.buffer, 0, 4).toString() === '0,1,128,255'");
-
-  i::v8_flags.experimental_wasm_threads = flag_was_enabled;
 }
 #endif  // V8_ENABLE_WEBASSEMBLY
 
diff -r -u --color up/v8/test/unittests/parser/decls-unittest.cc nw/v8/test/unittests/parser/decls-unittest.cc
--- up/v8/test/unittests/parser/decls-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/parser/decls-unittest.cc	2023-01-19 16:46:37.608942524 -0500
@@ -145,6 +145,8 @@
   InitializeIfNeeded();
   // A retry after a GC may pollute the counts, so perform gc now
   // to avoid that.
+  i::ScanStackModeScopeForTesting no_stack_scanning(
+      i_isolate()->heap(), i::Heap::ScanStackMode::kNone);
   i_isolate()->heap()->CollectGarbage(i::NEW_SPACE,
                                       i::GarbageCollectionReason::kTesting);
   HandleScope scope(isolate_);
diff -r -u --color up/v8/test/unittests/test-utils.cc nw/v8/test/unittests/test-utils.cc
--- up/v8/test/unittests/test-utils.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/test-utils.cc	2023-01-19 16:46:37.630609186 -0500
@@ -89,8 +89,10 @@
   // running by the time a ManualGCScope is created. Finalizing existing marking
   // prevents any undefined/unexpected behavior.
   if (isolate && isolate->heap()->incremental_marking()->IsMarking()) {
-    isolate->heap()->CollectGarbage(i::OLD_SPACE,
-                                    i::GarbageCollectionReason::kTesting);
+    ScanStackModeScopeForTesting no_stack_scanning(isolate->heap(),
+                                                   Heap::ScanStackMode::kNone);
+    isolate->heap()->CollectGarbage(OLD_SPACE,
+                                    GarbageCollectionReason::kTesting);
     // Make sure there is no concurrent sweeping running in the background.
     isolate->heap()->CompleteSweepingFull();
   }
@@ -98,7 +100,6 @@
   i::v8_flags.concurrent_marking = false;
   i::v8_flags.concurrent_sweeping = false;
   i::v8_flags.concurrent_minor_mc_marking = false;
-  i::v8_flags.concurrent_minor_mc_sweeping = false;
   i::v8_flags.stress_incremental_marking = false;
   i::v8_flags.stress_concurrent_allocation = false;
   // Parallel marking has a dependency on concurrent marking.
diff -r -u --color up/v8/test/unittests/test-utils.h nw/v8/test/unittests/test-utils.h
--- up/v8/test/unittests/test-utils.h	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/test-utils.h	2023-01-19 16:46:37.630609186 -0500
@@ -186,30 +186,40 @@
         .ToLocalChecked();
   }
 
-  void CollectGarbage(i::AllocationSpace space, i::Isolate* isolate = nullptr) {
+  // By default, the GC methods do not scan the stack conservatively.
+  void CollectGarbage(
+      i::AllocationSpace space, i::Isolate* isolate = nullptr,
+      i::Heap::ScanStackMode mode = i::Heap::ScanStackMode::kNone) {
     i::Isolate* iso = isolate ? isolate : i_isolate();
-    iso->heap()->CollectGarbage(space, i::GarbageCollectionReason::kTesting,
-                                kNoGCCallbackFlags);
+    i::ScanStackModeScopeForTesting scope(iso->heap(), mode);
+    iso->heap()->CollectGarbage(space, i::GarbageCollectionReason::kTesting);
   }
 
-  void CollectAllGarbage(i::Isolate* isolate = nullptr) {
+  void CollectAllGarbage(
+      i::Isolate* isolate = nullptr,
+      i::Heap::ScanStackMode mode = i::Heap::ScanStackMode::kNone) {
     i::Isolate* iso = isolate ? isolate : i_isolate();
+    i::ScanStackModeScopeForTesting scope(iso->heap(), mode);
     iso->heap()->CollectAllGarbage(i::Heap::kNoGCFlags,
-                                   i::GarbageCollectionReason::kTesting,
-                                   kNoGCCallbackFlags);
+                                   i::GarbageCollectionReason::kTesting);
   }
 
-  void CollectAllAvailableGarbage(i::Isolate* isolate = nullptr) {
+  void CollectAllAvailableGarbage(
+      i::Isolate* isolate = nullptr,
+      i::Heap::ScanStackMode mode = i::Heap::ScanStackMode::kNone) {
     i::Isolate* iso = isolate ? isolate : i_isolate();
+    i::ScanStackModeScopeForTesting scope(iso->heap(), mode);
     iso->heap()->CollectAllAvailableGarbage(
         i::GarbageCollectionReason::kTesting);
   }
 
-  void PreciseCollectAllGarbage(i::Isolate* isolate = nullptr) {
+  void PreciseCollectAllGarbage(
+      i::Isolate* isolate = nullptr,
+      i::Heap::ScanStackMode mode = i::Heap::ScanStackMode::kNone) {
     i::Isolate* iso = isolate ? isolate : i_isolate();
+    i::ScanStackModeScopeForTesting scope(iso->heap(), mode);
     iso->heap()->PreciseCollectAllGarbage(i::Heap::kNoGCFlags,
-                                          i::GarbageCollectionReason::kTesting,
-                                          kNoGCCallbackFlags);
+                                          i::GarbageCollectionReason::kTesting);
   }
 
   v8::Local<v8::String> NewString(const char* string) {
@@ -290,6 +300,18 @@
                 WithDefaultPlatformMixin<  //
                     ::testing::Test>>>>;
 
+// Use v8::internal::TestJSSharedMemoryWithNativeContext if you are testing
+// internals, aka. directly work with Handles.
+//
+// Using this will FATAL when !V8_CAN_CREATE_SHARED_HEAP_BOOL
+using TestJSSharedMemoryWithContext =                     //
+    WithContextMixin<                                     //
+        WithIsolateScopeMixin<                            //
+            WithIsolateMixin<                             //
+                WithDefaultPlatformMixin<                 //
+                    WithJSSharedMemoryFeatureFlagsMixin<  //
+                        ::testing::Test>>>>>;
+
 class PrintExtension : public v8::Extension {
  public:
   PrintExtension() : v8::Extension("v8/print", "native function print();") {}
diff -r -u --color up/v8/test/unittests/unittests.status nw/v8/test/unittests/unittests.status
--- up/v8/test/unittests/unittests.status	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/unittests.status	2023-01-19 16:46:37.630609186 -0500
@@ -92,6 +92,14 @@
 }],  # system == aix
 
 ##############################################################################
+['system == aix and component_build', {
+  # FreezeFlags relies on mprotect() method, which does not work by default on
+  # shared memory
+  # https://www.ibm.com/docs/en/aix/7.2?topic=m-mprotect-subroutine
+  'FlagDefinitionsTest.FreezeFlags': [SKIP],
+}],  # system == aix and component_build
+
+##############################################################################
 ['arch == ppc64', {
   # PPC Page size is too large for these tests.
   'HeapStatisticsCollectorTest.BriefStatisticsWithDiscardingOnNormalPage': [SKIP],
@@ -324,4 +332,9 @@
   'RegExpTestWithContext.UnicodePropertyEscapeCodeSize': [SKIP],
 }],  # no_i18n == True
 
+##############################################################################
+['no_simd_hardware == True', {
+  'WasmDisassemblerTest.Simd': [SKIP],
+}],  # no_simd_hardware == True
+
 ]
diff -r -u --color up/v8/test/unittests/wasm/control-transfer-unittest.cc nw/v8/test/unittests/wasm/control-transfer-unittest.cc
--- up/v8/test/unittests/wasm/control-transfer-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/wasm/control-transfer-unittest.cc	2023-01-19 16:46:37.630609186 -0500
@@ -59,19 +59,23 @@
 
 class ControlTransferTest : public TestWithZone {
  public:
-  template <int code_len>
+  static constexpr pc_t kPcShiftForLocalsCount = 1;
+
+  template <int kCodeLen>
   void CheckTransfers(
-      const byte (&code)[code_len],
+      const byte (&code)[kCodeLen],
       std::initializer_list<ExpectedControlTransfer> expected_transfers) {
-    byte code_with_end[code_len + 1];  // NOLINT: code_len is a constant here
-    memcpy(code_with_end, code, code_len);
-    code_with_end[code_len] = kExprEnd;
+    // Add a leading '0' for number of locals and append the 'end' opcode.
+    byte function_body[kCodeLen + 2];
+    function_body[0] = 0;
+    memcpy(function_body + 1, code, kCodeLen);
+    function_body[kCodeLen + 1] = kExprEnd;
 
     ControlTransferMap map = WasmInterpreter::ComputeControlTransfersForTesting(
-        zone(), nullptr, code_with_end, code_with_end + code_len + 1);
+        zone(), std::begin(function_body), std::end(function_body));
     // Check all control targets in the map.
     for (auto& expected_transfer : expected_transfers) {
-      pc_t pc = expected_transfer.pc;
+      pc_t pc = expected_transfer.pc + kPcShiftForLocalsCount;
       EXPECT_TRUE(map.map.count(pc) > 0) << "expected control target @" << pc;
       if (!map.map.count(pc)) continue;
       auto& entry = map.map[pc];
@@ -80,7 +84,7 @@
     }
 
     // Check there are no other control targets.
-    CheckNoOtherTargets(code_with_end, code_with_end + code_len + 1, map,
+    CheckNoOtherTargets(std::begin(function_body), std::end(function_body), map,
                         expected_transfers);
   }
 
@@ -91,7 +95,7 @@
     for (pc_t pc = 0; start + pc < end; pc++) {
       bool found = false;
       for (auto& target : targets) {
-        if (target.pc == pc) {
+        if (target.pc + kPcShiftForLocalsCount == pc) {
           found = true;
           break;
         }
diff -r -u --color up/v8/test/unittests/wasm/decoder-unittest.cc nw/v8/test/unittests/wasm/decoder-unittest.cc
--- up/v8/test/unittests/wasm/decoder-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/wasm/decoder-unittest.cc	2023-01-19 16:46:37.630609186 -0500
@@ -20,54 +20,54 @@
   Decoder decoder;
 };
 
-#define CHECK_UINT32V_INLINE(expected, expected_length, ...)               \
-  do {                                                                     \
-    const byte data[] = {__VA_ARGS__};                                     \
-    decoder.Reset(data, data + sizeof(data));                              \
-    unsigned length;                                                       \
-    EXPECT_EQ(static_cast<uint32_t>(expected),                             \
-              decoder.read_u32v<Decoder::kFullValidation>(decoder.start(), \
-                                                          &length));       \
-    EXPECT_EQ(static_cast<unsigned>(expected_length), length);             \
-    EXPECT_EQ(data, decoder.pc());                                         \
-    EXPECT_TRUE(decoder.ok());                                             \
-    EXPECT_EQ(static_cast<uint32_t>(expected), decoder.consume_u32v());    \
-    EXPECT_EQ(data + expected_length, decoder.pc());                       \
+#define CHECK_UINT32V_INLINE(expected, expected_length, ...)                 \
+  do {                                                                       \
+    const byte data[] = {__VA_ARGS__};                                       \
+    decoder.Reset(data, data + sizeof(data));                                \
+    unsigned length;                                                         \
+    EXPECT_EQ(static_cast<uint32_t>(expected),                               \
+              decoder.read_u32v<Decoder::FullValidationTag>(decoder.start(), \
+                                                            &length));       \
+    EXPECT_EQ(static_cast<unsigned>(expected_length), length);               \
+    EXPECT_EQ(data, decoder.pc());                                           \
+    EXPECT_TRUE(decoder.ok());                                               \
+    EXPECT_EQ(static_cast<uint32_t>(expected), decoder.consume_u32v());      \
+    EXPECT_EQ(data + expected_length, decoder.pc());                         \
   } while (false)
 
-#define CHECK_INT32V_INLINE(expected, expected_length, ...)          \
-  do {                                                               \
-    const byte data[] = {__VA_ARGS__};                               \
-    decoder.Reset(data, data + sizeof(data));                        \
-    unsigned length;                                                 \
-    EXPECT_EQ(expected, decoder.read_i32v<Decoder::kFullValidation>( \
-                            decoder.start(), &length));              \
-    EXPECT_EQ(static_cast<unsigned>(expected_length), length);       \
-    EXPECT_EQ(data, decoder.pc());                                   \
-    EXPECT_TRUE(decoder.ok());                                       \
-    EXPECT_EQ(expected, decoder.consume_i32v());                     \
-    EXPECT_EQ(data + expected_length, decoder.pc());                 \
+#define CHECK_INT32V_INLINE(expected, expected_length, ...)            \
+  do {                                                                 \
+    const byte data[] = {__VA_ARGS__};                                 \
+    decoder.Reset(data, data + sizeof(data));                          \
+    unsigned length;                                                   \
+    EXPECT_EQ(expected, decoder.read_i32v<Decoder::FullValidationTag>( \
+                            decoder.start(), &length));                \
+    EXPECT_EQ(static_cast<unsigned>(expected_length), length);         \
+    EXPECT_EQ(data, decoder.pc());                                     \
+    EXPECT_TRUE(decoder.ok());                                         \
+    EXPECT_EQ(expected, decoder.consume_i32v());                       \
+    EXPECT_EQ(data + expected_length, decoder.pc());                   \
   } while (false)
 
-#define CHECK_UINT64V_INLINE(expected, expected_length, ...)               \
-  do {                                                                     \
-    const byte data[] = {__VA_ARGS__};                                     \
-    decoder.Reset(data, data + sizeof(data));                              \
-    unsigned length;                                                       \
-    EXPECT_EQ(static_cast<uint64_t>(expected),                             \
-              decoder.read_u64v<Decoder::kFullValidation>(decoder.start(), \
-                                                          &length));       \
-    EXPECT_EQ(static_cast<unsigned>(expected_length), length);             \
+#define CHECK_UINT64V_INLINE(expected, expected_length, ...)                 \
+  do {                                                                       \
+    const byte data[] = {__VA_ARGS__};                                       \
+    decoder.Reset(data, data + sizeof(data));                                \
+    unsigned length;                                                         \
+    EXPECT_EQ(static_cast<uint64_t>(expected),                               \
+              decoder.read_u64v<Decoder::FullValidationTag>(decoder.start(), \
+                                                            &length));       \
+    EXPECT_EQ(static_cast<unsigned>(expected_length), length);               \
   } while (false)
 
-#define CHECK_INT64V_INLINE(expected, expected_length, ...)          \
-  do {                                                               \
-    const byte data[] = {__VA_ARGS__};                               \
-    decoder.Reset(data, data + sizeof(data));                        \
-    unsigned length;                                                 \
-    EXPECT_EQ(expected, decoder.read_i64v<Decoder::kFullValidation>( \
-                            decoder.start(), &length));              \
-    EXPECT_EQ(static_cast<unsigned>(expected_length), length);       \
+#define CHECK_INT64V_INLINE(expected, expected_length, ...)            \
+  do {                                                                 \
+    const byte data[] = {__VA_ARGS__};                                 \
+    decoder.Reset(data, data + sizeof(data));                          \
+    unsigned length;                                                   \
+    EXPECT_EQ(expected, decoder.read_i64v<Decoder::FullValidationTag>( \
+                            decoder.start(), &length));                \
+    EXPECT_EQ(static_cast<unsigned>(expected_length), length);         \
   } while (false)
 
 TEST_F(DecoderTest, ReadU32v_OneByte) {
@@ -379,7 +379,7 @@
   static const byte data[] = {U32V_1(11)};
   unsigned length = 0;
   decoder.Reset(data, data);
-  decoder.read_u32v<Decoder::kFullValidation>(decoder.start(), &length);
+  decoder.read_u32v<Decoder::FullValidationTag>(decoder.start(), &length);
   EXPECT_FALSE(decoder.ok());
 }
 
@@ -388,7 +388,7 @@
   for (size_t i = 0; i < sizeof(data); i++) {
     unsigned length = 0;
     decoder.Reset(data, data + i);
-    decoder.read_u32v<Decoder::kFullValidation>(decoder.start(), &length);
+    decoder.read_u32v<Decoder::FullValidationTag>(decoder.start(), &length);
     EXPECT_FALSE(decoder.ok());
   }
 }
@@ -398,7 +398,7 @@
   for (size_t i = 0; i < sizeof(data); i++) {
     unsigned length = 0;
     decoder.Reset(data, data + i);
-    decoder.read_u32v<Decoder::kFullValidation>(decoder.start(), &length);
+    decoder.read_u32v<Decoder::FullValidationTag>(decoder.start(), &length);
     EXPECT_FALSE(decoder.ok());
   }
 }
@@ -408,7 +408,7 @@
   for (size_t i = 0; i < sizeof(data); i++) {
     unsigned length = 0;
     decoder.Reset(data, data + i);
-    decoder.read_u32v<Decoder::kFullValidation>(decoder.start(), &length);
+    decoder.read_u32v<Decoder::FullValidationTag>(decoder.start(), &length);
     EXPECT_FALSE(decoder.ok());
   }
 }
@@ -418,7 +418,7 @@
   for (size_t i = 0; i < sizeof(data); i++) {
     unsigned length = 0;
     decoder.Reset(data, data + i);
-    decoder.read_u32v<Decoder::kFullValidation>(decoder.start(), &length);
+    decoder.read_u32v<Decoder::FullValidationTag>(decoder.start(), &length);
     EXPECT_FALSE(decoder.ok());
   }
 }
@@ -429,7 +429,7 @@
     data[4] = static_cast<byte>(i << 4);
     unsigned length = 0;
     decoder.Reset(data, data + sizeof(data));
-    decoder.read_u32v<Decoder::kFullValidation>(decoder.start(), &length);
+    decoder.read_u32v<Decoder::FullValidationTag>(decoder.start(), &length);
     EXPECT_FALSE(decoder.ok());
   }
 }
@@ -439,7 +439,7 @@
   unsigned length = 0;
   byte data[] = {0xFF, 0xFF, 0xFF, 0xFF, 0x7F};
   decoder.Reset(data, data + sizeof(data));
-  decoder.read_i32v<Decoder::kFullValidation>(decoder.start(), &length);
+  decoder.read_i32v<Decoder::FullValidationTag>(decoder.start(), &length);
   EXPECT_EQ(5u, length);
   EXPECT_TRUE(decoder.ok());
 }
@@ -449,7 +449,7 @@
   unsigned length = 0;
   byte data[] = {0x80, 0x80, 0x80, 0x80, 0x77};
   decoder.Reset(data, data + sizeof(data));
-  decoder.read_i32v<Decoder::kFullValidation>(decoder.start(), &length);
+  decoder.read_i32v<Decoder::FullValidationTag>(decoder.start(), &length);
   EXPECT_FALSE(decoder.ok());
 }
 
@@ -485,7 +485,7 @@
         decoder.Reset(data, data + limit);
         unsigned rlen;
         uint32_t result =
-            decoder.read_u32v<Decoder::kFullValidation>(data, &rlen);
+            decoder.read_u32v<Decoder::FullValidationTag>(data, &rlen);
         if (limit < length) {
           EXPECT_FALSE(decoder.ok());
         } else {
@@ -542,7 +542,7 @@
       decoder.Reset(data, data + limit);
       unsigned length;
       uint64_t result =
-          decoder.read_u64v<Decoder::kFullValidation>(data, &length);
+          decoder.read_u64v<Decoder::FullValidationTag>(data, &length);
       if (limit <= index) {
         EXPECT_FALSE(decoder.ok());
       } else {
@@ -584,7 +584,7 @@
         decoder.Reset(data, data + limit);
         unsigned rlen;
         uint64_t result =
-            decoder.read_u64v<Decoder::kFullValidation>(data, &rlen);
+            decoder.read_u64v<Decoder::FullValidationTag>(data, &rlen);
         if (limit < length) {
           EXPECT_FALSE(decoder.ok());
         } else {
@@ -628,7 +628,7 @@
         decoder.Reset(data, data + limit);
         unsigned rlen;
         int64_t result =
-            decoder.read_i64v<Decoder::kFullValidation>(data, &rlen);
+            decoder.read_i64v<Decoder::FullValidationTag>(data, &rlen);
         if (limit < length) {
           EXPECT_FALSE(decoder.ok());
         } else {
@@ -647,7 +647,7 @@
     data[9] = static_cast<byte>(i << 1);
     unsigned length = 0;
     decoder.Reset(data, data + sizeof(data));
-    decoder.read_u64v<Decoder::kFullValidation>(decoder.start(), &length);
+    decoder.read_u64v<Decoder::FullValidationTag>(decoder.start(), &length);
     EXPECT_FALSE(decoder.ok());
   }
 }
@@ -657,7 +657,7 @@
   unsigned length = 0;
   byte data[] = {0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0x7F};
   decoder.Reset(data, data + sizeof(data));
-  decoder.read_i64v<Decoder::kFullValidation>(decoder.start(), &length);
+  decoder.read_i64v<Decoder::FullValidationTag>(decoder.start(), &length);
   EXPECT_EQ(10u, length);
   EXPECT_TRUE(decoder.ok());
 }
@@ -667,7 +667,7 @@
   unsigned length = 0;
   byte data[] = {0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x77};
   decoder.Reset(data, data + sizeof(data));
-  decoder.read_i64v<Decoder::kFullValidation>(decoder.start(), &length);
+  decoder.read_i64v<Decoder::FullValidationTag>(decoder.start(), &length);
   EXPECT_FALSE(decoder.ok());
 }
 
diff -r -u --color up/v8/test/unittests/wasm/function-body-decoder-unittest.cc nw/v8/test/unittests/wasm/function-body-decoder-unittest.cc
--- up/v8/test/unittests/wasm/function-body-decoder-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/wasm/function-body-decoder-unittest.cc	2023-01-19 16:46:37.652275846 -0500
@@ -1160,8 +1160,9 @@
   ExpectValidates(sigs.i_v(),
                   {WASM_UNREACHABLE, WASM_GC_OP(kExprRefTest), kEqRefCode});
 
-  ExpectValidates(sigs.v_v(), {WASM_UNREACHABLE, WASM_GC_OP(kExprRefCast),
-                               struct_index, kExprDrop});
+  ExpectValidates(sigs.v_v(),
+                  {WASM_UNREACHABLE, WASM_GC_OP(kExprRefCastDeprecated),
+                   struct_index, kExprDrop});
   ExpectValidates(sigs.v_v(), {WASM_UNREACHABLE, WASM_GC_OP(kExprRefCast),
                                struct_index, kExprDrop});
 
@@ -1170,11 +1171,14 @@
   ExpectValidates(&sig_v_s, {WASM_UNREACHABLE, WASM_LOCAL_GET(0), kExprBrOnNull,
                              0, kExprCallFunction, struct_consumer});
 
+  ExpectValidates(
+      FunctionSig::Build(zone(), {struct_type}, {}),
+      {WASM_UNREACHABLE, WASM_GC_OP(kExprRefCastDeprecated), struct_index});
   ExpectValidates(FunctionSig::Build(zone(), {struct_type}, {}),
                   {WASM_UNREACHABLE, WASM_GC_OP(kExprRefCast), struct_index});
 
-  ExpectValidates(FunctionSig::Build(zone(), {kWasmDataRef}, {}),
-                  {WASM_UNREACHABLE, WASM_GC_OP(kExprRefAsData)});
+  ExpectValidates(FunctionSig::Build(zone(), {kWasmStructRef}, {}),
+                  {WASM_UNREACHABLE, WASM_GC_OP(kExprRefAsStruct)});
 
   ExpectValidates(FunctionSig::Build(zone(), {}, {struct_type_null}),
                   {WASM_UNREACHABLE, WASM_LOCAL_GET(0), kExprBrOnNull, 0,
@@ -2955,7 +2959,6 @@
 }
 
 TEST_F(FunctionBodyDecoderTest, Throw) {
-  WASM_FEATURE_SCOPE(eh);
   byte ex1 = builder.AddException(sigs.v_v());
   byte ex2 = builder.AddException(sigs.v_i());
   byte ex3 = builder.AddException(sigs.v_ii());
@@ -2968,7 +2971,6 @@
 }
 
 TEST_F(FunctionBodyDecoderTest, ThrowUnreachable) {
-  WASM_FEATURE_SCOPE(eh);
   byte ex1 = builder.AddException(sigs.v_v());
   byte ex2 = builder.AddException(sigs.v_i());
   ExpectValidates(sigs.i_i(), {WASM_LOCAL_GET(0), kExprThrow, ex1, WASM_NOP});
@@ -2984,7 +2986,6 @@
 #define WASM_TRY_OP kExprTry, kVoidCode
 
 TEST_F(FunctionBodyDecoderTest, TryCatch) {
-  WASM_FEATURE_SCOPE(eh);
   byte ex = builder.AddException(sigs.v_v());
   ExpectValidates(sigs.v_v(), {WASM_TRY_OP, kExprCatch, ex, kExprEnd});
   ExpectValidates(sigs.v_v(),
@@ -3001,7 +3002,6 @@
 }
 
 TEST_F(FunctionBodyDecoderTest, Rethrow) {
-  WASM_FEATURE_SCOPE(eh);
   ExpectValidates(sigs.v_v(),
                   {WASM_TRY_OP, kExprCatchAll, kExprRethrow, 0, kExprEnd});
   ExpectFailure(sigs.v_v(),
@@ -3014,7 +3014,6 @@
 }
 
 TEST_F(FunctionBodyDecoderTest, TryDelegate) {
-  WASM_FEATURE_SCOPE(eh);
   byte ex = builder.AddException(sigs.v_v());
 
   ExpectValidates(sigs.v_v(), {WASM_TRY_OP,
@@ -3649,9 +3648,8 @@
                 kAppendEnd,
                 "struct.new_default: struct type 1 has field 0 of "
                 "non-defaultable type (ref 0)");
-  ExpectFailure(
-      sigs.v_v(), {WASM_STRUCT_NEW_DEFAULT(struct_immutable_index), WASM_DROP},
-      kAppendEnd, "struct.new_default: struct_type 2 has immutable field 0");
+  ExpectValidates(sigs.v_v(),
+                  {WASM_STRUCT_NEW_DEFAULT(struct_immutable_index), WASM_DROP});
   ExpectValidates(
       sigs.v_v(),
       {WASM_ARRAY_NEW_DEFAULT(array_index, WASM_I32V(3)), WASM_DROP});
@@ -3661,10 +3659,9 @@
       kAppendEnd,
       "array.new_default: array type 4 has non-defaultable element type (ref "
       "3)");
-  ExpectFailure(
+  ExpectValidates(
       sigs.v_v(),
-      {WASM_ARRAY_NEW_DEFAULT(array_immutable_index, WASM_I32V(3)), WASM_DROP},
-      kAppendEnd, "array.new_default: array type 5 is immutable");
+      {WASM_ARRAY_NEW_DEFAULT(array_immutable_index, WASM_I32V(3)), WASM_DROP});
 }
 
 TEST_F(FunctionBodyDecoderTest, DefaultableLocal) {
@@ -3675,7 +3672,6 @@
 
 TEST_F(FunctionBodyDecoderTest, NonDefaultableLocals) {
   WASM_FEATURE_SCOPE(typed_funcref);
-  WASM_FEATURE_SCOPE(eh);
   WASM_FEATURE_SCOPE(gc);
   byte struct_type_index = builder.AddStruct({F(kWasmI32, true)});
   ValueType rep = ref(struct_type_index);
@@ -3753,7 +3749,6 @@
 }
 
 TEST_F(FunctionBodyDecoderTest, RefEq) {
-  WASM_FEATURE_SCOPE(eh);
   WASM_FEATURE_SCOPE(typed_funcref);
   WASM_FEATURE_SCOPE(simd);
   WASM_FEATURE_SCOPE(gc);
@@ -3796,7 +3791,6 @@
 }
 
 TEST_F(FunctionBodyDecoderTest, RefAsNonNull) {
-  WASM_FEATURE_SCOPE(eh);
   WASM_FEATURE_SCOPE(typed_funcref);
   WASM_FEATURE_SCOPE(simd);
   WASM_FEATURE_SCOPE(gc);
@@ -3835,7 +3829,6 @@
 }
 
 TEST_F(FunctionBodyDecoderTest, RefNull) {
-  WASM_FEATURE_SCOPE(eh);
   WASM_FEATURE_SCOPE(typed_funcref);
   WASM_FEATURE_SCOPE(gc);
 
@@ -3856,7 +3849,6 @@
 }
 
 TEST_F(FunctionBodyDecoderTest, RefIsNull) {
-  WASM_FEATURE_SCOPE(eh);
   WASM_FEATURE_SCOPE(typed_funcref);
   WASM_FEATURE_SCOPE(gc);
 
@@ -4311,8 +4303,8 @@
 
   std::tuple<HeapType::Representation, HeapType::Representation, bool, bool>
       tests[] = {
-          std::make_tuple(HeapType::kData, array_heap, true, true),
-          std::make_tuple(HeapType::kData, super_struct_heap, true, true),
+          std::make_tuple(HeapType::kArray, array_heap, true, true),
+          std::make_tuple(HeapType::kStruct, super_struct_heap, true, true),
           std::make_tuple(HeapType::kFunc, func_heap_1, true, true),
           std::make_tuple(func_heap_1, func_heap_1, true, true),
           std::make_tuple(func_heap_1, func_heap_2, true, true),
@@ -4331,7 +4323,7 @@
     HeapType from_heap = HeapType(std::get<0>(test));
     HeapType to_heap = HeapType(std::get<1>(test));
     bool should_pass = std::get<2>(test);
-    bool should_pass_ref_test = std::get<3>(test);
+    bool should_pass_new_ops = std::get<3>(test);
     SCOPED_TRACE("from_heap = " + from_heap.name() +
                  ", to_heap = " + to_heap.name());
 
@@ -4346,34 +4338,42 @@
       ExpectValidates(&test_sig,
                       {WASM_REF_TEST_DEPRECATED(WASM_LOCAL_GET(0),
                                                 WASM_HEAP_TYPE(to_heap))});
-      ExpectValidates(&cast_sig, {WASM_REF_CAST(WASM_LOCAL_GET(0),
+      ExpectValidates(&cast_sig,
+                      {WASM_REF_CAST_DEPRECATED(WASM_LOCAL_GET(0),
                                                 WASM_HEAP_TYPE(to_heap))});
     } else {
       std::string error_message =
-          "[0] expected subtype of (ref null func) or (ref null data), found "
-          "local.get of type " +
+          "[0] expected subtype of (ref null func), (ref null struct) or (ref "
+          "null array), found local.get of type " +
           test_reps[1].name();
       ExpectFailure(&test_sig,
                     {WASM_REF_TEST_DEPRECATED(WASM_LOCAL_GET(0),
                                               WASM_HEAP_TYPE(to_heap))},
                     kAppendEnd, ("ref.test" + error_message).c_str());
       ExpectFailure(&cast_sig,
-                    {WASM_REF_CAST(WASM_LOCAL_GET(0), WASM_HEAP_TYPE(to_heap))},
+                    {WASM_REF_CAST_DEPRECATED(WASM_LOCAL_GET(0),
+                                              WASM_HEAP_TYPE(to_heap))},
                     kAppendEnd, ("ref.cast" + error_message).c_str());
     }
 
-    if (should_pass_ref_test) {
+    if (should_pass_new_ops) {
       ExpectValidates(&test_sig, {WASM_REF_TEST(WASM_LOCAL_GET(0),
                                                 WASM_HEAP_TYPE(to_heap))});
+      ExpectValidates(&cast_sig, {WASM_REF_CAST(WASM_LOCAL_GET(0),
+                                                WASM_HEAP_TYPE(to_heap))});
     } else {
       std::string error_message =
-          "Invalid types for ref.test: local.get of type " +
-          cast_reps[1].name() +
+          "local.get of type " + cast_reps[1].name() +
           " has to be in the same reference type hierarchy as (ref " +
           to_heap.name() + ")";
       ExpectFailure(&test_sig,
                     {WASM_REF_TEST(WASM_LOCAL_GET(0), WASM_HEAP_TYPE(to_heap))},
-                    kAppendEnd, error_message.c_str());
+                    kAppendEnd,
+                    ("Invalid types for ref.test: " + error_message).c_str());
+      ExpectFailure(&cast_sig,
+                    {WASM_REF_CAST(WASM_LOCAL_GET(0), WASM_HEAP_TYPE(to_heap))},
+                    kAppendEnd,
+                    ("Invalid types for ref.cast: " + error_message).c_str());
     }
   }
 
@@ -4381,24 +4381,28 @@
   ExpectFailure(sigs.v_v(),
                 {WASM_REF_TEST_DEPRECATED(WASM_I32V(1), array_heap), kExprDrop},
                 kAppendEnd,
-                "ref.test[0] expected subtype of (ref null func) or "
-                "(ref null data), found i32.const of type i32");
+                "ref.test[0] expected subtype of (ref null func), (ref null "
+                "struct) or (ref null array), found i32.const of type i32");
   ExpectFailure(sigs.v_v(),
                 {WASM_REF_TEST(WASM_I32V(1), array_heap), kExprDrop},
                 kAppendEnd,
                 "Invalid types for ref.test: i32.const of type i32 has to be "
                 "in the same reference type hierarchy as (ref 0)");
   ExpectFailure(sigs.v_v(),
+                {WASM_REF_CAST_DEPRECATED(WASM_I32V(1), array_heap), kExprDrop},
+                kAppendEnd,
+                "ref.cast[0] expected subtype of (ref null func), (ref null "
+                "struct) or (ref null array), found i32.const of type i32");
+  ExpectFailure(sigs.v_v(),
                 {WASM_REF_CAST(WASM_I32V(1), array_heap), kExprDrop},
                 kAppendEnd,
-                "ref.cast[0] expected subtype of (ref null func) or "
-                "(ref null data), found i32.const of type i32");
+                "Invalid types for ref.cast: i32.const of type i32 has to be "
+                "in the same reference type hierarchy as (ref 0)");
 }
 
 TEST_F(FunctionBodyDecoderTest, BrOnCastOrCastFail) {
   WASM_FEATURE_SCOPE(typed_funcref);
   WASM_FEATURE_SCOPE(gc);
-  FLAG_SCOPE(experimental_wasm_gc);
 
   byte super_struct = builder.AddStruct({F(kWasmI16, true)});
   byte sub_struct =
@@ -4412,6 +4416,10 @@
       {WASM_I32V(42), WASM_LOCAL_GET(0), WASM_BR_ON_CAST(0, sub_struct),
        WASM_GC_OP(kExprRefCast), sub_struct});
   ExpectValidates(
+      FunctionSig::Build(this->zone(), {kWasmI32, subtype}, {supertype}),
+      {WASM_I32V(42), WASM_LOCAL_GET(0), WASM_BR_ON_CAST(0, sub_struct),
+       WASM_GC_OP(kExprRefCast), sub_struct});
+  ExpectValidates(
       FunctionSig::Build(this->zone(), {kWasmI32, supertype}, {supertype}),
       {WASM_I32V(42), WASM_LOCAL_GET(0), WASM_BR_ON_CAST_FAIL(0, sub_struct)});
 
@@ -4437,32 +4445,32 @@
       kAppendEnd, "type error in branch[0] (expected i32, got (ref null 0))");
 
   // Argument type error.
-  ExpectFailure(FunctionSig::Build(this->zone(), {subtype}, {kWasmExternRef}),
-                {WASM_LOCAL_GET(0), WASM_BR_ON_CAST(0, sub_struct),
-                 WASM_GC_OP(kExprRefCast), sub_struct},
-                kAppendEnd,
-                "br_on_cast[0] expected subtype of (ref null func) or "
-                "(ref null data), found local.get of type externref");
-  ExpectFailure(FunctionSig::Build(this->zone(), {supertype}, {kWasmExternRef}),
-                {WASM_LOCAL_GET(0), WASM_BR_ON_CAST_FAIL(0, sub_struct)},
-                kAppendEnd,
-                "br_on_cast_fail[0] expected subtype of (ref null func) "
-                "or (ref null data), found local.get of type externref");
+  ExpectFailure(
+      FunctionSig::Build(this->zone(), {subtype}, {kWasmExternRef}),
+      {WASM_LOCAL_GET(0), WASM_BR_ON_CAST(0, sub_struct),
+       WASM_GC_OP(kExprRefCast), sub_struct},
+      kAppendEnd,
+      "br_on_cast[0] expected subtype of (ref null func), (ref null struct) or "
+      "(ref null array), found local.get of type externref");
+  ExpectFailure(
+      FunctionSig::Build(this->zone(), {supertype}, {kWasmExternRef}),
+      {WASM_LOCAL_GET(0), WASM_BR_ON_CAST_FAIL(0, sub_struct)}, kAppendEnd,
+      "br_on_cast_fail[0] expected subtype of (ref null func), (ref null "
+      "struct) or (ref null array), found local.get of type externref");
 }
 
 TEST_F(FunctionBodyDecoderTest, BrOnAbstractType) {
   WASM_FEATURE_SCOPE(typed_funcref);
   WASM_FEATURE_SCOPE(gc);
-  FLAG_SCOPE(experimental_wasm_gc);
 
   ValueType kNonNullableFunc = ValueType::Ref(HeapType::kFunc);
 
   ExpectValidates(
-      FunctionSig::Build(this->zone(), {kWasmDataRef}, {kWasmAnyRef}),
-      {WASM_LOCAL_GET(0), WASM_BR_ON_DATA(0), WASM_GC_OP(kExprRefAsData)});
+      FunctionSig::Build(this->zone(), {kWasmStructRef}, {kWasmAnyRef}),
+      {WASM_LOCAL_GET(0), WASM_BR_ON_STRUCT(0), WASM_GC_OP(kExprRefAsStruct)});
   ExpectValidates(
       FunctionSig::Build(this->zone(), {kWasmAnyRef}, {kWasmAnyRef}),
-      {WASM_LOCAL_GET(0), WASM_BR_ON_NON_DATA(0)});
+      {WASM_LOCAL_GET(0), WASM_BR_ON_NON_STRUCT(0)});
   ExpectValidates(
       FunctionSig::Build(this->zone(), {kWasmI31Ref}, {kWasmAnyRef}),
       {WASM_LOCAL_GET(0), WASM_BR_ON_I31(0), WASM_GC_OP(kExprRefAsI31)});
@@ -4472,20 +4480,21 @@
 
   // Wrong branch type.
   ExpectFailure(FunctionSig::Build(this->zone(), {}, {kWasmAnyRef}),
-                {WASM_LOCAL_GET(0), WASM_BR_ON_DATA(0), WASM_UNREACHABLE},
+                {WASM_LOCAL_GET(0), WASM_BR_ON_STRUCT(0), WASM_UNREACHABLE},
                 kAppendEnd,
-                "br_on_data must target a branch of arity at least 1");
+                "br_on_struct must target a branch of arity at least 1");
   ExpectFailure(
       FunctionSig::Build(this->zone(), {kNonNullableFunc}, {kWasmAnyRef}),
-      {WASM_LOCAL_GET(0), WASM_BR_ON_NON_DATA(0)}, kAppendEnd,
+      {WASM_LOCAL_GET(0), WASM_BR_ON_NON_STRUCT(0)}, kAppendEnd,
       "type error in branch[0] (expected (ref func), got anyref)");
 
   // Wrong fallthrough type.
-  ExpectFailure(FunctionSig::Build(this->zone(), {kWasmDataRef}, {kWasmAnyRef}),
-                {WASM_LOCAL_GET(0), WASM_BR_ON_DATA(0)}, kAppendEnd,
-                "type error in fallthru[0] (expected dataref, got anyref)");
+  ExpectFailure(
+      FunctionSig::Build(this->zone(), {kWasmStructRef}, {kWasmAnyRef}),
+      {WASM_LOCAL_GET(0), WASM_BR_ON_STRUCT(0)}, kAppendEnd,
+      "type error in fallthru[0] (expected structref, got anyref)");
   ExpectFailure(FunctionSig::Build(this->zone(), {kWasmAnyRef}, {kWasmAnyRef}),
-                {WASM_BLOCK_I(WASM_LOCAL_GET(0), WASM_BR_ON_NON_DATA(0))},
+                {WASM_BLOCK_I(WASM_LOCAL_GET(0), WASM_BR_ON_NON_STRUCT(0))},
                 kAppendEnd,
                 "type error in branch[0] (expected i32, got anyref)");
 
@@ -4497,6 +4506,38 @@
       "br_on_i31[0] expected type anyref, found local.get of type i32");
 }
 
+TEST_F(FunctionBodyDecoderTest, BrWithBottom) {
+  WASM_FEATURE_SCOPE(typed_funcref);
+  WASM_FEATURE_SCOPE(gc);
+
+  // Merging an unsatisfiable non-nullable (ref none) into a target that
+  // expects a non-null struct is OK.
+  ExpectValidates(
+      FunctionSig::Build(this->zone(), {ValueType::Ref(HeapType::kStruct)},
+                         {ValueType::Ref(HeapType::kStruct)}),
+      {WASM_BR_ON_NON_NULL(0, WASM_REF_NULL(ValueTypeCode::kNoneCode)),
+       WASM_LOCAL_GET(0)});
+  // Merging the same value into a target that expects a value outside
+  // the "anyref" hierarchy is invalid...
+  ExpectFailure(
+      FunctionSig::Build(this->zone(), {kWasmFuncRef}, {kWasmFuncRef}),
+      {WASM_BR_ON_NON_NULL(0, WASM_REF_NULL(ValueTypeCode::kNoneCode)),
+       WASM_LOCAL_GET(0)},
+      kAppendEnd, "type error in branch[0] (expected funcref, got (ref none))");
+  // ...because it would have to be a (ref nofunc) in that case.
+  ExpectValidates(
+      FunctionSig::Build(this->zone(), {kWasmFuncRef}, {kWasmFuncRef}),
+      {WASM_BR_ON_NON_NULL(0, WASM_REF_NULL(ValueTypeCode::kNoFuncCode)),
+       WASM_LOCAL_GET(0)});
+  // (ref nofunc) in turn doesn't match anyref.
+  ExpectFailure(
+      FunctionSig::Build(this->zone(), {kWasmAnyRef}, {kWasmAnyRef}),
+      {WASM_BR_ON_NON_NULL(0, WASM_REF_NULL(ValueTypeCode::kNoFuncCode)),
+       WASM_LOCAL_GET(0)},
+      kAppendEnd,
+      "type error in branch[0] (expected anyref, got (ref nofunc))");
+}
+
 TEST_F(FunctionBodyDecoderTest, LocalTeeTyping) {
   WASM_FEATURE_SCOPE(typed_funcref);
   WASM_FEATURE_SCOPE(gc);
@@ -4602,15 +4643,15 @@
   BranchTableIteratorTest() : TestWithZone() {}
   void CheckBrTableSize(const byte* start, const byte* end) {
     Decoder decoder(start, end);
-    BranchTableImmediate<Decoder::kFullValidation> operand(&decoder, start + 1);
-    BranchTableIterator<Decoder::kFullValidation> iterator(&decoder, operand);
+    BranchTableImmediate operand(&decoder, start + 1, Decoder::kFullValidation);
+    BranchTableIterator<Decoder::FullValidationTag> iterator(&decoder, operand);
     EXPECT_EQ(end - start - 1u, iterator.length());
     EXPECT_OK(decoder);
   }
   void CheckBrTableError(const byte* start, const byte* end) {
     Decoder decoder(start, end);
-    BranchTableImmediate<Decoder::kFullValidation> operand(&decoder, start + 1);
-    BranchTableIterator<Decoder::kFullValidation> iterator(&decoder, operand);
+    BranchTableImmediate operand(&decoder, start + 1, Decoder::kFullValidation);
+    BranchTableIterator<Decoder::FullValidationTag> iterator(&decoder, operand);
     iterator.length();
     EXPECT_FALSE(decoder.ok());
   }
@@ -4704,10 +4745,10 @@
   void ExpectFailure(Bytes... bytes) {
     const byte code[] = {static_cast<byte>(bytes)..., 0, 0, 0, 0, 0, 0, 0, 0};
     WasmFeatures no_features = WasmFeatures::None();
-    WasmDecoder<Decoder::kFullValidation> decoder(
+    WasmDecoder<Decoder::FullValidationTag> decoder(
         this->zone(), nullptr, no_features, &no_features, nullptr, code,
         code + sizeof(code), 0);
-    WasmDecoder<Decoder::kFullValidation>::OpcodeLength(&decoder, code);
+    WasmDecoder<Decoder::FullValidationTag>::OpcodeLength(&decoder, code);
     EXPECT_TRUE(decoder.failed());
   }
 
@@ -4729,10 +4770,10 @@
       }
     }
     WasmFeatures detected;
-    WasmDecoder<Decoder::kBooleanValidation> decoder(
+    WasmDecoder<Decoder::BooleanValidationTag> decoder(
         this->zone(), nullptr, WasmFeatures::All(), &detected, nullptr, bytes,
         bytes + sizeof(bytes), 0);
-    WasmDecoder<Decoder::kBooleanValidation>::OpcodeLength(&decoder, bytes);
+    WasmDecoder<Decoder::BooleanValidationTag>::OpcodeLength(&decoder, bytes);
     EXPECT_TRUE(decoder.ok())
         << opcode << " aka " << WasmOpcodes::OpcodeName(opcode) << ": "
         << decoder.error().message();
@@ -4920,20 +4961,11 @@
 
 class TypeReaderTest : public TestWithZone {
  public:
-  ValueType DecodeValueType(const byte* start, const byte* end,
-                            const WasmModule* module) {
-    Decoder decoder(start, end);
-    uint32_t length;
-    return value_type_reader::read_value_type<Decoder::kFullValidation>(
-        &decoder, start, &length, module, enabled_features_);
-  }
-
-  HeapType DecodeHeapType(const byte* start, const byte* end,
-                          const WasmModule* module) {
+  HeapType DecodeHeapType(const byte* start, const byte* end) {
     Decoder decoder(start, end);
     uint32_t length;
-    return value_type_reader::read_heap_type<Decoder::kFullValidation>(
-        &decoder, start, &length, module, enabled_features_);
+    return value_type_reader::read_heap_type<Decoder::FullValidationTag>(
+        &decoder, start, &length, enabled_features_);
   }
 
   // This variable is modified by WASM_FEATURE_SCOPE.
@@ -4950,34 +4982,34 @@
   // 1- to 5-byte representation of kFuncRefCode.
   {
     const byte data[] = {kFuncRefCode};
-    HeapType result = DecodeHeapType(data, data + sizeof(data), nullptr);
+    HeapType result = DecodeHeapType(data, data + sizeof(data));
     EXPECT_TRUE(result == heap_func);
   }
   {
     const byte data[] = {kFuncRefCode | 0x80, 0x7F};
-    HeapType result = DecodeHeapType(data, data + sizeof(data), nullptr);
+    HeapType result = DecodeHeapType(data, data + sizeof(data));
     EXPECT_EQ(result, heap_func);
   }
   {
     const byte data[] = {kFuncRefCode | 0x80, 0xFF, 0x7F};
-    HeapType result = DecodeHeapType(data, data + sizeof(data), nullptr);
+    HeapType result = DecodeHeapType(data, data + sizeof(data));
     EXPECT_EQ(result, heap_func);
   }
   {
     const byte data[] = {kFuncRefCode | 0x80, 0xFF, 0xFF, 0x7F};
-    HeapType result = DecodeHeapType(data, data + sizeof(data), nullptr);
+    HeapType result = DecodeHeapType(data, data + sizeof(data));
     EXPECT_EQ(result, heap_func);
   }
   {
     const byte data[] = {kFuncRefCode | 0x80, 0xFF, 0xFF, 0xFF, 0x7F};
-    HeapType result = DecodeHeapType(data, data + sizeof(data), nullptr);
+    HeapType result = DecodeHeapType(data, data + sizeof(data));
     EXPECT_EQ(result, heap_func);
   }
 
   {
     // Some negative number.
     const byte data[] = {0xB4, 0x7F};
-    HeapType result = DecodeHeapType(data, data + sizeof(data), nullptr);
+    HeapType result = DecodeHeapType(data, data + sizeof(data));
     EXPECT_EQ(result, heap_bottom);
   }
 
@@ -4986,7 +5018,7 @@
     // range. This should therefore NOT be decoded as HeapType::kFunc and
     // instead fail.
     const byte data[] = {kFuncRefCode | 0x80, 0x6F};
-    HeapType result = DecodeHeapType(data, data + sizeof(data), nullptr);
+    HeapType result = DecodeHeapType(data, data + sizeof(data));
     EXPECT_EQ(result, heap_bottom);
   }
 }
@@ -5007,8 +5039,8 @@
   bool DecodeLocalDecls(BodyLocalDecls* decls, const byte* start,
                         const byte* end) {
     WasmModule module;
-    return i::wasm::DecodeLocalDecls(enabled_features_, decls, &module, start,
-                                     end, zone());
+    return ValidateAndDecodeLocalDeclsForTesting(enabled_features_, decls,
+                                                 &module, start, end, zone());
   }
 };
 
diff -r -u --color up/v8/test/unittests/wasm/leb-helper-unittest.cc nw/v8/test/unittests/wasm/leb-helper-unittest.cc
--- up/v8/test/unittests/wasm/leb-helper-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/wasm/leb-helper-unittest.cc	2023-01-19 16:46:37.652275846 -0500
@@ -99,7 +99,7 @@
     Decoder decoder(buffer, buffer + kSize);                               \
     unsigned length = 0;                                                   \
     ctype result =                                                         \
-        decoder.read_##name<Decoder::kNoValidation>(buffer, &length);      \
+        decoder.read_##name<Decoder::NoValidationTag>(buffer, &length);    \
     EXPECT_EQ(val, result);                                                \
     EXPECT_EQ(LEBHelper::sizeof_##name(val), static_cast<size_t>(length)); \
   }
diff -r -u --color up/v8/test/unittests/wasm/memory-protection-unittest.cc nw/v8/test/unittests/wasm/memory-protection-unittest.cc
--- up/v8/test/unittests/wasm/memory-protection-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/wasm/memory-protection-unittest.cc	2023-01-19 16:46:37.652275846 -0500
@@ -130,10 +130,11 @@
 
     ErrorThrower thrower(isolate(), "");
     constexpr int kNoCompilationId = 0;
+    constexpr ProfileInformation* kNoProfileInformation = nullptr;
     std::shared_ptr<NativeModule> native_module = CompileToNativeModule(
         isolate(), WasmFeatures::All(), &thrower, std::move(result).value(),
         ModuleWireBytes{base::ArrayVector(module_bytes)}, kNoCompilationId,
-        v8::metrics::Recorder::ContextId::Empty());
+        v8::metrics::Recorder::ContextId::Empty(), kNoProfileInformation);
     CHECK(!thrower.error());
     CHECK_NOT_NULL(native_module);
 
diff -r -u --color up/v8/test/unittests/wasm/module-decoder-unittest.cc nw/v8/test/unittests/wasm/module-decoder-unittest.cc
--- up/v8/test/unittests/wasm/module-decoder-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/wasm/module-decoder-unittest.cc	2023-01-19 16:46:37.652275846 -0500
@@ -78,12 +78,6 @@
                     'i', 'n', 't'),                                        \
           __VA_ARGS__)
 
-#define FAIL_IF_NO_EXPERIMENTAL_EH(data)                                 \
-  do {                                                                   \
-    ModuleResult result = DecodeModule((data), (data) + sizeof((data))); \
-    EXPECT_FALSE(result.ok());                                           \
-  } while (false)
-
 #define X1(...) __VA_ARGS__
 #define X2(...) __VA_ARGS__, __VA_ARGS__
 #define X3(...) __VA_ARGS__, __VA_ARGS__, __VA_ARGS__
@@ -193,7 +187,7 @@
     {kAnyRefCode, kWasmAnyRef},                    // --
     {kEqRefCode, kWasmEqRef},                      // --
     {kI31RefCode, kWasmI31Ref},                    // --
-    {kDataRefCode, kWasmDataRef},                  // --
+    {kStructRefCode, kWasmStructRef},              // --
     {kArrayRefCode, kWasmArrayRef},                // --
     {kNoneCode, kWasmNullRef},                     // --
     {kStringRefCode, kWasmStringRef},              // --
@@ -1082,9 +1076,6 @@
 
 TEST_F(WasmModuleVerifyTest, ZeroExceptions) {
   static const byte data[] = {SECTION(Tag, ENTRY_COUNT(0))};
-  FAIL_IF_NO_EXPERIMENTAL_EH(data);
-
-  WASM_FEATURE_SCOPE(eh);
   ModuleResult result = DecodeModule(data, data + sizeof(data));
   EXPECT_OK(result);
   EXPECT_EQ(0u, result.value()->tags.size());
@@ -1095,9 +1086,6 @@
       SECTION(Type, ENTRY_COUNT(1), SIG_ENTRY_v_x(kI32Code)),  // sig#0 (i32)
       SECTION(Tag, ENTRY_COUNT(1),
               EXCEPTION_ENTRY(SIG_INDEX(0)))};  // except[0] (sig#0)
-  FAIL_IF_NO_EXPERIMENTAL_EH(data);
-
-  WASM_FEATURE_SCOPE(eh);
   ModuleResult result = DecodeModule(data, data + sizeof(data));
   EXPECT_OK(result);
   EXPECT_EQ(1u, result.value()->tags.size());
@@ -1115,9 +1103,6 @@
       SECTION(Tag, ENTRY_COUNT(2),
               EXCEPTION_ENTRY(SIG_INDEX(1)),    // except[0] (sig#1)
               EXCEPTION_ENTRY(SIG_INDEX(0)))};  // except[1] (sig#0)
-  FAIL_IF_NO_EXPERIMENTAL_EH(data);
-
-  WASM_FEATURE_SCOPE(eh);
   ModuleResult result = DecodeModule(data, data + sizeof(data));
   EXPECT_OK(result);
   EXPECT_EQ(2u, result.value()->tags.size());
@@ -1135,10 +1120,7 @@
       SECTION(Tag, ENTRY_COUNT(1),
               EXCEPTION_ENTRY(
                   SIG_INDEX(23)))};  // except[0] (sig#23 [out-of-bounds])
-  FAIL_IF_NO_EXPERIMENTAL_EH(data);
-
   // Should fail decoding exception section.
-  WASM_FEATURE_SCOPE(eh);
   ModuleResult result = DecodeModule(data, data + sizeof(data));
   EXPECT_NOT_OK(result, "signature index 23 out of bounds");
 }
@@ -1149,10 +1131,7 @@
       SECTION(Tag, ENTRY_COUNT(1),
               EXCEPTION_ENTRY(
                   SIG_INDEX(0)))};  // except[0] (sig#0 [invalid-return-type])
-  FAIL_IF_NO_EXPERIMENTAL_EH(data);
-
   // Should fail decoding exception section.
-  WASM_FEATURE_SCOPE(eh);
   ModuleResult result = DecodeModule(data, data + sizeof(data));
   EXPECT_NOT_OK(result, "tag signature 0 has non-void return");
 }
@@ -1162,10 +1141,7 @@
       SECTION(Type, ENTRY_COUNT(1), SIG_ENTRY_i_i),
       SECTION(Tag, ENTRY_COUNT(1), 23,
               SIG_INDEX(0))};  // except[0] (sig#0) [invalid-attribute]
-  FAIL_IF_NO_EXPERIMENTAL_EH(data);
-
   // Should fail decoding exception section.
-  WASM_FEATURE_SCOPE(eh);
   ModuleResult result = DecodeModule(data, data + sizeof(data));
   EXPECT_NOT_OK(result, "exception attribute 23 not supported");
 }
@@ -1174,9 +1150,6 @@
   static const byte data[] = {SECTION(Memory, ENTRY_COUNT(0)),
                               SECTION(Tag, ENTRY_COUNT(0)),
                               SECTION(Global, ENTRY_COUNT(0))};
-  FAIL_IF_NO_EXPERIMENTAL_EH(data);
-
-  WASM_FEATURE_SCOPE(eh);
   ModuleResult result = DecodeModule(data, data + sizeof(data));
   EXPECT_OK(result);
 }
@@ -1184,9 +1157,6 @@
 TEST_F(WasmModuleVerifyTest, TagSectionAfterGlobal) {
   static const byte data[] = {SECTION(Global, ENTRY_COUNT(0)),
                               SECTION(Tag, ENTRY_COUNT(0))};
-  FAIL_IF_NO_EXPERIMENTAL_EH(data);
-
-  WASM_FEATURE_SCOPE(eh);
   ModuleResult result = DecodeModule(data, data + sizeof(data));
   EXPECT_NOT_OK(result,
                 "The Tag section must appear before the Global section");
@@ -1195,9 +1165,6 @@
 TEST_F(WasmModuleVerifyTest, TagSectionBeforeMemory) {
   static const byte data[] = {SECTION(Tag, ENTRY_COUNT(0)),
                               SECTION(Memory, ENTRY_COUNT(0))};
-  FAIL_IF_NO_EXPERIMENTAL_EH(data);
-
-  WASM_FEATURE_SCOPE(eh);
   ModuleResult result = DecodeModule(data, data + sizeof(data));
   EXPECT_NOT_OK(result, "unexpected section <Memory>");
 }
@@ -1207,9 +1174,6 @@
   static const byte data[] = {SECTION(Table, ENTRY_COUNT(0)),
                               SECTION(Tag, ENTRY_COUNT(0)),
                               SECTION(Memory, ENTRY_COUNT(0))};
-  FAIL_IF_NO_EXPERIMENTAL_EH(data);
-
-  WASM_FEATURE_SCOPE(eh);
   ModuleResult result = DecodeModule(data, data + sizeof(data));
   EXPECT_NOT_OK(result, "unexpected section <Memory>");
 }
@@ -1223,9 +1187,6 @@
               ADD_COUNT('e', 'x'),              // tag name
               kExternalTag,                     // import kind
               EXCEPTION_ENTRY(SIG_INDEX(0)))};  // except[0] (sig#0)
-  FAIL_IF_NO_EXPERIMENTAL_EH(data);
-
-  WASM_FEATURE_SCOPE(eh);
   ModuleResult result = DecodeModule(data, data + sizeof(data));
   EXPECT_OK(result);
   EXPECT_EQ(1u, result.value()->tags.size());
@@ -1241,9 +1202,6 @@
               NO_NAME,                         // --
               kExternalTag,                    // --
               EXCEPTION_INDEX(0))};
-  FAIL_IF_NO_EXPERIMENTAL_EH(data);
-
-  WASM_FEATURE_SCOPE(eh);
   ModuleResult result = DecodeModule(data, data + sizeof(data));
   EXPECT_OK(result);
   EXPECT_EQ(1u, result.value()->tags.size());
@@ -3459,7 +3417,6 @@
 #undef EMPTY_NAMES_SECTION
 #undef SECTION_SRC_MAP
 #undef SECTION_COMPILATION_HINTS
-#undef FAIL_IF_NO_EXPERIMENTAL_EH
 #undef X1
 #undef X2
 #undef X3
diff -r -u --color up/v8/test/unittests/wasm/subtyping-unittest.cc nw/v8/test/unittests/wasm/subtyping-unittest.cc
--- up/v8/test/unittests/wasm/subtyping-unittest.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/unittests/wasm/subtyping-unittest.cc	2023-01-19 16:46:37.652275846 -0500
@@ -64,6 +64,7 @@
 
 TEST_F(WasmSubtypingTest, Subtyping) {
   FLAG_SCOPE(experimental_wasm_gc);
+  FLAG_VALUE_SCOPE(wasm_gc_structref_as_dataref, false);
   v8::internal::AccountingAllocator allocator;
   WasmModule module1_(std::make_unique<Zone>(&allocator, ZONE_NAME));
   WasmModule module2_(std::make_unique<Zone>(&allocator, ZONE_NAME));
@@ -130,7 +131,7 @@
                                          kWasmS128};
   constexpr ValueType ref_types[] = {
       kWasmFuncRef,   kWasmEqRef,           // --
-      kWasmDataRef,   kWasmArrayRef,        // --
+      kWasmStructRef, kWasmArrayRef,        // --
       kWasmI31Ref,    kWasmAnyRef,          // --
       kWasmExternRef, kWasmNullExternRef,   // --
       kWasmNullRef,   kWasmNullFuncRef,     // --
@@ -201,17 +202,15 @@
                                   ref_type == kWasmStringViewWtf8 ||
                                   ref_type == kWasmStringViewWtf16;
       SCOPED_TRACE("ref_type: " + ref_type.name());
-      // Concrete reference types, i31ref and dataref are subtypes of eqref,
-      // externref/funcref/anyref/functions are not.
+      // Concrete reference types, i31ref, structref and arrayref are subtypes
+      // of eqref, externref/funcref/anyref/functions are not.
       SUBTYPE_IFF(ref_type, kWasmEqRef,
                   ref_type != kWasmAnyRef && !is_any_func && !is_extern &&
                       !is_string_view && ref_type != kWasmStringRef);
-      // Struct/array types are subtypes of dataref.
-      SUBTYPE_IFF(ref_type, kWasmDataRef,
-                  ref_type == kWasmDataRef || ref_type == kWasmArrayRef ||
-                      ref_type == kWasmNullRef || ref_type == ref(0) ||
-                      ref_type == ref(2) || ref_type == refNull(0) ||
-                      ref_type == refNull(2));
+      // Struct types are subtypes of structref.
+      SUBTYPE_IFF(ref_type, kWasmStructRef,
+                  ref_type == kWasmStructRef || ref_type == kWasmNullRef ||
+                      ref_type == ref(0) || ref_type == refNull(0));
       // Array types are subtypes of arrayref.
       SUBTYPE_IFF(ref_type, kWasmArrayRef,
                   ref_type == kWasmArrayRef || ref_type == ref(2) ||
@@ -379,33 +378,32 @@
     }
 
     // Abstract types vs abstract types.
-    UNION(kWasmEqRef, kWasmDataRef, kWasmEqRef);
+    UNION(kWasmEqRef, kWasmStructRef, kWasmEqRef);
     UNION(kWasmEqRef, kWasmI31Ref, kWasmEqRef);
     UNION(kWasmEqRef, kWasmArrayRef, kWasmEqRef);
     UNION(kWasmEqRef, kWasmNullRef, kWasmEqRef);
-    UNION(kWasmDataRef, kWasmI31Ref, kWasmEqRef);
-    UNION(kWasmDataRef, kWasmArrayRef, kWasmDataRef);
-    UNION(kWasmDataRef, kWasmNullRef, kWasmDataRef.AsNullable());
+    UNION(kWasmStructRef, kWasmI31Ref, kWasmEqRef);
+    UNION(kWasmStructRef, kWasmArrayRef, kWasmEqRef);
+    UNION(kWasmStructRef, kWasmNullRef, kWasmStructRef.AsNullable());
     UNION(kWasmI31Ref.AsNonNull(), kWasmArrayRef.AsNonNull(),
           kWasmEqRef.AsNonNull());
     UNION(kWasmI31Ref, kWasmNullRef, kWasmI31Ref.AsNullable());
     UNION(kWasmArrayRef, kWasmNullRef, kWasmArrayRef.AsNullable());
-    UNION(kWasmDataRef.AsNonNull(), kWasmI31Ref.AsNonNull(),
+    UNION(kWasmStructRef.AsNonNull(), kWasmI31Ref.AsNonNull(),
           kWasmEqRef.AsNonNull());
-    UNION(kWasmDataRef, kWasmArrayRef, kWasmDataRef);
     UNION(kWasmI31Ref.AsNonNull(), kWasmArrayRef, kWasmEqRef);
     UNION(kWasmAnyRef, kWasmNullRef, kWasmAnyRef);
     UNION(kWasmExternRef, kWasmNullExternRef, kWasmExternRef);
     UNION(kWasmFuncRef, kWasmNullFuncRef, kWasmFuncRef);
 
     INTERSECTION(kWasmExternRef, kWasmEqRef, kWasmBottom);
-    INTERSECTION(kWasmExternRef, kWasmDataRef, kWasmBottom);
+    INTERSECTION(kWasmExternRef, kWasmStructRef, kWasmBottom);
     INTERSECTION(kWasmExternRef, kWasmI31Ref.AsNonNull(), kWasmBottom);
     INTERSECTION(kWasmExternRef, kWasmArrayRef, kWasmBottom);
     INTERSECTION(kWasmExternRef, kWasmNullRef, kWasmBottom);
     INTERSECTION(kWasmExternRef, kWasmFuncRef, kWasmBottom);
     INTERSECTION(kWasmNullExternRef, kWasmEqRef, kWasmBottom);
-    INTERSECTION(kWasmNullExternRef, kWasmDataRef, kWasmBottom);
+    INTERSECTION(kWasmNullExternRef, kWasmStructRef, kWasmBottom);
     INTERSECTION(kWasmNullExternRef, kWasmI31Ref, kWasmBottom);
     INTERSECTION(kWasmNullExternRef, kWasmArrayRef, kWasmBottom);
     INTERSECTION(kWasmNullExternRef, kWasmNullRef, kWasmBottom);
@@ -413,13 +411,13 @@
     INTERSECTION(kWasmNullExternRef, kWasmExternRef.AsNonNull(), kWasmBottom);
 
     INTERSECTION(kWasmFuncRef, kWasmEqRef, kWasmBottom);
-    INTERSECTION(kWasmFuncRef, kWasmDataRef, kWasmBottom);
+    INTERSECTION(kWasmFuncRef, kWasmStructRef, kWasmBottom);
     INTERSECTION(kWasmFuncRef, kWasmI31Ref.AsNonNull(), kWasmBottom);
     INTERSECTION(kWasmFuncRef, kWasmArrayRef, kWasmBottom);
     INTERSECTION(kWasmFuncRef, kWasmNullRef, kWasmBottom);
     INTERSECTION(kWasmFuncRef, kWasmNullExternRef, kWasmBottom);
     INTERSECTION(kWasmNullFuncRef, kWasmEqRef, kWasmBottom);
-    INTERSECTION(kWasmNullFuncRef, kWasmDataRef, kWasmBottom);
+    INTERSECTION(kWasmNullFuncRef, kWasmStructRef, kWasmBottom);
     INTERSECTION(kWasmNullFuncRef, kWasmI31Ref, kWasmBottom);
     INTERSECTION(kWasmNullFuncRef, kWasmArrayRef, kWasmBottom);
     INTERSECTION(kWasmNullFuncRef, kWasmNullRef, kWasmBottom);
@@ -427,14 +425,14 @@
     INTERSECTION(kWasmNullFuncRef, kWasmFuncRef.AsNonNull(), kWasmBottom);
     INTERSECTION(kWasmNullFuncRef, kWasmNullExternRef, kWasmBottom);
 
-    INTERSECTION(kWasmEqRef, kWasmDataRef, kWasmDataRef);
+    INTERSECTION(kWasmEqRef, kWasmStructRef, kWasmStructRef);
     INTERSECTION(kWasmEqRef, kWasmI31Ref, kWasmI31Ref);
     INTERSECTION(kWasmEqRef, kWasmArrayRef, kWasmArrayRef);
     INTERSECTION(kWasmEqRef, kWasmNullRef, kWasmNullRef);
     INTERSECTION(kWasmEqRef, kWasmFuncRef, kWasmBottom);
-    INTERSECTION(kWasmDataRef, kWasmI31Ref, kWasmNullRef);
-    INTERSECTION(kWasmDataRef, kWasmArrayRef, kWasmArrayRef);
-    INTERSECTION(kWasmDataRef, kWasmNullRef, kWasmNullRef);
+    INTERSECTION(kWasmStructRef, kWasmI31Ref, kWasmNullRef);
+    INTERSECTION(kWasmStructRef, kWasmArrayRef, kWasmNullRef);
+    INTERSECTION(kWasmStructRef, kWasmNullRef, kWasmNullRef);
     INTERSECTION(kWasmI31Ref, kWasmArrayRef, kWasmNullRef);
     INTERSECTION(kWasmI31Ref.AsNonNull(), kWasmNullRef, kWasmBottom);
     INTERSECTION(kWasmArrayRef.AsNonNull(), kWasmNullRef, kWasmBottom);
@@ -464,11 +462,11 @@
     INTERSECTION(kWasmEqRef, array_type, array_type);
     INTERSECTION(kWasmEqRef, function_type, kWasmBottom);
 
-    UNION(kWasmDataRef, struct_type, kWasmDataRef);
-    UNION(kWasmDataRef, array_type, kWasmDataRef);
-    INTERSECTION(kWasmDataRef, struct_type, struct_type);
-    INTERSECTION(kWasmDataRef, array_type, array_type);
-    INTERSECTION(kWasmDataRef, function_type, kWasmBottom);
+    UNION(kWasmStructRef, struct_type, kWasmStructRef);
+    UNION(kWasmStructRef, array_type, kWasmEqRef);
+    INTERSECTION(kWasmStructRef, struct_type, struct_type);
+    INTERSECTION(kWasmStructRef, array_type, kWasmBottom);
+    INTERSECTION(kWasmStructRef, function_type, kWasmBottom);
 
     UNION(kWasmI31Ref, struct_type, kWasmEqRef);
     UNION(kWasmI31Ref, array_type, kWasmEqRef);
@@ -476,7 +474,7 @@
     INTERSECTION(kWasmI31Ref, array_type, kWasmBottom);
     INTERSECTION(kWasmI31Ref, function_type, kWasmBottom);
 
-    UNION(kWasmArrayRef, struct_type, kWasmDataRef);
+    UNION(kWasmArrayRef, struct_type, kWasmEqRef);
     UNION(kWasmArrayRef, array_type, kWasmArrayRef);
     INTERSECTION(kWasmArrayRef, struct_type, kWasmBottom);
     INTERSECTION(kWasmArrayRef, array_type, array_type);
@@ -490,7 +488,7 @@
     INTERSECTION(kWasmNullRef, function_type, kWasmBottom);
 
     // Indexed types of different kinds.
-    UNION(struct_type, array_type, kWasmDataRef.AsNonNull());
+    UNION(struct_type, array_type, kWasmEqRef.AsNonNull());
     INTERSECTION(struct_type, array_type, kWasmBottom);
     INTERSECTION(struct_type, function_type, kWasmBottom);
     INTERSECTION(array_type, function_type, kWasmBottom);
@@ -498,8 +496,9 @@
     // Nullable vs. non-nullable.
     UNION(struct_type, struct_type.AsNullable(), struct_type.AsNullable());
     INTERSECTION(struct_type, struct_type.AsNullable(), struct_type);
-    UNION(kWasmDataRef, kWasmDataRef.AsNullable(), kWasmDataRef.AsNullable());
-    INTERSECTION(kWasmDataRef, kWasmDataRef.AsNullable(), kWasmDataRef);
+    UNION(kWasmStructRef, kWasmStructRef.AsNullable(),
+          kWasmStructRef.AsNullable());
+    INTERSECTION(kWasmStructRef, kWasmStructRef.AsNullable(), kWasmStructRef);
 
     // Concrete types of the same kind.
     // Subtyping relation.
@@ -512,7 +511,7 @@
     // No common ancestor.
     UNION(ref(6), refNull(2), kWasmArrayRef.AsNullable());
     INTERSECTION(ref(6), refNull(2), kWasmBottom);
-    UNION(ref(0), ref(17), kWasmDataRef.AsNonNull());
+    UNION(ref(0), ref(17), kWasmStructRef.AsNonNull());
     INTERSECTION(ref(0), ref(17), kWasmBottom);
     UNION(ref(10), refNull(11), kWasmFuncRef);
     INTERSECTION(ref(10), refNull(11), kWasmBottom);
Only in nw/v8/test/unittests/wasm: wasm-disassembler-unittest-mvp.wasm.inc
Only in nw/v8/test/unittests/wasm: wasm-disassembler-unittest-mvp.wat.inc
Only in nw/v8/test/unittests/wasm: wasm-disassembler-unittest-names.wasm.inc
Only in nw/v8/test/unittests/wasm: wasm-disassembler-unittest-names.wat.inc
Only in nw/v8/test/unittests/wasm: wasm-disassembler-unittest-simd.wasm.inc
Only in nw/v8/test/unittests/wasm: wasm-disassembler-unittest-simd.wat.inc
Only in nw/v8/test/unittests/wasm: wasm-disassembler-unittest.cc
diff -r -u --color up/v8/test/wasm-api-tests/callbacks.cc nw/v8/test/wasm-api-tests/callbacks.cc
--- up/v8/test/wasm-api-tests/callbacks.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/wasm-api-tests/callbacks.cc	2023-01-19 16:46:37.663109175 -0500
@@ -30,9 +30,10 @@
 own<Trap> Stage4_GC(void* env, const Val args[], Val results[]) {
   printf("Stage4...\n");
   i::Isolate* isolate = reinterpret_cast<i::Isolate*>(env);
-  isolate->heap()->PreciseCollectAllGarbage(
-      i::Heap::kForcedGC, i::GarbageCollectionReason::kTesting,
-      v8::kNoGCCallbackFlags);
+  ScanStackModeScopeForTesting no_stack_scanning(isolate->heap(),
+                                                 Heap::ScanStackMode::kNone);
+  isolate->heap()->PreciseCollectAllGarbage(Heap::kForcedGC,
+                                            GarbageCollectionReason::kTesting);
   results[0] = Val::i32(args[0].i32() + 1);
   return nullptr;
 }
@@ -174,6 +175,36 @@
   return nullptr;
 }
 
+own<Trap> PlusOneWithManyArgs(const Val args[], Val results[]) {
+  int32_t a0 = args[0].i32();
+  results[0] = Val::i32(a0 + 1);
+  int64_t a1 = args[1].i64();
+  results[1] = Val::i64(a1 + 1);
+  float a2 = args[2].f32();
+  results[2] = Val::f32(a2 + 1);
+  double a3 = args[3].f64();
+  results[3] = Val::f64(a3 + 1);
+  results[4] = Val::ref(args[4].ref()->copy());  // No +1 for Refs.
+  int32_t a5 = args[5].i32();
+  results[5] = Val::i32(a5 + 1);
+  int64_t a6 = args[6].i64();
+  results[6] = Val::i64(a6 + 1);
+  float a7 = args[7].f32();
+  results[7] = Val::f32(a7 + 1);
+  double a8 = args[8].f64();
+  results[8] = Val::f64(a8 + 1);
+  int32_t a9 = args[9].i32();
+  results[9] = Val::i32(a9 + 1);
+  int64_t a10 = args[10].i64();
+  results[10] = Val::i64(a10 + 1);
+  float a11 = args[11].f32();
+  results[11] = Val::f32(a11 + 1);
+  double a12 = args[12].f64();
+  results[12] = Val::f64(a12 + 1);
+  int32_t a13 = args[13].i32();
+  results[13] = Val::i32(a13 + 1);
+  return nullptr;
+}
 }  // namespace
 
 TEST_F(WasmCapiTest, DirectCallCapiFunction) {
@@ -221,6 +252,84 @@
   EXPECT_TRUE(func->same(results[4].ref()));
 }
 
+TEST_F(WasmCapiTest, DirectCallCapiFunctionWithManyArgs) {
+  // Test with many arguments to make sure that CWasmArgumentsPacker won't use
+  // its buffer-on-stack optimization.
+  own<FuncType> cpp_sig = FuncType::make(
+      ownvec<ValType>::make(
+          ValType::make(::wasm::I32), ValType::make(::wasm::I64),
+          ValType::make(::wasm::F32), ValType::make(::wasm::F64),
+          ValType::make(::wasm::ANYREF), ValType::make(::wasm::I32),
+          ValType::make(::wasm::I64), ValType::make(::wasm::F32),
+          ValType::make(::wasm::F64), ValType::make(::wasm::I32),
+          ValType::make(::wasm::I64), ValType::make(::wasm::F32),
+          ValType::make(::wasm::F64), ValType::make(::wasm::I32)),
+      ownvec<ValType>::make(
+          ValType::make(::wasm::I32), ValType::make(::wasm::I64),
+          ValType::make(::wasm::F32), ValType::make(::wasm::F64),
+          ValType::make(::wasm::ANYREF), ValType::make(::wasm::I32),
+          ValType::make(::wasm::I64), ValType::make(::wasm::F32),
+          ValType::make(::wasm::F64), ValType::make(::wasm::I32),
+          ValType::make(::wasm::I64), ValType::make(::wasm::F32),
+          ValType::make(::wasm::F64), ValType::make(::wasm::I32)));
+  own<Func> func = Func::make(store(), cpp_sig.get(), PlusOneWithManyArgs);
+  Extern* imports[] = {func.get()};
+  ValueType wasm_types[] = {
+      kWasmI32,       kWasmI64, kWasmF32, kWasmF64, kWasmExternRef, kWasmI32,
+      kWasmI64,       kWasmF32, kWasmF64, kWasmI32, kWasmI64,       kWasmF32,
+      kWasmF64,       kWasmI32, kWasmI32, kWasmI64, kWasmF32,       kWasmF64,
+      kWasmExternRef, kWasmI32, kWasmI64, kWasmF32, kWasmF64,       kWasmI32,
+      kWasmI64,       kWasmF32, kWasmF64, kWasmI32};
+  FunctionSig wasm_sig(14, 14, wasm_types);
+  int func_index = builder()->AddImport(base::CStrVector("func"), &wasm_sig);
+  builder()->ExportImportedFunction(base::CStrVector("func"), func_index);
+  Instantiate(imports);
+  int32_t a0 = 42;
+  int64_t a1 = 0x1234c0ffee;
+  float a2 = 1234.5;
+  double a3 = 123.45;
+  Val args[] = {
+      Val::i32(a0),           Val::i64(a1), Val::f32(a2), Val::f64(a3),
+      Val::ref(func->copy()), Val::i32(a0), Val::i64(a1), Val::f32(a2),
+      Val::f64(a3),           Val::i32(a0), Val::i64(a1), Val::f32(a2),
+      Val::f64(a3),           Val::i32(a0)};
+  Val results[14];
+  // Test that {func} can be called directly.
+  own<Trap> trap = func->call(args, results);
+  EXPECT_EQ(nullptr, trap);
+  EXPECT_EQ(a0 + 1, results[0].i32());
+  EXPECT_EQ(a1 + 1, results[1].i64());
+  EXPECT_EQ(a2 + 1, results[2].f32());
+  EXPECT_EQ(a3 + 1, results[3].f64());
+  EXPECT_TRUE(func->same(results[4].ref()));
+  EXPECT_EQ(a0 + 1, results[5].i32());
+  EXPECT_EQ(a1 + 1, results[6].i64());
+  EXPECT_EQ(a2 + 1, results[7].f32());
+  EXPECT_EQ(a3 + 1, results[8].f64());
+  EXPECT_EQ(a0 + 1, results[9].i32());
+  EXPECT_EQ(a1 + 1, results[10].i64());
+  EXPECT_EQ(a2 + 1, results[11].f32());
+  EXPECT_EQ(a3 + 1, results[12].f64());
+  EXPECT_EQ(a0 + 1, results[13].i32());
+
+  // Test that {func} can be called after import/export round-tripping.
+  trap = GetExportedFunction(0)->call(args, results);
+  EXPECT_EQ(nullptr, trap);
+  EXPECT_EQ(a0 + 1, results[0].i32());
+  EXPECT_EQ(a1 + 1, results[1].i64());
+  EXPECT_EQ(a2 + 1, results[2].f32());
+  EXPECT_EQ(a3 + 1, results[3].f64());
+  EXPECT_TRUE(func->same(results[4].ref()));
+  EXPECT_EQ(a0 + 1, results[5].i32());
+  EXPECT_EQ(a1 + 1, results[6].i64());
+  EXPECT_EQ(a2 + 1, results[7].f32());
+  EXPECT_EQ(a3 + 1, results[8].f64());
+  EXPECT_EQ(a0 + 1, results[9].i32());
+  EXPECT_EQ(a1 + 1, results[10].i64());
+  EXPECT_EQ(a2 + 1, results[11].f32());
+  EXPECT_EQ(a3 + 1, results[12].f64());
+  EXPECT_EQ(a0 + 1, results[13].i32());
+}
 }  // namespace wasm
 }  // namespace internal
 }  // namespace v8
diff -r -u --color up/v8/test/wasm-api-tests/serialize.cc nw/v8/test/wasm-api-tests/serialize.cc
--- up/v8/test/wasm-api-tests/serialize.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/wasm-api-tests/serialize.cc	2023-01-19 16:46:37.663109175 -0500
@@ -35,14 +35,14 @@
   // We reset the module and collect it to make sure the NativeModuleCache does
   // not contain it anymore. Otherwise deserialization will not happen.
   ResetModule();
-  i::Isolate* isolate =
-      reinterpret_cast<::wasm::StoreImpl*>(store())->i_isolate();
-  isolate->heap()->PreciseCollectAllGarbage(
-      i::Heap::kForcedGC, i::GarbageCollectionReason::kTesting,
-      v8::kNoGCCallbackFlags);
-  isolate->heap()->PreciseCollectAllGarbage(
-      i::Heap::kForcedGC, i::GarbageCollectionReason::kTesting,
-      v8::kNoGCCallbackFlags);
+  Heap* heap =
+      reinterpret_cast<::wasm::StoreImpl*>(store())->i_isolate()->heap();
+  ScanStackModeScopeForTesting no_stack_scanning(heap,
+                                                 Heap::ScanStackMode::kNone);
+  heap->PreciseCollectAllGarbage(Heap::kForcedGC,
+                                 GarbageCollectionReason::kTesting);
+  heap->PreciseCollectAllGarbage(Heap::kForcedGC,
+                                 GarbageCollectionReason::kTesting);
   own<Module> deserialized = Module::deserialize(store(), serialized);
 
   // Try to serialize the module again. This can fail if deserialization does
diff -r -u --color up/v8/test/wasm-js/tests.tar.gz.sha1 nw/v8/test/wasm-js/tests.tar.gz.sha1
--- up/v8/test/wasm-js/tests.tar.gz.sha1	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/wasm-js/tests.tar.gz.sha1	2023-01-19 16:46:37.663109175 -0500
@@ -1 +1 @@
-1953e12380e62431000575179fed0814a4d58b57
\ No newline at end of file
+597fc3fde2a51bd2bb9be1c87505d5a82ca7e441
\ No newline at end of file
diff -r -u --color up/v8/test/wasm-spec-tests/tests.tar.gz.sha1 nw/v8/test/wasm-spec-tests/tests.tar.gz.sha1
--- up/v8/test/wasm-spec-tests/tests.tar.gz.sha1	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/wasm-spec-tests/tests.tar.gz.sha1	2023-01-19 16:46:37.663109175 -0500
@@ -1 +1 @@
-f53f497d815d85864350b100b16b5f21f20b617f
\ No newline at end of file
+ac7450624874b95cbdf3b5e216a466729663e7a7
\ No newline at end of file
diff -r -u --color up/v8/test/wasm-spec-tests/wasm-spec-tests.status nw/v8/test/wasm-spec-tests/wasm-spec-tests.status
--- up/v8/test/wasm-spec-tests/wasm-spec-tests.status	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/test/wasm-spec-tests/wasm-spec-tests.status	2023-01-19 16:46:37.663109175 -0500
@@ -5,9 +5,6 @@
 [
 [ALWAYS, {
   'skip-stack-guard-page': [PASS, ['((arch == ppc or arch == ppc64 or arch == s390 or arch == s390x) and simulator_run)', SKIP]],
-  # TODO(v8:10994): Failing spec test after update.
-  'proposals/js-types/data': [FAIL],
-
   # Missing rebase in the proposal repository.
   'proposals/js-types/table': [FAIL],
   'proposals/memory64/linking': [FAIL],
@@ -15,9 +12,7 @@
   'proposals/memory64/unreached-invalid': [FAIL],
 
   # TODO(wasm): Roll newest tests into "js-types" repository.
-  'proposals/js-types/elem': [FAIL],
   'proposals/js-types/globals': [FAIL],
-  'proposals/js-types/imports': [FAIL],
   'proposals/js-types/linking': [FAIL],
 
   # TODO(wasm): Roll newest tests into "tail-call" repository.
diff -r -u --color up/v8/third_party/v8/builtins/OWNERS nw/v8/third_party/v8/builtins/OWNERS
--- up/v8/third_party/v8/builtins/OWNERS	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/third_party/v8/builtins/OWNERS	2023-01-19 16:46:37.847275794 -0500
@@ -1,2 +1,3 @@
 jgruber@chromium.org
 szuend@chromium.org
+tebbi@chromium.org
diff -r -u --color up/v8/tools/builtins-pgo/arm.profile nw/v8/tools/builtins-pgo/arm.profile
--- up/v8/tools/builtins-pgo/arm.profile	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/builtins-pgo/arm.profile	2023-01-19 16:46:37.858109124 -0500
@@ -6,7 +6,7 @@
 block_hint,RecordWriteSaveFP,34,35,1
 block_hint,RecordWriteSaveFP,25,26,0
 block_hint,RecordWriteSaveFP,15,16,0
-block_hint,RecordWriteSaveFP,17,18,1
+block_hint,RecordWriteIgnoreFP,21,22,0
 block_hint,RecordWriteIgnoreFP,6,7,1
 block_hint,RecordWriteIgnoreFP,19,20,1
 block_hint,RecordWriteIgnoreFP,9,10,1
@@ -38,6 +38,7 @@
 block_hint,Call_ReceiverIsNotNullOrUndefined_Baseline_Compact,67,68,1
 block_hint,Call_ReceiverIsNotNullOrUndefined_Baseline_Compact,50,51,0
 block_hint,Call_ReceiverIsNotNullOrUndefined_Baseline_Compact,29,30,1
+block_hint,Call_ReceiverIsNotNullOrUndefined_Baseline_Compact,56,57,0
 block_hint,Call_ReceiverIsNotNullOrUndefined_Baseline_Compact,7,8,1
 block_hint,Call_ReceiverIsNotNullOrUndefined_Baseline_Compact,61,62,0
 block_hint,Call_ReceiverIsNotNullOrUndefined_Baseline_Compact,14,15,1
@@ -85,10 +86,21 @@
 block_hint,CallWithSpread_Baseline,102,103,0
 block_hint,CallWithSpread_Baseline,68,69,0
 block_hint,CallWithSpread_Baseline,33,34,1
+block_hint,CallWithSpread_Baseline,70,71,1
 block_hint,CallWithSpread_Baseline,53,54,0
 block_hint,CallWithSpread_Baseline,63,64,1
 block_hint,CallWithSpread_Baseline,65,66,1
 block_hint,CallWithSpread_Baseline,37,38,1
+block_hint,CallWithSpread_Baseline,39,40,1
+block_hint,CallWithSpread_Baseline,41,42,0
+block_hint,CallWithSpread_Baseline,91,92,1
+block_hint,CallWithSpread_Baseline,77,78,1
+block_hint,CallWithSpread_Baseline,23,24,0
+block_hint,CallWithSpread_Baseline,25,26,1
+block_hint,CallWithSpread_Baseline,27,28,0
+block_hint,CallWithSpread_Baseline,79,80,0
+block_hint,CallWithSpread_Baseline,93,94,1
+block_hint,CallWithSpread_Baseline,29,30,1
 block_hint,CallWithArrayLike,28,29,1
 block_hint,CallWithArrayLike,30,31,1
 block_hint,CallWithArrayLike,32,33,1
@@ -229,7 +241,6 @@
 block_hint,ToNumberConvertBigInt,20,21,0
 block_hint,ToNumberConvertBigInt,17,18,1
 block_hint,ToNumberConvertBigInt,9,10,1
-block_hint,Typeof,15,16,0
 block_hint,Typeof,17,18,0
 block_hint,Typeof,9,10,0
 block_hint,Typeof,13,14,1
@@ -248,6 +259,7 @@
 block_hint,KeyedLoadIC_PolymorphicName,335,336,1
 block_hint,KeyedLoadIC_PolymorphicName,110,111,0
 block_hint,KeyedLoadIC_PolymorphicName,175,176,0
+block_hint,KeyedLoadIC_PolymorphicName,112,113,1
 block_hint,KeyedLoadIC_PolymorphicName,45,46,1
 block_hint,KeyedLoadIC_PolymorphicName,74,75,0
 block_hint,KeyedLoadIC_PolymorphicName,253,254,0
@@ -428,7 +440,6 @@
 block_hint,LoadIC_NoFeedback,247,248,1
 block_hint,LoadIC_NoFeedback,59,60,0
 block_hint,LoadIC_NoFeedback,22,23,1
-block_hint,LoadIC_NoFeedback,113,114,0
 block_hint,LoadIC_NoFeedback,35,36,1
 block_hint,LoadIC_NoFeedback,130,131,1
 block_hint,LoadIC_NoFeedback,145,146,0
@@ -618,6 +629,7 @@
 block_hint,StoreFastElementIC_Standard,770,771,0
 block_hint,StoreFastElementIC_Standard,308,309,1
 block_hint,StoreFastElementIC_Standard,580,581,0
+block_hint,StoreFastElementIC_Standard,1097,1098,0
 block_hint,StoreFastElementIC_Standard,929,930,1
 block_hint,StoreFastElementIC_Standard,772,773,0
 block_hint,StoreFastElementIC_Standard,310,311,1
@@ -877,6 +889,7 @@
 block_hint,SetDataProperties,99,100,0
 block_hint,SetDataProperties,437,438,0
 block_hint,SetDataProperties,241,242,0
+block_hint,SetDataProperties,129,130,0
 block_hint,SetDataProperties,279,280,1
 block_hint,SetDataProperties,204,205,0
 block_hint,SetDataProperties,61,62,0
@@ -941,7 +954,7 @@
 block_hint,ArrayIncludesSmiOrObject,125,126,0
 block_hint,ArrayIncludesSmiOrObject,98,99,0
 block_hint,ArrayIncludes,52,53,1
-block_hint,ArrayIncludes,49,50,0
+block_hint,ArrayIncludes,49,50,1
 block_hint,ArrayIncludes,42,43,1
 block_hint,ArrayIncludes,44,45,1
 block_hint,ArrayIncludes,25,26,1
@@ -972,7 +985,7 @@
 block_hint,ArrayIndexOfSmiOrObject,94,95,1
 block_hint,ArrayIndexOfSmiOrObject,86,87,0
 block_hint,ArrayIndexOf,52,53,1
-block_hint,ArrayIndexOf,49,50,0
+block_hint,ArrayIndexOf,49,50,1
 block_hint,ArrayIndexOf,42,43,1
 block_hint,ArrayIndexOf,44,45,1
 block_hint,ArrayIndexOf,25,26,1
@@ -1227,6 +1240,7 @@
 block_hint,LoadIC,141,142,1
 block_hint,LoadIC,361,362,1
 block_hint,LoadIC,102,103,0
+block_hint,LoadIC,104,105,0
 block_hint,LoadIC,21,22,1
 block_hint,LoadIC,64,65,0
 block_hint,LoadIC,143,144,0
@@ -1519,6 +1533,7 @@
 block_hint,StoreIC,210,211,1
 block_hint,StoreIC,395,396,1
 block_hint,StoreIC,386,387,0
+block_hint,StoreIC,369,370,1
 block_hint,StoreIC,240,241,1
 block_hint,StoreIC,242,243,1
 block_hint,StoreIC,74,75,1
@@ -1615,7 +1630,6 @@
 block_hint,DefineNamedOwnIC,93,94,0
 block_hint,DefineNamedOwnIC,17,18,0
 block_hint,DefineNamedOwnIC,350,351,0
-block_hint,DefineNamedOwnIC,282,283,1
 block_hint,DefineNamedOwnIC,157,158,1
 block_hint,DefineNamedOwnIC,159,160,1
 block_hint,DefineNamedOwnIC,254,255,1
@@ -1827,7 +1841,7 @@
 block_hint,FindOrderedHashMapEntry,58,59,1
 block_hint,FindOrderedHashMapEntry,60,61,1
 block_hint,MapConstructor,328,329,1
-block_hint,MapConstructor,248,249,1
+block_hint,MapConstructor,248,249,0
 block_hint,MapConstructor,105,106,0
 block_hint,MapConstructor,13,14,1
 block_hint,MapConstructor,270,271,1
@@ -1887,7 +1901,7 @@
 block_hint,MapPrototypeGetSize,5,6,1
 block_hint,MapPrototypeGetSize,3,4,1
 block_hint,MapPrototypeForEach,33,34,1
-block_hint,MapPrototypeForEach,30,31,0
+block_hint,MapPrototypeForEach,30,31,1
 block_hint,MapPrototypeForEach,27,28,1
 block_hint,MapPrototypeForEach,20,21,1
 block_hint,MapPrototypeForEach,22,23,1
@@ -1987,19 +2001,19 @@
 block_hint,DivideSmi_Baseline,40,41,1
 block_hint,DivideSmi_Baseline,25,26,1
 block_hint,DivideSmi_Baseline,12,13,1
-block_hint,Modulus_Baseline,61,62,0
-block_hint,Modulus_Baseline,57,58,0
-block_hint,Modulus_Baseline,43,44,1
-block_hint,Modulus_Baseline,38,39,1
-block_hint,Modulus_Baseline,17,18,0
-block_hint,Modulus_Baseline,6,7,1
-block_hint,ModulusSmi_Baseline,43,44,1
-block_hint,ModulusSmi_Baseline,38,39,1
-block_hint,ModulusSmi_Baseline,17,18,0
-block_hint,ModulusSmi_Baseline,6,7,1
-block_hint,ModulusSmi_Baseline,32,33,1
-block_hint,ModulusSmi_Baseline,19,20,1
-block_hint,ModulusSmi_Baseline,8,9,1
+block_hint,Modulus_Baseline,68,69,0
+block_hint,Modulus_Baseline,64,65,0
+block_hint,Modulus_Baseline,50,51,1
+block_hint,Modulus_Baseline,45,46,1
+block_hint,Modulus_Baseline,22,23,0
+block_hint,Modulus_Baseline,10,11,1
+block_hint,ModulusSmi_Baseline,50,51,1
+block_hint,ModulusSmi_Baseline,45,46,1
+block_hint,ModulusSmi_Baseline,22,23,0
+block_hint,ModulusSmi_Baseline,10,11,1
+block_hint,ModulusSmi_Baseline,37,38,1
+block_hint,ModulusSmi_Baseline,24,25,1
+block_hint,ModulusSmi_Baseline,12,13,1
 block_hint,BitwiseAnd_Baseline,35,36,0
 block_hint,BitwiseAnd_Baseline,23,24,1
 block_hint,BitwiseAnd_Baseline,8,9,0
@@ -2057,8 +2071,6 @@
 block_hint,ShiftRightSmi_Baseline,9,10,1
 block_hint,ShiftRightLogical_Baseline,25,26,1
 block_hint,ShiftRightLogical_Baseline,10,11,0
-block_hint,ShiftRightLogical_Baseline,46,47,0
-block_hint,ShiftRightLogical_Baseline,29,30,0
 block_hint,ShiftRightLogical_Baseline,14,15,1
 block_hint,ShiftRightLogicalSmi_Baseline,35,36,1
 block_hint,ShiftRightLogicalSmi_Baseline,25,26,1
@@ -2077,12 +2089,12 @@
 block_hint,Subtract_WithFeedback,54,55,0
 block_hint,Subtract_WithFeedback,42,43,0
 block_hint,Subtract_WithFeedback,17,18,1
-block_hint,Modulus_WithFeedback,61,62,0
-block_hint,Modulus_WithFeedback,57,58,0
-block_hint,Modulus_WithFeedback,43,44,1
-block_hint,Modulus_WithFeedback,38,39,1
-block_hint,Modulus_WithFeedback,17,18,0
-block_hint,Modulus_WithFeedback,6,7,1
+block_hint,Modulus_WithFeedback,68,69,0
+block_hint,Modulus_WithFeedback,64,65,0
+block_hint,Modulus_WithFeedback,50,51,1
+block_hint,Modulus_WithFeedback,45,46,1
+block_hint,Modulus_WithFeedback,22,23,0
+block_hint,Modulus_WithFeedback,10,11,1
 block_hint,BitwiseOr_WithFeedback,6,7,1
 block_hint,BitwiseOr_WithFeedback,35,36,0
 block_hint,BitwiseOr_WithFeedback,23,24,0
@@ -2092,6 +2104,7 @@
 block_hint,BitwiseOr_WithFeedback,14,15,1
 block_hint,Equal_Baseline,48,49,0
 block_hint,Equal_Baseline,18,19,1
+block_hint,Equal_Baseline,22,23,1
 block_hint,Equal_Baseline,101,102,0
 block_hint,Equal_Baseline,14,15,1
 block_hint,Equal_Baseline,39,40,0
@@ -2139,6 +2152,7 @@
 block_hint,GreaterThan_Baseline,44,45,0
 block_hint,GreaterThan_Baseline,10,11,0
 block_hint,GreaterThan_Baseline,48,49,1
+block_hint,GreaterThan_Baseline,56,57,0
 block_hint,GreaterThan_Baseline,12,13,0
 block_hint,GreaterThan_Baseline,5,6,1
 block_hint,LessThanOrEqual_Baseline,44,45,0
@@ -2160,7 +2174,6 @@
 block_hint,Equal_WithFeedback,37,38,0
 block_hint,Equal_WithFeedback,48,49,0
 block_hint,Equal_WithFeedback,18,19,1
-block_hint,Equal_WithFeedback,8,9,0
 block_hint,Equal_WithFeedback,95,96,0
 block_hint,Equal_WithFeedback,101,102,0
 block_hint,Equal_WithFeedback,20,21,0
@@ -2252,13 +2265,13 @@
 block_hint,Negate_Baseline,18,19,1
 block_hint,Negate_Baseline,5,6,1
 block_hint,ObjectAssign,21,22,1
-block_hint,ObjectAssign,18,19,0
+block_hint,ObjectAssign,18,19,1
 block_hint,ObjectAssign,15,16,1
 block_hint,ObjectAssign,12,13,1
 block_hint,ObjectAssign,9,10,0
 block_hint,ObjectAssign,5,6,0
 block_hint,ObjectCreate,78,79,1
-block_hint,ObjectCreate,75,76,0
+block_hint,ObjectCreate,75,76,1
 block_hint,ObjectCreate,33,34,1
 block_hint,ObjectCreate,35,36,1
 block_hint,ObjectCreate,37,38,1
@@ -2282,7 +2295,7 @@
 block_hint,ObjectCreate,20,21,0
 block_hint,ObjectCreate,61,62,1
 block_hint,ObjectGetOwnPropertyDescriptor,517,518,1
-block_hint,ObjectGetOwnPropertyDescriptor,514,515,0
+block_hint,ObjectGetOwnPropertyDescriptor,514,515,1
 block_hint,ObjectGetOwnPropertyDescriptor,511,512,0
 block_hint,ObjectGetOwnPropertyDescriptor,503,504,1
 block_hint,ObjectGetOwnPropertyDescriptor,490,491,1
@@ -2355,14 +2368,14 @@
 block_hint,ObjectPrototypeHasOwnProperty,171,172,0
 block_hint,ObjectPrototypeHasOwnProperty,178,179,1
 block_hint,ObjectPrototypeHasOwnProperty,58,59,0
-block_hint,ObjectToString,42,43,0
-block_hint,ObjectToString,57,58,0
-block_hint,ObjectToString,65,66,0
-block_hint,ObjectToString,52,53,0
+block_hint,ObjectToString,45,46,0
+block_hint,ObjectToString,60,61,0
+block_hint,ObjectToString,68,69,0
+block_hint,ObjectToString,55,56,0
 block_hint,ObjectToString,7,8,1
 block_hint,ObjectToString,5,6,1
 block_hint,ObjectToString,11,12,1
-block_hint,ObjectToString,19,20,0
+block_hint,ObjectToString,20,21,0
 block_hint,InstanceOf_WithFeedback,50,51,1
 block_hint,InstanceOf_WithFeedback,52,53,0
 block_hint,InstanceOf_WithFeedback,54,55,1
@@ -2591,7 +2604,7 @@
 block_hint,StringPrototypeReplace,18,19,0
 block_hint,StringPrototypeReplace,26,27,1
 block_hint,StringPrototypeSplit,125,126,1
-block_hint,StringPrototypeSplit,112,113,0
+block_hint,StringPrototypeSplit,112,113,1
 block_hint,StringPrototypeSplit,92,93,1
 block_hint,StringPrototypeSplit,35,36,0
 block_hint,StringPrototypeSplit,114,115,1
@@ -2626,7 +2639,7 @@
 block_hint,StringPrototypeSplit,20,21,1
 block_hint,StringPrototypeSplit,50,51,1
 block_hint,TypedArrayConstructor,14,15,1
-block_hint,TypedArrayConstructor,11,12,0
+block_hint,TypedArrayConstructor,11,12,1
 block_hint,TypedArrayConstructor,2,3,0
 block_hint,TypedArrayPrototypeByteLength,69,70,1
 block_hint,TypedArrayPrototypeByteLength,43,44,1
@@ -2643,7 +2656,7 @@
 block_hint,TypedArrayPrototypeLength,28,29,0
 block_hint,TypedArrayPrototypeLength,19,20,0
 block_hint,WeakMapConstructor,351,352,1
-block_hint,WeakMapConstructor,271,272,1
+block_hint,WeakMapConstructor,271,272,0
 block_hint,WeakMapConstructor,119,120,0
 block_hint,WeakMapConstructor,14,15,1
 block_hint,WeakMapConstructor,293,294,1
@@ -2708,7 +2721,7 @@
 block_hint,AsyncGeneratorResumeNext,18,19,0
 block_hint,AsyncGeneratorResumeNext,14,15,0
 block_hint,AsyncGeneratorPrototypeNext,27,28,1
-block_hint,AsyncGeneratorPrototypeNext,16,17,1
+block_hint,AsyncGeneratorPrototypeNext,16,17,0
 block_hint,AsyncGeneratorPrototypeNext,4,5,1
 block_hint,AsyncGeneratorPrototypeNext,34,35,1
 block_hint,AsyncGeneratorPrototypeNext,29,30,0
@@ -2884,6 +2897,12 @@
 block_hint,CreateDataProperty,55,56,1
 block_hint,CreateDataProperty,543,544,1
 block_hint,CreateDataProperty,57,58,1
+block_hint,FindNonDefaultConstructorOrConstruct,12,13,0
+block_hint,FindNonDefaultConstructorOrConstruct,6,7,0
+block_hint,FindNonDefaultConstructorOrConstruct,14,15,1
+block_hint,FindNonDefaultConstructorOrConstruct,16,17,0
+block_hint,FindNonDefaultConstructorOrConstruct,4,5,1
+block_hint,FindNonDefaultConstructorOrConstruct,18,19,0
 block_hint,ArrayPrototypeConcat,79,80,1
 block_hint,ArrayPrototypeConcat,54,55,1
 block_hint,ArrayPrototypeConcat,63,64,1
@@ -2906,18 +2925,17 @@
 block_hint,ArrayPrototypeConcat,59,60,0
 block_hint,ArrayPrototypeConcat,66,67,0
 block_hint,ArrayPrototypeConcat,33,34,1
-block_hint,ArrayPrototypeConcat,68,69,0
+block_hint,ArrayPrototypeConcat,68,69,1
 block_hint,ArrayPrototypeConcat,35,36,1
 block_hint,ArrayPrototypeConcat,27,28,1
 block_hint,ArrayPrototypeConcat,11,12,1
 block_hint,ArrayEvery,73,74,1
 block_hint,ArrayEvery,31,32,0
-block_hint,ArrayEvery,125,126,1
-block_hint,ArrayEvery,117,118,1
+block_hint,ArrayEvery,122,123,1
+block_hint,ArrayEvery,116,117,1
 block_hint,ArrayEvery,91,92,1
 block_hint,ArrayEvery,93,94,1
 block_hint,ArrayEvery,99,100,1
-block_hint,ArrayEvery,121,122,0
 block_hint,ArrayEvery,105,106,1
 block_hint,ArrayEvery,107,108,1
 block_hint,ArrayEvery,97,98,1
@@ -2942,12 +2960,11 @@
 block_hint,ArrayEvery,79,80,0
 block_hint,ArrayFilter,194,195,1
 block_hint,ArrayFilter,84,85,0
-block_hint,ArrayFilter,304,305,1
-block_hint,ArrayFilter,293,294,1
+block_hint,ArrayFilter,301,302,1
+block_hint,ArrayFilter,292,293,1
 block_hint,ArrayFilter,228,229,1
 block_hint,ArrayFilter,230,231,1
 block_hint,ArrayFilter,249,250,1
-block_hint,ArrayFilter,302,303,0
 block_hint,ArrayFilter,274,275,1
 block_hint,ArrayFilter,276,277,1
 block_hint,ArrayFilter,242,243,0
@@ -3000,12 +3017,11 @@
 block_hint,ArrayFilter,50,51,1
 block_hint,ArrayForEach,70,71,1
 block_hint,ArrayForEach,29,30,0
-block_hint,ArrayForEach,102,103,1
-block_hint,ArrayForEach,96,97,1
+block_hint,ArrayForEach,99,100,1
+block_hint,ArrayForEach,95,96,1
 block_hint,ArrayForEach,76,77,1
 block_hint,ArrayForEach,78,79,1
 block_hint,ArrayForEach,84,85,1
-block_hint,ArrayForEach,100,101,0
 block_hint,ArrayForEach,90,91,1
 block_hint,ArrayForEach,92,93,1
 block_hint,ArrayForEach,47,48,0
@@ -3027,7 +3043,7 @@
 block_hint,ArrayFrom,78,79,1
 block_hint,ArrayFrom,8,9,1
 block_hint,ArrayFrom,342,343,1
-block_hint,ArrayFrom,338,339,0
+block_hint,ArrayFrom,338,339,1
 block_hint,ArrayFrom,327,328,0
 block_hint,ArrayFrom,311,312,1
 block_hint,ArrayFrom,309,310,0
@@ -3168,7 +3184,7 @@
 block_hint,ArrayPrototypeLastIndexOf,313,314,0
 block_hint,ArrayPrototypeLastIndexOf,298,299,0
 block_hint,ArrayPrototypeLastIndexOf,281,282,1
-block_hint,ArrayPrototypeLastIndexOf,252,253,0
+block_hint,ArrayPrototypeLastIndexOf,252,253,1
 block_hint,ArrayPrototypeLastIndexOf,194,195,1
 block_hint,ArrayPrototypeLastIndexOf,83,84,1
 block_hint,ArrayPrototypeLastIndexOf,73,74,1
@@ -3190,15 +3206,14 @@
 block_hint,ArrayPrototypeLastIndexOf,31,32,0
 block_hint,ArrayMap,163,164,1
 block_hint,ArrayMap,72,73,0
-block_hint,ArrayMap,270,271,1
-block_hint,ArrayMap,249,250,1
+block_hint,ArrayMap,267,268,1
+block_hint,ArrayMap,248,249,1
 block_hint,ArrayMap,194,195,1
 block_hint,ArrayMap,196,197,1
 block_hint,ArrayMap,215,216,1
-block_hint,ArrayMap,268,269,0
 block_hint,ArrayMap,229,230,1
 block_hint,ArrayMap,231,232,1
-block_hint,ArrayMap,258,259,1
+block_hint,ArrayMap,257,258,1
 block_hint,ArrayMap,212,213,0
 block_hint,ArrayMap,226,227,1
 block_hint,ArrayMap,233,234,1
@@ -3212,9 +3227,9 @@
 block_hint,ArrayMap,180,181,1
 block_hint,ArrayMap,159,160,1
 block_hint,ArrayMap,55,56,0
-block_hint,ArrayMap,283,284,1
-block_hint,ArrayMap,280,281,0
-block_hint,ArrayMap,261,262,0
+block_hint,ArrayMap,280,281,1
+block_hint,ArrayMap,277,278,0
+block_hint,ArrayMap,260,261,0
 block_hint,ArrayMap,235,236,0
 block_hint,ArrayMap,201,202,0
 block_hint,ArrayMap,116,117,0
@@ -3225,7 +3240,7 @@
 block_hint,ArrayMap,120,121,0
 block_hint,ArrayMap,37,38,1
 block_hint,ArrayMap,35,36,1
-block_hint,ArrayMap,266,267,0
+block_hint,ArrayMap,265,266,0
 block_hint,ArrayMap,209,210,0
 block_hint,ArrayMap,151,152,0
 block_hint,ArrayMap,45,46,1
@@ -3245,12 +3260,11 @@
 block_hint,ArrayMap,136,137,1
 block_hint,ArrayReduce,81,82,1
 block_hint,ArrayReduce,30,31,0
-block_hint,ArrayReduce,127,128,1
-block_hint,ArrayReduce,121,122,1
+block_hint,ArrayReduce,124,125,1
+block_hint,ArrayReduce,120,121,1
 block_hint,ArrayReduce,89,90,1
 block_hint,ArrayReduce,91,92,1
 block_hint,ArrayReduce,101,102,1
-block_hint,ArrayReduce,125,126,0
 block_hint,ArrayReduce,111,112,1
 block_hint,ArrayReduce,113,114,1
 block_hint,ArrayReduce,95,96,1
@@ -3402,12 +3416,11 @@
 block_hint,ArrayPrototypeSlice,73,74,0
 block_hint,ArraySome,88,89,1
 block_hint,ArraySome,31,32,0
-block_hint,ArraySome,122,123,1
-block_hint,ArraySome,116,117,1
+block_hint,ArraySome,119,120,1
+block_hint,ArraySome,115,116,1
 block_hint,ArraySome,93,94,1
 block_hint,ArraySome,95,96,1
 block_hint,ArraySome,101,102,1
-block_hint,ArraySome,120,121,0
 block_hint,ArraySome,108,109,1
 block_hint,ArraySome,110,111,1
 block_hint,ArraySome,99,100,1
@@ -3428,7 +3441,7 @@
 block_hint,ArrayPrototypeSplice,605,606,1
 block_hint,ArrayPrototypeSplice,450,451,1
 block_hint,ArrayPrototypeSplice,452,453,1
-block_hint,ArrayPrototypeSplice,1201,1202,0
+block_hint,ArrayPrototypeSplice,1201,1202,1
 block_hint,ArrayPrototypeSplice,1183,1184,0
 block_hint,ArrayPrototypeSplice,1159,1160,0
 block_hint,ArrayPrototypeSplice,1138,1139,0
@@ -3531,7 +3544,7 @@
 block_hint,ToInteger,4,5,1
 block_hint,ToInteger,6,7,0
 block_hint,BooleanConstructor,81,82,1
-block_hint,BooleanConstructor,74,75,0
+block_hint,BooleanConstructor,74,75,1
 block_hint,BooleanConstructor,57,58,0
 block_hint,BooleanConstructor,68,69,1
 block_hint,BooleanConstructor,59,60,0
@@ -3584,16 +3597,14 @@
 block_hint,StringPrototypeCodePointAt,72,73,0
 block_hint,StringPrototypeCodePointAt,48,49,0
 block_hint,StringPrototypeCodePointAt,18,19,1
-block_hint,StringConstructor,65,66,1
+block_hint,StringConstructor,64,65,1
 block_hint,StringConstructor,49,50,1
-block_hint,StringConstructor,63,64,0
 block_hint,StringConstructor,36,37,0
-block_hint,StringConstructor,78,79,0
-block_hint,StringConstructor,83,84,1
-block_hint,StringConstructor,81,82,1
-block_hint,StringConstructor,75,76,1
-block_hint,StringConstructor,59,60,0
-block_hint,StringConstructor,61,62,1
+block_hint,StringConstructor,78,79,1
+block_hint,StringConstructor,76,77,1
+block_hint,StringConstructor,73,74,1
+block_hint,StringConstructor,60,61,0
+block_hint,StringConstructor,62,63,1
 block_hint,StringConstructor,45,46,0
 block_hint,StringConstructor,24,25,0
 block_hint,StringConstructor,26,27,1
@@ -3652,7 +3663,7 @@
 block_hint,CreateShallowObjectLiteral,93,94,1
 block_hint,ObjectConstructor,27,28,1
 block_hint,ObjectConstructor,19,20,1
-block_hint,ObjectConstructor,29,30,0
+block_hint,ObjectConstructor,29,30,1
 block_hint,ObjectConstructor,23,24,0
 block_hint,ObjectConstructor,17,18,0
 block_hint,ObjectConstructor,11,12,0
@@ -3664,7 +3675,7 @@
 block_hint,CreateEmptyLiteralObject,6,7,0
 block_hint,NumberConstructor,18,19,1
 block_hint,NumberConstructor,6,7,1
-block_hint,NumberConstructor,28,29,0
+block_hint,NumberConstructor,28,29,1
 block_hint,NumberConstructor,12,13,0
 block_hint,NumberConstructor,34,35,0
 block_hint,NumberConstructor,32,33,1
@@ -3738,7 +3749,7 @@
 block_hint,DataViewPrototypeGetByteLength,12,13,0
 block_hint,DataViewPrototypeGetByteLength,10,11,0
 block_hint,DataViewPrototypeGetFloat64,101,102,1
-block_hint,DataViewPrototypeGetFloat64,87,88,0
+block_hint,DataViewPrototypeGetFloat64,87,88,1
 block_hint,DataViewPrototypeGetFloat64,56,57,0
 block_hint,DataViewPrototypeGetFloat64,17,18,1
 block_hint,DataViewPrototypeGetFloat64,19,20,1
@@ -3762,7 +3773,7 @@
 block_hint,DataViewPrototypeGetFloat64,54,55,0
 block_hint,DataViewPrototypeGetFloat64,14,15,1
 block_hint,DataViewPrototypeSetFloat64,116,117,1
-block_hint,DataViewPrototypeSetFloat64,104,105,0
+block_hint,DataViewPrototypeSetFloat64,104,105,1
 block_hint,DataViewPrototypeSetFloat64,82,83,0
 block_hint,DataViewPrototypeSetFloat64,49,50,0
 block_hint,DataViewPrototypeSetFloat64,16,17,1
@@ -3811,7 +3822,7 @@
 block_hint,FastFunctionPrototypeBind,11,12,1
 block_hint,FastFunctionPrototypeBind,35,36,1
 block_hint,FastFunctionPrototypeBind,81,82,1
-block_hint,FastFunctionPrototypeBind,73,74,0
+block_hint,FastFunctionPrototypeBind,73,74,1
 block_hint,FastFunctionPrototypeBind,27,28,1
 block_hint,ForInNext,2,3,1
 block_hint,ForInNext,7,8,1
@@ -3963,13 +3974,9 @@
 block_hint,NumberParseInt,3,4,1
 block_hint,Add,66,67,1
 block_hint,Add,24,25,0
-block_hint,Add,52,53,1
 block_hint,Add,68,69,0
 block_hint,Add,35,36,0
 block_hint,Add,40,41,0
-block_hint,Add,70,71,1
-block_hint,Add,26,27,0
-block_hint,Add,29,30,1
 block_hint,Subtract,24,25,0
 block_hint,Subtract,9,10,0
 block_hint,Subtract,22,23,0
@@ -4086,7 +4093,7 @@
 block_hint,ProxyGetProperty,171,172,0
 block_hint,ProxyGetProperty,60,61,0
 block_hint,ReflectGet,20,21,1
-block_hint,ReflectGet,15,16,0
+block_hint,ReflectGet,15,16,1
 block_hint,ReflectGet,5,6,1
 block_hint,ReflectGet,7,8,0
 block_hint,ReflectGet,18,19,0
@@ -4367,7 +4374,7 @@
 block_hint,RegExpPrototypeTestFast,26,27,0
 block_hint,RegExpPrototypeTestFast,42,43,0
 block_hint,StringPrototypeEndsWith,288,289,1
-block_hint,StringPrototypeEndsWith,271,272,0
+block_hint,StringPrototypeEndsWith,271,272,1
 block_hint,StringPrototypeEndsWith,251,252,1
 block_hint,StringPrototypeEndsWith,235,236,1
 block_hint,StringPrototypeEndsWith,174,175,1
@@ -4389,7 +4396,7 @@
 block_hint,StringPrototypeEndsWith,49,50,0
 block_hint,StringPrototypeEndsWith,116,117,0
 block_hint,StringPrototypeIndexOf,39,40,1
-block_hint,StringPrototypeIndexOf,36,37,0
+block_hint,StringPrototypeIndexOf,36,37,1
 block_hint,StringPrototypeIndexOf,19,20,1
 block_hint,StringPrototypeIndexOf,8,9,1
 block_hint,StringPrototypeIndexOf,28,29,1
@@ -4444,7 +4451,7 @@
 block_hint,StringPrototypeSlice,167,168,1
 block_hint,StringPrototypeSlice,136,137,1
 block_hint,StringPrototypeSlice,103,104,1
-block_hint,StringPrototypeSlice,189,190,0
+block_hint,StringPrototypeSlice,189,190,1
 block_hint,StringPrototypeSlice,175,176,0
 block_hint,StringPrototypeSlice,199,200,0
 block_hint,StringPrototypeSlice,196,197,0
@@ -4472,7 +4479,7 @@
 block_hint,StringPrototypeSlice,36,37,1
 block_hint,StringPrototypeSlice,33,34,0
 block_hint,StringPrototypeStartsWith,288,289,1
-block_hint,StringPrototypeStartsWith,271,272,0
+block_hint,StringPrototypeStartsWith,271,272,1
 block_hint,StringPrototypeStartsWith,251,252,1
 block_hint,StringPrototypeStartsWith,235,236,1
 block_hint,StringPrototypeStartsWith,174,175,1
@@ -4495,7 +4502,7 @@
 block_hint,StringPrototypeSubstr,163,164,1
 block_hint,StringPrototypeSubstr,141,142,1
 block_hint,StringPrototypeSubstr,103,104,1
-block_hint,StringPrototypeSubstr,182,183,0
+block_hint,StringPrototypeSubstr,182,183,1
 block_hint,StringPrototypeSubstr,171,172,0
 block_hint,StringPrototypeSubstr,192,193,0
 block_hint,StringPrototypeSubstr,189,190,0
@@ -4515,7 +4522,7 @@
 block_hint,StringPrototypeSubstring,147,148,1
 block_hint,StringPrototypeSubstring,127,128,1
 block_hint,StringPrototypeSubstring,99,100,1
-block_hint,StringPrototypeSubstring,182,183,0
+block_hint,StringPrototypeSubstring,182,183,1
 block_hint,StringPrototypeSubstring,169,170,0
 block_hint,StringPrototypeSubstring,186,187,0
 block_hint,StringPrototypeSubstring,180,181,0
@@ -4691,34 +4698,39 @@
 block_hint,CreateTypedArray,188,189,0
 block_hint,CreateTypedArray,451,452,0
 block_hint,CreateTypedArray,273,274,0
-block_hint,TypedArrayFrom,168,169,1
-block_hint,TypedArrayFrom,151,152,0
-block_hint,TypedArrayFrom,131,132,1
-block_hint,TypedArrayFrom,100,101,1
-block_hint,TypedArrayFrom,58,59,1
-block_hint,TypedArrayFrom,60,61,1
-block_hint,TypedArrayFrom,124,125,1
-block_hint,TypedArrayFrom,115,116,0
-block_hint,TypedArrayFrom,92,93,0
-block_hint,TypedArrayFrom,71,72,1
-block_hint,TypedArrayFrom,73,74,1
-block_hint,TypedArrayFrom,175,176,1
+block_hint,TypedArrayFrom,246,247,1
+block_hint,TypedArrayFrom,225,226,1
+block_hint,TypedArrayFrom,202,203,1
+block_hint,TypedArrayFrom,159,160,1
+block_hint,TypedArrayFrom,89,90,1
+block_hint,TypedArrayFrom,91,92,1
+block_hint,TypedArrayFrom,191,192,1
+block_hint,TypedArrayFrom,182,183,0
+block_hint,TypedArrayFrom,143,144,0
+block_hint,TypedArrayFrom,102,103,1
+block_hint,TypedArrayFrom,104,105,1
+block_hint,TypedArrayFrom,260,261,1
+block_hint,TypedArrayFrom,262,263,0
+block_hint,TypedArrayFrom,248,249,0
+block_hint,TypedArrayFrom,235,236,1
+block_hint,TypedArrayFrom,237,238,0
+block_hint,TypedArrayFrom,215,216,1
+block_hint,TypedArrayFrom,193,194,1
+block_hint,TypedArrayFrom,169,170,0
+block_hint,TypedArrayFrom,171,172,0
+block_hint,TypedArrayFrom,256,257,0
+block_hint,TypedArrayFrom,230,231,1
+block_hint,TypedArrayFrom,185,186,0
+block_hint,TypedArrayFrom,108,109,1
+block_hint,TypedArrayFrom,110,111,1
 block_hint,TypedArrayFrom,177,178,0
-block_hint,TypedArrayFrom,179,180,0
-block_hint,TypedArrayFrom,186,187,1
-block_hint,TypedArrayFrom,181,182,0
-block_hint,TypedArrayFrom,183,184,1
-block_hint,TypedArrayFrom,173,174,1
-block_hint,TypedArrayFrom,165,166,0
-block_hint,TypedArrayFrom,156,157,1
-block_hint,TypedArrayFrom,118,119,0
-block_hint,TypedArrayFrom,75,76,1
-block_hint,TypedArrayFrom,77,78,1
-block_hint,TypedArrayFrom,26,27,0
-block_hint,TypedArrayFrom,96,97,0
-block_hint,TypedArrayFrom,28,29,0
-block_hint,TypedArrayFrom,86,87,1
-block_hint,TypedArrayFrom,30,31,1
+block_hint,TypedArrayFrom,149,150,0
+block_hint,TypedArrayFrom,120,121,0
+block_hint,TypedArrayFrom,57,58,0
+block_hint,TypedArrayFrom,155,156,0
+block_hint,TypedArrayFrom,59,60,0
+block_hint,TypedArrayFrom,137,138,1
+block_hint,TypedArrayFrom,61,62,1
 block_hint,TypedArrayPrototypeSet,196,197,1
 block_hint,TypedArrayPrototypeSet,104,105,1
 block_hint,TypedArrayPrototypeSet,106,107,1
@@ -4731,7 +4743,7 @@
 block_hint,TypedArrayPrototypeSet,198,199,0
 block_hint,TypedArrayPrototypeSet,200,201,0
 block_hint,TypedArrayPrototypeSet,167,168,0
-block_hint,TypedArrayPrototypeSet,278,279,0
+block_hint,TypedArrayPrototypeSet,278,279,1
 block_hint,TypedArrayPrototypeSet,265,266,1
 block_hint,TypedArrayPrototypeSet,244,245,1
 block_hint,TypedArrayPrototypeSet,211,212,0
@@ -4756,7 +4768,7 @@
 block_hint,TypedArrayPrototypeSubArray,131,132,0
 block_hint,TypedArrayPrototypeSubArray,133,134,0
 block_hint,TypedArrayPrototypeSubArray,210,211,0
-block_hint,TypedArrayPrototypeSubArray,190,191,0
+block_hint,TypedArrayPrototypeSubArray,190,191,1
 block_hint,TypedArrayPrototypeSubArray,170,171,0
 block_hint,TypedArrayPrototypeSubArray,218,219,0
 block_hint,TypedArrayPrototypeSubArray,205,206,0
@@ -5056,7 +5068,7 @@
 block_hint,ArrayTimSort,113,114,0
 block_hint,ArrayTimSort,89,90,0
 block_hint,ArrayPrototypeSort,106,107,1
-block_hint,ArrayPrototypeSort,80,81,0
+block_hint,ArrayPrototypeSort,80,81,1
 block_hint,ArrayPrototypeSort,39,40,1
 block_hint,ArrayPrototypeSort,70,71,0
 block_hint,ArrayPrototypeSort,41,42,1
@@ -5200,14 +5212,14 @@
 block_hint,DivHandler,70,71,1
 block_hint,DivHandler,46,47,1
 block_hint,DivHandler,17,18,1
-block_hint,ModHandler,77,78,1
-block_hint,ModHandler,74,75,0
-block_hint,ModHandler,70,71,0
-block_hint,ModHandler,56,57,1
-block_hint,ModHandler,51,52,1
-block_hint,ModHandler,28,29,0
-block_hint,ModHandler,8,9,0
-block_hint,ModHandler,15,16,1
+block_hint,ModHandler,87,88,1
+block_hint,ModHandler,84,85,0
+block_hint,ModHandler,80,81,0
+block_hint,ModHandler,66,67,1
+block_hint,ModHandler,61,62,1
+block_hint,ModHandler,34,35,0
+block_hint,ModHandler,15,16,0
+block_hint,ModHandler,23,24,1
 block_hint,BitwiseOrHandler,42,43,0
 block_hint,BitwiseOrHandler,30,31,1
 block_hint,BitwiseOrHandler,8,9,1
@@ -5238,7 +5250,6 @@
 block_hint,AddSmiHandler,28,29,1
 block_hint,SubSmiHandler,35,36,0
 block_hint,SubSmiHandler,23,24,1
-block_hint,SubSmiHandler,19,20,1
 block_hint,MulSmiHandler,78,79,0
 block_hint,MulSmiHandler,63,64,0
 block_hint,MulSmiHandler,65,66,0
@@ -5250,10 +5261,10 @@
 block_hint,DivSmiHandler,43,44,1
 block_hint,DivSmiHandler,15,16,0
 block_hint,DivSmiHandler,23,24,1
-block_hint,ModSmiHandler,56,57,1
-block_hint,ModSmiHandler,51,52,1
-block_hint,ModSmiHandler,28,29,0
-block_hint,ModSmiHandler,15,16,1
+block_hint,ModSmiHandler,66,67,1
+block_hint,ModSmiHandler,61,62,1
+block_hint,ModSmiHandler,34,35,0
+block_hint,ModSmiHandler,23,24,1
 block_hint,BitwiseOrSmiHandler,31,32,1
 block_hint,BitwiseOrSmiHandler,37,38,1
 block_hint,BitwiseAndSmiHandler,6,7,0
@@ -5284,6 +5295,12 @@
 block_hint,ToBooleanLogicalNotHandler,21,22,0
 block_hint,ToBooleanLogicalNotHandler,7,8,0
 block_hint,TypeOfHandler,20,21,0
+block_hint,FindNonDefaultConstructorOrConstructHandler,12,13,0
+block_hint,FindNonDefaultConstructorOrConstructHandler,6,7,0
+block_hint,FindNonDefaultConstructorOrConstructHandler,14,15,1
+block_hint,FindNonDefaultConstructorOrConstructHandler,16,17,0
+block_hint,FindNonDefaultConstructorOrConstructHandler,4,5,1
+block_hint,FindNonDefaultConstructorOrConstructHandler,18,19,0
 block_hint,CallAnyReceiverHandler,21,22,1
 block_hint,CallProperty0Handler,7,8,1
 block_hint,CallProperty0Handler,62,63,0
@@ -5350,12 +5367,6 @@
 block_hint,CallUndefinedReceiver2Handler,70,71,1
 block_hint,CallUndefinedReceiver2Handler,51,52,0
 block_hint,CallUndefinedReceiver2Handler,29,30,1
-block_hint,CallUndefinedReceiver2Handler,7,8,1
-block_hint,CallUndefinedReceiver2Handler,62,63,0
-block_hint,CallUndefinedReceiver2Handler,14,15,1
-block_hint,CallUndefinedReceiver2Handler,16,17,0
-block_hint,CallUndefinedReceiver2Handler,72,73,0
-block_hint,CallUndefinedReceiver2Handler,55,56,1
 block_hint,CallWithSpreadHandler,23,24,1
 block_hint,ConstructHandler,52,53,0
 block_hint,ConstructHandler,41,42,1
@@ -5540,13 +5551,13 @@
 block_hint,MulSmiWideHandler,36,37,0
 block_hint,MulSmiWideHandler,42,43,1
 block_hint,MulSmiWideHandler,17,18,1
-block_hint,ModSmiWideHandler,67,68,1
-block_hint,ModSmiWideHandler,60,61,0
-block_hint,ModSmiWideHandler,56,57,1
-block_hint,ModSmiWideHandler,51,52,1
-block_hint,ModSmiWideHandler,28,29,0
-block_hint,ModSmiWideHandler,8,9,0
-block_hint,ModSmiWideHandler,15,16,1
+block_hint,ModSmiWideHandler,77,78,1
+block_hint,ModSmiWideHandler,70,71,0
+block_hint,ModSmiWideHandler,66,67,1
+block_hint,ModSmiWideHandler,61,62,1
+block_hint,ModSmiWideHandler,34,35,0
+block_hint,ModSmiWideHandler,15,16,0
+block_hint,ModSmiWideHandler,23,24,1
 block_hint,BitwiseOrSmiWideHandler,23,24,0
 block_hint,BitwiseOrSmiWideHandler,6,7,0
 block_hint,BitwiseOrSmiWideHandler,11,12,1
@@ -5640,720 +5651,721 @@
 block_hint,BitwiseAndSmiExtraWideHandler,18,19,1
 block_hint,CallUndefinedReceiver1ExtraWideHandler,68,69,0
 block_hint,CallUndefinedReceiver1ExtraWideHandler,19,20,0
-builtin_hash,RecordWriteSaveFP,626390513
-builtin_hash,RecordWriteIgnoreFP,626390513
-builtin_hash,EphemeronKeyBarrierSaveFP,-719755886
-builtin_hash,AdaptorWithBuiltinExitFrame,-506288945
-builtin_hash,Call_ReceiverIsNullOrUndefined_Baseline_Compact,-634465027
-builtin_hash,Call_ReceiverIsNotNullOrUndefined_Baseline_Compact,-585445450
-builtin_hash,Call_ReceiverIsAny_Baseline_Compact,-585445450
-builtin_hash,CallProxy,690784589
-builtin_hash,CallWithSpread,-1022926733
-builtin_hash,CallWithSpread_Baseline,-849069302
-builtin_hash,CallWithArrayLike,459945534
-builtin_hash,ConstructWithSpread,872318319
-builtin_hash,ConstructWithSpread_Baseline,-872100778
-builtin_hash,Construct_Baseline,-106849933
-builtin_hash,FastNewObject,-1018095454
-builtin_hash,FastNewClosure,-910014514
-builtin_hash,StringEqual,-711928119
-builtin_hash,StringGreaterThan,-416690019
-builtin_hash,StringGreaterThanOrEqual,-1054396153
-builtin_hash,StringLessThan,-1054396153
-builtin_hash,StringLessThanOrEqual,-416690019
-builtin_hash,StringSubstring,358251310
-builtin_hash,OrderedHashTableHealIndex,725955381
-builtin_hash,CompileLazy,504995397
-builtin_hash,CompileLazyDeoptimizedCode,748068919
-builtin_hash,InstantiateAsmJs,-697690741
-builtin_hash,AllocateInYoungGeneration,214124693
-builtin_hash,AllocateRegularInYoungGeneration,-141910266
-builtin_hash,AllocateRegularInOldGeneration,-141910266
-builtin_hash,CopyFastSmiOrObjectElements,-883587649
-builtin_hash,GrowFastDoubleElements,-1000340886
-builtin_hash,GrowFastSmiOrObjectElements,36929045
-builtin_hash,ToNumber,-151588116
-builtin_hash,ToNumber_Baseline,435851851
-builtin_hash,ToNumeric_Baseline,15745649
-builtin_hash,ToNumberConvertBigInt,251501190
-builtin_hash,Typeof,-685026400
-builtin_hash,KeyedLoadIC_PolymorphicName,59242921
-builtin_hash,KeyedStoreIC_Megamorphic,283484746
-builtin_hash,DefineKeyedOwnIC_Megamorphic,969546395
-builtin_hash,LoadGlobalIC_NoFeedback,-412177004
-builtin_hash,LoadIC_FunctionPrototype,-696483123
-builtin_hash,LoadIC_StringLength,695888016
-builtin_hash,LoadIC_StringWrapperLength,-366408146
-builtin_hash,LoadIC_NoFeedback,529908718
-builtin_hash,StoreIC_NoFeedback,519102631
-builtin_hash,DefineNamedOwnIC_NoFeedback,-983253079
-builtin_hash,KeyedLoadIC_SloppyArguments,-578047658
-builtin_hash,StoreFastElementIC_Standard,856806958
-builtin_hash,StoreFastElementIC_GrowNoTransitionHandleCOW,295024286
-builtin_hash,StoreFastElementIC_NoTransitionHandleCOW,825368452
-builtin_hash,ElementsTransitionAndStore_Standard,-891826531
-builtin_hash,ElementsTransitionAndStore_GrowNoTransitionHandleCOW,-562086358
-builtin_hash,ElementsTransitionAndStore_NoTransitionHandleCOW,154727773
-builtin_hash,KeyedHasIC_PolymorphicName,681066279
-builtin_hash,EnqueueMicrotask,997834086
-builtin_hash,RunMicrotasks,835697778
-builtin_hash,HasProperty,325671088
-builtin_hash,DeleteProperty,362124331
-builtin_hash,SetDataProperties,649615472
-builtin_hash,ReturnReceiver,-720171624
-builtin_hash,ArrayConstructor,-709634836
-builtin_hash,ArrayConstructorImpl,106723908
-builtin_hash,ArrayNoArgumentConstructor_PackedSmi_DontOverride,-22521131
-builtin_hash,ArrayNoArgumentConstructor_HoleySmi_DontOverride,-22521131
-builtin_hash,ArrayNoArgumentConstructor_PackedSmi_DisableAllocationSites,-1046128045
-builtin_hash,ArrayNoArgumentConstructor_Packed_DisableAllocationSites,-1046128045
-builtin_hash,ArrayNoArgumentConstructor_Holey_DisableAllocationSites,-1046128045
-builtin_hash,ArrayNoArgumentConstructor_PackedDouble_DisableAllocationSites,-89922726
-builtin_hash,ArraySingleArgumentConstructor_HoleySmi_DontOverride,-249912913
-builtin_hash,ArraySingleArgumentConstructor_HoleySmi_DisableAllocationSites,452895553
-builtin_hash,ArraySingleArgumentConstructor_Holey_DisableAllocationSites,452895553
-builtin_hash,ArraySingleArgumentConstructor_HoleyDouble_DisableAllocationSites,97007850
-builtin_hash,ArrayIncludesSmi,52014377
-builtin_hash,ArrayIncludesSmiOrObject,194648631
-builtin_hash,ArrayIncludes,-547277433
-builtin_hash,ArrayIndexOfSmi,-203639532
-builtin_hash,ArrayIndexOfSmiOrObject,547347825
-builtin_hash,ArrayIndexOf,917956553
-builtin_hash,ArrayPrototypePop,-480442047
-builtin_hash,ArrayPrototypePush,879036958
-builtin_hash,CloneFastJSArray,330965023
-builtin_hash,CloneFastJSArrayFillingHoles,356661348
-builtin_hash,ExtractFastJSArray,-420221067
-builtin_hash,ArrayPrototypeEntries,138422158
-builtin_hash,ArrayPrototypeKeys,-3226360
-builtin_hash,ArrayPrototypeValues,138422158
-builtin_hash,ArrayIteratorPrototypeNext,957293662
-builtin_hash,AsyncFunctionEnter,-1043254273
-builtin_hash,AsyncFunctionResolve,847009948
-builtin_hash,AsyncFunctionAwaitCaught,-616846371
-builtin_hash,AsyncFunctionAwaitUncaught,-616846371
-builtin_hash,AsyncFunctionAwaitResolveClosure,518113046
-builtin_hash,DatePrototypeGetDate,517103214
-builtin_hash,DatePrototypeGetDay,517103214
-builtin_hash,DatePrototypeGetFullYear,517103214
-builtin_hash,DatePrototypeGetHours,517103214
-builtin_hash,DatePrototypeGetMilliseconds,402424200
-builtin_hash,DatePrototypeGetMinutes,517103214
-builtin_hash,DatePrototypeGetMonth,517103214
-builtin_hash,DatePrototypeGetSeconds,517103214
-builtin_hash,DatePrototypeGetTime,-634509018
-builtin_hash,DatePrototypeGetTimezoneOffset,402424200
-builtin_hash,DatePrototypeValueOf,-634509018
-builtin_hash,DatePrototypeToPrimitive,601732193
-builtin_hash,CreateIterResultObject,277090833
-builtin_hash,CreateGeneratorObject,-109733150
-builtin_hash,GeneratorPrototypeNext,-337770274
-builtin_hash,GeneratorPrototypeReturn,-356725560
-builtin_hash,SuspendGeneratorBaseline,877095808
-builtin_hash,ResumeGeneratorBaseline,263125026
-builtin_hash,GlobalIsFinite,-487573831
-builtin_hash,GlobalIsNaN,-507424666
-builtin_hash,LoadIC,1072966044
-builtin_hash,LoadIC_Megamorphic,132390484
-builtin_hash,LoadIC_Noninlined,-1003101269
-builtin_hash,LoadICTrampoline,709645727
-builtin_hash,LoadICBaseline,150897937
-builtin_hash,LoadICTrampoline_Megamorphic,709645727
-builtin_hash,LoadSuperIC,-323552047
-builtin_hash,LoadSuperICBaseline,489833039
-builtin_hash,KeyedLoadIC,166545115
-builtin_hash,KeyedLoadIC_Megamorphic,483505882
-builtin_hash,KeyedLoadICTrampoline,709645727
-builtin_hash,KeyedLoadICBaseline,150897937
-builtin_hash,KeyedLoadICTrampoline_Megamorphic,709645727
-builtin_hash,StoreGlobalIC,-192111648
-builtin_hash,StoreGlobalICTrampoline,709645727
-builtin_hash,StoreGlobalICBaseline,150897937
-builtin_hash,StoreIC,1060265796
-builtin_hash,StoreICTrampoline,1046844053
-builtin_hash,StoreICBaseline,489833039
-builtin_hash,DefineNamedOwnIC,-883359916
-builtin_hash,DefineNamedOwnICBaseline,489833039
-builtin_hash,KeyedStoreIC,-187772846
-builtin_hash,KeyedStoreICTrampoline,1046844053
-builtin_hash,KeyedStoreICBaseline,489833039
-builtin_hash,DefineKeyedOwnIC,229093376
-builtin_hash,StoreInArrayLiteralIC,84243524
-builtin_hash,StoreInArrayLiteralICBaseline,489833039
-builtin_hash,LoadGlobalIC,443237854
-builtin_hash,LoadGlobalICInsideTypeof,34880284
-builtin_hash,LoadGlobalICTrampoline,-306966323
-builtin_hash,LoadGlobalICBaseline,162605878
-builtin_hash,LoadGlobalICInsideTypeofTrampoline,-306966323
-builtin_hash,LoadGlobalICInsideTypeofBaseline,162605878
-builtin_hash,LookupGlobalICBaseline,658263603
-builtin_hash,LookupGlobalICInsideTypeofBaseline,658263603
-builtin_hash,KeyedHasIC,-184159247
-builtin_hash,KeyedHasICBaseline,150897937
-builtin_hash,KeyedHasIC_Megamorphic,325671088
-builtin_hash,IterableToList,-927449562
-builtin_hash,IterableToListWithSymbolLookup,-708423733
-builtin_hash,IterableToListMayPreserveHoles,-1051869903
-builtin_hash,FindOrderedHashMapEntry,-320047027
-builtin_hash,MapConstructor,-273849543
-builtin_hash,MapPrototypeSet,-438010061
-builtin_hash,MapPrototypeDelete,-650460231
-builtin_hash,MapPrototypeGet,-169555850
-builtin_hash,MapPrototypeHas,801403049
-builtin_hash,MapPrototypeEntries,-1013383243
-builtin_hash,MapPrototypeGetSize,-643608746
-builtin_hash,MapPrototypeForEach,397971331
-builtin_hash,MapPrototypeKeys,-1013383243
-builtin_hash,MapPrototypeValues,-1013383243
-builtin_hash,MapIteratorPrototypeNext,-58230590
-builtin_hash,MapIteratorToList,-500405827
-builtin_hash,SameValueNumbersOnly,225193452
-builtin_hash,Add_Baseline,986710997
-builtin_hash,AddSmi_Baseline,422902597
-builtin_hash,Subtract_Baseline,515924102
-builtin_hash,SubtractSmi_Baseline,-270105773
-builtin_hash,Multiply_Baseline,1010707471
-builtin_hash,MultiplySmi_Baseline,-436355449
-builtin_hash,Divide_Baseline,-545594124
-builtin_hash,DivideSmi_Baseline,-730711433
-builtin_hash,Modulus_Baseline,368591854
-builtin_hash,ModulusSmi_Baseline,-170655443
-builtin_hash,Exponentiate_Baseline,957426187
-builtin_hash,BitwiseAnd_Baseline,354476460
-builtin_hash,BitwiseAndSmi_Baseline,828880803
-builtin_hash,BitwiseOr_Baseline,-583084875
-builtin_hash,BitwiseOrSmi_Baseline,-795226409
-builtin_hash,BitwiseXor_Baseline,1012539154
-builtin_hash,BitwiseXorSmi_Baseline,-392216447
-builtin_hash,ShiftLeft_Baseline,361604397
-builtin_hash,ShiftLeftSmi_Baseline,5007264
-builtin_hash,ShiftRight_Baseline,-420303545
-builtin_hash,ShiftRightSmi_Baseline,-112240434
-builtin_hash,ShiftRightLogical_Baseline,540201589
-builtin_hash,ShiftRightLogicalSmi_Baseline,788439127
-builtin_hash,Add_WithFeedback,-229648387
-builtin_hash,Subtract_WithFeedback,376218593
-builtin_hash,Modulus_WithFeedback,-94506121
-builtin_hash,BitwiseOr_WithFeedback,468043323
-builtin_hash,Equal_Baseline,124000003
-builtin_hash,StrictEqual_Baseline,-1036096086
-builtin_hash,LessThan_Baseline,215883171
-builtin_hash,GreaterThan_Baseline,-830551473
-builtin_hash,LessThanOrEqual_Baseline,580468296
-builtin_hash,GreaterThanOrEqual_Baseline,146507237
-builtin_hash,Equal_WithFeedback,-1006818792
-builtin_hash,StrictEqual_WithFeedback,-600679172
-builtin_hash,LessThan_WithFeedback,-280032142
-builtin_hash,GreaterThan_WithFeedback,-753289225
-builtin_hash,GreaterThanOrEqual_WithFeedback,-99952431
-builtin_hash,BitwiseNot_Baseline,-805521700
-builtin_hash,Decrement_Baseline,-802142449
-builtin_hash,Increment_Baseline,-165792101
-builtin_hash,Negate_Baseline,-490529739
-builtin_hash,ObjectAssign,-223190746
-builtin_hash,ObjectCreate,-412976913
-builtin_hash,ObjectEntries,-161513173
-builtin_hash,ObjectGetOwnPropertyDescriptor,-318975900
-builtin_hash,ObjectGetOwnPropertyNames,-771378931
-builtin_hash,ObjectIs,754529791
-builtin_hash,ObjectKeys,868266099
-builtin_hash,ObjectPrototypeHasOwnProperty,368139529
-builtin_hash,ObjectToString,-682499204
-builtin_hash,InstanceOf_WithFeedback,-1073126021
-builtin_hash,InstanceOf_Baseline,-322167849
-builtin_hash,ForInEnumerate,-155288347
-builtin_hash,ForInPrepare,-765799522
-builtin_hash,ForInFilter,562291169
-builtin_hash,RegExpConstructor,646833247
-builtin_hash,RegExpExecAtom,345347095
-builtin_hash,RegExpExecInternal,1011835160
-builtin_hash,FindOrderedHashSetEntry,-610957306
-builtin_hash,SetConstructor,-240775710
-builtin_hash,SetPrototypeHas,801403049
-builtin_hash,SetPrototypeAdd,173213477
-builtin_hash,SetPrototypeDelete,-593129092
-builtin_hash,SetPrototypeEntries,-1013383243
-builtin_hash,SetPrototypeGetSize,-643608746
-builtin_hash,SetPrototypeForEach,335951491
-builtin_hash,SetPrototypeValues,-1013383243
-builtin_hash,SetIteratorPrototypeNext,-665998715
-builtin_hash,SetOrSetIteratorToList,-885447300
-builtin_hash,StringFromCharCode,-912540455
-builtin_hash,StringPrototypeReplace,66129801
-builtin_hash,StringPrototypeSplit,-362413227
-builtin_hash,TypedArrayConstructor,784903226
-builtin_hash,TypedArrayPrototypeByteLength,-727961620
-builtin_hash,TypedArrayPrototypeLength,733765018
-builtin_hash,WeakMapConstructor,1062092506
-builtin_hash,WeakMapLookupHashIndex,-142601393
-builtin_hash,WeakMapGet,294891847
-builtin_hash,WeakMapPrototypeHas,849790636
-builtin_hash,WeakMapPrototypeSet,345042088
-builtin_hash,WeakSetConstructor,105581429
-builtin_hash,WeakSetPrototypeHas,849790636
-builtin_hash,WeakSetPrototypeAdd,617138317
-builtin_hash,WeakCollectionSet,166987452
-builtin_hash,AsyncGeneratorResolve,249154931
-builtin_hash,AsyncGeneratorYieldWithAwait,295024719
-builtin_hash,AsyncGeneratorResumeNext,-291585996
-builtin_hash,AsyncGeneratorPrototypeNext,-917251663
-builtin_hash,AsyncGeneratorAwaitUncaught,-475035157
-builtin_hash,AsyncGeneratorAwaitResolveClosure,252907365
-builtin_hash,AsyncGeneratorYieldWithAwaitResolveClosure,523444337
-builtin_hash,StringAdd_CheckNone,122784498
-builtin_hash,SubString,42338335
-builtin_hash,GetProperty,244104330
-builtin_hash,GetPropertyWithReceiver,-361896834
-builtin_hash,SetProperty,-14606917
-builtin_hash,CreateDataProperty,56144293
-builtin_hash,ArrayPrototypeConcat,-629648509
-builtin_hash,ArrayEvery,420214237
-builtin_hash,ArrayFilterLoopLazyDeoptContinuation,784278374
-builtin_hash,ArrayFilterLoopContinuation,-537020475
-builtin_hash,ArrayFilter,1060321948
-builtin_hash,ArrayPrototypeFind,-585495057
-builtin_hash,ArrayForEachLoopLazyDeoptContinuation,-696710490
-builtin_hash,ArrayForEachLoopContinuation,168795773
-builtin_hash,ArrayForEach,707321002
-builtin_hash,ArrayFrom,-659188931
-builtin_hash,ArrayIsArray,232602198
-builtin_hash,LoadJoinElement_FastSmiOrObjectElements_0,680814132
-builtin_hash,LoadJoinElement_FastDoubleElements_0,-901304444
-builtin_hash,JoinStackPush,-686648212
-builtin_hash,JoinStackPop,190013026
-builtin_hash,ArrayPrototypeJoin,-35537300
-builtin_hash,ArrayPrototypeToString,-918592343
-builtin_hash,ArrayPrototypeLastIndexOf,17251424
-builtin_hash,ArrayMapLoopLazyDeoptContinuation,571271309
-builtin_hash,ArrayMapLoopContinuation,323522123
-builtin_hash,ArrayMap,301006305
-builtin_hash,ArrayReduceLoopLazyDeoptContinuation,802810816
-builtin_hash,ArrayReduceLoopContinuation,90909940
-builtin_hash,ArrayReduce,-82429841
-builtin_hash,ArrayPrototypeReverse,419566432
-builtin_hash,ArrayPrototypeShift,70747297
-builtin_hash,ArrayPrototypeSlice,-52082583
-builtin_hash,ArraySome,793025165
-builtin_hash,ArrayPrototypeSplice,-778396927
-builtin_hash,ArrayPrototypeUnshift,874971167
-builtin_hash,ArrayBufferPrototypeGetByteLength,856452822
-builtin_hash,ArrayBufferIsView,1032022175
-builtin_hash,ToInteger,571151444
-builtin_hash,FastCreateDataProperty,870096873
-builtin_hash,BooleanConstructor,404195496
-builtin_hash,BooleanPrototypeToString,238471687
-builtin_hash,ToString,-884108556
-builtin_hash,StringPrototypeToString,-772653609
-builtin_hash,StringPrototypeValueOf,-772653609
-builtin_hash,StringPrototypeCharAt,888809448
-builtin_hash,StringPrototypeCharCodeAt,-227472234
-builtin_hash,StringPrototypeCodePointAt,61946543
-builtin_hash,StringPrototypeConcat,861254127
-builtin_hash,StringConstructor,-918118629
-builtin_hash,StringAddConvertLeft,-219299649
-builtin_hash,StringAddConvertRight,616228664
-builtin_hash,StringCharAt,-4884031
-builtin_hash,FastNewClosureBaseline,-52998587
-builtin_hash,FastNewFunctionContextFunction,-1009868545
-builtin_hash,CreateRegExpLiteral,1052197216
-builtin_hash,CreateShallowArrayLiteral,-964230745
-builtin_hash,CreateEmptyArrayLiteral,223771159
-builtin_hash,CreateShallowObjectLiteral,608722722
-builtin_hash,ObjectConstructor,964183658
-builtin_hash,CreateEmptyLiteralObject,-604735671
-builtin_hash,NumberConstructor,71247802
-builtin_hash,StringToNumber,338191864
-builtin_hash,NonNumberToNumber,-935639762
-builtin_hash,NonNumberToNumeric,772033950
-builtin_hash,ToNumeric,1038000251
-builtin_hash,NumberToString,349605053
-builtin_hash,ToBoolean,-651891533
-builtin_hash,ToBooleanForBaselineJump,-345306524
-builtin_hash,ToLength,334926030
-builtin_hash,ToName,-958704264
-builtin_hash,ToObject,466844704
-builtin_hash,NonPrimitiveToPrimitive_Default,-135149567
-builtin_hash,NonPrimitiveToPrimitive_Number,-135149567
-builtin_hash,NonPrimitiveToPrimitive_String,-135149567
-builtin_hash,OrdinaryToPrimitive_Number,977211335
-builtin_hash,OrdinaryToPrimitive_String,977211335
-builtin_hash,DataViewPrototypeGetByteLength,-813425866
-builtin_hash,DataViewPrototypeGetFloat64,-154734323
-builtin_hash,DataViewPrototypeSetUint32,816035861
-builtin_hash,DataViewPrototypeSetFloat64,779399418
-builtin_hash,FunctionPrototypeHasInstance,-95071236
-builtin_hash,FastFunctionPrototypeBind,-731291747
-builtin_hash,ForInNext,-935081187
-builtin_hash,GetIteratorWithFeedback,952367600
-builtin_hash,GetIteratorBaseline,-428929310
-builtin_hash,CallIteratorWithFeedback,-997498631
-builtin_hash,MathAbs,791053705
-builtin_hash,MathCeil,479583478
-builtin_hash,MathFloor,843767669
-builtin_hash,MathRound,679517696
-builtin_hash,MathPow,315117335
-builtin_hash,MathMax,170958432
-builtin_hash,MathMin,-697028591
-builtin_hash,MathAsin,546404306
-builtin_hash,MathAtan2,-509511765
-builtin_hash,MathCos,907147156
-builtin_hash,MathExp,-622287064
-builtin_hash,MathFround,951312971
-builtin_hash,MathImul,750440578
-builtin_hash,MathLog,-240154692
-builtin_hash,MathSin,-666255595
-builtin_hash,MathSign,358296598
-builtin_hash,MathSqrt,376524398
-builtin_hash,MathTan,509900921
-builtin_hash,MathTanh,-99681025
-builtin_hash,MathRandom,761144822
-builtin_hash,NumberPrototypeToString,-379708910
-builtin_hash,NumberIsInteger,-216955149
-builtin_hash,NumberIsNaN,-901582353
-builtin_hash,NumberParseFloat,-608188050
-builtin_hash,ParseInt,-160264841
-builtin_hash,NumberParseInt,-548370817
-builtin_hash,Add,564570422
-builtin_hash,Subtract,638154168
-builtin_hash,Multiply,492896985
-builtin_hash,Divide,-323417562
-builtin_hash,Modulus,-1073163320
-builtin_hash,CreateObjectWithoutProperties,-847663210
-builtin_hash,ObjectIsExtensible,152531438
-builtin_hash,ObjectPreventExtensions,-713378297
-builtin_hash,ObjectGetPrototypeOf,-270542452
-builtin_hash,ObjectSetPrototypeOf,-46279001
-builtin_hash,ObjectPrototypeToString,799867293
-builtin_hash,ObjectPrototypeValueOf,681014822
-builtin_hash,FulfillPromise,-157790719
-builtin_hash,NewPromiseCapability,-849832514
-builtin_hash,PromiseCapabilityDefaultResolve,198574955
-builtin_hash,PerformPromiseThen,-1003545837
-builtin_hash,PromiseAll,172257811
-builtin_hash,PromiseAllResolveElementClosure,-429070847
-builtin_hash,PromiseConstructor,-813002504
-builtin_hash,PromisePrototypeCatch,881185065
-builtin_hash,PromiseFulfillReactionJob,1069906663
-builtin_hash,PromiseResolveTrampoline,584305889
-builtin_hash,PromiseResolve,774376525
-builtin_hash,ResolvePromise,-348045499
-builtin_hash,PromisePrototypeThen,972876029
-builtin_hash,PromiseResolveThenableJob,-24601962
-builtin_hash,ProxyConstructor,462534935
-builtin_hash,ProxyGetProperty,-1026339756
-builtin_hash,ProxyIsExtensible,-863573816
-builtin_hash,ProxyPreventExtensions,139876653
-builtin_hash,ReflectGet,-917044458
-builtin_hash,ReflectHas,584305889
-builtin_hash,RegExpPrototypeExec,-1050512290
-builtin_hash,RegExpMatchFast,794780294
-builtin_hash,RegExpReplace,-599615598
-builtin_hash,RegExpPrototypeReplace,985947001
-builtin_hash,RegExpSearchFast,-655744379
-builtin_hash,RegExpPrototypeSourceGetter,-842127011
-builtin_hash,RegExpSplit,882917446
-builtin_hash,RegExpPrototypeTest,36603632
-builtin_hash,RegExpPrototypeTestFast,170577886
-builtin_hash,RegExpPrototypeGlobalGetter,-787799554
-builtin_hash,RegExpPrototypeIgnoreCaseGetter,198844413
-builtin_hash,RegExpPrototypeMultilineGetter,-910406363
-builtin_hash,RegExpPrototypeHasIndicesGetter,738086388
-builtin_hash,RegExpPrototypeDotAllGetter,334669429
-builtin_hash,RegExpPrototypeStickyGetter,800064458
-builtin_hash,RegExpPrototypeUnicodeGetter,489690780
-builtin_hash,RegExpPrototypeFlagsGetter,-224095936
-builtin_hash,StringPrototypeEndsWith,-987712417
-builtin_hash,StringPrototypeIncludes,-217252393
-builtin_hash,StringPrototypeIndexOf,633191246
-builtin_hash,StringPrototypeIterator,836933598
-builtin_hash,StringIteratorPrototypeNext,-913259598
-builtin_hash,StringPrototypeMatch,634725082
-builtin_hash,StringPrototypeSearch,634725082
-builtin_hash,StringRepeat,146146790
-builtin_hash,StringPrototypeSlice,-354759606
-builtin_hash,StringPrototypeStartsWith,-27064544
-builtin_hash,StringPrototypeSubstr,-580369335
-builtin_hash,StringPrototypeSubstring,432057087
-builtin_hash,StringPrototypeTrim,413903909
-builtin_hash,SymbolPrototypeToString,-299401864
-builtin_hash,CreateTypedArray,680205708
-builtin_hash,TypedArrayFrom,-1010794576
-builtin_hash,TypedArrayPrototypeSet,908791947
-builtin_hash,TypedArrayPrototypeSubArray,-259511191
-builtin_hash,NewSloppyArgumentsElements,987039656
-builtin_hash,NewStrictArgumentsElements,811685066
-builtin_hash,NewRestArgumentsElements,506972005
-builtin_hash,FastNewSloppyArguments,266310136
-builtin_hash,FastNewStrictArguments,292190921
-builtin_hash,FastNewRestArguments,757900916
-builtin_hash,StringSlowFlatten,-1052562445
-builtin_hash,StringIndexOf,257386893
-builtin_hash,Load_FastSmiElements_0,1073407008
-builtin_hash,Load_FastObjectElements_0,1073407008
-builtin_hash,Store_FastSmiElements_0,30666818
-builtin_hash,Store_FastObjectElements_0,951821143
-builtin_hash,SortCompareDefault,367817675
-builtin_hash,SortCompareUserFn,713062731
-builtin_hash,Copy,314823270
-builtin_hash,MergeAt,238953995
-builtin_hash,GallopLeft,-902489773
-builtin_hash,GallopRight,-89876014
-builtin_hash,ArrayTimSort,241000486
-builtin_hash,ArrayPrototypeSort,304686317
-builtin_hash,StringFastLocaleCompare,805312292
-builtin_hash,WasmInt32ToHeapNumber,-952774935
-builtin_hash,WasmTaggedNonSmiToInt32,-293378356
-builtin_hash,WasmTriggerTierUp,429384824
-builtin_hash,WasmStackGuard,1062523926
-builtin_hash,CanUseSameAccessor_FastSmiElements_0,185432218
-builtin_hash,CanUseSameAccessor_FastObjectElements_0,185432218
-builtin_hash,StringPrototypeToLowerCaseIntl,-318663010
-builtin_hash,StringToLowerCaseIntl,545745188
-builtin_hash,WideHandler,-1007335397
-builtin_hash,ExtraWideHandler,-1007335397
-builtin_hash,LdarHandler,-107846222
-builtin_hash,LdaZeroHandler,-1049743757
-builtin_hash,LdaSmiHandler,578097911
-builtin_hash,LdaUndefinedHandler,929864141
-builtin_hash,LdaNullHandler,929864141
-builtin_hash,LdaTheHoleHandler,929864141
-builtin_hash,LdaTrueHandler,-722702248
-builtin_hash,LdaFalseHandler,-722702248
-builtin_hash,LdaConstantHandler,16269541
-builtin_hash,LdaContextSlotHandler,-824239973
-builtin_hash,LdaImmutableContextSlotHandler,-824239973
-builtin_hash,LdaCurrentContextSlotHandler,797490234
-builtin_hash,LdaImmutableCurrentContextSlotHandler,797490234
-builtin_hash,StarHandler,833251632
-builtin_hash,MovHandler,154956537
-builtin_hash,PushContextHandler,-12168674
-builtin_hash,PopContextHandler,-245935867
-builtin_hash,TestReferenceEqualHandler,149010419
-builtin_hash,TestUndetectableHandler,-14656053
-builtin_hash,TestNullHandler,231206083
-builtin_hash,TestUndefinedHandler,231206083
-builtin_hash,TestTypeOfHandler,811742421
-builtin_hash,LdaGlobalHandler,-713975577
-builtin_hash,LdaGlobalInsideTypeofHandler,-1036637473
-builtin_hash,StaGlobalHandler,860070325
-builtin_hash,StaContextSlotHandler,506961255
-builtin_hash,StaCurrentContextSlotHandler,-476609776
-builtin_hash,LdaLookupGlobalSlotHandler,-579647044
-builtin_hash,LdaLookupGlobalSlotInsideTypeofHandler,-981566026
-builtin_hash,StaLookupSlotHandler,-26776709
-builtin_hash,GetNamedPropertyHandler,12677554
-builtin_hash,GetNamedPropertyFromSuperHandler,266004833
-builtin_hash,GetKeyedPropertyHandler,886814234
-builtin_hash,SetNamedPropertyHandler,535113985
-builtin_hash,DefineNamedOwnPropertyHandler,535113985
-builtin_hash,SetKeyedPropertyHandler,-966762662
-builtin_hash,DefineKeyedOwnPropertyHandler,-966762662
-builtin_hash,StaInArrayLiteralHandler,-966762662
-builtin_hash,DefineKeyedOwnPropertyInLiteralHandler,587951803
-builtin_hash,AddHandler,-1005617859
-builtin_hash,SubHandler,-392643217
-builtin_hash,MulHandler,774043098
-builtin_hash,DivHandler,-851134889
-builtin_hash,ModHandler,-974972127
-builtin_hash,ExpHandler,315906749
-builtin_hash,BitwiseOrHandler,-743595958
-builtin_hash,BitwiseXorHandler,250559085
-builtin_hash,BitwiseAndHandler,-517136702
-builtin_hash,ShiftLeftHandler,96191474
-builtin_hash,ShiftRightHandler,527145177
-builtin_hash,ShiftRightLogicalHandler,-262573075
-builtin_hash,AddSmiHandler,80685265
-builtin_hash,SubSmiHandler,3480492
-builtin_hash,MulSmiHandler,125802928
-builtin_hash,DivSmiHandler,639417969
-builtin_hash,ModSmiHandler,725314466
-builtin_hash,BitwiseOrSmiHandler,-982449315
-builtin_hash,BitwiseXorSmiHandler,-243744730
-builtin_hash,BitwiseAndSmiHandler,-177147003
-builtin_hash,ShiftLeftSmiHandler,-1064587438
-builtin_hash,ShiftRightSmiHandler,1063683921
-builtin_hash,ShiftRightLogicalSmiHandler,-471648394
-builtin_hash,IncHandler,-827986722
-builtin_hash,DecHandler,-353459880
-builtin_hash,NegateHandler,-400765437
-builtin_hash,BitwiseNotHandler,70967995
-builtin_hash,ToBooleanLogicalNotHandler,907700753
-builtin_hash,LogicalNotHandler,-207609416
-builtin_hash,TypeOfHandler,953802635
-builtin_hash,DeletePropertyStrictHandler,-316449707
-builtin_hash,DeletePropertySloppyHandler,808036376
-builtin_hash,GetSuperConstructorHandler,251341877
-builtin_hash,CallAnyReceiverHandler,439143156
-builtin_hash,CallPropertyHandler,439143156
-builtin_hash,CallProperty0Handler,-534603169
-builtin_hash,CallProperty1Handler,498249779
-builtin_hash,CallProperty2Handler,-819411032
-builtin_hash,CallUndefinedReceiverHandler,594463859
-builtin_hash,CallUndefinedReceiver0Handler,-846775891
-builtin_hash,CallUndefinedReceiver1Handler,-295634644
-builtin_hash,CallUndefinedReceiver2Handler,-184450155
-builtin_hash,CallWithSpreadHandler,439143156
-builtin_hash,CallRuntimeHandler,577184417
-builtin_hash,CallJSRuntimeHandler,611687015
-builtin_hash,InvokeIntrinsicHandler,-984127378
-builtin_hash,ConstructHandler,-721343708
-builtin_hash,ConstructWithSpreadHandler,-306460848
-builtin_hash,TestEqualHandler,808271421
-builtin_hash,TestEqualStrictHandler,329888735
-builtin_hash,TestLessThanHandler,470012740
-builtin_hash,TestGreaterThanHandler,-1062728522
-builtin_hash,TestLessThanOrEqualHandler,567090722
-builtin_hash,TestGreaterThanOrEqualHandler,386329922
-builtin_hash,TestInstanceOfHandler,490446195
-builtin_hash,TestInHandler,839610057
-builtin_hash,ToNameHandler,-725569033
-builtin_hash,ToNumberHandler,1020792744
-builtin_hash,ToNumericHandler,157199207
-builtin_hash,ToObjectHandler,-725569033
-builtin_hash,ToStringHandler,-98710919
-builtin_hash,CreateRegExpLiteralHandler,-121212255
-builtin_hash,CreateArrayLiteralHandler,266431889
-builtin_hash,CreateArrayFromIterableHandler,52334283
-builtin_hash,CreateEmptyArrayLiteralHandler,115537807
-builtin_hash,CreateObjectLiteralHandler,618199371
-builtin_hash,CreateEmptyObjectLiteralHandler,740591432
-builtin_hash,CreateClosureHandler,2501418
-builtin_hash,CreateBlockContextHandler,666399130
-builtin_hash,CreateCatchContextHandler,22630037
-builtin_hash,CreateFunctionContextHandler,-865490982
-builtin_hash,CreateMappedArgumentsHandler,697758196
-builtin_hash,CreateUnmappedArgumentsHandler,-1055788256
-builtin_hash,CreateRestParameterHandler,-320411861
-builtin_hash,JumpLoopHandler,-662751978
-builtin_hash,JumpHandler,182907248
-builtin_hash,JumpConstantHandler,736061566
-builtin_hash,JumpIfUndefinedConstantHandler,-462384230
-builtin_hash,JumpIfNotUndefinedConstantHandler,148504964
-builtin_hash,JumpIfUndefinedOrNullConstantHandler,444469070
-builtin_hash,JumpIfTrueConstantHandler,-462384230
-builtin_hash,JumpIfFalseConstantHandler,-462384230
-builtin_hash,JumpIfToBooleanTrueConstantHandler,-247852105
-builtin_hash,JumpIfToBooleanFalseConstantHandler,-525508881
-builtin_hash,JumpIfToBooleanTrueHandler,725690453
-builtin_hash,JumpIfToBooleanFalseHandler,-413432567
-builtin_hash,JumpIfTrueHandler,2511228
-builtin_hash,JumpIfFalseHandler,2511228
-builtin_hash,JumpIfNullHandler,2511228
-builtin_hash,JumpIfNotNullHandler,-489824053
-builtin_hash,JumpIfUndefinedHandler,2511228
-builtin_hash,JumpIfNotUndefinedHandler,-489824053
-builtin_hash,JumpIfUndefinedOrNullHandler,-718321269
-builtin_hash,JumpIfJSReceiverHandler,-171543406
-builtin_hash,SwitchOnSmiNoFeedbackHandler,-188066699
-builtin_hash,ForInEnumerateHandler,-998219819
-builtin_hash,ForInPrepareHandler,480083331
-builtin_hash,ForInContinueHandler,829644977
-builtin_hash,ForInNextHandler,-132478305
-builtin_hash,ForInStepHandler,115172731
-builtin_hash,SetPendingMessageHandler,962212923
-builtin_hash,ThrowHandler,781858395
-builtin_hash,ReThrowHandler,781858395
-builtin_hash,ReturnHandler,557919322
-builtin_hash,ThrowReferenceErrorIfHoleHandler,-516587462
-builtin_hash,ThrowSuperNotCalledIfHoleHandler,-773024801
-builtin_hash,ThrowSuperAlreadyCalledIfNotHoleHandler,357828706
-builtin_hash,ThrowIfNotSuperConstructorHandler,-125687893
-builtin_hash,SwitchOnGeneratorStateHandler,-423602269
-builtin_hash,SuspendGeneratorHandler,-380438202
-builtin_hash,ResumeGeneratorHandler,-518827895
-builtin_hash,GetIteratorHandler,-378048438
-builtin_hash,ShortStarHandler,368091777
-builtin_hash,LdarWideHandler,-405004763
-builtin_hash,LdaSmiWideHandler,976476814
-builtin_hash,LdaConstantWideHandler,-171575655
-builtin_hash,LdaContextSlotWideHandler,679792213
-builtin_hash,LdaImmutableContextSlotWideHandler,679792213
-builtin_hash,LdaImmutableCurrentContextSlotWideHandler,796095575
-builtin_hash,StarWideHandler,728945870
-builtin_hash,MovWideHandler,-483424163
-builtin_hash,PushContextWideHandler,325978832
-builtin_hash,PopContextWideHandler,1044899411
-builtin_hash,TestReferenceEqualWideHandler,-444014273
-builtin_hash,LdaGlobalWideHandler,976796507
-builtin_hash,LdaGlobalInsideTypeofWideHandler,-19868457
-builtin_hash,StaGlobalWideHandler,-216106571
-builtin_hash,StaContextSlotWideHandler,-553577385
-builtin_hash,StaCurrentContextSlotWideHandler,-429895179
-builtin_hash,LdaLookupGlobalSlotWideHandler,-291087637
-builtin_hash,GetNamedPropertyWideHandler,-241439423
-builtin_hash,GetKeyedPropertyWideHandler,364433897
-builtin_hash,SetNamedPropertyWideHandler,-31139520
-builtin_hash,DefineNamedOwnPropertyWideHandler,-31139520
-builtin_hash,SetKeyedPropertyWideHandler,-363367368
-builtin_hash,DefineKeyedOwnPropertyWideHandler,-363367368
-builtin_hash,StaInArrayLiteralWideHandler,-363367368
-builtin_hash,AddWideHandler,-667096135
-builtin_hash,SubWideHandler,-383279960
-builtin_hash,MulWideHandler,1029076402
-builtin_hash,DivWideHandler,-547887104
-builtin_hash,BitwiseOrWideHandler,9634898
-builtin_hash,BitwiseAndWideHandler,498024986
-builtin_hash,ShiftLeftWideHandler,-839529705
-builtin_hash,AddSmiWideHandler,-930183091
-builtin_hash,SubSmiWideHandler,-150212329
-builtin_hash,MulSmiWideHandler,591583886
-builtin_hash,DivSmiWideHandler,343395191
-builtin_hash,ModSmiWideHandler,-801743696
-builtin_hash,BitwiseOrSmiWideHandler,-944128869
-builtin_hash,BitwiseXorSmiWideHandler,-944761076
-builtin_hash,BitwiseAndSmiWideHandler,512282363
-builtin_hash,ShiftLeftSmiWideHandler,895748637
-builtin_hash,ShiftRightSmiWideHandler,-526800833
-builtin_hash,ShiftRightLogicalSmiWideHandler,306897250
-builtin_hash,IncWideHandler,243851885
-builtin_hash,DecWideHandler,573422516
-builtin_hash,NegateWideHandler,-256119499
-builtin_hash,CallPropertyWideHandler,899629797
-builtin_hash,CallProperty0WideHandler,649245559
-builtin_hash,CallProperty1WideHandler,-508092594
-builtin_hash,CallProperty2WideHandler,375883648
-builtin_hash,CallUndefinedReceiverWideHandler,-544740528
-builtin_hash,CallUndefinedReceiver0WideHandler,-908716826
-builtin_hash,CallUndefinedReceiver1WideHandler,-63091942
-builtin_hash,CallUndefinedReceiver2WideHandler,332968542
-builtin_hash,CallWithSpreadWideHandler,899629797
-builtin_hash,ConstructWideHandler,-133823750
-builtin_hash,TestEqualWideHandler,-560396321
-builtin_hash,TestEqualStrictWideHandler,-723086528
-builtin_hash,TestLessThanWideHandler,-954086139
-builtin_hash,TestGreaterThanWideHandler,8081046
-builtin_hash,TestLessThanOrEqualWideHandler,-869672701
-builtin_hash,TestGreaterThanOrEqualWideHandler,915149033
-builtin_hash,TestInstanceOfWideHandler,-907585471
-builtin_hash,TestInWideHandler,952743861
-builtin_hash,ToNumericWideHandler,848961913
-builtin_hash,CreateRegExpLiteralWideHandler,-434410141
-builtin_hash,CreateArrayLiteralWideHandler,-5646835
-builtin_hash,CreateEmptyArrayLiteralWideHandler,-585282797
-builtin_hash,CreateObjectLiteralWideHandler,484880724
-builtin_hash,CreateClosureWideHandler,158285856
-builtin_hash,CreateBlockContextWideHandler,63996092
-builtin_hash,CreateFunctionContextWideHandler,-965079469
-builtin_hash,JumpLoopWideHandler,999208902
-builtin_hash,JumpWideHandler,182907248
-builtin_hash,JumpIfToBooleanTrueWideHandler,592062439
-builtin_hash,JumpIfToBooleanFalseWideHandler,-182416778
-builtin_hash,JumpIfTrueWideHandler,675926914
-builtin_hash,JumpIfFalseWideHandler,675926914
-builtin_hash,SwitchOnSmiNoFeedbackWideHandler,438009109
-builtin_hash,ForInPrepareWideHandler,4093040
-builtin_hash,ForInNextWideHandler,1049458669
-builtin_hash,ThrowReferenceErrorIfHoleWideHandler,269750921
-builtin_hash,GetIteratorWideHandler,-359333408
-builtin_hash,LdaSmiExtraWideHandler,976476814
-builtin_hash,LdaGlobalExtraWideHandler,-300848501
-builtin_hash,AddSmiExtraWideHandler,-986018015
-builtin_hash,SubSmiExtraWideHandler,-1056721725
-builtin_hash,MulSmiExtraWideHandler,-437583101
-builtin_hash,DivSmiExtraWideHandler,91019683
-builtin_hash,BitwiseOrSmiExtraWideHandler,-280647725
-builtin_hash,BitwiseXorSmiExtraWideHandler,945663448
-builtin_hash,BitwiseAndSmiExtraWideHandler,764701867
-builtin_hash,CallUndefinedReceiverExtraWideHandler,-252891433
-builtin_hash,CallUndefinedReceiver1ExtraWideHandler,984557820
-builtin_hash,CallUndefinedReceiver2ExtraWideHandler,-972056227
+builtin_hash,RecordWriteSaveFP,-726777896
+builtin_hash,RecordWriteIgnoreFP,-726777896
+builtin_hash,EphemeronKeyBarrierSaveFP,-673045595
+builtin_hash,AdaptorWithBuiltinExitFrame,1058054117
+builtin_hash,Call_ReceiverIsNullOrUndefined_Baseline_Compact,858122912
+builtin_hash,Call_ReceiverIsNotNullOrUndefined_Baseline_Compact,646911826
+builtin_hash,Call_ReceiverIsAny_Baseline_Compact,646911826
+builtin_hash,CallProxy,-213096940
+builtin_hash,CallWithSpread,850146918
+builtin_hash,CallWithSpread_Baseline,28766541
+builtin_hash,CallWithArrayLike,863195024
+builtin_hash,ConstructWithSpread,-680697218
+builtin_hash,ConstructWithSpread_Baseline,289501324
+builtin_hash,Construct_Baseline,-759871673
+builtin_hash,FastNewObject,413535442
+builtin_hash,FastNewClosure,570052345
+builtin_hash,StringEqual,854835916
+builtin_hash,StringGreaterThan,-1046193569
+builtin_hash,StringGreaterThanOrEqual,-163480371
+builtin_hash,StringLessThan,-163480371
+builtin_hash,StringLessThanOrEqual,-1046193569
+builtin_hash,StringSubstring,919827347
+builtin_hash,OrderedHashTableHealIndex,534476200
+builtin_hash,CompileLazy,972488543
+builtin_hash,CompileLazyDeoptimizedCode,-421407567
+builtin_hash,InstantiateAsmJs,-1023179608
+builtin_hash,AllocateInYoungGeneration,-533397479
+builtin_hash,AllocateRegularInYoungGeneration,1027329059
+builtin_hash,AllocateRegularInOldGeneration,1027329059
+builtin_hash,CopyFastSmiOrObjectElements,683092240
+builtin_hash,GrowFastDoubleElements,-452014384
+builtin_hash,GrowFastSmiOrObjectElements,-864052523
+builtin_hash,ToNumber,-800974620
+builtin_hash,ToNumber_Baseline,109807865
+builtin_hash,ToNumeric_Baseline,-776212596
+builtin_hash,ToNumberConvertBigInt,716851903
+builtin_hash,Typeof,819455224
+builtin_hash,KeyedLoadIC_PolymorphicName,-689223336
+builtin_hash,KeyedStoreIC_Megamorphic,254473413
+builtin_hash,DefineKeyedOwnIC_Megamorphic,-460277254
+builtin_hash,LoadGlobalIC_NoFeedback,-110200517
+builtin_hash,LoadIC_FunctionPrototype,-712048234
+builtin_hash,LoadIC_StringLength,888036620
+builtin_hash,LoadIC_StringWrapperLength,-322863465
+builtin_hash,LoadIC_NoFeedback,-407781812
+builtin_hash,StoreIC_NoFeedback,498495916
+builtin_hash,DefineNamedOwnIC_NoFeedback,-674344650
+builtin_hash,KeyedLoadIC_SloppyArguments,-264177550
+builtin_hash,StoreFastElementIC_Standard,670144974
+builtin_hash,StoreFastElementIC_GrowNoTransitionHandleCOW,72441205
+builtin_hash,StoreFastElementIC_NoTransitionHandleCOW,-865837557
+builtin_hash,ElementsTransitionAndStore_Standard,602243835
+builtin_hash,ElementsTransitionAndStore_GrowNoTransitionHandleCOW,-149712742
+builtin_hash,ElementsTransitionAndStore_NoTransitionHandleCOW,594322143
+builtin_hash,KeyedHasIC_PolymorphicName,-821302781
+builtin_hash,EnqueueMicrotask,-302354621
+builtin_hash,RunMicrotasks,766385977
+builtin_hash,HasProperty,-714203747
+builtin_hash,DeleteProperty,-714863218
+builtin_hash,SetDataProperties,-859409816
+builtin_hash,ReturnReceiver,424286427
+builtin_hash,ArrayConstructor,374700129
+builtin_hash,ArrayConstructorImpl,-937016390
+builtin_hash,ArrayNoArgumentConstructor_PackedSmi_DontOverride,-610727288
+builtin_hash,ArrayNoArgumentConstructor_HoleySmi_DontOverride,-610727288
+builtin_hash,ArrayNoArgumentConstructor_PackedSmi_DisableAllocationSites,-920795459
+builtin_hash,ArrayNoArgumentConstructor_Packed_DisableAllocationSites,-920795459
+builtin_hash,ArrayNoArgumentConstructor_Holey_DisableAllocationSites,-920795459
+builtin_hash,ArrayNoArgumentConstructor_PackedDouble_DisableAllocationSites,-931536819
+builtin_hash,ArraySingleArgumentConstructor_HoleySmi_DontOverride,-163160499
+builtin_hash,ArraySingleArgumentConstructor_HoleySmi_DisableAllocationSites,-334236117
+builtin_hash,ArraySingleArgumentConstructor_Holey_DisableAllocationSites,-334236117
+builtin_hash,ArraySingleArgumentConstructor_HoleyDouble_DisableAllocationSites,-851781780
+builtin_hash,ArrayIncludesSmi,-782625397
+builtin_hash,ArrayIncludesSmiOrObject,-1019116117
+builtin_hash,ArrayIncludes,-700282247
+builtin_hash,ArrayIndexOfSmi,674398784
+builtin_hash,ArrayIndexOfSmiOrObject,-38210588
+builtin_hash,ArrayIndexOf,-630602255
+builtin_hash,ArrayPrototypePop,520833050
+builtin_hash,ArrayPrototypePush,-588957354
+builtin_hash,CloneFastJSArray,-940332184
+builtin_hash,CloneFastJSArrayFillingHoles,376125361
+builtin_hash,ExtractFastJSArray,69886191
+builtin_hash,ArrayPrototypeEntries,797181916
+builtin_hash,ArrayPrototypeKeys,-510793262
+builtin_hash,ArrayPrototypeValues,797181916
+builtin_hash,ArrayIteratorPrototypeNext,998895316
+builtin_hash,AsyncFunctionEnter,706651433
+builtin_hash,AsyncFunctionResolve,924354792
+builtin_hash,AsyncFunctionAwaitCaught,1029985317
+builtin_hash,AsyncFunctionAwaitUncaught,1029985317
+builtin_hash,AsyncFunctionAwaitResolveClosure,624834406
+builtin_hash,DatePrototypeGetDate,455533545
+builtin_hash,DatePrototypeGetDay,455533545
+builtin_hash,DatePrototypeGetFullYear,455533545
+builtin_hash,DatePrototypeGetHours,455533545
+builtin_hash,DatePrototypeGetMilliseconds,837814216
+builtin_hash,DatePrototypeGetMinutes,455533545
+builtin_hash,DatePrototypeGetMonth,455533545
+builtin_hash,DatePrototypeGetSeconds,455533545
+builtin_hash,DatePrototypeGetTime,-352023713
+builtin_hash,DatePrototypeGetTimezoneOffset,837814216
+builtin_hash,DatePrototypeValueOf,-352023713
+builtin_hash,DatePrototypeToPrimitive,-306581045
+builtin_hash,CreateIterResultObject,-369987292
+builtin_hash,CreateGeneratorObject,-51189745
+builtin_hash,GeneratorPrototypeNext,-979874589
+builtin_hash,GeneratorPrototypeReturn,-376232641
+builtin_hash,SuspendGeneratorBaseline,1044307139
+builtin_hash,ResumeGeneratorBaseline,-33214673
+builtin_hash,GlobalIsFinite,608945684
+builtin_hash,GlobalIsNaN,480533016
+builtin_hash,LoadIC,-159967869
+builtin_hash,LoadIC_Megamorphic,783558994
+builtin_hash,LoadIC_Noninlined,-1053430564
+builtin_hash,LoadICTrampoline,415259910
+builtin_hash,LoadICBaseline,-789670064
+builtin_hash,LoadICTrampoline_Megamorphic,415259910
+builtin_hash,LoadSuperIC,-382069116
+builtin_hash,LoadSuperICBaseline,350433880
+builtin_hash,KeyedLoadIC,-775829679
+builtin_hash,KeyedLoadIC_Megamorphic,535456695
+builtin_hash,KeyedLoadICTrampoline,415259910
+builtin_hash,KeyedLoadICBaseline,-789670064
+builtin_hash,KeyedLoadICTrampoline_Megamorphic,415259910
+builtin_hash,StoreGlobalIC,-549357916
+builtin_hash,StoreGlobalICTrampoline,415259910
+builtin_hash,StoreGlobalICBaseline,-789670064
+builtin_hash,StoreIC,131443759
+builtin_hash,StoreICTrampoline,664895372
+builtin_hash,StoreICBaseline,350433880
+builtin_hash,DefineNamedOwnIC,-470691823
+builtin_hash,DefineNamedOwnICBaseline,350433880
+builtin_hash,KeyedStoreIC,143012442
+builtin_hash,KeyedStoreICTrampoline,664895372
+builtin_hash,KeyedStoreICBaseline,350433880
+builtin_hash,DefineKeyedOwnIC,14337203
+builtin_hash,StoreInArrayLiteralIC,357518953
+builtin_hash,StoreInArrayLiteralICBaseline,350433880
+builtin_hash,LoadGlobalIC,-965378230
+builtin_hash,LoadGlobalICInsideTypeof,-962887989
+builtin_hash,LoadGlobalICTrampoline,1065241136
+builtin_hash,LoadGlobalICBaseline,-209484242
+builtin_hash,LoadGlobalICInsideTypeofTrampoline,1065241136
+builtin_hash,LoadGlobalICInsideTypeofBaseline,-209484242
+builtin_hash,LookupGlobalICBaseline,696922418
+builtin_hash,LookupGlobalICInsideTypeofBaseline,696922418
+builtin_hash,KeyedHasIC,926037557
+builtin_hash,KeyedHasICBaseline,-789670064
+builtin_hash,KeyedHasIC_Megamorphic,-714203747
+builtin_hash,IterableToList,216346690
+builtin_hash,IterableToListWithSymbolLookup,470463439
+builtin_hash,IterableToListMayPreserveHoles,140268820
+builtin_hash,FindOrderedHashMapEntry,-604979912
+builtin_hash,MapConstructor,-249864188
+builtin_hash,MapPrototypeSet,535944514
+builtin_hash,MapPrototypeDelete,823187063
+builtin_hash,MapPrototypeGet,-992707095
+builtin_hash,MapPrototypeHas,379455552
+builtin_hash,MapPrototypeEntries,908832154
+builtin_hash,MapPrototypeGetSize,846186029
+builtin_hash,MapPrototypeForEach,420961920
+builtin_hash,MapPrototypeKeys,908832154
+builtin_hash,MapPrototypeValues,908832154
+builtin_hash,MapIteratorPrototypeNext,-288779464
+builtin_hash,MapIteratorToList,-816260477
+builtin_hash,SameValueNumbersOnly,-25129126
+builtin_hash,Add_Baseline,227334744
+builtin_hash,AddSmi_Baseline,-803607255
+builtin_hash,Subtract_Baseline,-465432536
+builtin_hash,SubtractSmi_Baseline,-747934466
+builtin_hash,Multiply_Baseline,958145830
+builtin_hash,MultiplySmi_Baseline,-183633015
+builtin_hash,Divide_Baseline,-658775957
+builtin_hash,DivideSmi_Baseline,596645384
+builtin_hash,Modulus_Baseline,417939096
+builtin_hash,ModulusSmi_Baseline,274117178
+builtin_hash,Exponentiate_Baseline,-678138653
+builtin_hash,BitwiseAnd_Baseline,971917730
+builtin_hash,BitwiseAndSmi_Baseline,-427224399
+builtin_hash,BitwiseOr_Baseline,-86023826
+builtin_hash,BitwiseOrSmi_Baseline,1033800245
+builtin_hash,BitwiseXor_Baseline,1018309106
+builtin_hash,BitwiseXorSmi_Baseline,-776461247
+builtin_hash,ShiftLeft_Baseline,-563580356
+builtin_hash,ShiftLeftSmi_Baseline,-559342044
+builtin_hash,ShiftRight_Baseline,1046983317
+builtin_hash,ShiftRightSmi_Baseline,344949769
+builtin_hash,ShiftRightLogical_Baseline,-621551965
+builtin_hash,ShiftRightLogicalSmi_Baseline,-730271331
+builtin_hash,Add_WithFeedback,-655798137
+builtin_hash,Subtract_WithFeedback,-172753508
+builtin_hash,Modulus_WithFeedback,-1003748883
+builtin_hash,BitwiseOr_WithFeedback,981590503
+builtin_hash,Equal_Baseline,940608970
+builtin_hash,StrictEqual_Baseline,726432452
+builtin_hash,LessThan_Baseline,362404859
+builtin_hash,GreaterThan_Baseline,1053559793
+builtin_hash,LessThanOrEqual_Baseline,283953892
+builtin_hash,GreaterThanOrEqual_Baseline,734136323
+builtin_hash,Equal_WithFeedback,553553676
+builtin_hash,StrictEqual_WithFeedback,-536478159
+builtin_hash,LessThan_WithFeedback,-304778695
+builtin_hash,GreaterThan_WithFeedback,856459323
+builtin_hash,GreaterThanOrEqual_WithFeedback,547352059
+builtin_hash,BitwiseNot_Baseline,-649673224
+builtin_hash,Decrement_Baseline,596332548
+builtin_hash,Increment_Baseline,842384965
+builtin_hash,Negate_Baseline,15380030
+builtin_hash,ObjectAssign,521201809
+builtin_hash,ObjectCreate,-448176478
+builtin_hash,ObjectEntries,-68491710
+builtin_hash,ObjectGetOwnPropertyDescriptor,-542894089
+builtin_hash,ObjectGetOwnPropertyNames,650621679
+builtin_hash,ObjectIs,-986440389
+builtin_hash,ObjectKeys,-702112957
+builtin_hash,ObjectPrototypeHasOwnProperty,-29325300
+builtin_hash,ObjectToString,-165543538
+builtin_hash,InstanceOf_WithFeedback,336324400
+builtin_hash,InstanceOf_Baseline,-6863477
+builtin_hash,ForInEnumerate,-685322984
+builtin_hash,ForInPrepare,-247635412
+builtin_hash,ForInFilter,-833678486
+builtin_hash,RegExpConstructor,-954773819
+builtin_hash,RegExpExecAtom,-1012355680
+builtin_hash,RegExpExecInternal,714858405
+builtin_hash,FindOrderedHashSetEntry,726112787
+builtin_hash,SetConstructor,246660026
+builtin_hash,SetPrototypeHas,379455552
+builtin_hash,SetPrototypeAdd,-1038309920
+builtin_hash,SetPrototypeDelete,-870300530
+builtin_hash,SetPrototypeEntries,908832154
+builtin_hash,SetPrototypeGetSize,846186029
+builtin_hash,SetPrototypeForEach,-891643547
+builtin_hash,SetPrototypeValues,908832154
+builtin_hash,SetIteratorPrototypeNext,664557777
+builtin_hash,SetOrSetIteratorToList,-665943107
+builtin_hash,StringFromCharCode,-404331798
+builtin_hash,StringPrototypeReplace,-489667599
+builtin_hash,StringPrototypeSplit,882257544
+builtin_hash,TypedArrayConstructor,-808110108
+builtin_hash,TypedArrayPrototypeByteLength,450901727
+builtin_hash,TypedArrayPrototypeLength,51309302
+builtin_hash,WeakMapConstructor,-402303863
+builtin_hash,WeakMapLookupHashIndex,94941198
+builtin_hash,WeakMapGet,815434422
+builtin_hash,WeakMapPrototypeHas,514771298
+builtin_hash,WeakMapPrototypeSet,-349184670
+builtin_hash,WeakSetConstructor,-342477008
+builtin_hash,WeakSetPrototypeHas,514771298
+builtin_hash,WeakSetPrototypeAdd,-987480020
+builtin_hash,WeakCollectionSet,-1066277515
+builtin_hash,AsyncGeneratorResolve,-819132993
+builtin_hash,AsyncGeneratorYieldWithAwait,110676265
+builtin_hash,AsyncGeneratorResumeNext,923449615
+builtin_hash,AsyncGeneratorPrototypeNext,-1069429825
+builtin_hash,AsyncGeneratorAwaitUncaught,-489961876
+builtin_hash,AsyncGeneratorAwaitResolveClosure,811649112
+builtin_hash,AsyncGeneratorYieldWithAwaitResolveClosure,-998984780
+builtin_hash,StringAdd_CheckNone,-1024256598
+builtin_hash,SubString,-266358213
+builtin_hash,GetProperty,933894284
+builtin_hash,GetPropertyWithReceiver,-32510004
+builtin_hash,SetProperty,-495376823
+builtin_hash,CreateDataProperty,594399224
+builtin_hash,FindNonDefaultConstructorOrConstruct,366791757
+builtin_hash,ArrayPrototypeConcat,754436794
+builtin_hash,ArrayEvery,-455892525
+builtin_hash,ArrayFilterLoopLazyDeoptContinuation,611749953
+builtin_hash,ArrayFilterLoopContinuation,-833908508
+builtin_hash,ArrayFilter,-683369240
+builtin_hash,ArrayPrototypeFind,865719932
+builtin_hash,ArrayForEachLoopLazyDeoptContinuation,447647825
+builtin_hash,ArrayForEachLoopContinuation,1042683717
+builtin_hash,ArrayForEach,-45990657
+builtin_hash,ArrayFrom,-19171003
+builtin_hash,ArrayIsArray,-862006355
+builtin_hash,LoadJoinElement_FastSmiOrObjectElements_0,525543749
+builtin_hash,LoadJoinElement_FastDoubleElements_0,538796669
+builtin_hash,JoinStackPush,-77767476
+builtin_hash,JoinStackPop,-269387551
+builtin_hash,ArrayPrototypeJoin,-742859465
+builtin_hash,ArrayPrototypeToString,403405778
+builtin_hash,ArrayPrototypeLastIndexOf,536760057
+builtin_hash,ArrayMapLoopLazyDeoptContinuation,-554291199
+builtin_hash,ArrayMapLoopContinuation,746589647
+builtin_hash,ArrayMap,-987743445
+builtin_hash,ArrayReduceLoopLazyDeoptContinuation,-31323243
+builtin_hash,ArrayReduceLoopContinuation,586684706
+builtin_hash,ArrayReduce,-1036824447
+builtin_hash,ArrayPrototypeReverse,-85515243
+builtin_hash,ArrayPrototypeShift,-355152592
+builtin_hash,ArrayPrototypeSlice,-882387995
+builtin_hash,ArraySome,572787735
+builtin_hash,ArrayPrototypeSplice,-993386813
+builtin_hash,ArrayPrototypeUnshift,-757236194
+builtin_hash,ArrayBufferPrototypeGetByteLength,366611265
+builtin_hash,ArrayBufferIsView,-965145146
+builtin_hash,ToInteger,-1031727149
+builtin_hash,FastCreateDataProperty,-85250558
+builtin_hash,BooleanConstructor,-152052498
+builtin_hash,BooleanPrototypeToString,884033636
+builtin_hash,ToString,918542802
+builtin_hash,StringPrototypeToString,-980655357
+builtin_hash,StringPrototypeValueOf,-980655357
+builtin_hash,StringPrototypeCharAt,-180668138
+builtin_hash,StringPrototypeCharCodeAt,-872019738
+builtin_hash,StringPrototypeCodePointAt,187584823
+builtin_hash,StringPrototypeConcat,897315329
+builtin_hash,StringConstructor,-742161575
+builtin_hash,StringAddConvertLeft,464769062
+builtin_hash,StringAddConvertRight,319266024
+builtin_hash,StringCharAt,864970334
+builtin_hash,FastNewClosureBaseline,163793209
+builtin_hash,FastNewFunctionContextFunction,317653702
+builtin_hash,CreateRegExpLiteral,1058887503
+builtin_hash,CreateShallowArrayLiteral,-330514735
+builtin_hash,CreateEmptyArrayLiteral,833695600
+builtin_hash,CreateShallowObjectLiteral,-29649147
+builtin_hash,ObjectConstructor,741451926
+builtin_hash,CreateEmptyLiteralObject,-766695182
+builtin_hash,NumberConstructor,782234117
+builtin_hash,StringToNumber,-992596056
+builtin_hash,NonNumberToNumber,-970153117
+builtin_hash,NonNumberToNumeric,-280530476
+builtin_hash,ToNumeric,874672566
+builtin_hash,NumberToString,-397868165
+builtin_hash,ToBoolean,-1062838663
+builtin_hash,ToBooleanForBaselineJump,358716591
+builtin_hash,ToLength,-759915327
+builtin_hash,ToName,820424209
+builtin_hash,ToObject,-125924279
+builtin_hash,NonPrimitiveToPrimitive_Default,687286659
+builtin_hash,NonPrimitiveToPrimitive_Number,687286659
+builtin_hash,NonPrimitiveToPrimitive_String,687286659
+builtin_hash,OrdinaryToPrimitive_Number,172366650
+builtin_hash,OrdinaryToPrimitive_String,172366650
+builtin_hash,DataViewPrototypeGetByteLength,686110725
+builtin_hash,DataViewPrototypeGetFloat64,895164724
+builtin_hash,DataViewPrototypeSetUint32,729463948
+builtin_hash,DataViewPrototypeSetFloat64,946935329
+builtin_hash,FunctionPrototypeHasInstance,-111878689
+builtin_hash,FastFunctionPrototypeBind,733410738
+builtin_hash,ForInNext,695701361
+builtin_hash,GetIteratorWithFeedback,656782758
+builtin_hash,GetIteratorBaseline,561837844
+builtin_hash,CallIteratorWithFeedback,-555674017
+builtin_hash,MathAbs,679934506
+builtin_hash,MathCeil,-379506221
+builtin_hash,MathFloor,-953911531
+builtin_hash,MathRound,402490573
+builtin_hash,MathPow,-180289611
+builtin_hash,MathMax,178146456
+builtin_hash,MathMin,751748840
+builtin_hash,MathAsin,816104342
+builtin_hash,MathAtan2,-86622779
+builtin_hash,MathCos,-22074993
+builtin_hash,MathExp,-588443238
+builtin_hash,MathFround,400378764
+builtin_hash,MathImul,1019157426
+builtin_hash,MathLog,-97836428
+builtin_hash,MathSin,14431085
+builtin_hash,MathSign,728598572
+builtin_hash,MathSqrt,229011554
+builtin_hash,MathTan,-1012302844
+builtin_hash,MathTanh,1011823357
+builtin_hash,MathRandom,-676034213
+builtin_hash,NumberPrototypeToString,-162258996
+builtin_hash,NumberIsInteger,-384800121
+builtin_hash,NumberIsNaN,820755905
+builtin_hash,NumberParseFloat,-375043449
+builtin_hash,ParseInt,18014642
+builtin_hash,NumberParseInt,63997416
+builtin_hash,Add,-954609031
+builtin_hash,Subtract,289942982
+builtin_hash,Multiply,523374153
+builtin_hash,Divide,804521858
+builtin_hash,Modulus,949112112
+builtin_hash,CreateObjectWithoutProperties,-1064462911
+builtin_hash,ObjectIsExtensible,284344234
+builtin_hash,ObjectPreventExtensions,-111120885
+builtin_hash,ObjectGetPrototypeOf,913438424
+builtin_hash,ObjectSetPrototypeOf,121738554
+builtin_hash,ObjectPrototypeToString,-282402028
+builtin_hash,ObjectPrototypeValueOf,-694058811
+builtin_hash,FulfillPromise,-550390507
+builtin_hash,NewPromiseCapability,505672440
+builtin_hash,PromiseCapabilityDefaultResolve,-866352423
+builtin_hash,PerformPromiseThen,-560636632
+builtin_hash,PromiseAll,17219024
+builtin_hash,PromiseAllResolveElementClosure,95365838
+builtin_hash,PromiseConstructor,161136799
+builtin_hash,PromisePrototypeCatch,815820860
+builtin_hash,PromiseFulfillReactionJob,274407576
+builtin_hash,PromiseResolveTrampoline,-167703057
+builtin_hash,PromiseResolve,907966935
+builtin_hash,ResolvePromise,223087443
+builtin_hash,PromisePrototypeThen,844580362
+builtin_hash,PromiseResolveThenableJob,655377974
+builtin_hash,ProxyConstructor,-22029616
+builtin_hash,ProxyGetProperty,272083385
+builtin_hash,ProxyIsExtensible,-454160590
+builtin_hash,ProxyPreventExtensions,-632485167
+builtin_hash,ReflectGet,326106219
+builtin_hash,ReflectHas,-167703057
+builtin_hash,RegExpPrototypeExec,-813728440
+builtin_hash,RegExpMatchFast,166162007
+builtin_hash,RegExpReplace,-143637874
+builtin_hash,RegExpPrototypeReplace,-459816201
+builtin_hash,RegExpSearchFast,1000243168
+builtin_hash,RegExpPrototypeSourceGetter,648178085
+builtin_hash,RegExpSplit,-714662060
+builtin_hash,RegExpPrototypeTest,1046472002
+builtin_hash,RegExpPrototypeTestFast,-798233488
+builtin_hash,RegExpPrototypeGlobalGetter,-203267211
+builtin_hash,RegExpPrototypeIgnoreCaseGetter,-465412178
+builtin_hash,RegExpPrototypeMultilineGetter,-508767992
+builtin_hash,RegExpPrototypeHasIndicesGetter,-282842269
+builtin_hash,RegExpPrototypeDotAllGetter,192996811
+builtin_hash,RegExpPrototypeStickyGetter,134953677
+builtin_hash,RegExpPrototypeUnicodeGetter,-249845547
+builtin_hash,RegExpPrototypeFlagsGetter,921172498
+builtin_hash,StringPrototypeEndsWith,-918116877
+builtin_hash,StringPrototypeIncludes,941923285
+builtin_hash,StringPrototypeIndexOf,806674611
+builtin_hash,StringPrototypeIterator,-876353408
+builtin_hash,StringIteratorPrototypeNext,-654477549
+builtin_hash,StringPrototypeMatch,-595127381
+builtin_hash,StringPrototypeSearch,-595127381
+builtin_hash,StringRepeat,-561250183
+builtin_hash,StringPrototypeSlice,-775888791
+builtin_hash,StringPrototypeStartsWith,-277204680
+builtin_hash,StringPrototypeSubstr,-947069151
+builtin_hash,StringPrototypeSubstring,-978368636
+builtin_hash,StringPrototypeTrim,274893597
+builtin_hash,SymbolPrototypeToString,-185434595
+builtin_hash,CreateTypedArray,1033298460
+builtin_hash,TypedArrayFrom,-367584045
+builtin_hash,TypedArrayPrototypeSet,-63776151
+builtin_hash,TypedArrayPrototypeSubArray,-493435057
+builtin_hash,NewSloppyArgumentsElements,678456613
+builtin_hash,NewStrictArgumentsElements,185097327
+builtin_hash,NewRestArgumentsElements,619724658
+builtin_hash,FastNewSloppyArguments,-80034412
+builtin_hash,FastNewStrictArguments,-464051478
+builtin_hash,FastNewRestArguments,685808482
+builtin_hash,StringSlowFlatten,-784143839
+builtin_hash,StringIndexOf,-474260096
+builtin_hash,Load_FastSmiElements_0,379894944
+builtin_hash,Load_FastObjectElements_0,379894944
+builtin_hash,Store_FastSmiElements_0,-902376010
+builtin_hash,Store_FastObjectElements_0,66846279
+builtin_hash,SortCompareDefault,622990416
+builtin_hash,SortCompareUserFn,-475979535
+builtin_hash,Copy,-597822670
+builtin_hash,MergeAt,623377186
+builtin_hash,GallopLeft,-889285900
+builtin_hash,GallopRight,-999899286
+builtin_hash,ArrayTimSort,439597300
+builtin_hash,ArrayPrototypeSort,-257800026
+builtin_hash,StringFastLocaleCompare,-322387781
+builtin_hash,WasmInt32ToHeapNumber,642966430
+builtin_hash,WasmTaggedNonSmiToInt32,721903651
+builtin_hash,WasmTriggerTierUp,-81706558
+builtin_hash,WasmStackGuard,-758328907
+builtin_hash,CanUseSameAccessor_FastSmiElements_0,895246524
+builtin_hash,CanUseSameAccessor_FastObjectElements_0,895246524
+builtin_hash,StringPrototypeToLowerCaseIntl,-989408672
+builtin_hash,StringToLowerCaseIntl,-921557299
+builtin_hash,WideHandler,-909873635
+builtin_hash,ExtraWideHandler,-909873635
+builtin_hash,LdarHandler,436211825
+builtin_hash,LdaZeroHandler,-600556494
+builtin_hash,LdaSmiHandler,-976000524
+builtin_hash,LdaUndefinedHandler,-443904601
+builtin_hash,LdaNullHandler,-443904601
+builtin_hash,LdaTheHoleHandler,-443904601
+builtin_hash,LdaTrueHandler,-1005639650
+builtin_hash,LdaFalseHandler,-1005639650
+builtin_hash,LdaConstantHandler,266070913
+builtin_hash,LdaContextSlotHandler,660734865
+builtin_hash,LdaImmutableContextSlotHandler,660734865
+builtin_hash,LdaCurrentContextSlotHandler,864850451
+builtin_hash,LdaImmutableCurrentContextSlotHandler,864850451
+builtin_hash,StarHandler,-205182408
+builtin_hash,MovHandler,-701250297
+builtin_hash,PushContextHandler,-20849192
+builtin_hash,PopContextHandler,711206742
+builtin_hash,TestReferenceEqualHandler,-497662813
+builtin_hash,TestUndetectableHandler,-791662564
+builtin_hash,TestNullHandler,-70530481
+builtin_hash,TestUndefinedHandler,-70530481
+builtin_hash,TestTypeOfHandler,-392059484
+builtin_hash,LdaGlobalHandler,875012517
+builtin_hash,LdaGlobalInsideTypeofHandler,-679405654
+builtin_hash,StaGlobalHandler,-566520598
+builtin_hash,StaContextSlotHandler,-1033911292
+builtin_hash,StaCurrentContextSlotHandler,-78459139
+builtin_hash,LdaLookupGlobalSlotHandler,779325960
+builtin_hash,LdaLookupGlobalSlotInsideTypeofHandler,788806266
+builtin_hash,StaLookupSlotHandler,440700011
+builtin_hash,GetNamedPropertyHandler,309200467
+builtin_hash,GetNamedPropertyFromSuperHandler,-449343494
+builtin_hash,GetKeyedPropertyHandler,-379193937
+builtin_hash,SetNamedPropertyHandler,514822198
+builtin_hash,DefineNamedOwnPropertyHandler,514822198
+builtin_hash,SetKeyedPropertyHandler,642510192
+builtin_hash,DefineKeyedOwnPropertyHandler,642510192
+builtin_hash,StaInArrayLiteralHandler,642510192
+builtin_hash,DefineKeyedOwnPropertyInLiteralHandler,1040795086
+builtin_hash,AddHandler,-846364031
+builtin_hash,SubHandler,-947310537
+builtin_hash,MulHandler,-898838623
+builtin_hash,DivHandler,-601015277
+builtin_hash,ModHandler,-618002607
+builtin_hash,ExpHandler,48455184
+builtin_hash,BitwiseOrHandler,-782248736
+builtin_hash,BitwiseXorHandler,355393221
+builtin_hash,BitwiseAndHandler,568661089
+builtin_hash,ShiftLeftHandler,-289287193
+builtin_hash,ShiftRightHandler,468833118
+builtin_hash,ShiftRightLogicalHandler,1067162459
+builtin_hash,AddSmiHandler,-819173902
+builtin_hash,SubSmiHandler,-89503362
+builtin_hash,MulSmiHandler,-67264102
+builtin_hash,DivSmiHandler,200036936
+builtin_hash,ModSmiHandler,430435649
+builtin_hash,BitwiseOrSmiHandler,755686576
+builtin_hash,BitwiseXorSmiHandler,-713121388
+builtin_hash,BitwiseAndSmiHandler,-35533849
+builtin_hash,ShiftLeftSmiHandler,219116013
+builtin_hash,ShiftRightSmiHandler,-486366353
+builtin_hash,ShiftRightLogicalSmiHandler,561278751
+builtin_hash,IncHandler,-31417017
+builtin_hash,DecHandler,-90685787
+builtin_hash,NegateHandler,145072098
+builtin_hash,BitwiseNotHandler,401993486
+builtin_hash,ToBooleanLogicalNotHandler,741088768
+builtin_hash,LogicalNotHandler,-58795339
+builtin_hash,TypeOfHandler,-388431360
+builtin_hash,DeletePropertyStrictHandler,-1037108511
+builtin_hash,DeletePropertySloppyHandler,-37188680
+builtin_hash,FindNonDefaultConstructorOrConstructHandler,-861606229
+builtin_hash,CallAnyReceiverHandler,1020222647
+builtin_hash,CallPropertyHandler,1020222647
+builtin_hash,CallProperty0Handler,224132667
+builtin_hash,CallProperty1Handler,-100562732
+builtin_hash,CallProperty2Handler,263714692
+builtin_hash,CallUndefinedReceiverHandler,7784819
+builtin_hash,CallUndefinedReceiver0Handler,-108466145
+builtin_hash,CallUndefinedReceiver1Handler,-579782138
+builtin_hash,CallUndefinedReceiver2Handler,615684548
+builtin_hash,CallWithSpreadHandler,1020222647
+builtin_hash,CallRuntimeHandler,-822340864
+builtin_hash,CallJSRuntimeHandler,-926246438
+builtin_hash,InvokeIntrinsicHandler,-8360555
+builtin_hash,ConstructHandler,-272587889
+builtin_hash,ConstructWithSpreadHandler,-372839015
+builtin_hash,TestEqualHandler,374283527
+builtin_hash,TestEqualStrictHandler,626523511
+builtin_hash,TestLessThanHandler,-794234108
+builtin_hash,TestGreaterThanHandler,735449711
+builtin_hash,TestLessThanOrEqualHandler,297739786
+builtin_hash,TestGreaterThanOrEqualHandler,-670596979
+builtin_hash,TestInstanceOfHandler,296390618
+builtin_hash,TestInHandler,510814407
+builtin_hash,ToNameHandler,137741473
+builtin_hash,ToNumberHandler,-476034023
+builtin_hash,ToNumericHandler,-258785245
+builtin_hash,ToObjectHandler,137741473
+builtin_hash,ToStringHandler,-664830208
+builtin_hash,CreateRegExpLiteralHandler,367775340
+builtin_hash,CreateArrayLiteralHandler,1024172358
+builtin_hash,CreateArrayFromIterableHandler,-1040575832
+builtin_hash,CreateEmptyArrayLiteralHandler,-266274507
+builtin_hash,CreateObjectLiteralHandler,-1061604397
+builtin_hash,CreateEmptyObjectLiteralHandler,725849533
+builtin_hash,CreateClosureHandler,-785694825
+builtin_hash,CreateBlockContextHandler,336437386
+builtin_hash,CreateCatchContextHandler,-591858337
+builtin_hash,CreateFunctionContextHandler,831535155
+builtin_hash,CreateMappedArgumentsHandler,690946433
+builtin_hash,CreateUnmappedArgumentsHandler,91752283
+builtin_hash,CreateRestParameterHandler,171972945
+builtin_hash,JumpLoopHandler,-538188868
+builtin_hash,JumpHandler,-127048246
+builtin_hash,JumpConstantHandler,9123603
+builtin_hash,JumpIfUndefinedConstantHandler,-235965049
+builtin_hash,JumpIfNotUndefinedConstantHandler,1037899806
+builtin_hash,JumpIfUndefinedOrNullConstantHandler,142717992
+builtin_hash,JumpIfTrueConstantHandler,-235965049
+builtin_hash,JumpIfFalseConstantHandler,-235965049
+builtin_hash,JumpIfToBooleanTrueConstantHandler,-924023865
+builtin_hash,JumpIfToBooleanFalseConstantHandler,163722448
+builtin_hash,JumpIfToBooleanTrueHandler,143047998
+builtin_hash,JumpIfToBooleanFalseHandler,132155748
+builtin_hash,JumpIfTrueHandler,509710878
+builtin_hash,JumpIfFalseHandler,509710878
+builtin_hash,JumpIfNullHandler,509710878
+builtin_hash,JumpIfNotNullHandler,1067368128
+builtin_hash,JumpIfUndefinedHandler,509710878
+builtin_hash,JumpIfNotUndefinedHandler,1067368128
+builtin_hash,JumpIfUndefinedOrNullHandler,388158191
+builtin_hash,JumpIfJSReceiverHandler,-1017385873
+builtin_hash,SwitchOnSmiNoFeedbackHandler,-713553393
+builtin_hash,ForInEnumerateHandler,-856728459
+builtin_hash,ForInPrepareHandler,218804922
+builtin_hash,ForInContinueHandler,-793028055
+builtin_hash,ForInNextHandler,715608170
+builtin_hash,ForInStepHandler,904745987
+builtin_hash,SetPendingMessageHandler,-797838352
+builtin_hash,ThrowHandler,640637055
+builtin_hash,ReThrowHandler,640637055
+builtin_hash,ReturnHandler,-751460073
+builtin_hash,ThrowReferenceErrorIfHoleHandler,544325925
+builtin_hash,ThrowSuperNotCalledIfHoleHandler,799933248
+builtin_hash,ThrowSuperAlreadyCalledIfNotHoleHandler,397804955
+builtin_hash,ThrowIfNotSuperConstructorHandler,-527607615
+builtin_hash,SwitchOnGeneratorStateHandler,91785332
+builtin_hash,SuspendGeneratorHandler,-82069111
+builtin_hash,ResumeGeneratorHandler,-346239077
+builtin_hash,GetIteratorHandler,828882125
+builtin_hash,ShortStarHandler,773625974
+builtin_hash,LdarWideHandler,-558525052
+builtin_hash,LdaSmiWideHandler,564581323
+builtin_hash,LdaConstantWideHandler,548763663
+builtin_hash,LdaContextSlotWideHandler,618436299
+builtin_hash,LdaImmutableContextSlotWideHandler,618436299
+builtin_hash,LdaImmutableCurrentContextSlotWideHandler,882741548
+builtin_hash,StarWideHandler,-757786971
+builtin_hash,MovWideHandler,-238891687
+builtin_hash,PushContextWideHandler,32753133
+builtin_hash,PopContextWideHandler,476886005
+builtin_hash,TestReferenceEqualWideHandler,753678489
+builtin_hash,LdaGlobalWideHandler,-888894444
+builtin_hash,LdaGlobalInsideTypeofWideHandler,500447961
+builtin_hash,StaGlobalWideHandler,-1015395305
+builtin_hash,StaContextSlotWideHandler,526187909
+builtin_hash,StaCurrentContextSlotWideHandler,475958509
+builtin_hash,LdaLookupGlobalSlotWideHandler,-1071096984
+builtin_hash,GetNamedPropertyWideHandler,817982323
+builtin_hash,GetKeyedPropertyWideHandler,857306089
+builtin_hash,SetNamedPropertyWideHandler,-960786341
+builtin_hash,DefineNamedOwnPropertyWideHandler,-960786341
+builtin_hash,SetKeyedPropertyWideHandler,591604085
+builtin_hash,DefineKeyedOwnPropertyWideHandler,591604085
+builtin_hash,StaInArrayLiteralWideHandler,591604085
+builtin_hash,AddWideHandler,-232204010
+builtin_hash,SubWideHandler,-384964032
+builtin_hash,MulWideHandler,580603117
+builtin_hash,DivWideHandler,63550206
+builtin_hash,BitwiseOrWideHandler,238584526
+builtin_hash,BitwiseAndWideHandler,199219068
+builtin_hash,ShiftLeftWideHandler,-289702637
+builtin_hash,AddSmiWideHandler,-181214576
+builtin_hash,SubSmiWideHandler,797195243
+builtin_hash,MulSmiWideHandler,84206304
+builtin_hash,DivSmiWideHandler,-713648372
+builtin_hash,ModSmiWideHandler,1034577565
+builtin_hash,BitwiseOrSmiWideHandler,636336979
+builtin_hash,BitwiseXorSmiWideHandler,83711264
+builtin_hash,BitwiseAndSmiWideHandler,981310414
+builtin_hash,ShiftLeftSmiWideHandler,-108872348
+builtin_hash,ShiftRightSmiWideHandler,-499188540
+builtin_hash,ShiftRightLogicalSmiWideHandler,868152887
+builtin_hash,IncWideHandler,23321587
+builtin_hash,DecWideHandler,775873375
+builtin_hash,NegateWideHandler,-106212454
+builtin_hash,CallPropertyWideHandler,-98363630
+builtin_hash,CallProperty0WideHandler,376756798
+builtin_hash,CallProperty1WideHandler,232683870
+builtin_hash,CallProperty2WideHandler,-495857513
+builtin_hash,CallUndefinedReceiverWideHandler,-1065355738
+builtin_hash,CallUndefinedReceiver0WideHandler,-94257150
+builtin_hash,CallUndefinedReceiver1WideHandler,639212152
+builtin_hash,CallUndefinedReceiver2WideHandler,1002448783
+builtin_hash,CallWithSpreadWideHandler,-98363630
+builtin_hash,ConstructWideHandler,-449134944
+builtin_hash,TestEqualWideHandler,32535298
+builtin_hash,TestEqualStrictWideHandler,438216084
+builtin_hash,TestLessThanWideHandler,278610142
+builtin_hash,TestGreaterThanWideHandler,468841842
+builtin_hash,TestLessThanOrEqualWideHandler,-427559307
+builtin_hash,TestGreaterThanOrEqualWideHandler,553075788
+builtin_hash,TestInstanceOfWideHandler,-1012072182
+builtin_hash,TestInWideHandler,452328218
+builtin_hash,ToNumericWideHandler,-1069018434
+builtin_hash,CreateRegExpLiteralWideHandler,-907932416
+builtin_hash,CreateArrayLiteralWideHandler,747775518
+builtin_hash,CreateEmptyArrayLiteralWideHandler,-1062432317
+builtin_hash,CreateObjectLiteralWideHandler,711507686
+builtin_hash,CreateClosureWideHandler,-755746622
+builtin_hash,CreateBlockContextWideHandler,-298013473
+builtin_hash,CreateFunctionContextWideHandler,-125332949
+builtin_hash,JumpLoopWideHandler,243486717
+builtin_hash,JumpWideHandler,-127048246
+builtin_hash,JumpIfToBooleanTrueWideHandler,527083439
+builtin_hash,JumpIfToBooleanFalseWideHandler,22533852
+builtin_hash,JumpIfTrueWideHandler,-988555685
+builtin_hash,JumpIfFalseWideHandler,-988555685
+builtin_hash,SwitchOnSmiNoFeedbackWideHandler,-1073273649
+builtin_hash,ForInPrepareWideHandler,364788909
+builtin_hash,ForInNextWideHandler,618775213
+builtin_hash,ThrowReferenceErrorIfHoleWideHandler,-906539593
+builtin_hash,GetIteratorWideHandler,987616579
+builtin_hash,LdaSmiExtraWideHandler,564581323
+builtin_hash,LdaGlobalExtraWideHandler,335586579
+builtin_hash,AddSmiExtraWideHandler,409910492
+builtin_hash,SubSmiExtraWideHandler,-439082683
+builtin_hash,MulSmiExtraWideHandler,-535192892
+builtin_hash,DivSmiExtraWideHandler,-779784508
+builtin_hash,BitwiseOrSmiExtraWideHandler,962616587
+builtin_hash,BitwiseXorSmiExtraWideHandler,-2659951
+builtin_hash,BitwiseAndSmiExtraWideHandler,161080602
+builtin_hash,CallUndefinedReceiverExtraWideHandler,-80876123
+builtin_hash,CallUndefinedReceiver1ExtraWideHandler,-222714708
+builtin_hash,CallUndefinedReceiver2ExtraWideHandler,-437922167
diff -r -u --color up/v8/tools/builtins-pgo/arm64.profile nw/v8/tools/builtins-pgo/arm64.profile
--- up/v8/tools/builtins-pgo/arm64.profile	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/builtins-pgo/arm64.profile	2023-01-19 16:46:37.858109124 -0500
@@ -805,7 +805,6 @@
 block_hint,HasProperty,44,45,0
 block_hint,HasProperty,239,240,1
 block_hint,HasProperty,48,49,0
-block_hint,HasProperty,207,208,1
 block_hint,HasProperty,272,273,0
 block_hint,HasProperty,228,229,0
 block_hint,HasProperty,38,39,0
@@ -889,7 +888,6 @@
 block_hint,SetDataProperties,99,100,0
 block_hint,SetDataProperties,437,438,0
 block_hint,SetDataProperties,241,242,0
-block_hint,SetDataProperties,129,130,0
 block_hint,SetDataProperties,279,280,1
 block_hint,SetDataProperties,204,205,0
 block_hint,SetDataProperties,61,62,0
@@ -1065,7 +1063,6 @@
 block_hint,CloneFastJSArray,37,38,1
 block_hint,CloneFastJSArray,34,35,1
 block_hint,CloneFastJSArray,19,20,1
-block_hint,CloneFastJSArray,8,9,0
 block_hint,CloneFastJSArray,12,13,0
 block_hint,CloneFastJSArray,14,15,1
 block_hint,CloneFastJSArray,42,43,1
@@ -1092,7 +1089,6 @@
 block_hint,ExtractFastJSArray,39,40,1
 block_hint,ExtractFastJSArray,35,36,1
 block_hint,ExtractFastJSArray,20,21,1
-block_hint,ExtractFastJSArray,6,7,0
 block_hint,ExtractFastJSArray,12,13,0
 block_hint,ExtractFastJSArray,14,15,1
 block_hint,ExtractFastJSArray,37,38,1
@@ -1279,6 +1275,7 @@
 block_hint,LoadIC_Megamorphic,282,283,1
 block_hint,LoadIC_Megamorphic,328,329,1
 block_hint,LoadIC_Megamorphic,95,96,0
+block_hint,LoadIC_Megamorphic,97,98,0
 block_hint,LoadIC_Megamorphic,20,21,1
 block_hint,LoadIC_Megamorphic,162,163,0
 block_hint,LoadIC_Megamorphic,287,288,0
@@ -1437,7 +1434,6 @@
 block_hint,KeyedLoadIC_Megamorphic,987,988,1
 block_hint,KeyedLoadIC_Megamorphic,256,257,0
 block_hint,KeyedLoadIC_Megamorphic,656,657,0
-block_hint,KeyedLoadIC_Megamorphic,258,259,1
 block_hint,KeyedLoadIC_Megamorphic,1076,1077,0
 block_hint,KeyedLoadIC_Megamorphic,1169,1170,0
 block_hint,KeyedLoadIC_Megamorphic,1206,1207,1
@@ -1568,6 +1564,7 @@
 block_hint,StoreIC,293,294,1
 block_hint,StoreIC,312,313,1
 block_hint,StoreIC,76,77,0
+block_hint,StoreIC,246,247,0
 block_hint,StoreIC,176,177,0
 block_hint,StoreIC,43,44,1
 block_hint,StoreIC,112,113,0
@@ -1623,7 +1620,6 @@
 block_hint,DefineNamedOwnIC,93,94,0
 block_hint,DefineNamedOwnIC,17,18,0
 block_hint,DefineNamedOwnIC,350,351,0
-block_hint,DefineNamedOwnIC,282,283,1
 block_hint,DefineNamedOwnIC,157,158,1
 block_hint,DefineNamedOwnIC,159,160,1
 block_hint,DefineNamedOwnIC,254,255,1
@@ -1913,7 +1909,6 @@
 block_hint,MapIteratorPrototypeNext,32,33,1
 block_hint,MapIteratorPrototypeNext,19,20,0
 block_hint,MapIteratorPrototypeNext,21,22,0
-block_hint,MapIteratorPrototypeNext,34,35,0
 block_hint,MapIteratorPrototypeNext,7,8,1
 block_hint,MapIteratorPrototypeNext,39,40,1
 block_hint,MapIteratorPrototypeNext,9,10,1
@@ -1975,40 +1970,40 @@
 block_hint,MultiplySmi_Baseline,51,52,1
 block_hint,MultiplySmi_Baseline,34,35,1
 block_hint,MultiplySmi_Baseline,15,16,1
+block_hint,Divide_Baseline,89,90,0
+block_hint,Divide_Baseline,91,92,0
 block_hint,Divide_Baseline,69,70,0
-block_hint,Divide_Baseline,71,72,0
-block_hint,Divide_Baseline,50,51,0
-block_hint,Divide_Baseline,31,32,1
-block_hint,Divide_Baseline,10,11,1
-block_hint,Divide_Baseline,54,55,1
-block_hint,Divide_Baseline,79,80,1
-block_hint,Divide_Baseline,56,57,1
-block_hint,Divide_Baseline,39,40,0
-block_hint,Divide_Baseline,19,20,1
-block_hint,Divide_Baseline,25,26,1
-block_hint,Divide_Baseline,12,13,1
-block_hint,DivideSmi_Baseline,63,64,0
-block_hint,DivideSmi_Baseline,76,77,0
-block_hint,DivideSmi_Baseline,65,66,0
-block_hint,DivideSmi_Baseline,50,51,0
-block_hint,DivideSmi_Baseline,31,32,1
-block_hint,DivideSmi_Baseline,10,11,1
-block_hint,DivideSmi_Baseline,41,42,1
-block_hint,DivideSmi_Baseline,25,26,1
-block_hint,DivideSmi_Baseline,12,13,1
-block_hint,Modulus_Baseline,76,77,0
-block_hint,Modulus_Baseline,72,73,0
-block_hint,Modulus_Baseline,55,56,1
-block_hint,Modulus_Baseline,50,51,1
-block_hint,Modulus_Baseline,18,19,0
-block_hint,Modulus_Baseline,6,7,1
-block_hint,ModulusSmi_Baseline,55,56,1
-block_hint,ModulusSmi_Baseline,50,51,1
-block_hint,ModulusSmi_Baseline,18,19,0
-block_hint,ModulusSmi_Baseline,6,7,1
-block_hint,ModulusSmi_Baseline,40,41,1
-block_hint,ModulusSmi_Baseline,20,21,1
-block_hint,ModulusSmi_Baseline,8,9,1
+block_hint,Divide_Baseline,47,48,1
+block_hint,Divide_Baseline,14,15,1
+block_hint,Divide_Baseline,73,74,1
+block_hint,Divide_Baseline,97,98,1
+block_hint,Divide_Baseline,75,76,1
+block_hint,Divide_Baseline,55,56,0
+block_hint,Divide_Baseline,28,29,1
+block_hint,Divide_Baseline,40,41,1
+block_hint,Divide_Baseline,16,17,1
+block_hint,DivideSmi_Baseline,83,84,0
+block_hint,DivideSmi_Baseline,99,100,0
+block_hint,DivideSmi_Baseline,85,86,0
+block_hint,DivideSmi_Baseline,69,70,0
+block_hint,DivideSmi_Baseline,47,48,1
+block_hint,DivideSmi_Baseline,14,15,1
+block_hint,DivideSmi_Baseline,57,58,1
+block_hint,DivideSmi_Baseline,40,41,1
+block_hint,DivideSmi_Baseline,16,17,1
+block_hint,Modulus_Baseline,108,109,0
+block_hint,Modulus_Baseline,94,95,0
+block_hint,Modulus_Baseline,71,72,1
+block_hint,Modulus_Baseline,66,67,1
+block_hint,Modulus_Baseline,37,38,0
+block_hint,Modulus_Baseline,14,15,1
+block_hint,ModulusSmi_Baseline,71,72,1
+block_hint,ModulusSmi_Baseline,66,67,1
+block_hint,ModulusSmi_Baseline,37,38,0
+block_hint,ModulusSmi_Baseline,14,15,1
+block_hint,ModulusSmi_Baseline,54,55,1
+block_hint,ModulusSmi_Baseline,39,40,1
+block_hint,ModulusSmi_Baseline,16,17,1
 block_hint,BitwiseAnd_Baseline,35,36,0
 block_hint,BitwiseAnd_Baseline,23,24,1
 block_hint,BitwiseAnd_Baseline,8,9,0
@@ -2083,12 +2078,12 @@
 block_hint,Subtract_WithFeedback,76,77,0
 block_hint,Subtract_WithFeedback,53,54,0
 block_hint,Subtract_WithFeedback,23,24,1
-block_hint,Modulus_WithFeedback,76,77,0
-block_hint,Modulus_WithFeedback,72,73,0
-block_hint,Modulus_WithFeedback,55,56,1
-block_hint,Modulus_WithFeedback,50,51,1
-block_hint,Modulus_WithFeedback,18,19,0
-block_hint,Modulus_WithFeedback,6,7,1
+block_hint,Modulus_WithFeedback,108,109,0
+block_hint,Modulus_WithFeedback,94,95,0
+block_hint,Modulus_WithFeedback,71,72,1
+block_hint,Modulus_WithFeedback,66,67,1
+block_hint,Modulus_WithFeedback,37,38,0
+block_hint,Modulus_WithFeedback,14,15,1
 block_hint,BitwiseOr_WithFeedback,6,7,1
 block_hint,BitwiseOr_WithFeedback,35,36,0
 block_hint,BitwiseOr_WithFeedback,23,24,0
@@ -2366,14 +2361,14 @@
 block_hint,ObjectPrototypeHasOwnProperty,171,172,0
 block_hint,ObjectPrototypeHasOwnProperty,178,179,1
 block_hint,ObjectPrototypeHasOwnProperty,58,59,0
-block_hint,ObjectToString,42,43,0
-block_hint,ObjectToString,57,58,0
-block_hint,ObjectToString,65,66,0
-block_hint,ObjectToString,52,53,0
+block_hint,ObjectToString,45,46,0
+block_hint,ObjectToString,60,61,0
+block_hint,ObjectToString,68,69,0
+block_hint,ObjectToString,55,56,0
 block_hint,ObjectToString,7,8,1
 block_hint,ObjectToString,5,6,1
 block_hint,ObjectToString,11,12,1
-block_hint,ObjectToString,19,20,0
+block_hint,ObjectToString,20,21,0
 block_hint,InstanceOf_WithFeedback,50,51,1
 block_hint,InstanceOf_WithFeedback,52,53,0
 block_hint,InstanceOf_WithFeedback,54,55,1
@@ -2813,7 +2808,7 @@
 block_hint,GetProperty,147,148,0
 block_hint,GetProperty,187,188,1
 block_hint,GetProperty,41,42,0
-block_hint,GetProperty,149,150,1
+block_hint,GetProperty,43,44,0
 block_hint,GetProperty,157,158,0
 block_hint,GetProperty,161,162,1
 block_hint,GetProperty,151,152,0
@@ -2899,6 +2894,12 @@
 block_hint,CreateDataProperty,55,56,1
 block_hint,CreateDataProperty,543,544,1
 block_hint,CreateDataProperty,57,58,1
+block_hint,FindNonDefaultConstructorOrConstruct,12,13,0
+block_hint,FindNonDefaultConstructorOrConstruct,6,7,0
+block_hint,FindNonDefaultConstructorOrConstruct,14,15,1
+block_hint,FindNonDefaultConstructorOrConstruct,16,17,0
+block_hint,FindNonDefaultConstructorOrConstruct,4,5,1
+block_hint,FindNonDefaultConstructorOrConstruct,18,19,1
 block_hint,ArrayPrototypeConcat,79,80,1
 block_hint,ArrayPrototypeConcat,54,55,0
 block_hint,ArrayPrototypeConcat,63,64,1
@@ -2927,12 +2928,11 @@
 block_hint,ArrayPrototypeConcat,11,12,1
 block_hint,ArrayEvery,73,74,1
 block_hint,ArrayEvery,31,32,0
-block_hint,ArrayEvery,125,126,1
-block_hint,ArrayEvery,117,118,1
+block_hint,ArrayEvery,122,123,1
+block_hint,ArrayEvery,116,117,1
 block_hint,ArrayEvery,91,92,1
 block_hint,ArrayEvery,93,94,1
 block_hint,ArrayEvery,99,100,0
-block_hint,ArrayEvery,121,122,0
 block_hint,ArrayEvery,105,106,1
 block_hint,ArrayEvery,107,108,1
 block_hint,ArrayEvery,97,98,1
@@ -2955,72 +2955,70 @@
 block_hint,ArrayEvery,89,90,0
 block_hint,ArrayEvery,111,112,0
 block_hint,ArrayEvery,79,80,0
-block_hint,ArrayFilter,198,199,1
-block_hint,ArrayFilter,83,84,0
-block_hint,ArrayFilter,301,302,1
-block_hint,ArrayFilter,290,291,1
-block_hint,ArrayFilter,228,229,1
-block_hint,ArrayFilter,230,231,1
-block_hint,ArrayFilter,249,250,0
-block_hint,ArrayFilter,299,300,0
-block_hint,ArrayFilter,273,274,1
-block_hint,ArrayFilter,275,276,1
-block_hint,ArrayFilter,242,243,0
-block_hint,ArrayFilter,279,280,1
-block_hint,ArrayFilter,200,201,1
-block_hint,ArrayFilter,126,127,1
-block_hint,ArrayFilter,23,24,1
-block_hint,ArrayFilter,202,203,1
-block_hint,ArrayFilter,128,129,0
-block_hint,ArrayFilter,25,26,1
-block_hint,ArrayFilter,270,271,1
-block_hint,ArrayFilter,169,170,0
-block_hint,ArrayFilter,281,282,1
-block_hint,ArrayFilter,204,205,1
-block_hint,ArrayFilter,130,131,1
-block_hint,ArrayFilter,27,28,1
-block_hint,ArrayFilter,214,215,1
-block_hint,ArrayFilter,216,217,0
+block_hint,ArrayFilter,195,196,1
+block_hint,ArrayFilter,84,85,0
+block_hint,ArrayFilter,295,296,1
 block_hint,ArrayFilter,286,287,1
-block_hint,ArrayFilter,218,219,1
-block_hint,ArrayFilter,220,221,1
-block_hint,ArrayFilter,222,223,1
-block_hint,ArrayFilter,206,207,1
-block_hint,ArrayFilter,132,133,0
-block_hint,ArrayFilter,29,30,1
-block_hint,ArrayFilter,174,175,0
-block_hint,ArrayFilter,106,107,0
-block_hint,ArrayFilter,245,246,1
-block_hint,ArrayFilter,247,248,0
-block_hint,ArrayFilter,208,209,0
-block_hint,ArrayFilter,134,135,0
-block_hint,ArrayFilter,41,42,0
-block_hint,ArrayFilter,43,44,1
-block_hint,ArrayFilter,152,153,0
-block_hint,ArrayFilter,252,253,1
-block_hint,ArrayFilter,180,181,0
-block_hint,ArrayFilter,182,183,0
-block_hint,ArrayFilter,254,255,0
-block_hint,ArrayFilter,256,257,0
-block_hint,ArrayFilter,258,259,1
-block_hint,ArrayFilter,260,261,0
-block_hint,ArrayFilter,262,263,1
-block_hint,ArrayFilter,283,284,0
-block_hint,ArrayFilter,240,241,0
-block_hint,ArrayFilter,164,165,0
-block_hint,ArrayFilter,98,99,0
-block_hint,ArrayFilter,190,191,1
-block_hint,ArrayFilter,59,60,0
-block_hint,ArrayFilter,63,64,1
-block_hint,ArrayFilter,49,50,1
+block_hint,ArrayFilter,225,226,1
+block_hint,ArrayFilter,227,228,1
+block_hint,ArrayFilter,246,247,0
+block_hint,ArrayFilter,270,271,1
+block_hint,ArrayFilter,272,273,1
+block_hint,ArrayFilter,239,240,0
+block_hint,ArrayFilter,276,277,1
+block_hint,ArrayFilter,197,198,1
+block_hint,ArrayFilter,123,124,1
+block_hint,ArrayFilter,22,23,1
+block_hint,ArrayFilter,199,200,1
+block_hint,ArrayFilter,125,126,0
+block_hint,ArrayFilter,24,25,1
+block_hint,ArrayFilter,267,268,1
+block_hint,ArrayFilter,166,167,0
+block_hint,ArrayFilter,278,279,1
+block_hint,ArrayFilter,201,202,1
+block_hint,ArrayFilter,127,128,1
+block_hint,ArrayFilter,26,27,1
+block_hint,ArrayFilter,211,212,1
+block_hint,ArrayFilter,213,214,0
+block_hint,ArrayFilter,283,284,1
+block_hint,ArrayFilter,215,216,1
+block_hint,ArrayFilter,217,218,1
+block_hint,ArrayFilter,219,220,1
+block_hint,ArrayFilter,203,204,1
+block_hint,ArrayFilter,129,130,0
+block_hint,ArrayFilter,28,29,1
+block_hint,ArrayFilter,171,172,0
+block_hint,ArrayFilter,103,104,0
+block_hint,ArrayFilter,242,243,1
+block_hint,ArrayFilter,244,245,0
+block_hint,ArrayFilter,205,206,0
+block_hint,ArrayFilter,131,132,0
+block_hint,ArrayFilter,42,43,0
+block_hint,ArrayFilter,44,45,1
+block_hint,ArrayFilter,149,150,0
+block_hint,ArrayFilter,249,250,1
+block_hint,ArrayFilter,177,178,0
+block_hint,ArrayFilter,179,180,0
+block_hint,ArrayFilter,251,252,0
+block_hint,ArrayFilter,253,254,0
+block_hint,ArrayFilter,255,256,1
+block_hint,ArrayFilter,257,258,0
+block_hint,ArrayFilter,259,260,1
+block_hint,ArrayFilter,280,281,0
+block_hint,ArrayFilter,237,238,0
+block_hint,ArrayFilter,161,162,0
+block_hint,ArrayFilter,95,96,0
+block_hint,ArrayFilter,187,188,1
+block_hint,ArrayFilter,60,61,0
+block_hint,ArrayFilter,64,65,1
+block_hint,ArrayFilter,50,51,1
 block_hint,ArrayForEach,70,71,1
 block_hint,ArrayForEach,29,30,0
-block_hint,ArrayForEach,102,103,1
-block_hint,ArrayForEach,96,97,1
+block_hint,ArrayForEach,99,100,1
+block_hint,ArrayForEach,95,96,1
 block_hint,ArrayForEach,76,77,1
 block_hint,ArrayForEach,78,79,1
 block_hint,ArrayForEach,84,85,0
-block_hint,ArrayForEach,100,101,0
 block_hint,ArrayForEach,90,91,1
 block_hint,ArrayForEach,92,93,1
 block_hint,ArrayForEach,47,48,0
@@ -3073,93 +3071,93 @@
 block_hint,LoadJoinElement_FastDoubleElements_0,3,4,1
 block_hint,LoadJoinElement_FastDoubleElements_0,5,6,0
 block_hint,LoadJoinElement_FastDoubleElements_0,7,8,1
-block_hint,JoinStackPush,30,31,1
+block_hint,JoinStackPush,28,29,1
 block_hint,JoinStackPush,6,7,1
 block_hint,JoinStackPush,10,11,0
 block_hint,JoinStackPop,9,10,1
 block_hint,JoinStackPop,4,5,1
-block_hint,ArrayPrototypeJoin,524,525,1
-block_hint,ArrayPrototypeJoin,462,463,1
-block_hint,ArrayPrototypeJoin,425,426,1
-block_hint,ArrayPrototypeJoin,340,341,1
-block_hint,ArrayPrototypeJoin,342,343,1
-block_hint,ArrayPrototypeJoin,373,374,1
-block_hint,ArrayPrototypeJoin,346,347,0
-block_hint,ArrayPrototypeJoin,185,186,0
-block_hint,ArrayPrototypeJoin,480,481,1
-block_hint,ArrayPrototypeJoin,446,447,1
-block_hint,ArrayPrototypeJoin,334,335,0
-block_hint,ArrayPrototypeJoin,234,235,1
+block_hint,ArrayPrototypeJoin,518,519,1
+block_hint,ArrayPrototypeJoin,456,457,1
+block_hint,ArrayPrototypeJoin,419,420,1
+block_hint,ArrayPrototypeJoin,334,335,1
+block_hint,ArrayPrototypeJoin,336,337,1
+block_hint,ArrayPrototypeJoin,367,368,1
+block_hint,ArrayPrototypeJoin,340,341,0
+block_hint,ArrayPrototypeJoin,179,180,0
+block_hint,ArrayPrototypeJoin,474,475,1
+block_hint,ArrayPrototypeJoin,440,441,1
+block_hint,ArrayPrototypeJoin,328,329,0
+block_hint,ArrayPrototypeJoin,228,229,1
 block_hint,ArrayPrototypeJoin,30,31,1
-block_hint,ArrayPrototypeJoin,187,188,0
+block_hint,ArrayPrototypeJoin,181,182,0
 block_hint,ArrayPrototypeJoin,32,33,1
-block_hint,ArrayPrototypeJoin,393,394,1
-block_hint,ArrayPrototypeJoin,331,332,0
-block_hint,ArrayPrototypeJoin,149,150,1
-block_hint,ArrayPrototypeJoin,499,500,1
-block_hint,ArrayPrototypeJoin,464,465,0
-block_hint,ArrayPrototypeJoin,429,430,0
-block_hint,ArrayPrototypeJoin,375,376,1
-block_hint,ArrayPrototypeJoin,189,190,1
-block_hint,ArrayPrototypeJoin,38,39,1
-block_hint,ArrayPrototypeJoin,466,467,1
-block_hint,ArrayPrototypeJoin,431,432,0
-block_hint,ArrayPrototypeJoin,301,302,1
-block_hint,ArrayPrototypeJoin,434,435,0
-block_hint,ArrayPrototypeJoin,351,352,0
-block_hint,ArrayPrototypeJoin,195,196,0
-block_hint,ArrayPrototypeJoin,238,239,1
-block_hint,ArrayPrototypeJoin,151,152,1
-block_hint,ArrayPrototypeJoin,489,490,0
+block_hint,ArrayPrototypeJoin,387,388,1
+block_hint,ArrayPrototypeJoin,325,326,0
+block_hint,ArrayPrototypeJoin,143,144,1
 block_hint,ArrayPrototypeJoin,493,494,1
-block_hint,ArrayPrototypeJoin,531,532,0
-block_hint,ArrayPrototypeJoin,527,528,0
-block_hint,ArrayPrototypeJoin,520,521,1
-block_hint,ArrayPrototypeJoin,495,496,1
-block_hint,ArrayPrototypeJoin,491,492,0
-block_hint,ArrayPrototypeJoin,153,154,0
-block_hint,ArrayPrototypeJoin,155,156,0
-block_hint,ArrayPrototypeJoin,475,476,0
-block_hint,ArrayPrototypeJoin,477,478,0
+block_hint,ArrayPrototypeJoin,458,459,0
+block_hint,ArrayPrototypeJoin,423,424,0
+block_hint,ArrayPrototypeJoin,369,370,1
+block_hint,ArrayPrototypeJoin,183,184,1
+block_hint,ArrayPrototypeJoin,38,39,1
 block_hint,ArrayPrototypeJoin,460,461,1
+block_hint,ArrayPrototypeJoin,425,426,0
+block_hint,ArrayPrototypeJoin,295,296,1
+block_hint,ArrayPrototypeJoin,428,429,0
+block_hint,ArrayPrototypeJoin,345,346,0
+block_hint,ArrayPrototypeJoin,189,190,0
+block_hint,ArrayPrototypeJoin,232,233,1
+block_hint,ArrayPrototypeJoin,145,146,1
+block_hint,ArrayPrototypeJoin,483,484,0
+block_hint,ArrayPrototypeJoin,487,488,1
+block_hint,ArrayPrototypeJoin,525,526,0
+block_hint,ArrayPrototypeJoin,521,522,0
+block_hint,ArrayPrototypeJoin,514,515,1
+block_hint,ArrayPrototypeJoin,489,490,1
+block_hint,ArrayPrototypeJoin,485,486,0
+block_hint,ArrayPrototypeJoin,147,148,0
+block_hint,ArrayPrototypeJoin,149,150,0
+block_hint,ArrayPrototypeJoin,469,470,0
+block_hint,ArrayPrototypeJoin,471,472,0
+block_hint,ArrayPrototypeJoin,454,455,1
+block_hint,ArrayPrototypeJoin,407,408,1
+block_hint,ArrayPrototypeJoin,409,410,1
+block_hint,ArrayPrototypeJoin,411,412,1
 block_hint,ArrayPrototypeJoin,413,414,1
-block_hint,ArrayPrototypeJoin,415,416,1
-block_hint,ArrayPrototypeJoin,417,418,1
-block_hint,ArrayPrototypeJoin,419,420,1
-block_hint,ArrayPrototypeJoin,203,204,1
-block_hint,ArrayPrototypeJoin,260,261,0
+block_hint,ArrayPrototypeJoin,197,198,1
+block_hint,ArrayPrototypeJoin,254,255,0
+block_hint,ArrayPrototypeJoin,256,257,0
+block_hint,ArrayPrototypeJoin,302,303,0
 block_hint,ArrayPrototypeJoin,262,263,0
-block_hint,ArrayPrototypeJoin,308,309,0
-block_hint,ArrayPrototypeJoin,268,269,0
+block_hint,ArrayPrototypeJoin,264,265,0
+block_hint,ArrayPrototypeJoin,203,204,1
+block_hint,ArrayPrototypeJoin,72,73,1
+block_hint,ArrayPrototypeJoin,381,382,1
+block_hint,ArrayPrototypeJoin,305,306,0
+block_hint,ArrayPrototypeJoin,207,208,1
 block_hint,ArrayPrototypeJoin,270,271,0
+block_hint,ArrayPrototypeJoin,272,273,0
 block_hint,ArrayPrototypeJoin,209,210,1
-block_hint,ArrayPrototypeJoin,74,75,1
-block_hint,ArrayPrototypeJoin,387,388,1
+block_hint,ArrayPrototypeJoin,86,87,1
+block_hint,ArrayPrototypeJoin,307,308,0
+block_hint,ArrayPrototypeJoin,219,220,1
+block_hint,ArrayPrototypeJoin,102,103,0
+block_hint,ArrayPrototypeJoin,104,105,0
+block_hint,ArrayPrototypeJoin,435,436,1
+block_hint,ArrayPrototypeJoin,403,404,1
+block_hint,ArrayPrototypeJoin,217,218,1
+block_hint,ArrayPrototypeJoin,100,101,0
+block_hint,ArrayPrototypeJoin,433,434,1
+block_hint,ArrayPrototypeJoin,401,402,1
+block_hint,ArrayPrototypeJoin,96,97,1
+block_hint,ArrayPrototypeJoin,352,353,1
 block_hint,ArrayPrototypeJoin,311,312,0
-block_hint,ArrayPrototypeJoin,213,214,1
-block_hint,ArrayPrototypeJoin,276,277,0
-block_hint,ArrayPrototypeJoin,278,279,0
-block_hint,ArrayPrototypeJoin,215,216,1
-block_hint,ArrayPrototypeJoin,90,91,1
-block_hint,ArrayPrototypeJoin,313,314,0
-block_hint,ArrayPrototypeJoin,225,226,1
+block_hint,ArrayPrototypeJoin,215,216,0
+block_hint,ArrayPrototypeJoin,106,107,1
 block_hint,ArrayPrototypeJoin,108,109,0
-block_hint,ArrayPrototypeJoin,110,111,0
-block_hint,ArrayPrototypeJoin,441,442,1
-block_hint,ArrayPrototypeJoin,409,410,1
-block_hint,ArrayPrototypeJoin,223,224,1
-block_hint,ArrayPrototypeJoin,106,107,0
-block_hint,ArrayPrototypeJoin,439,440,1
-block_hint,ArrayPrototypeJoin,407,408,1
-block_hint,ArrayPrototypeJoin,102,103,1
-block_hint,ArrayPrototypeJoin,358,359,1
-block_hint,ArrayPrototypeJoin,317,318,0
-block_hint,ArrayPrototypeJoin,221,222,0
-block_hint,ArrayPrototypeJoin,112,113,1
-block_hint,ArrayPrototypeJoin,114,115,0
-block_hint,ArrayPrototypeJoin,116,117,1
-block_hint,ArrayPrototypeJoin,290,291,1
-block_hint,ArrayPrototypeJoin,145,146,1
+block_hint,ArrayPrototypeJoin,110,111,1
+block_hint,ArrayPrototypeJoin,284,285,1
+block_hint,ArrayPrototypeJoin,139,140,1
 block_hint,ArrayPrototypeToString,14,15,1
 block_hint,ArrayPrototypeToString,11,12,1
 block_hint,ArrayPrototypeToString,8,9,1
@@ -3207,15 +3205,14 @@
 block_hint,ArrayPrototypeLastIndexOf,31,32,0
 block_hint,ArrayMap,165,166,1
 block_hint,ArrayMap,72,73,0
-block_hint,ArrayMap,258,259,1
-block_hint,ArrayMap,237,238,1
+block_hint,ArrayMap,255,256,1
+block_hint,ArrayMap,236,237,1
 block_hint,ArrayMap,188,189,1
 block_hint,ArrayMap,190,191,1
 block_hint,ArrayMap,209,210,0
-block_hint,ArrayMap,256,257,0
 block_hint,ArrayMap,221,222,1
 block_hint,ArrayMap,223,224,1
-block_hint,ArrayMap,246,247,1
+block_hint,ArrayMap,245,246,1
 block_hint,ArrayMap,206,207,0
 block_hint,ArrayMap,218,219,1
 block_hint,ArrayMap,225,226,1
@@ -3229,9 +3226,9 @@
 block_hint,ArrayMap,182,183,1
 block_hint,ArrayMap,157,158,1
 block_hint,ArrayMap,55,56,0
-block_hint,ArrayMap,271,272,1
-block_hint,ArrayMap,268,269,0
-block_hint,ArrayMap,249,250,0
+block_hint,ArrayMap,268,269,1
+block_hint,ArrayMap,265,266,0
+block_hint,ArrayMap,248,249,0
 block_hint,ArrayMap,227,228,0
 block_hint,ArrayMap,195,196,0
 block_hint,ArrayMap,116,117,0
@@ -3242,7 +3239,7 @@
 block_hint,ArrayMap,120,121,0
 block_hint,ArrayMap,37,38,1
 block_hint,ArrayMap,35,36,1
-block_hint,ArrayMap,254,255,0
+block_hint,ArrayMap,253,254,0
 block_hint,ArrayMap,203,204,0
 block_hint,ArrayMap,149,150,0
 block_hint,ArrayMap,45,46,1
@@ -3262,12 +3259,11 @@
 block_hint,ArrayMap,135,136,1
 block_hint,ArrayReduce,81,82,1
 block_hint,ArrayReduce,30,31,0
-block_hint,ArrayReduce,127,128,1
-block_hint,ArrayReduce,121,122,1
+block_hint,ArrayReduce,124,125,1
+block_hint,ArrayReduce,120,121,1
 block_hint,ArrayReduce,89,90,1
 block_hint,ArrayReduce,91,92,1
 block_hint,ArrayReduce,101,102,0
-block_hint,ArrayReduce,125,126,0
 block_hint,ArrayReduce,111,112,1
 block_hint,ArrayReduce,113,114,1
 block_hint,ArrayReduce,95,96,1
@@ -3291,42 +3287,42 @@
 block_hint,ArrayReduce,57,58,0
 block_hint,ArrayReduce,59,60,0
 block_hint,ArrayReduce,23,24,0
-block_hint,ArrayPrototypeReverse,242,243,1
-block_hint,ArrayPrototypeReverse,216,217,1
-block_hint,ArrayPrototypeReverse,196,197,1
-block_hint,ArrayPrototypeReverse,158,159,1
-block_hint,ArrayPrototypeReverse,109,110,1
-block_hint,ArrayPrototypeReverse,20,21,1
-block_hint,ArrayPrototypeReverse,198,199,1
-block_hint,ArrayPrototypeReverse,175,176,0
-block_hint,ArrayPrototypeReverse,146,147,1
-block_hint,ArrayPrototypeReverse,124,125,1
-block_hint,ArrayPrototypeReverse,95,96,0
-block_hint,ArrayPrototypeShift,240,241,1
-block_hint,ArrayPrototypeShift,208,209,1
-block_hint,ArrayPrototypeShift,188,189,1
-block_hint,ArrayPrototypeShift,135,136,1
-block_hint,ArrayPrototypeShift,84,85,1
-block_hint,ArrayPrototypeShift,12,13,1
-block_hint,ArrayPrototypeShift,199,200,1
-block_hint,ArrayPrototypeShift,171,172,0
-block_hint,ArrayPrototypeShift,137,138,1
-block_hint,ArrayPrototypeShift,86,87,0
-block_hint,ArrayPrototypeShift,14,15,1
-block_hint,ArrayPrototypeShift,139,140,0
-block_hint,ArrayPrototypeShift,88,89,0
-block_hint,ArrayPrototypeShift,71,72,0
-block_hint,ArrayPrototypeShift,90,91,0
-block_hint,ArrayPrototypeShift,26,27,0
-block_hint,ArrayPrototypeShift,28,29,1
-block_hint,ArrayPrototypeShift,173,174,0
-block_hint,ArrayPrototypeShift,92,93,0
-block_hint,ArrayPrototypeShift,32,33,0
-block_hint,ArrayPrototypeShift,151,152,0
-block_hint,ArrayPrototypeShift,114,115,1
-block_hint,ArrayPrototypeShift,94,95,0
-block_hint,ArrayPrototypeShift,38,39,0
-block_hint,ArrayPrototypeShift,40,41,1
+block_hint,ArrayPrototypeReverse,236,237,1
+block_hint,ArrayPrototypeReverse,210,211,1
+block_hint,ArrayPrototypeReverse,190,191,1
+block_hint,ArrayPrototypeReverse,152,153,1
+block_hint,ArrayPrototypeReverse,103,104,1
+block_hint,ArrayPrototypeReverse,18,19,1
+block_hint,ArrayPrototypeReverse,192,193,1
+block_hint,ArrayPrototypeReverse,169,170,0
+block_hint,ArrayPrototypeReverse,140,141,1
+block_hint,ArrayPrototypeReverse,118,119,1
+block_hint,ArrayPrototypeReverse,89,90,0
+block_hint,ArrayPrototypeShift,237,238,1
+block_hint,ArrayPrototypeShift,205,206,1
+block_hint,ArrayPrototypeShift,185,186,1
+block_hint,ArrayPrototypeShift,132,133,1
+block_hint,ArrayPrototypeShift,81,82,1
+block_hint,ArrayPrototypeShift,11,12,1
+block_hint,ArrayPrototypeShift,196,197,1
+block_hint,ArrayPrototypeShift,168,169,0
+block_hint,ArrayPrototypeShift,134,135,1
+block_hint,ArrayPrototypeShift,83,84,0
+block_hint,ArrayPrototypeShift,13,14,1
+block_hint,ArrayPrototypeShift,136,137,0
+block_hint,ArrayPrototypeShift,85,86,0
+block_hint,ArrayPrototypeShift,68,69,0
+block_hint,ArrayPrototypeShift,87,88,0
+block_hint,ArrayPrototypeShift,27,28,0
+block_hint,ArrayPrototypeShift,29,30,1
+block_hint,ArrayPrototypeShift,170,171,0
+block_hint,ArrayPrototypeShift,89,90,0
+block_hint,ArrayPrototypeShift,33,34,0
+block_hint,ArrayPrototypeShift,148,149,0
+block_hint,ArrayPrototypeShift,111,112,1
+block_hint,ArrayPrototypeShift,91,92,0
+block_hint,ArrayPrototypeShift,39,40,0
+block_hint,ArrayPrototypeShift,41,42,1
 block_hint,ArrayPrototypeSlice,288,289,1
 block_hint,ArrayPrototypeSlice,267,268,1
 block_hint,ArrayPrototypeSlice,245,246,1
@@ -3419,12 +3415,11 @@
 block_hint,ArrayPrototypeSlice,73,74,1
 block_hint,ArraySome,88,89,1
 block_hint,ArraySome,31,32,0
-block_hint,ArraySome,122,123,1
-block_hint,ArraySome,116,117,1
+block_hint,ArraySome,119,120,1
+block_hint,ArraySome,115,116,1
 block_hint,ArraySome,93,94,1
 block_hint,ArraySome,95,96,1
 block_hint,ArraySome,101,102,0
-block_hint,ArraySome,120,121,0
 block_hint,ArraySome,108,109,1
 block_hint,ArraySome,110,111,1
 block_hint,ArraySome,99,100,1
@@ -3442,101 +3437,99 @@
 block_hint,ArraySome,19,20,0
 block_hint,ArraySome,21,22,1
 block_hint,ArraySome,66,67,0
-block_hint,ArrayPrototypeSplice,605,606,1
-block_hint,ArrayPrototypeSplice,453,454,1
-block_hint,ArrayPrototypeSplice,455,456,1
-block_hint,ArrayPrototypeSplice,1201,1202,0
-block_hint,ArrayPrototypeSplice,1183,1184,0
-block_hint,ArrayPrototypeSplice,1159,1160,0
-block_hint,ArrayPrototypeSplice,1138,1139,0
-block_hint,ArrayPrototypeSplice,1104,1105,0
+block_hint,ArrayPrototypeSplice,599,600,1
+block_hint,ArrayPrototypeSplice,447,448,1
+block_hint,ArrayPrototypeSplice,449,450,1
+block_hint,ArrayPrototypeSplice,1195,1196,0
+block_hint,ArrayPrototypeSplice,1177,1178,0
+block_hint,ArrayPrototypeSplice,1153,1154,0
+block_hint,ArrayPrototypeSplice,1132,1133,0
+block_hint,ArrayPrototypeSplice,1098,1099,0
+block_hint,ArrayPrototypeSplice,1066,1067,0
+block_hint,ArrayPrototypeSplice,1025,1026,0
+block_hint,ArrayPrototypeSplice,1096,1097,0
+block_hint,ArrayPrototypeSplice,1064,1065,0
+block_hint,ArrayPrototypeSplice,1021,1022,1
+block_hint,ArrayPrototypeSplice,934,935,0
+block_hint,ArrayPrototypeSplice,1087,1088,0
+block_hint,ArrayPrototypeSplice,1220,1221,0
+block_hint,ArrayPrototypeSplice,1212,1213,0
+block_hint,ArrayPrototypeSplice,1199,1200,0
+block_hint,ArrayPrototypeSplice,1187,1188,1
+block_hint,ArrayPrototypeSplice,1156,1157,0
+block_hint,ArrayPrototypeSplice,1136,1137,0
+block_hint,ArrayPrototypeSplice,1115,1116,0
 block_hint,ArrayPrototypeSplice,1072,1073,0
-block_hint,ArrayPrototypeSplice,1031,1032,0
-block_hint,ArrayPrototypeSplice,1102,1103,0
+block_hint,ArrayPrototypeSplice,1032,1033,0
 block_hint,ArrayPrototypeSplice,1070,1071,0
-block_hint,ArrayPrototypeSplice,1027,1028,1
-block_hint,ArrayPrototypeSplice,940,941,0
-block_hint,ArrayPrototypeSplice,1093,1094,0
-block_hint,ArrayPrototypeSplice,1226,1227,0
-block_hint,ArrayPrototypeSplice,1218,1219,0
-block_hint,ArrayPrototypeSplice,1205,1206,0
-block_hint,ArrayPrototypeSplice,1193,1194,1
-block_hint,ArrayPrototypeSplice,1162,1163,0
-block_hint,ArrayPrototypeSplice,1142,1143,0
-block_hint,ArrayPrototypeSplice,1121,1122,0
-block_hint,ArrayPrototypeSplice,1078,1079,0
-block_hint,ArrayPrototypeSplice,1038,1039,0
-block_hint,ArrayPrototypeSplice,1076,1077,0
-block_hint,ArrayPrototypeSplice,1036,1037,0
-block_hint,ArrayPrototypeSplice,987,988,1
-block_hint,ArrayPrototypeSplice,877,878,0
-block_hint,ArrayPrototypeSplice,842,843,0
-block_hint,ArrayPrototypeSplice,810,811,0
-block_hint,ArrayPrototypeSplice,731,732,0
-block_hint,ArrayPrototypeSplice,677,678,0
-block_hint,ArrayPrototypeSplice,607,608,0
-block_hint,ArrayPrototypeSplice,513,514,1
-block_hint,ArrayPrototypeSplice,459,460,0
-block_hint,ArrayPrototypeSplice,229,230,0
-block_hint,ArrayPrototypeSplice,339,340,0
+block_hint,ArrayPrototypeSplice,1030,1031,0
+block_hint,ArrayPrototypeSplice,981,982,1
+block_hint,ArrayPrototypeSplice,871,872,0
+block_hint,ArrayPrototypeSplice,836,837,0
+block_hint,ArrayPrototypeSplice,804,805,0
+block_hint,ArrayPrototypeSplice,725,726,0
+block_hint,ArrayPrototypeSplice,671,672,0
+block_hint,ArrayPrototypeSplice,601,602,0
+block_hint,ArrayPrototypeSplice,507,508,1
+block_hint,ArrayPrototypeSplice,453,454,0
+block_hint,ArrayPrototypeSplice,223,224,0
+block_hint,ArrayPrototypeSplice,333,334,0
+block_hint,ArrayPrototypeSplice,335,336,0
+block_hint,ArrayPrototypeSplice,337,338,0
+block_hint,ArrayPrototypeSplice,225,226,1
+block_hint,ArrayPrototypeSplice,51,52,1
+block_hint,ArrayPrototypeSplice,339,340,1
 block_hint,ArrayPrototypeSplice,341,342,0
 block_hint,ArrayPrototypeSplice,343,344,0
-block_hint,ArrayPrototypeSplice,231,232,1
+block_hint,ArrayPrototypeSplice,388,389,1
+block_hint,ArrayPrototypeSplice,227,228,0
 block_hint,ArrayPrototypeSplice,53,54,1
-block_hint,ArrayPrototypeSplice,345,346,1
-block_hint,ArrayPrototypeSplice,347,348,0
-block_hint,ArrayPrototypeSplice,349,350,0
-block_hint,ArrayPrototypeSplice,394,395,1
-block_hint,ArrayPrototypeSplice,233,234,0
-block_hint,ArrayPrototypeSplice,55,56,1
-block_hint,ArrayPrototypeSplice,250,251,0
-block_hint,ArrayPrototypeSplice,91,92,1
-block_hint,ArrayPrototypeSplice,558,559,0
-block_hint,ArrayPrototypeSplice,407,408,0
-block_hint,ArrayPrototypeSplice,590,591,0
-block_hint,ArrayPrototypeSplice,530,531,1
-block_hint,ArrayPrototypeSplice,352,353,0
-block_hint,ArrayPrototypeSplice,354,355,1
-block_hint,ArrayPrototypeSplice,241,242,0
-block_hint,ArrayPrototypeSplice,263,264,1
-block_hint,ArrayPrototypeSplice,103,104,0
+block_hint,ArrayPrototypeSplice,244,245,0
+block_hint,ArrayPrototypeSplice,93,94,1
+block_hint,ArrayPrototypeSplice,552,553,0
+block_hint,ArrayPrototypeSplice,401,402,0
+block_hint,ArrayPrototypeSplice,584,585,0
+block_hint,ArrayPrototypeSplice,524,525,1
+block_hint,ArrayPrototypeSplice,346,347,0
+block_hint,ArrayPrototypeSplice,348,349,1
 block_hint,ArrayPrototypeSplice,235,236,0
-block_hint,ArrayPrototypeSplice,336,337,0
-block_hint,ArrayPrototypeSplice,334,335,0
-block_hint,ArrayPrototypeSplice,398,399,1
+block_hint,ArrayPrototypeSplice,257,258,1
+block_hint,ArrayPrototypeSplice,105,106,0
+block_hint,ArrayPrototypeSplice,229,230,0
+block_hint,ArrayPrototypeSplice,330,331,0
+block_hint,ArrayPrototypeSplice,328,329,0
+block_hint,ArrayPrototypeSplice,392,393,1
 block_hint,ArrayPrototypeSplice,65,66,1
-block_hint,ArrayPrototypeSplice,299,300,0
-block_hint,ArrayPrototypeSplice,145,146,0
+block_hint,ArrayPrototypeSplice,293,294,0
+block_hint,ArrayPrototypeSplice,143,144,0
 block_hint,ArrayPrototypeSplice,67,68,0
 block_hint,ArrayPrototypeSplice,69,70,0
-block_hint,ArrayPrototypeSplice,268,269,1
-block_hint,ArrayPrototypeSplice,184,185,1
-block_hint,ArrayPrototypeSplice,332,333,0
-block_hint,ArrayPrototypeSplice,428,429,1
-block_hint,ArrayPrototypeSplice,270,271,1
-block_hint,ArrayPrototypeSplice,109,110,0
-block_hint,ArrayPrototypeSplice,430,431,1
-block_hint,ArrayPrototypeSplice,272,273,0
+block_hint,ArrayPrototypeSplice,262,263,1
+block_hint,ArrayPrototypeSplice,178,179,1
+block_hint,ArrayPrototypeSplice,326,327,0
+block_hint,ArrayPrototypeSplice,422,423,1
+block_hint,ArrayPrototypeSplice,264,265,1
 block_hint,ArrayPrototypeSplice,111,112,0
-block_hint,ArrayPrototypeSplice,113,114,0
-block_hint,ArrayPrototypeSplice,188,189,1
+block_hint,ArrayPrototypeSplice,424,425,1
+block_hint,ArrayPrototypeSplice,115,116,0
+block_hint,ArrayPrototypeSplice,182,183,1
 block_hint,ArrayPrototypeSplice,63,64,1
-block_hint,ArrayPrototypeSplice,129,130,1
-block_hint,ArrayPrototypeSplice,301,302,0
+block_hint,ArrayPrototypeSplice,131,132,1
+block_hint,ArrayPrototypeSplice,295,296,0
 block_hint,ArrayPrototypeSplice,71,72,1
-block_hint,ArrayPrototypeUnshift,188,189,1
-block_hint,ArrayPrototypeUnshift,161,162,1
-block_hint,ArrayPrototypeUnshift,143,144,1
-block_hint,ArrayPrototypeUnshift,99,100,1
-block_hint,ArrayPrototypeUnshift,58,59,1
-block_hint,ArrayPrototypeUnshift,11,12,1
-block_hint,ArrayPrototypeUnshift,132,133,1
-block_hint,ArrayPrototypeUnshift,101,102,0
-block_hint,ArrayPrototypeUnshift,60,61,0
-block_hint,ArrayPrototypeUnshift,103,104,1
-block_hint,ArrayPrototypeUnshift,62,63,0
-block_hint,ArrayPrototypeUnshift,19,20,1
-block_hint,ArrayPrototypeUnshift,21,22,0
+block_hint,ArrayPrototypeUnshift,185,186,1
+block_hint,ArrayPrototypeUnshift,158,159,1
+block_hint,ArrayPrototypeUnshift,140,141,1
+block_hint,ArrayPrototypeUnshift,96,97,1
+block_hint,ArrayPrototypeUnshift,55,56,1
+block_hint,ArrayPrototypeUnshift,10,11,1
+block_hint,ArrayPrototypeUnshift,129,130,1
+block_hint,ArrayPrototypeUnshift,98,99,0
+block_hint,ArrayPrototypeUnshift,57,58,0
+block_hint,ArrayPrototypeUnshift,100,101,1
+block_hint,ArrayPrototypeUnshift,59,60,0
+block_hint,ArrayPrototypeUnshift,20,21,1
+block_hint,ArrayPrototypeUnshift,22,23,0
 block_hint,ArrayBufferPrototypeGetByteLength,15,16,1
 block_hint,ArrayBufferPrototypeGetByteLength,10,11,1
 block_hint,ArrayBufferPrototypeGetByteLength,12,13,1
@@ -3603,16 +3596,14 @@
 block_hint,StringPrototypeCodePointAt,72,73,0
 block_hint,StringPrototypeCodePointAt,48,49,0
 block_hint,StringPrototypeCodePointAt,18,19,1
-block_hint,StringConstructor,65,66,1
+block_hint,StringConstructor,64,65,1
 block_hint,StringConstructor,49,50,0
-block_hint,StringConstructor,63,64,0
 block_hint,StringConstructor,36,37,0
-block_hint,StringConstructor,78,79,0
-block_hint,StringConstructor,83,84,1
-block_hint,StringConstructor,81,82,1
-block_hint,StringConstructor,75,76,1
-block_hint,StringConstructor,59,60,0
-block_hint,StringConstructor,61,62,1
+block_hint,StringConstructor,78,79,1
+block_hint,StringConstructor,76,77,1
+block_hint,StringConstructor,73,74,1
+block_hint,StringConstructor,60,61,0
+block_hint,StringConstructor,62,63,1
 block_hint,StringConstructor,45,46,0
 block_hint,StringConstructor,24,25,0
 block_hint,StringConstructor,26,27,1
@@ -3827,6 +3818,7 @@
 block_hint,FunctionPrototypeHasInstance,33,34,1
 block_hint,FunctionPrototypeHasInstance,23,24,0
 block_hint,FunctionPrototypeHasInstance,13,14,0
+block_hint,FunctionPrototypeHasInstance,31,32,0
 block_hint,FunctionPrototypeHasInstance,25,26,0
 block_hint,FunctionPrototypeHasInstance,27,28,0
 block_hint,FastFunctionPrototypeBind,91,92,1
@@ -4119,7 +4111,6 @@
 block_hint,ProxyGetProperty,198,199,0
 block_hint,ProxyGetProperty,149,150,1
 block_hint,ProxyGetProperty,28,29,0
-block_hint,ProxyGetProperty,48,49,1
 block_hint,ProxyGetProperty,167,168,0
 block_hint,ProxyGetProperty,187,188,1
 block_hint,ProxyGetProperty,131,132,1
@@ -4169,73 +4160,71 @@
 block_hint,RegExpPrototypeExec,144,145,1
 block_hint,RegExpPrototypeExec,116,117,1
 block_hint,RegExpPrototypeExec,156,157,1
-block_hint,RegExpMatchFast,363,364,0
-block_hint,RegExpMatchFast,293,294,1
-block_hint,RegExpMatchFast,34,35,1
+block_hint,RegExpMatchFast,357,358,0
+block_hint,RegExpMatchFast,289,290,1
+block_hint,RegExpMatchFast,32,33,1
+block_hint,RegExpMatchFast,326,327,0
+block_hint,RegExpMatchFast,234,235,0
+block_hint,RegExpMatchFast,283,284,0
+block_hint,RegExpMatchFast,448,449,0
+block_hint,RegExpMatchFast,392,393,1
+block_hint,RegExpMatchFast,291,292,1
+block_hint,RegExpMatchFast,285,286,0
+block_hint,RegExpMatchFast,129,130,0
+block_hint,RegExpMatchFast,236,237,1
+block_hint,RegExpMatchFast,238,239,1
+block_hint,RegExpMatchFast,40,41,1
 block_hint,RegExpMatchFast,331,332,0
-block_hint,RegExpMatchFast,240,241,0
-block_hint,RegExpMatchFast,287,288,0
-block_hint,RegExpMatchFast,454,455,0
-block_hint,RegExpMatchFast,397,398,1
-block_hint,RegExpMatchFast,295,296,1
-block_hint,RegExpMatchFast,289,290,0
-block_hint,RegExpMatchFast,127,128,0
-block_hint,RegExpMatchFast,242,243,1
+block_hint,RegExpMatchFast,240,241,1
+block_hint,RegExpMatchFast,456,457,1
+block_hint,RegExpMatchFast,394,395,0
+block_hint,RegExpMatchFast,320,321,1
+block_hint,RegExpMatchFast,133,134,0
+block_hint,RegExpMatchFast,48,49,1
 block_hint,RegExpMatchFast,244,245,1
-block_hint,RegExpMatchFast,42,43,1
-block_hint,RegExpMatchFast,336,337,0
-block_hint,RegExpMatchFast,246,247,1
-block_hint,RegExpMatchFast,462,463,1
-block_hint,RegExpMatchFast,399,400,0
-block_hint,RegExpMatchFast,325,326,1
-block_hint,RegExpMatchFast,131,132,0
-block_hint,RegExpMatchFast,50,51,1
-block_hint,RegExpMatchFast,250,251,1
-block_hint,RegExpMatchFast,186,187,1
-block_hint,RegExpMatchFast,263,264,1
-block_hint,RegExpMatchFast,301,302,0
+block_hint,RegExpMatchFast,180,181,1
+block_hint,RegExpMatchFast,259,260,1
+block_hint,RegExpMatchFast,297,298,0
+block_hint,RegExpMatchFast,82,83,1
 block_hint,RegExpMatchFast,84,85,1
+block_hint,RegExpMatchFast,301,302,0
+block_hint,RegExpMatchFast,344,345,0
+block_hint,RegExpMatchFast,379,380,0
+block_hint,RegExpMatchFast,299,300,0
 block_hint,RegExpMatchFast,86,87,1
-block_hint,RegExpMatchFast,305,306,0
-block_hint,RegExpMatchFast,350,351,0
-block_hint,RegExpMatchFast,383,384,0
-block_hint,RegExpMatchFast,303,304,0
-block_hint,RegExpMatchFast,88,89,1
-block_hint,RegExpMatchFast,345,346,0
-block_hint,RegExpMatchFast,254,255,0
-block_hint,RegExpMatchFast,279,280,0
-block_hint,RegExpMatchFast,196,197,1
-block_hint,RegExpMatchFast,456,457,0
-block_hint,RegExpMatchFast,442,443,1
-block_hint,RegExpMatchFast,395,396,1
-block_hint,RegExpMatchFast,307,308,1
-block_hint,RegExpMatchFast,281,282,0
-block_hint,RegExpMatchFast,115,116,0
-block_hint,RegExpMatchFast,347,348,0
-block_hint,RegExpMatchFast,256,257,0
-block_hint,RegExpMatchFast,94,95,1
-block_hint,RegExpMatchFast,387,388,1
-block_hint,RegExpMatchFast,309,310,0
-block_hint,RegExpMatchFast,181,182,1
-block_hint,RegExpMatchFast,179,180,1
-block_hint,RegExpMatchFast,311,312,0
-block_hint,RegExpMatchFast,183,184,0
-block_hint,RegExpMatchFast,102,103,0
+block_hint,RegExpMatchFast,340,341,0
+block_hint,RegExpMatchFast,248,249,0
+block_hint,RegExpMatchFast,275,276,0
+block_hint,RegExpMatchFast,190,191,1
+block_hint,RegExpMatchFast,450,451,0
+block_hint,RegExpMatchFast,436,437,1
+block_hint,RegExpMatchFast,390,391,1
+block_hint,RegExpMatchFast,303,304,1
+block_hint,RegExpMatchFast,277,278,0
+block_hint,RegExpMatchFast,117,118,0
+block_hint,RegExpMatchFast,342,343,0
+block_hint,RegExpMatchFast,250,251,0
+block_hint,RegExpMatchFast,92,93,1
+block_hint,RegExpMatchFast,362,363,1
+block_hint,RegExpMatchFast,252,253,0
+block_hint,RegExpMatchFast,102,103,1
+block_hint,RegExpMatchFast,306,307,0
+block_hint,RegExpMatchFast,177,178,0
 block_hint,RegExpMatchFast,104,105,0
-block_hint,RegExpMatchFast,204,205,1
-block_hint,RegExpMatchFast,322,323,0
-block_hint,RegExpMatchFast,106,107,1
-block_hint,RegExpMatchFast,193,194,1
-block_hint,RegExpMatchFast,352,353,0
+block_hint,RegExpMatchFast,106,107,0
+block_hint,RegExpMatchFast,198,199,1
+block_hint,RegExpMatchFast,317,318,0
+block_hint,RegExpMatchFast,108,109,1
+block_hint,RegExpMatchFast,187,188,1
+block_hint,RegExpMatchFast,346,347,0
+block_hint,RegExpMatchFast,94,95,1
 block_hint,RegExpMatchFast,96,97,1
-block_hint,RegExpMatchFast,175,176,1
-block_hint,RegExpMatchFast,173,174,1
-block_hint,RegExpMatchFast,177,178,0
+block_hint,RegExpMatchFast,175,176,0
 block_hint,RegExpMatchFast,98,99,0
 block_hint,RegExpMatchFast,100,101,0
-block_hint,RegExpMatchFast,224,225,1
-block_hint,RegExpMatchFast,314,315,0
-block_hint,RegExpMatchFast,226,227,0
+block_hint,RegExpMatchFast,218,219,1
+block_hint,RegExpMatchFast,309,310,0
+block_hint,RegExpMatchFast,220,221,0
 block_hint,RegExpReplace,261,262,1
 block_hint,RegExpReplace,299,300,1
 block_hint,RegExpReplace,251,252,1
@@ -4744,34 +4733,39 @@
 block_hint,CreateTypedArray,391,392,0
 block_hint,CreateTypedArray,60,61,1
 block_hint,CreateTypedArray,62,63,1
-block_hint,TypedArrayFrom,156,157,1
-block_hint,TypedArrayFrom,140,141,0
-block_hint,TypedArrayFrom,124,125,1
-block_hint,TypedArrayFrom,95,96,1
-block_hint,TypedArrayFrom,56,57,1
-block_hint,TypedArrayFrom,58,59,1
-block_hint,TypedArrayFrom,117,118,1
-block_hint,TypedArrayFrom,109,110,0
-block_hint,TypedArrayFrom,88,89,0
-block_hint,TypedArrayFrom,69,70,1
-block_hint,TypedArrayFrom,71,72,1
-block_hint,TypedArrayFrom,163,164,1
-block_hint,TypedArrayFrom,165,166,0
-block_hint,TypedArrayFrom,167,168,0
-block_hint,TypedArrayFrom,180,181,1
-block_hint,TypedArrayFrom,173,174,0
-block_hint,TypedArrayFrom,175,176,1
-block_hint,TypedArrayFrom,161,162,1
-block_hint,TypedArrayFrom,152,153,0
-block_hint,TypedArrayFrom,143,144,1
-block_hint,TypedArrayFrom,111,112,0
-block_hint,TypedArrayFrom,73,74,1
-block_hint,TypedArrayFrom,75,76,1
-block_hint,TypedArrayFrom,24,25,0
-block_hint,TypedArrayFrom,91,92,0
-block_hint,TypedArrayFrom,26,27,0
-block_hint,TypedArrayFrom,82,83,1
-block_hint,TypedArrayFrom,28,29,0
+block_hint,TypedArrayFrom,234,235,1
+block_hint,TypedArrayFrom,214,215,0
+block_hint,TypedArrayFrom,195,196,1
+block_hint,TypedArrayFrom,154,155,1
+block_hint,TypedArrayFrom,87,88,1
+block_hint,TypedArrayFrom,89,90,1
+block_hint,TypedArrayFrom,184,185,1
+block_hint,TypedArrayFrom,176,177,0
+block_hint,TypedArrayFrom,139,140,0
+block_hint,TypedArrayFrom,100,101,1
+block_hint,TypedArrayFrom,102,103,1
+block_hint,TypedArrayFrom,248,249,1
+block_hint,TypedArrayFrom,250,251,0
+block_hint,TypedArrayFrom,236,237,0
+block_hint,TypedArrayFrom,223,224,1
+block_hint,TypedArrayFrom,225,226,0
+block_hint,TypedArrayFrom,204,205,1
+block_hint,TypedArrayFrom,186,187,1
+block_hint,TypedArrayFrom,164,165,0
+block_hint,TypedArrayFrom,166,167,0
+block_hint,TypedArrayFrom,244,245,0
+block_hint,TypedArrayFrom,217,218,1
+block_hint,TypedArrayFrom,178,179,0
+block_hint,TypedArrayFrom,106,107,1
+block_hint,TypedArrayFrom,108,109,1
+block_hint,TypedArrayFrom,171,172,0
+block_hint,TypedArrayFrom,144,145,0
+block_hint,TypedArrayFrom,118,119,0
+block_hint,TypedArrayFrom,55,56,0
+block_hint,TypedArrayFrom,150,151,0
+block_hint,TypedArrayFrom,57,58,0
+block_hint,TypedArrayFrom,133,134,1
+block_hint,TypedArrayFrom,59,60,0
 block_hint,TypedArrayPrototypeSet,189,190,1
 block_hint,TypedArrayPrototypeSet,104,105,1
 block_hint,TypedArrayPrototypeSet,106,107,1
@@ -4985,7 +4979,6 @@
 block_hint,MergeAt,91,92,1
 block_hint,MergeAt,93,94,1
 block_hint,MergeAt,95,96,1
-block_hint,MergeAt,107,108,1
 block_hint,MergeAt,194,195,1
 block_hint,MergeAt,97,98,1
 block_hint,MergeAt,99,100,1
@@ -5049,92 +5042,92 @@
 block_hint,GallopRight,39,40,0
 block_hint,GallopRight,17,18,1
 block_hint,GallopRight,61,62,0
-block_hint,ArrayTimSort,123,124,0
-block_hint,ArrayTimSort,243,244,0
+block_hint,ArrayTimSort,120,121,0
+block_hint,ArrayTimSort,240,241,0
+block_hint,ArrayTimSort,227,228,0
+block_hint,ArrayTimSort,122,123,0
+block_hint,ArrayTimSort,163,164,0
+block_hint,ArrayTimSort,140,141,0
+block_hint,ArrayTimSort,33,34,1
+block_hint,ArrayTimSort,93,94,0
+block_hint,ArrayTimSort,95,96,0
+block_hint,ArrayTimSort,143,144,0
+block_hint,ArrayTimSort,35,36,1
+block_hint,ArrayTimSort,37,38,1
+block_hint,ArrayTimSort,214,215,0
+block_hint,ArrayTimSort,145,146,1
+block_hint,ArrayTimSort,39,40,1
+block_hint,ArrayTimSort,218,219,0
+block_hint,ArrayTimSort,216,217,0
+block_hint,ArrayTimSort,41,42,1
+block_hint,ArrayTimSort,43,44,1
+block_hint,ArrayTimSort,45,46,1
+block_hint,ArrayTimSort,134,135,0
+block_hint,ArrayTimSort,47,48,1
+block_hint,ArrayTimSort,49,50,1
+block_hint,ArrayTimSort,222,223,0
+block_hint,ArrayTimSort,51,52,1
+block_hint,ArrayTimSort,53,54,1
+block_hint,ArrayTimSort,55,56,1
+block_hint,ArrayTimSort,57,58,1
+block_hint,ArrayTimSort,59,60,1
+block_hint,ArrayTimSort,61,62,1
+block_hint,ArrayTimSort,63,64,1
+block_hint,ArrayTimSort,65,66,1
+block_hint,ArrayTimSort,67,68,1
+block_hint,ArrayTimSort,69,70,1
+block_hint,ArrayTimSort,71,72,1
+block_hint,ArrayTimSort,157,158,1
+block_hint,ArrayTimSort,73,74,1
+block_hint,ArrayTimSort,75,76,1
+block_hint,ArrayTimSort,204,205,0
+block_hint,ArrayTimSort,77,78,1
+block_hint,ArrayTimSort,79,80,1
+block_hint,ArrayTimSort,209,210,0
+block_hint,ArrayTimSort,81,82,1
+block_hint,ArrayTimSort,83,84,1
+block_hint,ArrayTimSort,186,187,0
+block_hint,ArrayTimSort,236,237,1
+block_hint,ArrayTimSort,238,239,1
+block_hint,ArrayTimSort,211,212,1
+block_hint,ArrayTimSort,161,162,1
+block_hint,ArrayTimSort,85,86,1
+block_hint,ArrayTimSort,243,244,1
 block_hint,ArrayTimSort,230,231,0
-block_hint,ArrayTimSort,125,126,0
-block_hint,ArrayTimSort,164,165,0
-block_hint,ArrayTimSort,191,192,0
-block_hint,ArrayTimSort,32,33,1
-block_hint,ArrayTimSort,92,93,0
-block_hint,ArrayTimSort,94,95,0
-block_hint,ArrayTimSort,144,145,0
-block_hint,ArrayTimSort,34,35,1
-block_hint,ArrayTimSort,36,37,1
-block_hint,ArrayTimSort,217,218,0
-block_hint,ArrayTimSort,146,147,1
-block_hint,ArrayTimSort,38,39,1
-block_hint,ArrayTimSort,221,222,0
-block_hint,ArrayTimSort,219,220,0
-block_hint,ArrayTimSort,40,41,1
-block_hint,ArrayTimSort,42,43,1
-block_hint,ArrayTimSort,44,45,1
-block_hint,ArrayTimSort,137,138,0
-block_hint,ArrayTimSort,46,47,1
-block_hint,ArrayTimSort,48,49,1
-block_hint,ArrayTimSort,225,226,0
-block_hint,ArrayTimSort,50,51,1
-block_hint,ArrayTimSort,52,53,1
-block_hint,ArrayTimSort,54,55,1
-block_hint,ArrayTimSort,56,57,1
-block_hint,ArrayTimSort,58,59,1
-block_hint,ArrayTimSort,60,61,1
-block_hint,ArrayTimSort,62,63,1
-block_hint,ArrayTimSort,64,65,1
-block_hint,ArrayTimSort,66,67,1
-block_hint,ArrayTimSort,68,69,1
-block_hint,ArrayTimSort,70,71,1
-block_hint,ArrayTimSort,158,159,1
-block_hint,ArrayTimSort,72,73,1
-block_hint,ArrayTimSort,74,75,1
-block_hint,ArrayTimSort,207,208,0
-block_hint,ArrayTimSort,76,77,1
-block_hint,ArrayTimSort,78,79,1
-block_hint,ArrayTimSort,212,213,0
-block_hint,ArrayTimSort,80,81,1
-block_hint,ArrayTimSort,82,83,1
-block_hint,ArrayTimSort,187,188,0
-block_hint,ArrayTimSort,239,240,1
-block_hint,ArrayTimSort,241,242,1
-block_hint,ArrayTimSort,214,215,1
-block_hint,ArrayTimSort,162,163,1
-block_hint,ArrayTimSort,84,85,1
-block_hint,ArrayTimSort,246,247,1
-block_hint,ArrayTimSort,233,234,0
-block_hint,ArrayTimSort,189,190,1
-block_hint,ArrayTimSort,141,142,0
-block_hint,ArrayTimSort,86,87,1
-block_hint,ArrayTimSort,112,113,0
-block_hint,ArrayTimSort,88,89,0
-block_hint,ArrayPrototypeSort,109,110,1
-block_hint,ArrayPrototypeSort,83,84,0
-block_hint,ArrayPrototypeSort,42,43,1
-block_hint,ArrayPrototypeSort,73,74,0
-block_hint,ArrayPrototypeSort,44,45,1
-block_hint,ArrayPrototypeSort,85,86,1
-block_hint,ArrayPrototypeSort,87,88,1
-block_hint,ArrayPrototypeSort,66,67,0
-block_hint,ArrayPrototypeSort,26,27,0
-block_hint,ArrayPrototypeSort,123,124,0
-block_hint,ArrayPrototypeSort,104,105,1
-block_hint,ArrayPrototypeSort,76,77,1
-block_hint,ArrayPrototypeSort,54,55,1
-block_hint,ArrayPrototypeSort,16,17,1
-block_hint,ArrayPrototypeSort,98,99,1
-block_hint,ArrayPrototypeSort,78,79,0
-block_hint,ArrayPrototypeSort,56,57,0
-block_hint,ArrayPrototypeSort,139,140,0
-block_hint,ArrayPrototypeSort,142,143,0
-block_hint,ArrayPrototypeSort,133,134,0
-block_hint,ArrayPrototypeSort,125,126,0
-block_hint,ArrayPrototypeSort,106,107,0
-block_hint,ArrayPrototypeSort,117,118,0
-block_hint,ArrayPrototypeSort,120,121,1
-block_hint,ArrayPrototypeSort,80,81,1
-block_hint,ArrayPrototypeSort,36,37,0
+block_hint,ArrayTimSort,188,189,1
+block_hint,ArrayTimSort,138,139,0
+block_hint,ArrayTimSort,87,88,1
+block_hint,ArrayTimSort,113,114,0
+block_hint,ArrayTimSort,89,90,0
+block_hint,ArrayPrototypeSort,106,107,1
+block_hint,ArrayPrototypeSort,80,81,0
+block_hint,ArrayPrototypeSort,39,40,1
+block_hint,ArrayPrototypeSort,70,71,0
+block_hint,ArrayPrototypeSort,41,42,1
+block_hint,ArrayPrototypeSort,82,83,1
+block_hint,ArrayPrototypeSort,84,85,1
+block_hint,ArrayPrototypeSort,63,64,0
+block_hint,ArrayPrototypeSort,27,28,0
+block_hint,ArrayPrototypeSort,120,121,0
 block_hint,ArrayPrototypeSort,101,102,1
-block_hint,ArrayPrototypeSort,94,95,1
-block_hint,ArrayPrototypeSort,59,60,1
+block_hint,ArrayPrototypeSort,73,74,1
+block_hint,ArrayPrototypeSort,51,52,1
+block_hint,ArrayPrototypeSort,15,16,1
+block_hint,ArrayPrototypeSort,95,96,1
+block_hint,ArrayPrototypeSort,75,76,0
+block_hint,ArrayPrototypeSort,53,54,0
+block_hint,ArrayPrototypeSort,136,137,0
+block_hint,ArrayPrototypeSort,139,140,0
+block_hint,ArrayPrototypeSort,130,131,0
+block_hint,ArrayPrototypeSort,122,123,0
+block_hint,ArrayPrototypeSort,103,104,0
+block_hint,ArrayPrototypeSort,114,115,0
+block_hint,ArrayPrototypeSort,117,118,1
+block_hint,ArrayPrototypeSort,77,78,1
+block_hint,ArrayPrototypeSort,33,34,0
+block_hint,ArrayPrototypeSort,98,99,1
+block_hint,ArrayPrototypeSort,91,92,1
+block_hint,ArrayPrototypeSort,56,57,1
 block_hint,StringFastLocaleCompare,315,316,1
 block_hint,StringFastLocaleCompare,239,240,0
 block_hint,StringFastLocaleCompare,303,304,1
@@ -5202,6 +5195,7 @@
 block_hint,GetNamedPropertyHandler,290,291,0
 block_hint,GetNamedPropertyHandler,220,221,0
 block_hint,GetNamedPropertyHandler,294,295,1
+block_hint,GetNamedPropertyHandler,39,40,0
 block_hint,GetNamedPropertyHandler,98,99,1
 block_hint,GetNamedPropertyHandler,347,348,0
 block_hint,GetNamedPropertyHandler,242,243,0
@@ -5243,20 +5237,20 @@
 block_hint,MulHandler,91,92,1
 block_hint,MulHandler,59,60,1
 block_hint,MulHandler,23,24,1
-block_hint,DivHandler,85,86,0
-block_hint,DivHandler,66,67,0
-block_hint,DivHandler,43,44,1
+block_hint,DivHandler,109,110,0
+block_hint,DivHandler,90,91,0
+block_hint,DivHandler,63,64,1
+block_hint,DivHandler,33,34,1
+block_hint,DivHandler,121,122,1
+block_hint,DivHandler,96,97,1
+block_hint,DivHandler,66,67,1
 block_hint,DivHandler,23,24,1
-block_hint,DivHandler,95,96,1
-block_hint,DivHandler,72,73,1
-block_hint,DivHandler,46,47,1
-block_hint,DivHandler,17,18,1
-block_hint,ModHandler,89,90,0
-block_hint,ModHandler,85,86,0
-block_hint,ModHandler,68,69,1
-block_hint,ModHandler,63,64,1
-block_hint,ModHandler,29,30,0
-block_hint,ModHandler,15,16,1
+block_hint,ModHandler,129,130,0
+block_hint,ModHandler,118,119,0
+block_hint,ModHandler,92,93,1
+block_hint,ModHandler,87,88,1
+block_hint,ModHandler,50,51,0
+block_hint,ModHandler,33,34,1
 block_hint,BitwiseOrHandler,42,43,0
 block_hint,BitwiseOrHandler,30,31,1
 block_hint,BitwiseOrHandler,8,9,1
@@ -5330,10 +5324,17 @@
 block_hint,DecHandler,18,19,1
 block_hint,NegateHandler,26,27,1
 block_hint,NegateHandler,24,25,1
+block_hint,NegateHandler,14,15,1
 block_hint,ToBooleanLogicalNotHandler,15,16,0
 block_hint,ToBooleanLogicalNotHandler,21,22,0
 block_hint,ToBooleanLogicalNotHandler,7,8,0
 block_hint,TypeOfHandler,20,21,0
+block_hint,FindNonDefaultConstructorOrConstructHandler,12,13,0
+block_hint,FindNonDefaultConstructorOrConstructHandler,6,7,0
+block_hint,FindNonDefaultConstructorOrConstructHandler,14,15,1
+block_hint,FindNonDefaultConstructorOrConstructHandler,16,17,0
+block_hint,FindNonDefaultConstructorOrConstructHandler,4,5,1
+block_hint,FindNonDefaultConstructorOrConstructHandler,18,19,1
 block_hint,CallAnyReceiverHandler,21,22,1
 block_hint,CallProperty0Handler,7,8,1
 block_hint,CallProperty0Handler,62,63,0
@@ -5437,7 +5438,6 @@
 block_hint,TestGreaterThanHandler,9,10,1
 block_hint,TestLessThanOrEqualHandler,41,42,0
 block_hint,TestLessThanOrEqualHandler,9,10,1
-block_hint,TestGreaterThanOrEqualHandler,61,62,0
 block_hint,TestGreaterThanOrEqualHandler,41,42,0
 block_hint,TestGreaterThanOrEqualHandler,9,10,1
 block_hint,TestInstanceOfHandler,17,18,1
@@ -5683,720 +5683,721 @@
 block_hint,BitwiseAndSmiExtraWideHandler,18,19,1
 block_hint,CallUndefinedReceiver1ExtraWideHandler,68,69,0
 block_hint,CallUndefinedReceiver1ExtraWideHandler,19,20,0
-builtin_hash,RecordWriteSaveFP,-613048523
-builtin_hash,RecordWriteIgnoreFP,-613048523
-builtin_hash,EphemeronKeyBarrierSaveFP,-874028499
-builtin_hash,AdaptorWithBuiltinExitFrame,-50443338
-builtin_hash,Call_ReceiverIsNullOrUndefined_Baseline_Compact,277963652
-builtin_hash,Call_ReceiverIsNotNullOrUndefined_Baseline_Compact,739975018
-builtin_hash,Call_ReceiverIsAny_Baseline_Compact,739975018
-builtin_hash,CallProxy,265720458
-builtin_hash,CallWithSpread,690518666
-builtin_hash,CallWithSpread_Baseline,14944167
-builtin_hash,CallWithArrayLike,-463192950
-builtin_hash,ConstructWithSpread,1026222363
-builtin_hash,ConstructWithSpread_Baseline,-954447059
-builtin_hash,Construct_Baseline,242132798
-builtin_hash,FastNewObject,812115752
-builtin_hash,FastNewClosure,-22842529
-builtin_hash,StringEqual,552928703
-builtin_hash,StringGreaterThan,814990350
-builtin_hash,StringGreaterThanOrEqual,-931415038
-builtin_hash,StringLessThan,-931415038
-builtin_hash,StringLessThanOrEqual,814990350
-builtin_hash,StringSubstring,679034293
-builtin_hash,OrderedHashTableHealIndex,-480837431
-builtin_hash,CompileLazy,-913572652
-builtin_hash,CompileLazyDeoptimizedCode,797435305
-builtin_hash,InstantiateAsmJs,-824208537
-builtin_hash,AllocateInYoungGeneration,-589367571
-builtin_hash,AllocateRegularInYoungGeneration,549206964
-builtin_hash,AllocateRegularInOldGeneration,549206964
-builtin_hash,CopyFastSmiOrObjectElements,-664166620
-builtin_hash,GrowFastDoubleElements,-794207344
-builtin_hash,GrowFastSmiOrObjectElements,-727031326
-builtin_hash,ToNumber,87194511
-builtin_hash,ToNumber_Baseline,-245107362
-builtin_hash,ToNumeric_Baseline,765738096
-builtin_hash,ToNumberConvertBigInt,-809735249
-builtin_hash,Typeof,554300746
-builtin_hash,KeyedLoadIC_PolymorphicName,808866465
-builtin_hash,KeyedStoreIC_Megamorphic,355428822
-builtin_hash,DefineKeyedOwnIC_Megamorphic,-254774567
-builtin_hash,LoadGlobalIC_NoFeedback,567497889
-builtin_hash,LoadIC_FunctionPrototype,440547932
-builtin_hash,LoadIC_StringLength,631981109
-builtin_hash,LoadIC_StringWrapperLength,957410129
-builtin_hash,LoadIC_NoFeedback,-673925088
-builtin_hash,StoreIC_NoFeedback,599149807
-builtin_hash,DefineNamedOwnIC_NoFeedback,-684443605
-builtin_hash,KeyedLoadIC_SloppyArguments,732273933
-builtin_hash,StoreFastElementIC_Standard,-310030150
-builtin_hash,StoreFastElementIC_GrowNoTransitionHandleCOW,-894353505
-builtin_hash,StoreFastElementIC_NoTransitionHandleCOW,-684092303
-builtin_hash,ElementsTransitionAndStore_Standard,-313637466
-builtin_hash,ElementsTransitionAndStore_GrowNoTransitionHandleCOW,887654385
-builtin_hash,ElementsTransitionAndStore_NoTransitionHandleCOW,-730942180
-builtin_hash,KeyedHasIC_PolymorphicName,-900991969
-builtin_hash,EnqueueMicrotask,-201594324
-builtin_hash,RunMicrotasks,226014440
-builtin_hash,HasProperty,-179991880
-builtin_hash,DeleteProperty,-417791504
-builtin_hash,SetDataProperties,-676389083
-builtin_hash,ReturnReceiver,-253986889
-builtin_hash,ArrayConstructor,-132723945
-builtin_hash,ArrayConstructorImpl,-940010648
-builtin_hash,ArrayNoArgumentConstructor_PackedSmi_DontOverride,-419508170
-builtin_hash,ArrayNoArgumentConstructor_HoleySmi_DontOverride,-419508170
-builtin_hash,ArrayNoArgumentConstructor_PackedSmi_DisableAllocationSites,605372040
-builtin_hash,ArrayNoArgumentConstructor_Packed_DisableAllocationSites,605372040
-builtin_hash,ArrayNoArgumentConstructor_Holey_DisableAllocationSites,605372040
-builtin_hash,ArrayNoArgumentConstructor_PackedDouble_DisableAllocationSites,-118459699
-builtin_hash,ArraySingleArgumentConstructor_HoleySmi_DontOverride,-533026482
-builtin_hash,ArraySingleArgumentConstructor_HoleySmi_DisableAllocationSites,276667194
-builtin_hash,ArraySingleArgumentConstructor_Holey_DisableAllocationSites,276667194
-builtin_hash,ArraySingleArgumentConstructor_HoleyDouble_DisableAllocationSites,-533601049
-builtin_hash,ArrayIncludesSmi,-836179006
-builtin_hash,ArrayIncludesSmiOrObject,162670788
-builtin_hash,ArrayIncludes,508610041
-builtin_hash,ArrayIndexOfSmi,-144958716
-builtin_hash,ArrayIndexOfSmiOrObject,-560665373
-builtin_hash,ArrayIndexOf,659016893
-builtin_hash,ArrayPrototypePop,-672159034
-builtin_hash,ArrayPrototypePush,-828516926
-builtin_hash,CloneFastJSArray,330962956
-builtin_hash,CloneFastJSArrayFillingHoles,-114093580
-builtin_hash,ExtractFastJSArray,-899029625
-builtin_hash,ArrayPrototypeEntries,-846534049
-builtin_hash,ArrayPrototypeKeys,-432117890
-builtin_hash,ArrayPrototypeValues,-846534049
-builtin_hash,ArrayIteratorPrototypeNext,167355436
-builtin_hash,AsyncFunctionEnter,-860415031
-builtin_hash,AsyncFunctionResolve,910332485
-builtin_hash,AsyncFunctionAwaitCaught,-619125883
-builtin_hash,AsyncFunctionAwaitUncaught,-619125883
-builtin_hash,AsyncFunctionAwaitResolveClosure,-441313583
-builtin_hash,DatePrototypeGetDate,596885245
-builtin_hash,DatePrototypeGetDay,596885245
-builtin_hash,DatePrototypeGetFullYear,596885245
-builtin_hash,DatePrototypeGetHours,596885245
-builtin_hash,DatePrototypeGetMilliseconds,-147735130
-builtin_hash,DatePrototypeGetMinutes,596885245
-builtin_hash,DatePrototypeGetMonth,596885245
-builtin_hash,DatePrototypeGetSeconds,596885245
-builtin_hash,DatePrototypeGetTime,842589209
-builtin_hash,DatePrototypeGetTimezoneOffset,-147735130
-builtin_hash,DatePrototypeValueOf,842589209
-builtin_hash,DatePrototypeToPrimitive,-469261030
-builtin_hash,CreateIterResultObject,-236239497
-builtin_hash,CreateGeneratorObject,-989601020
-builtin_hash,GeneratorPrototypeNext,-532167070
-builtin_hash,GeneratorPrototypeReturn,204056688
-builtin_hash,SuspendGeneratorBaseline,-716242694
-builtin_hash,ResumeGeneratorBaseline,600643426
-builtin_hash,GlobalIsFinite,-28742852
-builtin_hash,GlobalIsNaN,-414427038
-builtin_hash,LoadIC,-1028921753
-builtin_hash,LoadIC_Megamorphic,604208967
-builtin_hash,LoadIC_Noninlined,-411987614
-builtin_hash,LoadICTrampoline,800274028
-builtin_hash,LoadICBaseline,470944725
-builtin_hash,LoadICTrampoline_Megamorphic,800274028
-builtin_hash,LoadSuperIC,-145652312
-builtin_hash,LoadSuperICBaseline,-463763660
-builtin_hash,KeyedLoadIC,-400473566
-builtin_hash,KeyedLoadIC_Megamorphic,41817838
-builtin_hash,KeyedLoadICTrampoline,800274028
-builtin_hash,KeyedLoadICBaseline,470944725
-builtin_hash,KeyedLoadICTrampoline_Megamorphic,800274028
-builtin_hash,StoreGlobalIC,-985598929
-builtin_hash,StoreGlobalICTrampoline,800274028
-builtin_hash,StoreGlobalICBaseline,470944725
-builtin_hash,StoreIC,107868822
-builtin_hash,StoreICTrampoline,515324941
-builtin_hash,StoreICBaseline,-463763660
-builtin_hash,DefineNamedOwnIC,293425336
-builtin_hash,DefineNamedOwnICBaseline,-463763660
-builtin_hash,KeyedStoreIC,-634858106
-builtin_hash,KeyedStoreICTrampoline,515324941
-builtin_hash,KeyedStoreICBaseline,-463763660
-builtin_hash,DefineKeyedOwnIC,-567510982
-builtin_hash,StoreInArrayLiteralIC,336733574
-builtin_hash,StoreInArrayLiteralICBaseline,-463763660
-builtin_hash,LoadGlobalIC,-994002095
-builtin_hash,LoadGlobalICInsideTypeof,131610143
-builtin_hash,LoadGlobalICTrampoline,-356577892
-builtin_hash,LoadGlobalICBaseline,-87390287
-builtin_hash,LoadGlobalICInsideTypeofTrampoline,-356577892
-builtin_hash,LoadGlobalICInsideTypeofBaseline,-87390287
-builtin_hash,LookupGlobalICBaseline,195819709
-builtin_hash,LookupGlobalICInsideTypeofBaseline,195819709
-builtin_hash,KeyedHasIC,-581893205
-builtin_hash,KeyedHasICBaseline,470944725
-builtin_hash,KeyedHasIC_Megamorphic,-179991880
-builtin_hash,IterableToList,-847583682
-builtin_hash,IterableToListWithSymbolLookup,639766325
-builtin_hash,IterableToListMayPreserveHoles,915672519
-builtin_hash,FindOrderedHashMapEntry,257985360
-builtin_hash,MapConstructor,173900465
-builtin_hash,MapPrototypeSet,-909373880
-builtin_hash,MapPrototypeDelete,-182536468
-builtin_hash,MapPrototypeGet,-10028336
-builtin_hash,MapPrototypeHas,-139761843
-builtin_hash,MapPrototypeEntries,-344495525
-builtin_hash,MapPrototypeGetSize,1002199563
-builtin_hash,MapPrototypeForEach,666422496
-builtin_hash,MapPrototypeKeys,-344495525
-builtin_hash,MapPrototypeValues,-344495525
-builtin_hash,MapIteratorPrototypeNext,824163271
-builtin_hash,MapIteratorToList,-171739571
-builtin_hash,SameValueNumbersOnly,-385008716
-builtin_hash,Add_Baseline,-279802821
-builtin_hash,AddSmi_Baseline,-180294218
-builtin_hash,Subtract_Baseline,422911741
-builtin_hash,SubtractSmi_Baseline,593938918
-builtin_hash,Multiply_Baseline,-390820476
-builtin_hash,MultiplySmi_Baseline,325873812
-builtin_hash,Divide_Baseline,-303206156
-builtin_hash,DivideSmi_Baseline,-760734875
-builtin_hash,Modulus_Baseline,-56419644
-builtin_hash,ModulusSmi_Baseline,-723448
-builtin_hash,Exponentiate_Baseline,-897267514
-builtin_hash,BitwiseAnd_Baseline,368212144
-builtin_hash,BitwiseAndSmi_Baseline,-1040430105
-builtin_hash,BitwiseOr_Baseline,-468458668
-builtin_hash,BitwiseOrSmi_Baseline,688726246
-builtin_hash,BitwiseXor_Baseline,-113074811
-builtin_hash,BitwiseXorSmi_Baseline,601401020
-builtin_hash,ShiftLeft_Baseline,-775732772
-builtin_hash,ShiftLeftSmi_Baseline,-78665210
-builtin_hash,ShiftRight_Baseline,748634885
-builtin_hash,ShiftRightSmi_Baseline,886941283
-builtin_hash,ShiftRightLogical_Baseline,561208446
-builtin_hash,ShiftRightLogicalSmi_Baseline,-31850172
-builtin_hash,Add_WithFeedback,-713508648
-builtin_hash,Subtract_WithFeedback,-1006518356
-builtin_hash,Modulus_WithFeedback,673708690
-builtin_hash,BitwiseOr_WithFeedback,-71811840
-builtin_hash,Equal_Baseline,-449571287
-builtin_hash,StrictEqual_Baseline,-311709296
-builtin_hash,LessThan_Baseline,-1041710075
-builtin_hash,GreaterThan_Baseline,763769306
-builtin_hash,LessThanOrEqual_Baseline,-289600196
-builtin_hash,GreaterThanOrEqual_Baseline,-964000144
-builtin_hash,Equal_WithFeedback,-804822195
-builtin_hash,StrictEqual_WithFeedback,316409561
-builtin_hash,LessThan_WithFeedback,-1041748847
-builtin_hash,GreaterThan_WithFeedback,208079969
-builtin_hash,GreaterThanOrEqual_WithFeedback,50039232
-builtin_hash,BitwiseNot_Baseline,574212378
-builtin_hash,Decrement_Baseline,740961552
-builtin_hash,Increment_Baseline,-482954167
-builtin_hash,Negate_Baseline,257429052
-builtin_hash,ObjectAssign,415745977
-builtin_hash,ObjectCreate,152352347
-builtin_hash,ObjectEntries,-267361188
-builtin_hash,ObjectGetOwnPropertyDescriptor,-1005546404
-builtin_hash,ObjectGetOwnPropertyNames,-10249982
-builtin_hash,ObjectIs,947042700
-builtin_hash,ObjectKeys,276395735
-builtin_hash,ObjectPrototypeHasOwnProperty,-366540189
-builtin_hash,ObjectToString,-680252272
-builtin_hash,InstanceOf_WithFeedback,-814385450
-builtin_hash,InstanceOf_Baseline,-567095434
-builtin_hash,ForInEnumerate,329908035
-builtin_hash,ForInPrepare,731557174
-builtin_hash,ForInFilter,884185984
-builtin_hash,RegExpConstructor,-1029370119
-builtin_hash,RegExpExecAtom,181372809
-builtin_hash,RegExpExecInternal,317900879
-builtin_hash,FindOrderedHashSetEntry,482436035
-builtin_hash,SetConstructor,692235107
-builtin_hash,SetPrototypeHas,-139761843
-builtin_hash,SetPrototypeAdd,-596680080
-builtin_hash,SetPrototypeDelete,331633635
-builtin_hash,SetPrototypeEntries,-344495525
-builtin_hash,SetPrototypeGetSize,1002199563
-builtin_hash,SetPrototypeForEach,97244170
-builtin_hash,SetPrototypeValues,-344495525
-builtin_hash,SetIteratorPrototypeNext,-441725951
-builtin_hash,SetOrSetIteratorToList,623342942
-builtin_hash,StringFromCharCode,-123751380
-builtin_hash,StringPrototypeReplace,-921072145
-builtin_hash,StringPrototypeSplit,415613472
-builtin_hash,TypedArrayConstructor,32466415
-builtin_hash,TypedArrayPrototypeByteLength,864895308
-builtin_hash,TypedArrayPrototypeLength,539604699
-builtin_hash,WeakMapConstructor,814764494
-builtin_hash,WeakMapLookupHashIndex,-464287185
-builtin_hash,WeakMapGet,925651553
-builtin_hash,WeakMapPrototypeHas,947465532
-builtin_hash,WeakMapPrototypeSet,-976760951
-builtin_hash,WeakSetConstructor,694246453
-builtin_hash,WeakSetPrototypeHas,947465532
-builtin_hash,WeakSetPrototypeAdd,-160318733
-builtin_hash,WeakCollectionSet,578996244
-builtin_hash,AsyncGeneratorResolve,-83028412
-builtin_hash,AsyncGeneratorYieldWithAwait,-366463177
-builtin_hash,AsyncGeneratorResumeNext,220127321
-builtin_hash,AsyncGeneratorPrototypeNext,1069549757
-builtin_hash,AsyncGeneratorAwaitUncaught,-628599896
-builtin_hash,AsyncGeneratorAwaitResolveClosure,1062097477
-builtin_hash,AsyncGeneratorYieldWithAwaitResolveClosure,793122606
-builtin_hash,StringAdd_CheckNone,113370168
-builtin_hash,SubString,895503589
-builtin_hash,GetProperty,1052862169
-builtin_hash,GetPropertyWithReceiver,1045827042
-builtin_hash,SetProperty,908643608
-builtin_hash,CreateDataProperty,-314133834
-builtin_hash,ArrayPrototypeConcat,-557766770
-builtin_hash,ArrayEvery,-740699383
-builtin_hash,ArrayFilterLoopLazyDeoptContinuation,-463893516
-builtin_hash,ArrayFilterLoopContinuation,-636224543
-builtin_hash,ArrayFilter,-1006837550
-builtin_hash,ArrayPrototypeFind,358067331
-builtin_hash,ArrayForEachLoopLazyDeoptContinuation,-227856192
-builtin_hash,ArrayForEachLoopContinuation,498815593
-builtin_hash,ArrayForEach,-465472618
-builtin_hash,ArrayFrom,559791774
-builtin_hash,ArrayIsArray,556045869
-builtin_hash,LoadJoinElement_FastSmiOrObjectElements_0,4464260
-builtin_hash,LoadJoinElement_FastDoubleElements_0,-669389930
-builtin_hash,JoinStackPush,932509525
-builtin_hash,JoinStackPop,97051696
-builtin_hash,ArrayPrototypeJoin,638420418
-builtin_hash,ArrayPrototypeToString,571363693
-builtin_hash,ArrayPrototypeLastIndexOf,-262998450
-builtin_hash,ArrayMapLoopLazyDeoptContinuation,992596139
-builtin_hash,ArrayMapLoopContinuation,852679435
-builtin_hash,ArrayMap,237015696
-builtin_hash,ArrayReduceLoopLazyDeoptContinuation,-1021360101
-builtin_hash,ArrayReduceLoopContinuation,736239909
-builtin_hash,ArrayReduce,550306639
-builtin_hash,ArrayPrototypeReverse,-848939503
-builtin_hash,ArrayPrototypeShift,510698980
-builtin_hash,ArrayPrototypeSlice,-226926113
-builtin_hash,ArraySome,616986483
-builtin_hash,ArrayPrototypeSplice,318122997
-builtin_hash,ArrayPrototypeUnshift,942293281
-builtin_hash,ArrayBufferPrototypeGetByteLength,8155127
-builtin_hash,ArrayBufferIsView,-92420774
-builtin_hash,ToInteger,114221870
-builtin_hash,FastCreateDataProperty,683077437
-builtin_hash,BooleanConstructor,104847507
-builtin_hash,BooleanPrototypeToString,496844333
-builtin_hash,ToString,-492204321
-builtin_hash,StringPrototypeToString,232130928
-builtin_hash,StringPrototypeValueOf,232130928
-builtin_hash,StringPrototypeCharAt,-493882295
-builtin_hash,StringPrototypeCharCodeAt,-70476469
-builtin_hash,StringPrototypeCodePointAt,958343749
-builtin_hash,StringPrototypeConcat,122908250
-builtin_hash,StringConstructor,36941296
-builtin_hash,StringAddConvertLeft,895631940
-builtin_hash,StringAddConvertRight,620894196
-builtin_hash,StringCharAt,-771156702
-builtin_hash,FastNewClosureBaseline,-345301780
-builtin_hash,FastNewFunctionContextFunction,393493853
-builtin_hash,CreateRegExpLiteral,1052274841
-builtin_hash,CreateShallowArrayLiteral,758569216
-builtin_hash,CreateEmptyArrayLiteral,-244361805
-builtin_hash,CreateShallowObjectLiteral,429596211
-builtin_hash,ObjectConstructor,792071103
-builtin_hash,CreateEmptyLiteralObject,792021411
-builtin_hash,NumberConstructor,-545912408
-builtin_hash,StringToNumber,-567475001
-builtin_hash,NonNumberToNumber,-75339598
-builtin_hash,NonNumberToNumeric,-163611573
-builtin_hash,ToNumeric,1067114169
-builtin_hash,NumberToString,808056721
-builtin_hash,ToBoolean,474893826
-builtin_hash,ToBooleanForBaselineJump,-1000387172
-builtin_hash,ToLength,-752062439
-builtin_hash,ToName,-893589751
-builtin_hash,ToObject,-995611522
-builtin_hash,NonPrimitiveToPrimitive_Default,-741936834
-builtin_hash,NonPrimitiveToPrimitive_Number,-741936834
-builtin_hash,NonPrimitiveToPrimitive_String,-741936834
-builtin_hash,OrdinaryToPrimitive_Number,940682530
-builtin_hash,OrdinaryToPrimitive_String,940682530
-builtin_hash,DataViewPrototypeGetByteLength,-344862281
-builtin_hash,DataViewPrototypeGetFloat64,-710736378
-builtin_hash,DataViewPrototypeSetUint32,561326289
-builtin_hash,DataViewPrototypeSetFloat64,224815643
-builtin_hash,FunctionPrototypeHasInstance,-159239165
-builtin_hash,FastFunctionPrototypeBind,-835190429
-builtin_hash,ForInNext,-628108871
-builtin_hash,GetIteratorWithFeedback,412632852
-builtin_hash,GetIteratorBaseline,878549031
-builtin_hash,CallIteratorWithFeedback,-173921836
-builtin_hash,MathAbs,-418374171
-builtin_hash,MathCeil,466763348
-builtin_hash,MathFloor,471221640
-builtin_hash,MathRound,-989099866
-builtin_hash,MathPow,510691647
-builtin_hash,MathMax,45115699
-builtin_hash,MathMin,-996382942
-builtin_hash,MathAsin,261451622
-builtin_hash,MathAtan2,605332815
-builtin_hash,MathCos,515079504
-builtin_hash,MathExp,551351922
-builtin_hash,MathFround,564706237
-builtin_hash,MathImul,685265173
-builtin_hash,MathLog,-553256829
-builtin_hash,MathSin,302395292
-builtin_hash,MathSign,611819739
-builtin_hash,MathSqrt,55107225
-builtin_hash,MathTan,-332405887
-builtin_hash,MathTanh,939045985
-builtin_hash,MathRandom,-504157126
-builtin_hash,NumberPrototypeToString,145247584
-builtin_hash,NumberIsInteger,910409330
-builtin_hash,NumberIsNaN,343619286
-builtin_hash,NumberParseFloat,-745268146
-builtin_hash,ParseInt,423449565
-builtin_hash,NumberParseInt,348325306
-builtin_hash,Add,-712082634
-builtin_hash,Subtract,860006498
-builtin_hash,Multiply,966938552
-builtin_hash,Divide,501339465
-builtin_hash,Modulus,556264773
-builtin_hash,CreateObjectWithoutProperties,911390056
-builtin_hash,ObjectIsExtensible,-376770424
-builtin_hash,ObjectPreventExtensions,-675757061
-builtin_hash,ObjectGetPrototypeOf,-694816240
-builtin_hash,ObjectSetPrototypeOf,-335823538
-builtin_hash,ObjectPrototypeToString,158685312
-builtin_hash,ObjectPrototypeValueOf,-993024104
-builtin_hash,FulfillPromise,-68874675
-builtin_hash,NewPromiseCapability,-880232666
-builtin_hash,PromiseCapabilityDefaultResolve,694927325
-builtin_hash,PerformPromiseThen,-238303189
-builtin_hash,PromiseAll,-121414633
-builtin_hash,PromiseAllResolveElementClosure,797273436
-builtin_hash,PromiseConstructor,-424149894
-builtin_hash,PromisePrototypeCatch,235262026
-builtin_hash,PromiseFulfillReactionJob,927825363
-builtin_hash,PromiseResolveTrampoline,-549629094
-builtin_hash,PromiseResolve,-366429795
-builtin_hash,ResolvePromise,526061379
-builtin_hash,PromisePrototypeThen,959282415
-builtin_hash,PromiseResolveThenableJob,-977786068
-builtin_hash,ProxyConstructor,-54504231
-builtin_hash,ProxyGetProperty,-692505715
-builtin_hash,ProxyIsExtensible,-120987472
-builtin_hash,ProxyPreventExtensions,739592105
-builtin_hash,ReflectGet,1006327680
-builtin_hash,ReflectHas,-549629094
-builtin_hash,RegExpPrototypeExec,866694176
-builtin_hash,RegExpMatchFast,556779044
-builtin_hash,RegExpReplace,1037671691
-builtin_hash,RegExpPrototypeReplace,-488505709
-builtin_hash,RegExpSearchFast,744647901
-builtin_hash,RegExpPrototypeSourceGetter,-69902772
-builtin_hash,RegExpSplit,418335022
-builtin_hash,RegExpPrototypeTest,-893509849
-builtin_hash,RegExpPrototypeTestFast,-541676085
-builtin_hash,RegExpPrototypeGlobalGetter,612394650
-builtin_hash,RegExpPrototypeIgnoreCaseGetter,-595775382
-builtin_hash,RegExpPrototypeMultilineGetter,368200363
-builtin_hash,RegExpPrototypeHasIndicesGetter,99570183
-builtin_hash,RegExpPrototypeDotAllGetter,99570183
-builtin_hash,RegExpPrototypeStickyGetter,471291660
-builtin_hash,RegExpPrototypeUnicodeGetter,471291660
-builtin_hash,RegExpPrototypeFlagsGetter,-493351549
-builtin_hash,StringPrototypeEndsWith,-140669855
-builtin_hash,StringPrototypeIncludes,-538712449
-builtin_hash,StringPrototypeIndexOf,-279080867
-builtin_hash,StringPrototypeIterator,-906814404
-builtin_hash,StringIteratorPrototypeNext,-459023719
-builtin_hash,StringPrototypeMatch,950777323
-builtin_hash,StringPrototypeSearch,950777323
-builtin_hash,StringRepeat,333496990
-builtin_hash,StringPrototypeSlice,147923310
-builtin_hash,StringPrototypeStartsWith,-916453690
-builtin_hash,StringPrototypeSubstr,93046303
-builtin_hash,StringPrototypeSubstring,-486167723
-builtin_hash,StringPrototypeTrim,-537839064
-builtin_hash,SymbolPrototypeToString,-331094885
-builtin_hash,CreateTypedArray,946007034
-builtin_hash,TypedArrayFrom,-383816322
-builtin_hash,TypedArrayPrototypeSet,183639399
-builtin_hash,TypedArrayPrototypeSubArray,-654743264
-builtin_hash,NewSloppyArgumentsElements,-733955106
-builtin_hash,NewStrictArgumentsElements,27861461
-builtin_hash,NewRestArgumentsElements,-158196826
-builtin_hash,FastNewSloppyArguments,701807193
-builtin_hash,FastNewStrictArguments,-400637158
-builtin_hash,FastNewRestArguments,771398605
-builtin_hash,StringSlowFlatten,758688335
-builtin_hash,StringIndexOf,893861646
-builtin_hash,Load_FastSmiElements_0,41377987
-builtin_hash,Load_FastObjectElements_0,41377987
-builtin_hash,Store_FastSmiElements_0,987491586
-builtin_hash,Store_FastObjectElements_0,-907039137
-builtin_hash,SortCompareDefault,-712046902
-builtin_hash,SortCompareUserFn,-498446944
-builtin_hash,Copy,1005972100
-builtin_hash,MergeAt,-238184884
-builtin_hash,GallopLeft,-228579918
-builtin_hash,GallopRight,508662767
-builtin_hash,ArrayTimSort,-584574007
-builtin_hash,ArrayPrototypeSort,-446345392
-builtin_hash,StringFastLocaleCompare,-805723901
-builtin_hash,WasmInt32ToHeapNumber,186218317
-builtin_hash,WasmTaggedNonSmiToInt32,644195797
-builtin_hash,WasmTriggerTierUp,-448026998
-builtin_hash,WasmStackGuard,929375954
-builtin_hash,CanUseSameAccessor_FastSmiElements_0,333215288
-builtin_hash,CanUseSameAccessor_FastObjectElements_0,333215288
-builtin_hash,StringPrototypeToLowerCaseIntl,325118773
-builtin_hash,StringToLowerCaseIntl,729618594
-builtin_hash,WideHandler,-985531040
-builtin_hash,ExtraWideHandler,-985531040
-builtin_hash,LdarHandler,1066069071
-builtin_hash,LdaZeroHandler,697098880
-builtin_hash,LdaSmiHandler,-92763154
-builtin_hash,LdaUndefinedHandler,94159659
-builtin_hash,LdaNullHandler,94159659
-builtin_hash,LdaTheHoleHandler,94159659
-builtin_hash,LdaTrueHandler,66190034
-builtin_hash,LdaFalseHandler,66190034
-builtin_hash,LdaConstantHandler,-234672240
-builtin_hash,LdaContextSlotHandler,999512170
-builtin_hash,LdaImmutableContextSlotHandler,999512170
-builtin_hash,LdaCurrentContextSlotHandler,-705029165
-builtin_hash,LdaImmutableCurrentContextSlotHandler,-705029165
-builtin_hash,StarHandler,-825981541
-builtin_hash,MovHandler,-222623368
-builtin_hash,PushContextHandler,239039195
-builtin_hash,PopContextHandler,663403390
-builtin_hash,TestReferenceEqualHandler,107959616
-builtin_hash,TestUndetectableHandler,768306054
-builtin_hash,TestNullHandler,317848228
-builtin_hash,TestUndefinedHandler,317848228
-builtin_hash,TestTypeOfHandler,-585531608
-builtin_hash,LdaGlobalHandler,680542536
-builtin_hash,LdaGlobalInsideTypeofHandler,-812384965
-builtin_hash,StaGlobalHandler,-849976646
-builtin_hash,StaContextSlotHandler,-642236485
-builtin_hash,StaCurrentContextSlotHandler,515612512
-builtin_hash,LdaLookupGlobalSlotHandler,328181263
-builtin_hash,LdaLookupGlobalSlotInsideTypeofHandler,-152487163
-builtin_hash,StaLookupSlotHandler,1043986971
-builtin_hash,GetNamedPropertyHandler,-918198086
-builtin_hash,GetNamedPropertyFromSuperHandler,-605958764
-builtin_hash,GetKeyedPropertyHandler,-368783501
-builtin_hash,SetNamedPropertyHandler,512867069
-builtin_hash,DefineNamedOwnPropertyHandler,512867069
-builtin_hash,SetKeyedPropertyHandler,-529790650
-builtin_hash,DefineKeyedOwnPropertyHandler,-529790650
-builtin_hash,StaInArrayLiteralHandler,-529790650
-builtin_hash,DefineKeyedOwnPropertyInLiteralHandler,519916231
-builtin_hash,AddHandler,-1014428769
-builtin_hash,SubHandler,-971645828
-builtin_hash,MulHandler,-1072793455
-builtin_hash,DivHandler,-162323805
-builtin_hash,ModHandler,-485795098
-builtin_hash,ExpHandler,795159955
-builtin_hash,BitwiseOrHandler,-974394049
-builtin_hash,BitwiseXorHandler,580834482
-builtin_hash,BitwiseAndHandler,614318128
-builtin_hash,ShiftLeftHandler,-423182377
-builtin_hash,ShiftRightHandler,-255081510
-builtin_hash,ShiftRightLogicalHandler,735938776
-builtin_hash,AddSmiHandler,107839307
-builtin_hash,SubSmiHandler,-363881533
-builtin_hash,MulSmiHandler,169761579
-builtin_hash,DivSmiHandler,-681265328
-builtin_hash,ModSmiHandler,861935655
-builtin_hash,BitwiseOrSmiHandler,-680303745
-builtin_hash,BitwiseXorSmiHandler,576458108
-builtin_hash,BitwiseAndSmiHandler,-994511503
-builtin_hash,ShiftLeftSmiHandler,-728693655
-builtin_hash,ShiftRightSmiHandler,975905832
-builtin_hash,ShiftRightLogicalSmiHandler,686146238
-builtin_hash,IncHandler,117772531
-builtin_hash,DecHandler,-691015839
-builtin_hash,NegateHandler,212889736
-builtin_hash,BitwiseNotHandler,-960473652
-builtin_hash,ToBooleanLogicalNotHandler,-997041363
-builtin_hash,LogicalNotHandler,-404436240
-builtin_hash,TypeOfHandler,-868029172
-builtin_hash,DeletePropertyStrictHandler,-310645655
-builtin_hash,DeletePropertySloppyHandler,-884621901
-builtin_hash,GetSuperConstructorHandler,-336144805
-builtin_hash,CallAnyReceiverHandler,-483788286
-builtin_hash,CallPropertyHandler,-483788286
-builtin_hash,CallProperty0Handler,234175094
-builtin_hash,CallProperty1Handler,354307341
-builtin_hash,CallProperty2Handler,968021051
-builtin_hash,CallUndefinedReceiverHandler,472718464
-builtin_hash,CallUndefinedReceiver0Handler,1020191467
-builtin_hash,CallUndefinedReceiver1Handler,785762305
-builtin_hash,CallUndefinedReceiver2Handler,-921863582
-builtin_hash,CallWithSpreadHandler,-483788286
-builtin_hash,CallRuntimeHandler,575543766
-builtin_hash,CallJSRuntimeHandler,-279970155
-builtin_hash,InvokeIntrinsicHandler,315814934
-builtin_hash,ConstructHandler,750653559
-builtin_hash,ConstructWithSpreadHandler,-950529667
-builtin_hash,TestEqualHandler,469957169
-builtin_hash,TestEqualStrictHandler,774972588
-builtin_hash,TestLessThanHandler,876731233
-builtin_hash,TestGreaterThanHandler,854370589
-builtin_hash,TestLessThanOrEqualHandler,-616820445
-builtin_hash,TestGreaterThanOrEqualHandler,128578007
-builtin_hash,TestInstanceOfHandler,437146777
-builtin_hash,TestInHandler,-595986293
-builtin_hash,ToNameHandler,-388837341
-builtin_hash,ToNumberHandler,172727215
-builtin_hash,ToNumericHandler,518340123
-builtin_hash,ToObjectHandler,-388837341
-builtin_hash,ToStringHandler,-736791596
-builtin_hash,CreateRegExpLiteralHandler,-387261303
-builtin_hash,CreateArrayLiteralHandler,544722821
-builtin_hash,CreateArrayFromIterableHandler,-590862374
-builtin_hash,CreateEmptyArrayLiteralHandler,-215104396
-builtin_hash,CreateObjectLiteralHandler,536615992
-builtin_hash,CreateEmptyObjectLiteralHandler,810635729
-builtin_hash,CreateClosureHandler,-899658211
-builtin_hash,CreateBlockContextHandler,-125556632
-builtin_hash,CreateCatchContextHandler,551209828
-builtin_hash,CreateFunctionContextHandler,-65684761
-builtin_hash,CreateMappedArgumentsHandler,67709625
-builtin_hash,CreateUnmappedArgumentsHandler,608258279
-builtin_hash,CreateRestParameterHandler,1042430952
-builtin_hash,JumpLoopHandler,77742379
-builtin_hash,JumpHandler,-420188660
-builtin_hash,JumpConstantHandler,-998825364
-builtin_hash,JumpIfUndefinedConstantHandler,-326209739
-builtin_hash,JumpIfNotUndefinedConstantHandler,37208057
-builtin_hash,JumpIfUndefinedOrNullConstantHandler,-104381115
-builtin_hash,JumpIfTrueConstantHandler,-326209739
-builtin_hash,JumpIfFalseConstantHandler,-326209739
-builtin_hash,JumpIfToBooleanTrueConstantHandler,-234142841
-builtin_hash,JumpIfToBooleanFalseConstantHandler,-602774868
-builtin_hash,JumpIfToBooleanTrueHandler,-297635325
-builtin_hash,JumpIfToBooleanFalseHandler,1015367976
-builtin_hash,JumpIfTrueHandler,862147447
-builtin_hash,JumpIfFalseHandler,862147447
-builtin_hash,JumpIfNullHandler,862147447
-builtin_hash,JumpIfNotNullHandler,-481058680
-builtin_hash,JumpIfUndefinedHandler,862147447
-builtin_hash,JumpIfNotUndefinedHandler,-481058680
-builtin_hash,JumpIfUndefinedOrNullHandler,14126870
-builtin_hash,JumpIfJSReceiverHandler,-723850389
-builtin_hash,SwitchOnSmiNoFeedbackHandler,-902670490
-builtin_hash,ForInEnumerateHandler,-322331924
-builtin_hash,ForInPrepareHandler,20034175
-builtin_hash,ForInContinueHandler,827732360
-builtin_hash,ForInNextHandler,119110335
-builtin_hash,ForInStepHandler,757646701
-builtin_hash,SetPendingMessageHandler,996401409
-builtin_hash,ThrowHandler,122680912
-builtin_hash,ReThrowHandler,122680912
-builtin_hash,ReturnHandler,47039723
-builtin_hash,ThrowReferenceErrorIfHoleHandler,-342650955
-builtin_hash,ThrowSuperNotCalledIfHoleHandler,-285583864
-builtin_hash,ThrowSuperAlreadyCalledIfNotHoleHandler,-827541184
-builtin_hash,ThrowIfNotSuperConstructorHandler,1018623070
-builtin_hash,SwitchOnGeneratorStateHandler,717471818
-builtin_hash,SuspendGeneratorHandler,547514791
-builtin_hash,ResumeGeneratorHandler,-860485588
-builtin_hash,GetIteratorHandler,-6630463
-builtin_hash,ShortStarHandler,721894508
-builtin_hash,LdarWideHandler,-978392409
-builtin_hash,LdaSmiWideHandler,-366656871
-builtin_hash,LdaConstantWideHandler,972813981
-builtin_hash,LdaContextSlotWideHandler,628329787
-builtin_hash,LdaImmutableContextSlotWideHandler,628329787
-builtin_hash,LdaImmutableCurrentContextSlotWideHandler,489858159
-builtin_hash,StarWideHandler,-1038662456
-builtin_hash,MovWideHandler,483803273
-builtin_hash,PushContextWideHandler,216419588
-builtin_hash,PopContextWideHandler,272986324
-builtin_hash,TestReferenceEqualWideHandler,-4739833
-builtin_hash,LdaGlobalWideHandler,-434470564
-builtin_hash,LdaGlobalInsideTypeofWideHandler,888730933
-builtin_hash,StaGlobalWideHandler,459118950
-builtin_hash,StaContextSlotWideHandler,888275701
-builtin_hash,StaCurrentContextSlotWideHandler,-317584552
-builtin_hash,LdaLookupGlobalSlotWideHandler,1026575020
-builtin_hash,GetNamedPropertyWideHandler,664403992
-builtin_hash,GetKeyedPropertyWideHandler,322108853
-builtin_hash,SetNamedPropertyWideHandler,784668777
-builtin_hash,DefineNamedOwnPropertyWideHandler,784668777
-builtin_hash,SetKeyedPropertyWideHandler,1015904043
-builtin_hash,DefineKeyedOwnPropertyWideHandler,1015904043
-builtin_hash,StaInArrayLiteralWideHandler,1015904043
-builtin_hash,AddWideHandler,1006647977
-builtin_hash,SubWideHandler,212325320
-builtin_hash,MulWideHandler,-922622067
-builtin_hash,DivWideHandler,145054418
-builtin_hash,BitwiseOrWideHandler,805505097
-builtin_hash,BitwiseAndWideHandler,563101073
-builtin_hash,ShiftLeftWideHandler,448918085
-builtin_hash,AddSmiWideHandler,-135072104
-builtin_hash,SubSmiWideHandler,-169078418
-builtin_hash,MulSmiWideHandler,793690226
-builtin_hash,DivSmiWideHandler,-657180043
-builtin_hash,ModSmiWideHandler,335754550
-builtin_hash,BitwiseOrSmiWideHandler,-1067934836
-builtin_hash,BitwiseXorSmiWideHandler,-668709153
-builtin_hash,BitwiseAndSmiWideHandler,-90084544
-builtin_hash,ShiftLeftSmiWideHandler,-381247703
-builtin_hash,ShiftRightSmiWideHandler,-38676513
-builtin_hash,ShiftRightLogicalSmiWideHandler,-1026231042
-builtin_hash,IncWideHandler,389395178
-builtin_hash,DecWideHandler,1062128797
-builtin_hash,NegateWideHandler,375542705
-builtin_hash,CallPropertyWideHandler,479651507
-builtin_hash,CallProperty0WideHandler,402451236
-builtin_hash,CallProperty1WideHandler,864866147
-builtin_hash,CallProperty2WideHandler,672960581
-builtin_hash,CallUndefinedReceiverWideHandler,633606056
-builtin_hash,CallUndefinedReceiver0WideHandler,-782323787
-builtin_hash,CallUndefinedReceiver1WideHandler,52355318
-builtin_hash,CallUndefinedReceiver2WideHandler,297430331
-builtin_hash,CallWithSpreadWideHandler,479651507
-builtin_hash,ConstructWideHandler,-923801363
-builtin_hash,TestEqualWideHandler,745861994
-builtin_hash,TestEqualStrictWideHandler,982796365
-builtin_hash,TestLessThanWideHandler,665221830
-builtin_hash,TestGreaterThanWideHandler,776130121
-builtin_hash,TestLessThanOrEqualWideHandler,-299580558
-builtin_hash,TestGreaterThanOrEqualWideHandler,-356242933
-builtin_hash,TestInstanceOfWideHandler,406240218
-builtin_hash,TestInWideHandler,-754759119
-builtin_hash,ToNumericWideHandler,1034444948
-builtin_hash,CreateRegExpLiteralWideHandler,1015965077
-builtin_hash,CreateArrayLiteralWideHandler,238187057
-builtin_hash,CreateEmptyArrayLiteralWideHandler,-21075025
-builtin_hash,CreateObjectLiteralWideHandler,570835533
-builtin_hash,CreateClosureWideHandler,912422636
-builtin_hash,CreateBlockContextWideHandler,499748521
-builtin_hash,CreateFunctionContextWideHandler,-887672919
-builtin_hash,JumpLoopWideHandler,714317010
-builtin_hash,JumpWideHandler,-420188660
-builtin_hash,JumpIfToBooleanTrueWideHandler,230302934
-builtin_hash,JumpIfToBooleanFalseWideHandler,237768975
-builtin_hash,JumpIfTrueWideHandler,814624851
-builtin_hash,JumpIfFalseWideHandler,814624851
-builtin_hash,SwitchOnSmiNoFeedbackWideHandler,623977068
-builtin_hash,ForInPrepareWideHandler,430965432
-builtin_hash,ForInNextWideHandler,-899950637
-builtin_hash,ThrowReferenceErrorIfHoleWideHandler,-575574526
-builtin_hash,GetIteratorWideHandler,-626454663
-builtin_hash,LdaSmiExtraWideHandler,465680004
-builtin_hash,LdaGlobalExtraWideHandler,1016564513
-builtin_hash,AddSmiExtraWideHandler,585533206
-builtin_hash,SubSmiExtraWideHandler,-88717151
-builtin_hash,MulSmiExtraWideHandler,-508453390
-builtin_hash,DivSmiExtraWideHandler,-542490757
-builtin_hash,BitwiseOrSmiExtraWideHandler,776661340
-builtin_hash,BitwiseXorSmiExtraWideHandler,276228867
-builtin_hash,BitwiseAndSmiExtraWideHandler,739058259
-builtin_hash,CallUndefinedReceiverExtraWideHandler,488508421
-builtin_hash,CallUndefinedReceiver1ExtraWideHandler,700320270
-builtin_hash,CallUndefinedReceiver2ExtraWideHandler,-7276189
+builtin_hash,RecordWriteSaveFP,-787985789
+builtin_hash,RecordWriteIgnoreFP,-787985789
+builtin_hash,EphemeronKeyBarrierSaveFP,-762846067
+builtin_hash,AdaptorWithBuiltinExitFrame,245562366
+builtin_hash,Call_ReceiverIsNullOrUndefined_Baseline_Compact,-701969451
+builtin_hash,Call_ReceiverIsNotNullOrUndefined_Baseline_Compact,-324308522
+builtin_hash,Call_ReceiverIsAny_Baseline_Compact,-324308522
+builtin_hash,CallProxy,1028339399
+builtin_hash,CallWithSpread,535056033
+builtin_hash,CallWithSpread_Baseline,-119914143
+builtin_hash,CallWithArrayLike,-122249728
+builtin_hash,ConstructWithSpread,246592083
+builtin_hash,ConstructWithSpread_Baseline,150379974
+builtin_hash,Construct_Baseline,62706048
+builtin_hash,FastNewObject,958443730
+builtin_hash,FastNewClosure,344670909
+builtin_hash,StringEqual,747283806
+builtin_hash,StringGreaterThan,-181364078
+builtin_hash,StringGreaterThanOrEqual,-462881432
+builtin_hash,StringLessThan,-462881432
+builtin_hash,StringLessThanOrEqual,-181364078
+builtin_hash,StringSubstring,-615814018
+builtin_hash,OrderedHashTableHealIndex,-1059061674
+builtin_hash,CompileLazy,-1040787392
+builtin_hash,CompileLazyDeoptimizedCode,254075260
+builtin_hash,InstantiateAsmJs,-162781474
+builtin_hash,AllocateInYoungGeneration,504130749
+builtin_hash,AllocateRegularInYoungGeneration,-967770913
+builtin_hash,AllocateRegularInOldGeneration,-967770913
+builtin_hash,CopyFastSmiOrObjectElements,-184201389
+builtin_hash,GrowFastDoubleElements,933478036
+builtin_hash,GrowFastSmiOrObjectElements,62812155
+builtin_hash,ToNumber,-536181652
+builtin_hash,ToNumber_Baseline,-361624131
+builtin_hash,ToNumeric_Baseline,-968362129
+builtin_hash,ToNumberConvertBigInt,-484303877
+builtin_hash,Typeof,-292943239
+builtin_hash,KeyedLoadIC_PolymorphicName,-445639640
+builtin_hash,KeyedStoreIC_Megamorphic,228109775
+builtin_hash,DefineKeyedOwnIC_Megamorphic,587942691
+builtin_hash,LoadGlobalIC_NoFeedback,-506168140
+builtin_hash,LoadIC_FunctionPrototype,-217294724
+builtin_hash,LoadIC_StringLength,876788958
+builtin_hash,LoadIC_StringWrapperLength,-105737329
+builtin_hash,LoadIC_NoFeedback,796730020
+builtin_hash,StoreIC_NoFeedback,-771215689
+builtin_hash,DefineNamedOwnIC_NoFeedback,610029223
+builtin_hash,KeyedLoadIC_SloppyArguments,1037341519
+builtin_hash,StoreFastElementIC_Standard,511933864
+builtin_hash,StoreFastElementIC_GrowNoTransitionHandleCOW,-733182579
+builtin_hash,StoreFastElementIC_NoTransitionHandleCOW,14002747
+builtin_hash,ElementsTransitionAndStore_Standard,-303995099
+builtin_hash,ElementsTransitionAndStore_GrowNoTransitionHandleCOW,-620039698
+builtin_hash,ElementsTransitionAndStore_NoTransitionHandleCOW,387221171
+builtin_hash,KeyedHasIC_PolymorphicName,481900135
+builtin_hash,EnqueueMicrotask,987190055
+builtin_hash,RunMicrotasks,-606800144
+builtin_hash,HasProperty,-958876308
+builtin_hash,DeleteProperty,-583543539
+builtin_hash,SetDataProperties,-633970258
+builtin_hash,ReturnReceiver,386533367
+builtin_hash,ArrayConstructor,-862505040
+builtin_hash,ArrayConstructorImpl,-772732436
+builtin_hash,ArrayNoArgumentConstructor_PackedSmi_DontOverride,546753803
+builtin_hash,ArrayNoArgumentConstructor_HoleySmi_DontOverride,546753803
+builtin_hash,ArrayNoArgumentConstructor_PackedSmi_DisableAllocationSites,76921937
+builtin_hash,ArrayNoArgumentConstructor_Packed_DisableAllocationSites,76921937
+builtin_hash,ArrayNoArgumentConstructor_Holey_DisableAllocationSites,76921937
+builtin_hash,ArrayNoArgumentConstructor_PackedDouble_DisableAllocationSites,-916490644
+builtin_hash,ArraySingleArgumentConstructor_HoleySmi_DontOverride,924187471
+builtin_hash,ArraySingleArgumentConstructor_HoleySmi_DisableAllocationSites,-90166804
+builtin_hash,ArraySingleArgumentConstructor_Holey_DisableAllocationSites,-90166804
+builtin_hash,ArraySingleArgumentConstructor_HoleyDouble_DisableAllocationSites,377718997
+builtin_hash,ArrayIncludesSmi,833613331
+builtin_hash,ArrayIncludesSmiOrObject,-439120197
+builtin_hash,ArrayIncludes,-557378221
+builtin_hash,ArrayIndexOfSmi,818318721
+builtin_hash,ArrayIndexOfSmiOrObject,1027851539
+builtin_hash,ArrayIndexOf,344845802
+builtin_hash,ArrayPrototypePop,127416215
+builtin_hash,ArrayPrototypePush,611743176
+builtin_hash,CloneFastJSArray,1060615555
+builtin_hash,CloneFastJSArrayFillingHoles,1003395618
+builtin_hash,ExtractFastJSArray,-517393151
+builtin_hash,ArrayPrototypeEntries,-332667431
+builtin_hash,ArrayPrototypeKeys,110264383
+builtin_hash,ArrayPrototypeValues,-332667431
+builtin_hash,ArrayIteratorPrototypeNext,-858892834
+builtin_hash,AsyncFunctionEnter,423723147
+builtin_hash,AsyncFunctionResolve,265196636
+builtin_hash,AsyncFunctionAwaitCaught,960969853
+builtin_hash,AsyncFunctionAwaitUncaught,960969853
+builtin_hash,AsyncFunctionAwaitResolveClosure,-1057297202
+builtin_hash,DatePrototypeGetDate,905028372
+builtin_hash,DatePrototypeGetDay,905028372
+builtin_hash,DatePrototypeGetFullYear,905028372
+builtin_hash,DatePrototypeGetHours,905028372
+builtin_hash,DatePrototypeGetMilliseconds,-707287527
+builtin_hash,DatePrototypeGetMinutes,905028372
+builtin_hash,DatePrototypeGetMonth,905028372
+builtin_hash,DatePrototypeGetSeconds,905028372
+builtin_hash,DatePrototypeGetTime,665014006
+builtin_hash,DatePrototypeGetTimezoneOffset,-707287527
+builtin_hash,DatePrototypeValueOf,665014006
+builtin_hash,DatePrototypeToPrimitive,23745105
+builtin_hash,CreateIterResultObject,833507199
+builtin_hash,CreateGeneratorObject,-898656785
+builtin_hash,GeneratorPrototypeNext,-29771038
+builtin_hash,GeneratorPrototypeReturn,-279661376
+builtin_hash,SuspendGeneratorBaseline,-49499079
+builtin_hash,ResumeGeneratorBaseline,145201245
+builtin_hash,GlobalIsFinite,805204024
+builtin_hash,GlobalIsNaN,413622277
+builtin_hash,LoadIC,79924816
+builtin_hash,LoadIC_Megamorphic,682925528
+builtin_hash,LoadIC_Noninlined,-767250044
+builtin_hash,LoadICTrampoline,-803254542
+builtin_hash,LoadICBaseline,-628874782
+builtin_hash,LoadICTrampoline_Megamorphic,-803254542
+builtin_hash,LoadSuperIC,-238282119
+builtin_hash,LoadSuperICBaseline,841397561
+builtin_hash,KeyedLoadIC,78355712
+builtin_hash,KeyedLoadIC_Megamorphic,-391277039
+builtin_hash,KeyedLoadICTrampoline,-803254542
+builtin_hash,KeyedLoadICBaseline,-628874782
+builtin_hash,KeyedLoadICTrampoline_Megamorphic,-803254542
+builtin_hash,StoreGlobalIC,-33330877
+builtin_hash,StoreGlobalICTrampoline,-803254542
+builtin_hash,StoreGlobalICBaseline,-628874782
+builtin_hash,StoreIC,-959753225
+builtin_hash,StoreICTrampoline,756382466
+builtin_hash,StoreICBaseline,841397561
+builtin_hash,DefineNamedOwnIC,464622021
+builtin_hash,DefineNamedOwnICBaseline,841397561
+builtin_hash,KeyedStoreIC,-538069768
+builtin_hash,KeyedStoreICTrampoline,756382466
+builtin_hash,KeyedStoreICBaseline,841397561
+builtin_hash,DefineKeyedOwnIC,458562905
+builtin_hash,StoreInArrayLiteralIC,-604069917
+builtin_hash,StoreInArrayLiteralICBaseline,841397561
+builtin_hash,LoadGlobalIC,274757270
+builtin_hash,LoadGlobalICInsideTypeof,303475129
+builtin_hash,LoadGlobalICTrampoline,-833311190
+builtin_hash,LoadGlobalICBaseline,-77255126
+builtin_hash,LoadGlobalICInsideTypeofTrampoline,-833311190
+builtin_hash,LoadGlobalICInsideTypeofBaseline,-77255126
+builtin_hash,LookupGlobalICBaseline,-1021507359
+builtin_hash,LookupGlobalICInsideTypeofBaseline,-1021507359
+builtin_hash,KeyedHasIC,-204183308
+builtin_hash,KeyedHasICBaseline,-628874782
+builtin_hash,KeyedHasIC_Megamorphic,-958876308
+builtin_hash,IterableToList,-4651130
+builtin_hash,IterableToListWithSymbolLookup,977588013
+builtin_hash,IterableToListMayPreserveHoles,908990960
+builtin_hash,FindOrderedHashMapEntry,196242182
+builtin_hash,MapConstructor,127220366
+builtin_hash,MapPrototypeSet,529910141
+builtin_hash,MapPrototypeDelete,-553855034
+builtin_hash,MapPrototypeGet,-312429732
+builtin_hash,MapPrototypeHas,-908577859
+builtin_hash,MapPrototypeEntries,898519671
+builtin_hash,MapPrototypeGetSize,548120946
+builtin_hash,MapPrototypeForEach,600253966
+builtin_hash,MapPrototypeKeys,898519671
+builtin_hash,MapPrototypeValues,898519671
+builtin_hash,MapIteratorPrototypeNext,581031622
+builtin_hash,MapIteratorToList,-668334452
+builtin_hash,SameValueNumbersOnly,1046023669
+builtin_hash,Add_Baseline,-819537320
+builtin_hash,AddSmi_Baseline,-468458532
+builtin_hash,Subtract_Baseline,65596691
+builtin_hash,SubtractSmi_Baseline,-149584042
+builtin_hash,Multiply_Baseline,294831898
+builtin_hash,MultiplySmi_Baseline,996262660
+builtin_hash,Divide_Baseline,-446061441
+builtin_hash,DivideSmi_Baseline,-447600168
+builtin_hash,Modulus_Baseline,-832082339
+builtin_hash,ModulusSmi_Baseline,413347859
+builtin_hash,Exponentiate_Baseline,129594833
+builtin_hash,BitwiseAnd_Baseline,807317245
+builtin_hash,BitwiseAndSmi_Baseline,-299694524
+builtin_hash,BitwiseOr_Baseline,517046253
+builtin_hash,BitwiseOrSmi_Baseline,986547189
+builtin_hash,BitwiseXor_Baseline,-23876279
+builtin_hash,BitwiseXorSmi_Baseline,-1002138133
+builtin_hash,ShiftLeft_Baseline,500850188
+builtin_hash,ShiftLeftSmi_Baseline,-633960771
+builtin_hash,ShiftRight_Baseline,-32080745
+builtin_hash,ShiftRightSmi_Baseline,315819990
+builtin_hash,ShiftRightLogical_Baseline,479447240
+builtin_hash,ShiftRightLogicalSmi_Baseline,-519393226
+builtin_hash,Add_WithFeedback,-206794177
+builtin_hash,Subtract_WithFeedback,347362352
+builtin_hash,Modulus_WithFeedback,920841751
+builtin_hash,BitwiseOr_WithFeedback,-74343708
+builtin_hash,Equal_Baseline,-896951542
+builtin_hash,StrictEqual_Baseline,87581778
+builtin_hash,LessThan_Baseline,-374004445
+builtin_hash,GreaterThan_Baseline,-368668942
+builtin_hash,LessThanOrEqual_Baseline,301132954
+builtin_hash,GreaterThanOrEqual_Baseline,756925202
+builtin_hash,Equal_WithFeedback,-1040295188
+builtin_hash,StrictEqual_WithFeedback,-1052414211
+builtin_hash,LessThan_WithFeedback,948983301
+builtin_hash,GreaterThan_WithFeedback,-258688563
+builtin_hash,GreaterThanOrEqual_WithFeedback,691471117
+builtin_hash,BitwiseNot_Baseline,182142082
+builtin_hash,Decrement_Baseline,-544743600
+builtin_hash,Increment_Baseline,-307783174
+builtin_hash,Negate_Baseline,434902398
+builtin_hash,ObjectAssign,-786777006
+builtin_hash,ObjectCreate,-543317475
+builtin_hash,ObjectEntries,-465524320
+builtin_hash,ObjectGetOwnPropertyDescriptor,862856609
+builtin_hash,ObjectGetOwnPropertyNames,409260893
+builtin_hash,ObjectIs,-428110665
+builtin_hash,ObjectKeys,-711238005
+builtin_hash,ObjectPrototypeHasOwnProperty,-338192343
+builtin_hash,ObjectToString,993745228
+builtin_hash,InstanceOf_WithFeedback,-50284518
+builtin_hash,InstanceOf_Baseline,992223159
+builtin_hash,ForInEnumerate,-857152067
+builtin_hash,ForInPrepare,-602567485
+builtin_hash,ForInFilter,-142224411
+builtin_hash,RegExpConstructor,-862541618
+builtin_hash,RegExpExecAtom,-837574121
+builtin_hash,RegExpExecInternal,549675176
+builtin_hash,FindOrderedHashSetEntry,-166628054
+builtin_hash,SetConstructor,-778640968
+builtin_hash,SetPrototypeHas,-908577859
+builtin_hash,SetPrototypeAdd,-427333429
+builtin_hash,SetPrototypeDelete,-871946847
+builtin_hash,SetPrototypeEntries,898519671
+builtin_hash,SetPrototypeGetSize,548120946
+builtin_hash,SetPrototypeForEach,-501810916
+builtin_hash,SetPrototypeValues,898519671
+builtin_hash,SetIteratorPrototypeNext,182871241
+builtin_hash,SetOrSetIteratorToList,-33118696
+builtin_hash,StringFromCharCode,-971392951
+builtin_hash,StringPrototypeReplace,211421001
+builtin_hash,StringPrototypeSplit,-56509999
+builtin_hash,TypedArrayConstructor,618386097
+builtin_hash,TypedArrayPrototypeByteLength,-587563610
+builtin_hash,TypedArrayPrototypeLength,-163278974
+builtin_hash,WeakMapConstructor,-808541690
+builtin_hash,WeakMapLookupHashIndex,-619048905
+builtin_hash,WeakMapGet,276986520
+builtin_hash,WeakMapPrototypeHas,-285904254
+builtin_hash,WeakMapPrototypeSet,629680419
+builtin_hash,WeakSetConstructor,-367435631
+builtin_hash,WeakSetPrototypeHas,-285904254
+builtin_hash,WeakSetPrototypeAdd,-301255294
+builtin_hash,WeakCollectionSet,217583952
+builtin_hash,AsyncGeneratorResolve,242317686
+builtin_hash,AsyncGeneratorYieldWithAwait,302667528
+builtin_hash,AsyncGeneratorResumeNext,-265907726
+builtin_hash,AsyncGeneratorPrototypeNext,-194499830
+builtin_hash,AsyncGeneratorAwaitUncaught,-398074132
+builtin_hash,AsyncGeneratorAwaitResolveClosure,-245656056
+builtin_hash,AsyncGeneratorYieldWithAwaitResolveClosure,-649252259
+builtin_hash,StringAdd_CheckNone,1037172071
+builtin_hash,SubString,-701927326
+builtin_hash,GetProperty,-433765894
+builtin_hash,GetPropertyWithReceiver,636771451
+builtin_hash,SetProperty,-985618808
+builtin_hash,CreateDataProperty,952942021
+builtin_hash,FindNonDefaultConstructorOrConstruct,1020851957
+builtin_hash,ArrayPrototypeConcat,-711562967
+builtin_hash,ArrayEvery,732127203
+builtin_hash,ArrayFilterLoopLazyDeoptContinuation,782264259
+builtin_hash,ArrayFilterLoopContinuation,292635770
+builtin_hash,ArrayFilter,-585622372
+builtin_hash,ArrayPrototypeFind,410534083
+builtin_hash,ArrayForEachLoopLazyDeoptContinuation,-299794382
+builtin_hash,ArrayForEachLoopContinuation,350033182
+builtin_hash,ArrayForEach,729108989
+builtin_hash,ArrayFrom,1055630901
+builtin_hash,ArrayIsArray,-970031738
+builtin_hash,LoadJoinElement_FastSmiOrObjectElements_0,228167807
+builtin_hash,LoadJoinElement_FastDoubleElements_0,580988969
+builtin_hash,JoinStackPush,751439150
+builtin_hash,JoinStackPop,128574663
+builtin_hash,ArrayPrototypeJoin,89295304
+builtin_hash,ArrayPrototypeToString,-66500098
+builtin_hash,ArrayPrototypeLastIndexOf,1073113005
+builtin_hash,ArrayMapLoopLazyDeoptContinuation,-47088981
+builtin_hash,ArrayMapLoopContinuation,-794603673
+builtin_hash,ArrayMap,-326417675
+builtin_hash,ArrayReduceLoopLazyDeoptContinuation,-1014597388
+builtin_hash,ArrayReduceLoopContinuation,-1067144759
+builtin_hash,ArrayReduce,-407776620
+builtin_hash,ArrayPrototypeReverse,-121874294
+builtin_hash,ArrayPrototypeShift,-928108750
+builtin_hash,ArrayPrototypeSlice,214735037
+builtin_hash,ArraySome,466290774
+builtin_hash,ArrayPrototypeSplice,1001942992
+builtin_hash,ArrayPrototypeUnshift,-1052845134
+builtin_hash,ArrayBufferPrototypeGetByteLength,445258508
+builtin_hash,ArrayBufferIsView,-78532109
+builtin_hash,ToInteger,713419327
+builtin_hash,FastCreateDataProperty,-278611029
+builtin_hash,BooleanConstructor,-809457299
+builtin_hash,BooleanPrototypeToString,-798757106
+builtin_hash,ToString,436846720
+builtin_hash,StringPrototypeToString,-794700080
+builtin_hash,StringPrototypeValueOf,-794700080
+builtin_hash,StringPrototypeCharAt,915103217
+builtin_hash,StringPrototypeCharCodeAt,-272108096
+builtin_hash,StringPrototypeCodePointAt,-596824984
+builtin_hash,StringPrototypeConcat,-577571398
+builtin_hash,StringConstructor,-65593142
+builtin_hash,StringAddConvertLeft,51926197
+builtin_hash,StringAddConvertRight,115066033
+builtin_hash,StringCharAt,959950211
+builtin_hash,FastNewClosureBaseline,-532908706
+builtin_hash,FastNewFunctionContextFunction,977993537
+builtin_hash,CreateRegExpLiteral,64770172
+builtin_hash,CreateShallowArrayLiteral,866949735
+builtin_hash,CreateEmptyArrayLiteral,-862242730
+builtin_hash,CreateShallowObjectLiteral,991590480
+builtin_hash,ObjectConstructor,-384944316
+builtin_hash,CreateEmptyLiteralObject,-310219292
+builtin_hash,NumberConstructor,-974450450
+builtin_hash,StringToNumber,-446317754
+builtin_hash,NonNumberToNumber,504608456
+builtin_hash,NonNumberToNumeric,-570033562
+builtin_hash,ToNumeric,-772194204
+builtin_hash,NumberToString,674929388
+builtin_hash,ToBoolean,856538717
+builtin_hash,ToBooleanForBaselineJump,-446512949
+builtin_hash,ToLength,999641871
+builtin_hash,ToName,645844037
+builtin_hash,ToObject,119745243
+builtin_hash,NonPrimitiveToPrimitive_Default,-151838227
+builtin_hash,NonPrimitiveToPrimitive_Number,-151838227
+builtin_hash,NonPrimitiveToPrimitive_String,-151838227
+builtin_hash,OrdinaryToPrimitive_Number,-337334591
+builtin_hash,OrdinaryToPrimitive_String,-337334591
+builtin_hash,DataViewPrototypeGetByteLength,750091486
+builtin_hash,DataViewPrototypeGetFloat64,544637297
+builtin_hash,DataViewPrototypeSetUint32,366892025
+builtin_hash,DataViewPrototypeSetFloat64,267831220
+builtin_hash,FunctionPrototypeHasInstance,-911487777
+builtin_hash,FastFunctionPrototypeBind,-29755211
+builtin_hash,ForInNext,547638943
+builtin_hash,GetIteratorWithFeedback,935596039
+builtin_hash,GetIteratorBaseline,-124236956
+builtin_hash,CallIteratorWithFeedback,174322508
+builtin_hash,MathAbs,111472406
+builtin_hash,MathCeil,-288711730
+builtin_hash,MathFloor,558720012
+builtin_hash,MathRound,-893522347
+builtin_hash,MathPow,-432438626
+builtin_hash,MathMax,-914923816
+builtin_hash,MathMin,-435430851
+builtin_hash,MathAsin,-865319143
+builtin_hash,MathAtan2,-706534972
+builtin_hash,MathCos,705415335
+builtin_hash,MathExp,1065131032
+builtin_hash,MathFround,-135252655
+builtin_hash,MathImul,773832811
+builtin_hash,MathLog,540909033
+builtin_hash,MathSin,-688911662
+builtin_hash,MathSign,-523407079
+builtin_hash,MathSqrt,-794868693
+builtin_hash,MathTan,537052027
+builtin_hash,MathTanh,-300840302
+builtin_hash,MathRandom,966867537
+builtin_hash,NumberPrototypeToString,-382822730
+builtin_hash,NumberIsInteger,-213604804
+builtin_hash,NumberIsNaN,788813704
+builtin_hash,NumberParseFloat,-741561968
+builtin_hash,ParseInt,998287919
+builtin_hash,NumberParseInt,-382916138
+builtin_hash,Add,-136527337
+builtin_hash,Subtract,-213501900
+builtin_hash,Multiply,7472525
+builtin_hash,Divide,-344347312
+builtin_hash,Modulus,-582417614
+builtin_hash,CreateObjectWithoutProperties,339671006
+builtin_hash,ObjectIsExtensible,-329082141
+builtin_hash,ObjectPreventExtensions,940542631
+builtin_hash,ObjectGetPrototypeOf,157540923
+builtin_hash,ObjectSetPrototypeOf,187356384
+builtin_hash,ObjectPrototypeToString,-483254038
+builtin_hash,ObjectPrototypeValueOf,193287106
+builtin_hash,FulfillPromise,272197869
+builtin_hash,NewPromiseCapability,-508522709
+builtin_hash,PromiseCapabilityDefaultResolve,-402797269
+builtin_hash,PerformPromiseThen,330989248
+builtin_hash,PromiseAll,697437536
+builtin_hash,PromiseAllResolveElementClosure,-862999565
+builtin_hash,PromiseConstructor,762524591
+builtin_hash,PromisePrototypeCatch,756171957
+builtin_hash,PromiseFulfillReactionJob,-630924263
+builtin_hash,PromiseResolveTrampoline,-167249272
+builtin_hash,PromiseResolve,-412690059
+builtin_hash,ResolvePromise,756044362
+builtin_hash,PromisePrototypeThen,3713531
+builtin_hash,PromiseResolveThenableJob,-14213172
+builtin_hash,ProxyConstructor,459230341
+builtin_hash,ProxyGetProperty,1054163992
+builtin_hash,ProxyIsExtensible,308384776
+builtin_hash,ProxyPreventExtensions,399450299
+builtin_hash,ReflectGet,-434221017
+builtin_hash,ReflectHas,-167249272
+builtin_hash,RegExpPrototypeExec,963999476
+builtin_hash,RegExpMatchFast,384654261
+builtin_hash,RegExpReplace,-475275041
+builtin_hash,RegExpPrototypeReplace,860372377
+builtin_hash,RegExpSearchFast,907750005
+builtin_hash,RegExpPrototypeSourceGetter,-747085084
+builtin_hash,RegExpSplit,-607180644
+builtin_hash,RegExpPrototypeTest,-585829947
+builtin_hash,RegExpPrototypeTestFast,-1071276448
+builtin_hash,RegExpPrototypeGlobalGetter,-718555192
+builtin_hash,RegExpPrototypeIgnoreCaseGetter,1070990033
+builtin_hash,RegExpPrototypeMultilineGetter,216999873
+builtin_hash,RegExpPrototypeHasIndicesGetter,390292067
+builtin_hash,RegExpPrototypeDotAllGetter,390292067
+builtin_hash,RegExpPrototypeStickyGetter,1055105538
+builtin_hash,RegExpPrototypeUnicodeGetter,1055105538
+builtin_hash,RegExpPrototypeFlagsGetter,-646009057
+builtin_hash,StringPrototypeEndsWith,565371891
+builtin_hash,StringPrototypeIncludes,480948081
+builtin_hash,StringPrototypeIndexOf,619068194
+builtin_hash,StringPrototypeIterator,-532566456
+builtin_hash,StringIteratorPrototypeNext,-1034386014
+builtin_hash,StringPrototypeMatch,127768813
+builtin_hash,StringPrototypeSearch,127768813
+builtin_hash,StringRepeat,92491602
+builtin_hash,StringPrototypeSlice,111174165
+builtin_hash,StringPrototypeStartsWith,-951440779
+builtin_hash,StringPrototypeSubstr,716425893
+builtin_hash,StringPrototypeSubstring,769385864
+builtin_hash,StringPrototypeTrim,-151587513
+builtin_hash,SymbolPrototypeToString,697341238
+builtin_hash,CreateTypedArray,100324164
+builtin_hash,TypedArrayFrom,-508079252
+builtin_hash,TypedArrayPrototypeSet,241292735
+builtin_hash,TypedArrayPrototypeSubArray,-638094120
+builtin_hash,NewSloppyArgumentsElements,745494442
+builtin_hash,NewStrictArgumentsElements,-81425804
+builtin_hash,NewRestArgumentsElements,-823345459
+builtin_hash,FastNewSloppyArguments,-174863955
+builtin_hash,FastNewStrictArguments,-75939795
+builtin_hash,FastNewRestArguments,-680285498
+builtin_hash,StringSlowFlatten,108774605
+builtin_hash,StringIndexOf,119327941
+builtin_hash,Load_FastSmiElements_0,-418523514
+builtin_hash,Load_FastObjectElements_0,-418523514
+builtin_hash,Store_FastSmiElements_0,975980653
+builtin_hash,Store_FastObjectElements_0,311513691
+builtin_hash,SortCompareDefault,842664214
+builtin_hash,SortCompareUserFn,1059126141
+builtin_hash,Copy,-750738169
+builtin_hash,MergeAt,944447896
+builtin_hash,GallopLeft,368113946
+builtin_hash,GallopRight,186729557
+builtin_hash,ArrayTimSort,-475205137
+builtin_hash,ArrayPrototypeSort,-366911589
+builtin_hash,StringFastLocaleCompare,15452983
+builtin_hash,WasmInt32ToHeapNumber,751194511
+builtin_hash,WasmTaggedNonSmiToInt32,-202443862
+builtin_hash,WasmTriggerTierUp,-980759280
+builtin_hash,WasmStackGuard,-1024124053
+builtin_hash,CanUseSameAccessor_FastSmiElements_0,-756700379
+builtin_hash,CanUseSameAccessor_FastObjectElements_0,-756700379
+builtin_hash,StringPrototypeToLowerCaseIntl,-966367732
+builtin_hash,StringToLowerCaseIntl,-481509366
+builtin_hash,WideHandler,-298201266
+builtin_hash,ExtraWideHandler,-298201266
+builtin_hash,LdarHandler,-745598094
+builtin_hash,LdaZeroHandler,368748633
+builtin_hash,LdaSmiHandler,-545227529
+builtin_hash,LdaUndefinedHandler,1011673901
+builtin_hash,LdaNullHandler,1011673901
+builtin_hash,LdaTheHoleHandler,1011673901
+builtin_hash,LdaTrueHandler,827753247
+builtin_hash,LdaFalseHandler,827753247
+builtin_hash,LdaConstantHandler,407548785
+builtin_hash,LdaContextSlotHandler,506452989
+builtin_hash,LdaImmutableContextSlotHandler,506452989
+builtin_hash,LdaCurrentContextSlotHandler,327557270
+builtin_hash,LdaImmutableCurrentContextSlotHandler,327557270
+builtin_hash,StarHandler,305217552
+builtin_hash,MovHandler,-283701884
+builtin_hash,PushContextHandler,177425195
+builtin_hash,PopContextHandler,-1044986385
+builtin_hash,TestReferenceEqualHandler,-651544719
+builtin_hash,TestUndetectableHandler,-830971105
+builtin_hash,TestNullHandler,1005522396
+builtin_hash,TestUndefinedHandler,1005522396
+builtin_hash,TestTypeOfHandler,-1028477858
+builtin_hash,LdaGlobalHandler,965344129
+builtin_hash,LdaGlobalInsideTypeofHandler,585777250
+builtin_hash,StaGlobalHandler,1056951542
+builtin_hash,StaContextSlotHandler,-675927710
+builtin_hash,StaCurrentContextSlotHandler,-997669083
+builtin_hash,LdaLookupGlobalSlotHandler,-84752131
+builtin_hash,LdaLookupGlobalSlotInsideTypeofHandler,49834142
+builtin_hash,StaLookupSlotHandler,-381579342
+builtin_hash,GetNamedPropertyHandler,-27764824
+builtin_hash,GetNamedPropertyFromSuperHandler,-724989944
+builtin_hash,GetKeyedPropertyHandler,-56635454
+builtin_hash,SetNamedPropertyHandler,448782548
+builtin_hash,DefineNamedOwnPropertyHandler,448782548
+builtin_hash,SetKeyedPropertyHandler,941278116
+builtin_hash,DefineKeyedOwnPropertyHandler,941278116
+builtin_hash,StaInArrayLiteralHandler,941278116
+builtin_hash,DefineKeyedOwnPropertyInLiteralHandler,1045494813
+builtin_hash,AddHandler,-518783725
+builtin_hash,SubHandler,505104408
+builtin_hash,MulHandler,-222850853
+builtin_hash,DivHandler,-1028262634
+builtin_hash,ModHandler,143526297
+builtin_hash,ExpHandler,-727777022
+builtin_hash,BitwiseOrHandler,-522781712
+builtin_hash,BitwiseXorHandler,-419955523
+builtin_hash,BitwiseAndHandler,530208341
+builtin_hash,ShiftLeftHandler,-804444955
+builtin_hash,ShiftRightHandler,-104335215
+builtin_hash,ShiftRightLogicalHandler,1050635494
+builtin_hash,AddSmiHandler,-161508067
+builtin_hash,SubSmiHandler,-609360326
+builtin_hash,MulSmiHandler,282822605
+builtin_hash,DivSmiHandler,292906952
+builtin_hash,ModSmiHandler,-917212490
+builtin_hash,BitwiseOrSmiHandler,172148322
+builtin_hash,BitwiseXorSmiHandler,1046550901
+builtin_hash,BitwiseAndSmiHandler,-808862341
+builtin_hash,ShiftLeftSmiHandler,862845296
+builtin_hash,ShiftRightSmiHandler,183483372
+builtin_hash,ShiftRightLogicalSmiHandler,31369673
+builtin_hash,IncHandler,-318834355
+builtin_hash,DecHandler,938496699
+builtin_hash,NegateHandler,-590726041
+builtin_hash,BitwiseNotHandler,322709376
+builtin_hash,ToBooleanLogicalNotHandler,-972724513
+builtin_hash,LogicalNotHandler,-706273800
+builtin_hash,TypeOfHandler,-751823
+builtin_hash,DeletePropertyStrictHandler,-724253277
+builtin_hash,DeletePropertySloppyHandler,-476722269
+builtin_hash,FindNonDefaultConstructorOrConstructHandler,-746857468
+builtin_hash,CallAnyReceiverHandler,87393745
+builtin_hash,CallPropertyHandler,87393745
+builtin_hash,CallProperty0Handler,956548008
+builtin_hash,CallProperty1Handler,-471075746
+builtin_hash,CallProperty2Handler,-1043814952
+builtin_hash,CallUndefinedReceiverHandler,126620186
+builtin_hash,CallUndefinedReceiver0Handler,-286191860
+builtin_hash,CallUndefinedReceiver1Handler,-357856703
+builtin_hash,CallUndefinedReceiver2Handler,798828847
+builtin_hash,CallWithSpreadHandler,87393745
+builtin_hash,CallRuntimeHandler,624123308
+builtin_hash,CallJSRuntimeHandler,1005113218
+builtin_hash,InvokeIntrinsicHandler,-566159390
+builtin_hash,ConstructHandler,543386518
+builtin_hash,ConstructWithSpreadHandler,595837553
+builtin_hash,TestEqualHandler,-157366914
+builtin_hash,TestEqualStrictHandler,998643852
+builtin_hash,TestLessThanHandler,1046936290
+builtin_hash,TestGreaterThanHandler,-369508260
+builtin_hash,TestLessThanOrEqualHandler,-412750652
+builtin_hash,TestGreaterThanOrEqualHandler,-364267636
+builtin_hash,TestInstanceOfHandler,-607728916
+builtin_hash,TestInHandler,539847065
+builtin_hash,ToNameHandler,701699245
+builtin_hash,ToNumberHandler,-512585428
+builtin_hash,ToNumericHandler,459707132
+builtin_hash,ToObjectHandler,701699245
+builtin_hash,ToStringHandler,620423288
+builtin_hash,CreateRegExpLiteralHandler,848340986
+builtin_hash,CreateArrayLiteralHandler,101333771
+builtin_hash,CreateArrayFromIterableHandler,-18783057
+builtin_hash,CreateEmptyArrayLiteralHandler,-289337896
+builtin_hash,CreateObjectLiteralHandler,-711473910
+builtin_hash,CreateEmptyObjectLiteralHandler,795228443
+builtin_hash,CreateClosureHandler,877324634
+builtin_hash,CreateBlockContextHandler,-344466857
+builtin_hash,CreateCatchContextHandler,-214012965
+builtin_hash,CreateFunctionContextHandler,729147868
+builtin_hash,CreateMappedArgumentsHandler,-124182926
+builtin_hash,CreateUnmappedArgumentsHandler,758781228
+builtin_hash,CreateRestParameterHandler,-10099522
+builtin_hash,JumpLoopHandler,-166037043
+builtin_hash,JumpHandler,-79617432
+builtin_hash,JumpConstantHandler,906507762
+builtin_hash,JumpIfUndefinedConstantHandler,250257394
+builtin_hash,JumpIfNotUndefinedConstantHandler,-587816710
+builtin_hash,JumpIfUndefinedOrNullConstantHandler,53751011
+builtin_hash,JumpIfTrueConstantHandler,250257394
+builtin_hash,JumpIfFalseConstantHandler,250257394
+builtin_hash,JumpIfToBooleanTrueConstantHandler,15176103
+builtin_hash,JumpIfToBooleanFalseConstantHandler,422983862
+builtin_hash,JumpIfToBooleanTrueHandler,635201116
+builtin_hash,JumpIfToBooleanFalseHandler,408147223
+builtin_hash,JumpIfTrueHandler,801953084
+builtin_hash,JumpIfFalseHandler,801953084
+builtin_hash,JumpIfNullHandler,801953084
+builtin_hash,JumpIfNotNullHandler,1026829001
+builtin_hash,JumpIfUndefinedHandler,801953084
+builtin_hash,JumpIfNotUndefinedHandler,1026829001
+builtin_hash,JumpIfUndefinedOrNullHandler,1021601552
+builtin_hash,JumpIfJSReceiverHandler,65469341
+builtin_hash,SwitchOnSmiNoFeedbackHandler,807681990
+builtin_hash,ForInEnumerateHandler,510063374
+builtin_hash,ForInPrepareHandler,630466074
+builtin_hash,ForInContinueHandler,-691562887
+builtin_hash,ForInNextHandler,512834227
+builtin_hash,ForInStepHandler,942618821
+builtin_hash,SetPendingMessageHandler,401946975
+builtin_hash,ThrowHandler,50431783
+builtin_hash,ReThrowHandler,50431783
+builtin_hash,ReturnHandler,-117530186
+builtin_hash,ThrowReferenceErrorIfHoleHandler,512852920
+builtin_hash,ThrowSuperNotCalledIfHoleHandler,717642155
+builtin_hash,ThrowSuperAlreadyCalledIfNotHoleHandler,-546144205
+builtin_hash,ThrowIfNotSuperConstructorHandler,-460002303
+builtin_hash,SwitchOnGeneratorStateHandler,10710931
+builtin_hash,SuspendGeneratorHandler,-500612975
+builtin_hash,ResumeGeneratorHandler,1068636019
+builtin_hash,GetIteratorHandler,-71006498
+builtin_hash,ShortStarHandler,356943682
+builtin_hash,LdarWideHandler,-249230336
+builtin_hash,LdaSmiWideHandler,-31881096
+builtin_hash,LdaConstantWideHandler,-758989820
+builtin_hash,LdaContextSlotWideHandler,687146226
+builtin_hash,LdaImmutableContextSlotWideHandler,687146226
+builtin_hash,LdaImmutableCurrentContextSlotWideHandler,-836770052
+builtin_hash,StarWideHandler,501248040
+builtin_hash,MovWideHandler,-871657303
+builtin_hash,PushContextWideHandler,844522230
+builtin_hash,PopContextWideHandler,744748597
+builtin_hash,TestReferenceEqualWideHandler,-118913544
+builtin_hash,LdaGlobalWideHandler,-661487412
+builtin_hash,LdaGlobalInsideTypeofWideHandler,-572343212
+builtin_hash,StaGlobalWideHandler,555909381
+builtin_hash,StaContextSlotWideHandler,478877471
+builtin_hash,StaCurrentContextSlotWideHandler,-615279276
+builtin_hash,LdaLookupGlobalSlotWideHandler,-1002268065
+builtin_hash,GetNamedPropertyWideHandler,-241462706
+builtin_hash,GetKeyedPropertyWideHandler,641533107
+builtin_hash,SetNamedPropertyWideHandler,-58064714
+builtin_hash,DefineNamedOwnPropertyWideHandler,-58064714
+builtin_hash,SetKeyedPropertyWideHandler,686171362
+builtin_hash,DefineKeyedOwnPropertyWideHandler,686171362
+builtin_hash,StaInArrayLiteralWideHandler,686171362
+builtin_hash,AddWideHandler,-617481681
+builtin_hash,SubWideHandler,145242966
+builtin_hash,MulWideHandler,166175890
+builtin_hash,DivWideHandler,829768719
+builtin_hash,BitwiseOrWideHandler,-671352735
+builtin_hash,BitwiseAndWideHandler,-748389668
+builtin_hash,ShiftLeftWideHandler,-722355824
+builtin_hash,AddSmiWideHandler,-503151286
+builtin_hash,SubSmiWideHandler,266762310
+builtin_hash,MulSmiWideHandler,767307001
+builtin_hash,DivSmiWideHandler,1050619977
+builtin_hash,ModSmiWideHandler,-653636504
+builtin_hash,BitwiseOrSmiWideHandler,905206733
+builtin_hash,BitwiseXorSmiWideHandler,1044063990
+builtin_hash,BitwiseAndSmiWideHandler,-376485258
+builtin_hash,ShiftLeftSmiWideHandler,-1004091795
+builtin_hash,ShiftRightSmiWideHandler,-397666497
+builtin_hash,ShiftRightLogicalSmiWideHandler,54662547
+builtin_hash,IncWideHandler,331971916
+builtin_hash,DecWideHandler,279024516
+builtin_hash,NegateWideHandler,-781916260
+builtin_hash,CallPropertyWideHandler,-998392170
+builtin_hash,CallProperty0WideHandler,54487119
+builtin_hash,CallProperty1WideHandler,-147592428
+builtin_hash,CallProperty2WideHandler,-58614287
+builtin_hash,CallUndefinedReceiverWideHandler,400495181
+builtin_hash,CallUndefinedReceiver0WideHandler,-1000686597
+builtin_hash,CallUndefinedReceiver1WideHandler,-299347389
+builtin_hash,CallUndefinedReceiver2WideHandler,525189648
+builtin_hash,CallWithSpreadWideHandler,-998392170
+builtin_hash,ConstructWideHandler,193926631
+builtin_hash,TestEqualWideHandler,-797631551
+builtin_hash,TestEqualStrictWideHandler,753248660
+builtin_hash,TestLessThanWideHandler,-210582608
+builtin_hash,TestGreaterThanWideHandler,543018087
+builtin_hash,TestLessThanOrEqualWideHandler,-1053789276
+builtin_hash,TestGreaterThanOrEqualWideHandler,-582678107
+builtin_hash,TestInstanceOfWideHandler,-280937039
+builtin_hash,TestInWideHandler,817647574
+builtin_hash,ToNumericWideHandler,868695670
+builtin_hash,CreateRegExpLiteralWideHandler,-1006765965
+builtin_hash,CreateArrayLiteralWideHandler,-826485513
+builtin_hash,CreateEmptyArrayLiteralWideHandler,559300434
+builtin_hash,CreateObjectLiteralWideHandler,455963528
+builtin_hash,CreateClosureWideHandler,1061873155
+builtin_hash,CreateBlockContextWideHandler,271729622
+builtin_hash,CreateFunctionContextWideHandler,527181803
+builtin_hash,JumpLoopWideHandler,941891518
+builtin_hash,JumpWideHandler,-79617432
+builtin_hash,JumpIfToBooleanTrueWideHandler,923993949
+builtin_hash,JumpIfToBooleanFalseWideHandler,145370961
+builtin_hash,JumpIfTrueWideHandler,-1042889789
+builtin_hash,JumpIfFalseWideHandler,-1042889789
+builtin_hash,SwitchOnSmiNoFeedbackWideHandler,-773907277
+builtin_hash,ForInPrepareWideHandler,-483036360
+builtin_hash,ForInNextWideHandler,-173595160
+builtin_hash,ThrowReferenceErrorIfHoleWideHandler,-254407930
+builtin_hash,GetIteratorWideHandler,-412149326
+builtin_hash,LdaSmiExtraWideHandler,65806156
+builtin_hash,LdaGlobalExtraWideHandler,411460668
+builtin_hash,AddSmiExtraWideHandler,553152400
+builtin_hash,SubSmiExtraWideHandler,446395338
+builtin_hash,MulSmiExtraWideHandler,105494980
+builtin_hash,DivSmiExtraWideHandler,-317292269
+builtin_hash,BitwiseOrSmiExtraWideHandler,604681516
+builtin_hash,BitwiseXorSmiExtraWideHandler,-91329781
+builtin_hash,BitwiseAndSmiExtraWideHandler,150048166
+builtin_hash,CallUndefinedReceiverExtraWideHandler,423421950
+builtin_hash,CallUndefinedReceiver1ExtraWideHandler,168432499
+builtin_hash,CallUndefinedReceiver2ExtraWideHandler,524973830
diff -r -u --color up/v8/tools/builtins-pgo/x64.profile nw/v8/tools/builtins-pgo/x64.profile
--- up/v8/tools/builtins-pgo/x64.profile	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/builtins-pgo/x64.profile	2023-01-19 16:46:37.858109124 -0500
@@ -12,7 +12,6 @@
 block_hint,RecordWriteIgnoreFP,19,20,0
 block_hint,RecordWriteIgnoreFP,9,10,0
 block_hint,RecordWriteIgnoreFP,25,26,0
-block_hint,RecordWriteIgnoreFP,15,16,1
 block_hint,Call_ReceiverIsNullOrUndefined_Baseline_Compact,19,20,1
 block_hint,Call_ReceiverIsNullOrUndefined_Baseline_Compact,43,44,0
 block_hint,Call_ReceiverIsNullOrUndefined_Baseline_Compact,83,84,0
@@ -254,7 +253,7 @@
 block_hint,KeyedLoadIC_PolymorphicName,298,299,1
 block_hint,KeyedLoadIC_PolymorphicName,330,331,1
 block_hint,KeyedLoadIC_PolymorphicName,98,99,0
-block_hint,KeyedLoadIC_PolymorphicName,279,280,1
+block_hint,KeyedLoadIC_PolymorphicName,100,101,0
 block_hint,KeyedLoadIC_PolymorphicName,22,23,1
 block_hint,KeyedLoadIC_PolymorphicName,165,166,0
 block_hint,KeyedLoadIC_PolymorphicName,122,123,1
@@ -767,7 +766,7 @@
 block_hint,KeyedHasIC_PolymorphicName,89,90,1
 block_hint,KeyedHasIC_PolymorphicName,93,94,1
 block_hint,KeyedHasIC_PolymorphicName,30,31,0
-block_hint,KeyedHasIC_PolymorphicName,78,79,1
+block_hint,KeyedHasIC_PolymorphicName,32,33,0
 block_hint,KeyedHasIC_PolymorphicName,14,15,1
 block_hint,KeyedHasIC_PolymorphicName,16,17,1
 block_hint,EnqueueMicrotask,4,5,0
@@ -1064,6 +1063,7 @@
 block_hint,CloneFastJSArray,37,38,1
 block_hint,CloneFastJSArray,34,35,1
 block_hint,CloneFastJSArray,19,20,1
+block_hint,CloneFastJSArray,8,9,0
 block_hint,CloneFastJSArray,12,13,0
 block_hint,CloneFastJSArray,14,15,1
 block_hint,CloneFastJSArray,42,43,1
@@ -1090,7 +1090,7 @@
 block_hint,ExtractFastJSArray,39,40,1
 block_hint,ExtractFastJSArray,35,36,1
 block_hint,ExtractFastJSArray,20,21,1
-block_hint,ExtractFastJSArray,8,9,0
+block_hint,ExtractFastJSArray,6,7,0
 block_hint,ExtractFastJSArray,12,13,0
 block_hint,ExtractFastJSArray,14,15,1
 block_hint,ExtractFastJSArray,37,38,1
@@ -1265,7 +1265,6 @@
 block_hint,LoadIC,135,136,0
 block_hint,LoadIC_Megamorphic,355,356,1
 block_hint,LoadIC_Megamorphic,352,353,0
-block_hint,LoadIC_Megamorphic,349,350,1
 block_hint,LoadIC_Megamorphic,257,258,1
 block_hint,LoadIC_Megamorphic,259,260,1
 block_hint,LoadIC_Megamorphic,255,256,0
@@ -1527,6 +1526,7 @@
 block_hint,StoreIC,210,211,1
 block_hint,StoreIC,395,396,1
 block_hint,StoreIC,386,387,0
+block_hint,StoreIC,369,370,1
 block_hint,StoreIC,240,241,1
 block_hint,StoreIC,242,243,1
 block_hint,StoreIC,74,75,1
@@ -1623,7 +1623,6 @@
 block_hint,DefineNamedOwnIC,93,94,0
 block_hint,DefineNamedOwnIC,17,18,0
 block_hint,DefineNamedOwnIC,350,351,0
-block_hint,DefineNamedOwnIC,282,283,1
 block_hint,DefineNamedOwnIC,157,158,1
 block_hint,DefineNamedOwnIC,159,160,1
 block_hint,DefineNamedOwnIC,254,255,1
@@ -1974,40 +1973,40 @@
 block_hint,MultiplySmi_Baseline,51,52,1
 block_hint,MultiplySmi_Baseline,34,35,1
 block_hint,MultiplySmi_Baseline,15,16,1
+block_hint,Divide_Baseline,89,90,0
+block_hint,Divide_Baseline,91,92,0
 block_hint,Divide_Baseline,69,70,0
-block_hint,Divide_Baseline,71,72,0
-block_hint,Divide_Baseline,50,51,0
-block_hint,Divide_Baseline,31,32,1
-block_hint,Divide_Baseline,10,11,1
-block_hint,Divide_Baseline,54,55,1
-block_hint,Divide_Baseline,79,80,1
-block_hint,Divide_Baseline,56,57,1
-block_hint,Divide_Baseline,39,40,0
-block_hint,Divide_Baseline,19,20,1
-block_hint,Divide_Baseline,25,26,1
-block_hint,Divide_Baseline,12,13,1
-block_hint,DivideSmi_Baseline,63,64,0
-block_hint,DivideSmi_Baseline,76,77,0
-block_hint,DivideSmi_Baseline,65,66,0
-block_hint,DivideSmi_Baseline,50,51,0
-block_hint,DivideSmi_Baseline,31,32,1
-block_hint,DivideSmi_Baseline,10,11,1
-block_hint,DivideSmi_Baseline,41,42,1
-block_hint,DivideSmi_Baseline,25,26,1
-block_hint,DivideSmi_Baseline,12,13,1
-block_hint,Modulus_Baseline,76,77,0
-block_hint,Modulus_Baseline,72,73,0
-block_hint,Modulus_Baseline,55,56,1
-block_hint,Modulus_Baseline,50,51,1
-block_hint,Modulus_Baseline,18,19,0
-block_hint,Modulus_Baseline,6,7,1
-block_hint,ModulusSmi_Baseline,55,56,1
-block_hint,ModulusSmi_Baseline,50,51,1
-block_hint,ModulusSmi_Baseline,18,19,0
-block_hint,ModulusSmi_Baseline,6,7,1
-block_hint,ModulusSmi_Baseline,40,41,1
-block_hint,ModulusSmi_Baseline,20,21,1
-block_hint,ModulusSmi_Baseline,8,9,1
+block_hint,Divide_Baseline,47,48,1
+block_hint,Divide_Baseline,14,15,1
+block_hint,Divide_Baseline,73,74,1
+block_hint,Divide_Baseline,97,98,1
+block_hint,Divide_Baseline,75,76,1
+block_hint,Divide_Baseline,55,56,0
+block_hint,Divide_Baseline,28,29,1
+block_hint,Divide_Baseline,40,41,1
+block_hint,Divide_Baseline,16,17,1
+block_hint,DivideSmi_Baseline,83,84,0
+block_hint,DivideSmi_Baseline,99,100,0
+block_hint,DivideSmi_Baseline,85,86,0
+block_hint,DivideSmi_Baseline,69,70,0
+block_hint,DivideSmi_Baseline,47,48,1
+block_hint,DivideSmi_Baseline,14,15,1
+block_hint,DivideSmi_Baseline,57,58,1
+block_hint,DivideSmi_Baseline,40,41,1
+block_hint,DivideSmi_Baseline,16,17,1
+block_hint,Modulus_Baseline,108,109,0
+block_hint,Modulus_Baseline,94,95,0
+block_hint,Modulus_Baseline,71,72,1
+block_hint,Modulus_Baseline,66,67,1
+block_hint,Modulus_Baseline,37,38,0
+block_hint,Modulus_Baseline,14,15,1
+block_hint,ModulusSmi_Baseline,71,72,1
+block_hint,ModulusSmi_Baseline,66,67,1
+block_hint,ModulusSmi_Baseline,37,38,0
+block_hint,ModulusSmi_Baseline,14,15,1
+block_hint,ModulusSmi_Baseline,54,55,1
+block_hint,ModulusSmi_Baseline,39,40,1
+block_hint,ModulusSmi_Baseline,16,17,1
 block_hint,BitwiseAnd_Baseline,35,36,0
 block_hint,BitwiseAnd_Baseline,23,24,1
 block_hint,BitwiseAnd_Baseline,8,9,0
@@ -2029,7 +2028,6 @@
 block_hint,BitwiseOr_Baseline,50,51,1
 block_hint,BitwiseOr_Baseline,14,15,1
 block_hint,BitwiseOrSmi_Baseline,5,6,0
-block_hint,BitwiseOrSmi_Baseline,22,23,1
 block_hint,BitwiseOrSmi_Baseline,18,19,0
 block_hint,BitwiseOrSmi_Baseline,16,17,0
 block_hint,BitwiseOrSmi_Baseline,28,29,1
@@ -2085,12 +2083,12 @@
 block_hint,Subtract_WithFeedback,76,77,0
 block_hint,Subtract_WithFeedback,53,54,0
 block_hint,Subtract_WithFeedback,23,24,1
-block_hint,Modulus_WithFeedback,76,77,0
-block_hint,Modulus_WithFeedback,72,73,0
-block_hint,Modulus_WithFeedback,55,56,1
-block_hint,Modulus_WithFeedback,50,51,1
-block_hint,Modulus_WithFeedback,18,19,0
-block_hint,Modulus_WithFeedback,6,7,1
+block_hint,Modulus_WithFeedback,108,109,0
+block_hint,Modulus_WithFeedback,94,95,0
+block_hint,Modulus_WithFeedback,71,72,1
+block_hint,Modulus_WithFeedback,66,67,1
+block_hint,Modulus_WithFeedback,37,38,0
+block_hint,Modulus_WithFeedback,14,15,1
 block_hint,BitwiseOr_WithFeedback,6,7,1
 block_hint,BitwiseOr_WithFeedback,35,36,0
 block_hint,BitwiseOr_WithFeedback,23,24,0
@@ -2374,14 +2372,14 @@
 block_hint,ObjectPrototypeHasOwnProperty,171,172,0
 block_hint,ObjectPrototypeHasOwnProperty,178,179,1
 block_hint,ObjectPrototypeHasOwnProperty,58,59,0
-block_hint,ObjectToString,42,43,0
-block_hint,ObjectToString,57,58,0
-block_hint,ObjectToString,65,66,0
-block_hint,ObjectToString,52,53,0
+block_hint,ObjectToString,45,46,0
+block_hint,ObjectToString,60,61,0
+block_hint,ObjectToString,68,69,0
+block_hint,ObjectToString,55,56,0
 block_hint,ObjectToString,7,8,1
 block_hint,ObjectToString,5,6,1
 block_hint,ObjectToString,11,12,1
-block_hint,ObjectToString,19,20,0
+block_hint,ObjectToString,20,21,0
 block_hint,InstanceOf_WithFeedback,50,51,1
 block_hint,InstanceOf_WithFeedback,52,53,0
 block_hint,InstanceOf_WithFeedback,54,55,1
@@ -2821,7 +2819,6 @@
 block_hint,GetProperty,147,148,0
 block_hint,GetProperty,187,188,1
 block_hint,GetProperty,41,42,0
-block_hint,GetProperty,43,44,0
 block_hint,GetProperty,157,158,0
 block_hint,GetProperty,161,162,1
 block_hint,GetProperty,151,152,0
@@ -2868,6 +2865,8 @@
 block_hint,GetPropertyWithReceiver,34,35,0
 block_hint,GetPropertyWithReceiver,231,232,1
 block_hint,GetPropertyWithReceiver,205,206,0
+block_hint,GetPropertyWithReceiver,181,182,1
+block_hint,GetPropertyWithReceiver,108,109,1
 block_hint,SetProperty,379,380,1
 block_hint,SetProperty,381,382,0
 block_hint,SetProperty,1201,1202,0
@@ -2905,6 +2904,12 @@
 block_hint,CreateDataProperty,55,56,1
 block_hint,CreateDataProperty,543,544,1
 block_hint,CreateDataProperty,57,58,1
+block_hint,FindNonDefaultConstructorOrConstruct,12,13,0
+block_hint,FindNonDefaultConstructorOrConstruct,6,7,0
+block_hint,FindNonDefaultConstructorOrConstruct,14,15,1
+block_hint,FindNonDefaultConstructorOrConstruct,16,17,0
+block_hint,FindNonDefaultConstructorOrConstruct,4,5,1
+block_hint,FindNonDefaultConstructorOrConstruct,18,19,1
 block_hint,ArrayPrototypeConcat,79,80,1
 block_hint,ArrayPrototypeConcat,54,55,0
 block_hint,ArrayPrototypeConcat,63,64,1
@@ -2933,12 +2938,11 @@
 block_hint,ArrayPrototypeConcat,11,12,1
 block_hint,ArrayEvery,73,74,1
 block_hint,ArrayEvery,31,32,0
-block_hint,ArrayEvery,125,126,1
-block_hint,ArrayEvery,117,118,1
+block_hint,ArrayEvery,122,123,1
+block_hint,ArrayEvery,116,117,1
 block_hint,ArrayEvery,91,92,1
 block_hint,ArrayEvery,93,94,1
 block_hint,ArrayEvery,99,100,0
-block_hint,ArrayEvery,121,122,0
 block_hint,ArrayEvery,105,106,1
 block_hint,ArrayEvery,107,108,1
 block_hint,ArrayEvery,97,98,1
@@ -2961,72 +2965,70 @@
 block_hint,ArrayEvery,89,90,0
 block_hint,ArrayEvery,111,112,0
 block_hint,ArrayEvery,79,80,0
-block_hint,ArrayFilter,198,199,1
-block_hint,ArrayFilter,83,84,0
-block_hint,ArrayFilter,301,302,1
-block_hint,ArrayFilter,290,291,1
-block_hint,ArrayFilter,228,229,1
-block_hint,ArrayFilter,230,231,1
-block_hint,ArrayFilter,249,250,0
-block_hint,ArrayFilter,299,300,0
-block_hint,ArrayFilter,273,274,1
-block_hint,ArrayFilter,275,276,1
-block_hint,ArrayFilter,242,243,0
-block_hint,ArrayFilter,279,280,1
-block_hint,ArrayFilter,200,201,1
-block_hint,ArrayFilter,126,127,1
-block_hint,ArrayFilter,23,24,1
-block_hint,ArrayFilter,202,203,1
-block_hint,ArrayFilter,128,129,0
-block_hint,ArrayFilter,25,26,1
-block_hint,ArrayFilter,270,271,1
-block_hint,ArrayFilter,169,170,0
-block_hint,ArrayFilter,281,282,1
-block_hint,ArrayFilter,204,205,1
-block_hint,ArrayFilter,130,131,1
-block_hint,ArrayFilter,27,28,1
-block_hint,ArrayFilter,214,215,1
-block_hint,ArrayFilter,216,217,0
+block_hint,ArrayFilter,195,196,1
+block_hint,ArrayFilter,84,85,0
+block_hint,ArrayFilter,295,296,1
 block_hint,ArrayFilter,286,287,1
-block_hint,ArrayFilter,218,219,1
-block_hint,ArrayFilter,220,221,1
-block_hint,ArrayFilter,222,223,1
-block_hint,ArrayFilter,206,207,1
-block_hint,ArrayFilter,132,133,0
-block_hint,ArrayFilter,29,30,1
-block_hint,ArrayFilter,174,175,0
-block_hint,ArrayFilter,106,107,0
-block_hint,ArrayFilter,245,246,1
-block_hint,ArrayFilter,247,248,0
-block_hint,ArrayFilter,208,209,0
-block_hint,ArrayFilter,134,135,0
-block_hint,ArrayFilter,41,42,0
-block_hint,ArrayFilter,43,44,1
-block_hint,ArrayFilter,152,153,0
-block_hint,ArrayFilter,252,253,1
-block_hint,ArrayFilter,180,181,0
-block_hint,ArrayFilter,182,183,0
-block_hint,ArrayFilter,254,255,0
-block_hint,ArrayFilter,256,257,0
-block_hint,ArrayFilter,258,259,1
-block_hint,ArrayFilter,260,261,0
-block_hint,ArrayFilter,262,263,1
-block_hint,ArrayFilter,283,284,0
-block_hint,ArrayFilter,240,241,0
-block_hint,ArrayFilter,164,165,0
-block_hint,ArrayFilter,98,99,0
-block_hint,ArrayFilter,190,191,1
-block_hint,ArrayFilter,59,60,0
-block_hint,ArrayFilter,63,64,1
-block_hint,ArrayFilter,49,50,1
+block_hint,ArrayFilter,225,226,1
+block_hint,ArrayFilter,227,228,1
+block_hint,ArrayFilter,246,247,0
+block_hint,ArrayFilter,270,271,1
+block_hint,ArrayFilter,272,273,1
+block_hint,ArrayFilter,239,240,0
+block_hint,ArrayFilter,276,277,1
+block_hint,ArrayFilter,197,198,1
+block_hint,ArrayFilter,123,124,1
+block_hint,ArrayFilter,22,23,1
+block_hint,ArrayFilter,199,200,1
+block_hint,ArrayFilter,125,126,0
+block_hint,ArrayFilter,24,25,1
+block_hint,ArrayFilter,267,268,1
+block_hint,ArrayFilter,166,167,0
+block_hint,ArrayFilter,278,279,1
+block_hint,ArrayFilter,201,202,1
+block_hint,ArrayFilter,127,128,1
+block_hint,ArrayFilter,26,27,1
+block_hint,ArrayFilter,211,212,1
+block_hint,ArrayFilter,213,214,0
+block_hint,ArrayFilter,283,284,1
+block_hint,ArrayFilter,215,216,1
+block_hint,ArrayFilter,217,218,1
+block_hint,ArrayFilter,219,220,1
+block_hint,ArrayFilter,203,204,1
+block_hint,ArrayFilter,129,130,0
+block_hint,ArrayFilter,28,29,1
+block_hint,ArrayFilter,171,172,0
+block_hint,ArrayFilter,103,104,0
+block_hint,ArrayFilter,242,243,1
+block_hint,ArrayFilter,244,245,0
+block_hint,ArrayFilter,205,206,0
+block_hint,ArrayFilter,131,132,0
+block_hint,ArrayFilter,42,43,0
+block_hint,ArrayFilter,44,45,1
+block_hint,ArrayFilter,149,150,0
+block_hint,ArrayFilter,249,250,1
+block_hint,ArrayFilter,177,178,0
+block_hint,ArrayFilter,179,180,0
+block_hint,ArrayFilter,251,252,0
+block_hint,ArrayFilter,253,254,0
+block_hint,ArrayFilter,255,256,1
+block_hint,ArrayFilter,257,258,0
+block_hint,ArrayFilter,259,260,1
+block_hint,ArrayFilter,280,281,0
+block_hint,ArrayFilter,237,238,0
+block_hint,ArrayFilter,161,162,0
+block_hint,ArrayFilter,95,96,0
+block_hint,ArrayFilter,187,188,1
+block_hint,ArrayFilter,60,61,0
+block_hint,ArrayFilter,64,65,1
+block_hint,ArrayFilter,50,51,1
 block_hint,ArrayForEach,70,71,1
 block_hint,ArrayForEach,29,30,0
-block_hint,ArrayForEach,102,103,1
-block_hint,ArrayForEach,96,97,1
+block_hint,ArrayForEach,99,100,1
+block_hint,ArrayForEach,95,96,1
 block_hint,ArrayForEach,76,77,1
 block_hint,ArrayForEach,78,79,1
 block_hint,ArrayForEach,84,85,0
-block_hint,ArrayForEach,100,101,0
 block_hint,ArrayForEach,90,91,1
 block_hint,ArrayForEach,92,93,1
 block_hint,ArrayForEach,47,48,0
@@ -3079,93 +3081,93 @@
 block_hint,LoadJoinElement_FastDoubleElements_0,3,4,1
 block_hint,LoadJoinElement_FastDoubleElements_0,5,6,0
 block_hint,LoadJoinElement_FastDoubleElements_0,7,8,1
-block_hint,JoinStackPush,30,31,1
+block_hint,JoinStackPush,28,29,1
 block_hint,JoinStackPush,6,7,1
 block_hint,JoinStackPush,10,11,0
 block_hint,JoinStackPop,9,10,1
 block_hint,JoinStackPop,4,5,1
-block_hint,ArrayPrototypeJoin,524,525,1
-block_hint,ArrayPrototypeJoin,462,463,1
-block_hint,ArrayPrototypeJoin,425,426,1
-block_hint,ArrayPrototypeJoin,340,341,1
-block_hint,ArrayPrototypeJoin,342,343,1
-block_hint,ArrayPrototypeJoin,373,374,1
-block_hint,ArrayPrototypeJoin,346,347,0
-block_hint,ArrayPrototypeJoin,185,186,0
-block_hint,ArrayPrototypeJoin,480,481,1
-block_hint,ArrayPrototypeJoin,446,447,1
-block_hint,ArrayPrototypeJoin,334,335,0
-block_hint,ArrayPrototypeJoin,234,235,1
+block_hint,ArrayPrototypeJoin,518,519,1
+block_hint,ArrayPrototypeJoin,456,457,1
+block_hint,ArrayPrototypeJoin,419,420,1
+block_hint,ArrayPrototypeJoin,334,335,1
+block_hint,ArrayPrototypeJoin,336,337,1
+block_hint,ArrayPrototypeJoin,367,368,1
+block_hint,ArrayPrototypeJoin,340,341,0
+block_hint,ArrayPrototypeJoin,179,180,0
+block_hint,ArrayPrototypeJoin,474,475,1
+block_hint,ArrayPrototypeJoin,440,441,1
+block_hint,ArrayPrototypeJoin,328,329,0
+block_hint,ArrayPrototypeJoin,228,229,1
 block_hint,ArrayPrototypeJoin,30,31,1
-block_hint,ArrayPrototypeJoin,187,188,0
+block_hint,ArrayPrototypeJoin,181,182,0
 block_hint,ArrayPrototypeJoin,32,33,1
-block_hint,ArrayPrototypeJoin,393,394,1
-block_hint,ArrayPrototypeJoin,331,332,0
-block_hint,ArrayPrototypeJoin,149,150,1
-block_hint,ArrayPrototypeJoin,499,500,1
-block_hint,ArrayPrototypeJoin,464,465,0
-block_hint,ArrayPrototypeJoin,429,430,0
-block_hint,ArrayPrototypeJoin,375,376,1
-block_hint,ArrayPrototypeJoin,189,190,1
-block_hint,ArrayPrototypeJoin,38,39,1
-block_hint,ArrayPrototypeJoin,466,467,1
-block_hint,ArrayPrototypeJoin,431,432,0
-block_hint,ArrayPrototypeJoin,301,302,1
-block_hint,ArrayPrototypeJoin,434,435,0
-block_hint,ArrayPrototypeJoin,351,352,0
-block_hint,ArrayPrototypeJoin,195,196,0
-block_hint,ArrayPrototypeJoin,238,239,1
-block_hint,ArrayPrototypeJoin,151,152,1
-block_hint,ArrayPrototypeJoin,489,490,0
+block_hint,ArrayPrototypeJoin,387,388,1
+block_hint,ArrayPrototypeJoin,325,326,0
+block_hint,ArrayPrototypeJoin,143,144,1
 block_hint,ArrayPrototypeJoin,493,494,1
-block_hint,ArrayPrototypeJoin,531,532,0
-block_hint,ArrayPrototypeJoin,527,528,0
-block_hint,ArrayPrototypeJoin,520,521,1
-block_hint,ArrayPrototypeJoin,495,496,1
-block_hint,ArrayPrototypeJoin,491,492,0
-block_hint,ArrayPrototypeJoin,153,154,0
-block_hint,ArrayPrototypeJoin,155,156,0
-block_hint,ArrayPrototypeJoin,475,476,0
-block_hint,ArrayPrototypeJoin,477,478,0
+block_hint,ArrayPrototypeJoin,458,459,0
+block_hint,ArrayPrototypeJoin,423,424,0
+block_hint,ArrayPrototypeJoin,369,370,1
+block_hint,ArrayPrototypeJoin,183,184,1
+block_hint,ArrayPrototypeJoin,38,39,1
 block_hint,ArrayPrototypeJoin,460,461,1
+block_hint,ArrayPrototypeJoin,425,426,0
+block_hint,ArrayPrototypeJoin,295,296,1
+block_hint,ArrayPrototypeJoin,428,429,0
+block_hint,ArrayPrototypeJoin,345,346,0
+block_hint,ArrayPrototypeJoin,189,190,0
+block_hint,ArrayPrototypeJoin,232,233,1
+block_hint,ArrayPrototypeJoin,145,146,1
+block_hint,ArrayPrototypeJoin,483,484,0
+block_hint,ArrayPrototypeJoin,487,488,1
+block_hint,ArrayPrototypeJoin,525,526,0
+block_hint,ArrayPrototypeJoin,521,522,0
+block_hint,ArrayPrototypeJoin,514,515,1
+block_hint,ArrayPrototypeJoin,489,490,1
+block_hint,ArrayPrototypeJoin,485,486,0
+block_hint,ArrayPrototypeJoin,147,148,0
+block_hint,ArrayPrototypeJoin,149,150,0
+block_hint,ArrayPrototypeJoin,469,470,0
+block_hint,ArrayPrototypeJoin,471,472,0
+block_hint,ArrayPrototypeJoin,454,455,1
+block_hint,ArrayPrototypeJoin,407,408,1
+block_hint,ArrayPrototypeJoin,409,410,1
+block_hint,ArrayPrototypeJoin,411,412,1
 block_hint,ArrayPrototypeJoin,413,414,1
-block_hint,ArrayPrototypeJoin,415,416,1
-block_hint,ArrayPrototypeJoin,417,418,1
-block_hint,ArrayPrototypeJoin,419,420,1
-block_hint,ArrayPrototypeJoin,203,204,1
-block_hint,ArrayPrototypeJoin,260,261,0
+block_hint,ArrayPrototypeJoin,197,198,1
+block_hint,ArrayPrototypeJoin,254,255,0
+block_hint,ArrayPrototypeJoin,256,257,0
+block_hint,ArrayPrototypeJoin,302,303,0
 block_hint,ArrayPrototypeJoin,262,263,0
-block_hint,ArrayPrototypeJoin,308,309,0
-block_hint,ArrayPrototypeJoin,268,269,0
+block_hint,ArrayPrototypeJoin,264,265,0
+block_hint,ArrayPrototypeJoin,203,204,1
+block_hint,ArrayPrototypeJoin,72,73,1
+block_hint,ArrayPrototypeJoin,381,382,1
+block_hint,ArrayPrototypeJoin,305,306,0
+block_hint,ArrayPrototypeJoin,207,208,1
 block_hint,ArrayPrototypeJoin,270,271,0
+block_hint,ArrayPrototypeJoin,272,273,0
 block_hint,ArrayPrototypeJoin,209,210,1
-block_hint,ArrayPrototypeJoin,74,75,1
-block_hint,ArrayPrototypeJoin,387,388,1
+block_hint,ArrayPrototypeJoin,86,87,1
+block_hint,ArrayPrototypeJoin,307,308,0
+block_hint,ArrayPrototypeJoin,219,220,1
+block_hint,ArrayPrototypeJoin,102,103,0
+block_hint,ArrayPrototypeJoin,104,105,0
+block_hint,ArrayPrototypeJoin,435,436,1
+block_hint,ArrayPrototypeJoin,403,404,1
+block_hint,ArrayPrototypeJoin,217,218,1
+block_hint,ArrayPrototypeJoin,100,101,0
+block_hint,ArrayPrototypeJoin,433,434,1
+block_hint,ArrayPrototypeJoin,401,402,1
+block_hint,ArrayPrototypeJoin,96,97,1
+block_hint,ArrayPrototypeJoin,352,353,1
 block_hint,ArrayPrototypeJoin,311,312,0
-block_hint,ArrayPrototypeJoin,213,214,1
-block_hint,ArrayPrototypeJoin,276,277,0
-block_hint,ArrayPrototypeJoin,278,279,0
-block_hint,ArrayPrototypeJoin,215,216,1
-block_hint,ArrayPrototypeJoin,90,91,1
-block_hint,ArrayPrototypeJoin,313,314,0
-block_hint,ArrayPrototypeJoin,225,226,1
+block_hint,ArrayPrototypeJoin,215,216,0
+block_hint,ArrayPrototypeJoin,106,107,1
 block_hint,ArrayPrototypeJoin,108,109,0
-block_hint,ArrayPrototypeJoin,110,111,0
-block_hint,ArrayPrototypeJoin,441,442,1
-block_hint,ArrayPrototypeJoin,409,410,1
-block_hint,ArrayPrototypeJoin,223,224,1
-block_hint,ArrayPrototypeJoin,106,107,0
-block_hint,ArrayPrototypeJoin,439,440,1
-block_hint,ArrayPrototypeJoin,407,408,1
-block_hint,ArrayPrototypeJoin,102,103,1
-block_hint,ArrayPrototypeJoin,358,359,1
-block_hint,ArrayPrototypeJoin,317,318,0
-block_hint,ArrayPrototypeJoin,221,222,0
-block_hint,ArrayPrototypeJoin,112,113,1
-block_hint,ArrayPrototypeJoin,114,115,0
-block_hint,ArrayPrototypeJoin,116,117,1
-block_hint,ArrayPrototypeJoin,290,291,1
-block_hint,ArrayPrototypeJoin,145,146,1
+block_hint,ArrayPrototypeJoin,110,111,1
+block_hint,ArrayPrototypeJoin,284,285,1
+block_hint,ArrayPrototypeJoin,139,140,1
 block_hint,ArrayPrototypeToString,14,15,1
 block_hint,ArrayPrototypeToString,11,12,1
 block_hint,ArrayPrototypeToString,8,9,1
@@ -3213,15 +3215,14 @@
 block_hint,ArrayPrototypeLastIndexOf,31,32,0
 block_hint,ArrayMap,165,166,1
 block_hint,ArrayMap,72,73,0
-block_hint,ArrayMap,258,259,1
-block_hint,ArrayMap,237,238,1
+block_hint,ArrayMap,255,256,1
+block_hint,ArrayMap,236,237,1
 block_hint,ArrayMap,188,189,1
 block_hint,ArrayMap,190,191,1
 block_hint,ArrayMap,209,210,0
-block_hint,ArrayMap,256,257,0
 block_hint,ArrayMap,221,222,1
 block_hint,ArrayMap,223,224,1
-block_hint,ArrayMap,246,247,1
+block_hint,ArrayMap,245,246,1
 block_hint,ArrayMap,206,207,0
 block_hint,ArrayMap,218,219,1
 block_hint,ArrayMap,225,226,1
@@ -3235,9 +3236,9 @@
 block_hint,ArrayMap,182,183,1
 block_hint,ArrayMap,157,158,1
 block_hint,ArrayMap,55,56,0
-block_hint,ArrayMap,271,272,1
-block_hint,ArrayMap,268,269,0
-block_hint,ArrayMap,249,250,0
+block_hint,ArrayMap,268,269,1
+block_hint,ArrayMap,265,266,0
+block_hint,ArrayMap,248,249,0
 block_hint,ArrayMap,227,228,0
 block_hint,ArrayMap,195,196,0
 block_hint,ArrayMap,116,117,0
@@ -3248,7 +3249,7 @@
 block_hint,ArrayMap,120,121,0
 block_hint,ArrayMap,37,38,1
 block_hint,ArrayMap,35,36,1
-block_hint,ArrayMap,254,255,0
+block_hint,ArrayMap,253,254,0
 block_hint,ArrayMap,203,204,0
 block_hint,ArrayMap,149,150,0
 block_hint,ArrayMap,45,46,1
@@ -3268,12 +3269,11 @@
 block_hint,ArrayMap,135,136,1
 block_hint,ArrayReduce,81,82,1
 block_hint,ArrayReduce,30,31,0
-block_hint,ArrayReduce,127,128,1
-block_hint,ArrayReduce,121,122,1
+block_hint,ArrayReduce,124,125,1
+block_hint,ArrayReduce,120,121,1
 block_hint,ArrayReduce,89,90,1
 block_hint,ArrayReduce,91,92,1
 block_hint,ArrayReduce,101,102,0
-block_hint,ArrayReduce,125,126,0
 block_hint,ArrayReduce,111,112,1
 block_hint,ArrayReduce,113,114,1
 block_hint,ArrayReduce,95,96,1
@@ -3297,42 +3297,42 @@
 block_hint,ArrayReduce,57,58,0
 block_hint,ArrayReduce,59,60,0
 block_hint,ArrayReduce,23,24,0
-block_hint,ArrayPrototypeReverse,242,243,1
-block_hint,ArrayPrototypeReverse,216,217,1
-block_hint,ArrayPrototypeReverse,196,197,1
-block_hint,ArrayPrototypeReverse,158,159,1
-block_hint,ArrayPrototypeReverse,109,110,1
-block_hint,ArrayPrototypeReverse,20,21,1
-block_hint,ArrayPrototypeReverse,198,199,1
-block_hint,ArrayPrototypeReverse,175,176,0
-block_hint,ArrayPrototypeReverse,146,147,1
-block_hint,ArrayPrototypeReverse,124,125,1
-block_hint,ArrayPrototypeReverse,95,96,0
-block_hint,ArrayPrototypeShift,240,241,1
-block_hint,ArrayPrototypeShift,208,209,1
-block_hint,ArrayPrototypeShift,188,189,1
-block_hint,ArrayPrototypeShift,135,136,1
-block_hint,ArrayPrototypeShift,84,85,1
-block_hint,ArrayPrototypeShift,12,13,1
-block_hint,ArrayPrototypeShift,199,200,1
-block_hint,ArrayPrototypeShift,171,172,0
-block_hint,ArrayPrototypeShift,137,138,1
-block_hint,ArrayPrototypeShift,86,87,0
-block_hint,ArrayPrototypeShift,14,15,1
-block_hint,ArrayPrototypeShift,139,140,0
-block_hint,ArrayPrototypeShift,88,89,0
-block_hint,ArrayPrototypeShift,71,72,0
-block_hint,ArrayPrototypeShift,90,91,0
-block_hint,ArrayPrototypeShift,26,27,0
-block_hint,ArrayPrototypeShift,28,29,1
-block_hint,ArrayPrototypeShift,173,174,0
-block_hint,ArrayPrototypeShift,92,93,0
-block_hint,ArrayPrototypeShift,32,33,0
-block_hint,ArrayPrototypeShift,151,152,0
-block_hint,ArrayPrototypeShift,114,115,1
-block_hint,ArrayPrototypeShift,94,95,0
-block_hint,ArrayPrototypeShift,38,39,0
-block_hint,ArrayPrototypeShift,40,41,1
+block_hint,ArrayPrototypeReverse,236,237,1
+block_hint,ArrayPrototypeReverse,210,211,1
+block_hint,ArrayPrototypeReverse,190,191,1
+block_hint,ArrayPrototypeReverse,152,153,1
+block_hint,ArrayPrototypeReverse,103,104,1
+block_hint,ArrayPrototypeReverse,18,19,1
+block_hint,ArrayPrototypeReverse,192,193,1
+block_hint,ArrayPrototypeReverse,169,170,0
+block_hint,ArrayPrototypeReverse,140,141,1
+block_hint,ArrayPrototypeReverse,118,119,1
+block_hint,ArrayPrototypeReverse,89,90,0
+block_hint,ArrayPrototypeShift,237,238,1
+block_hint,ArrayPrototypeShift,205,206,1
+block_hint,ArrayPrototypeShift,185,186,1
+block_hint,ArrayPrototypeShift,132,133,1
+block_hint,ArrayPrototypeShift,81,82,1
+block_hint,ArrayPrototypeShift,11,12,1
+block_hint,ArrayPrototypeShift,196,197,1
+block_hint,ArrayPrototypeShift,168,169,0
+block_hint,ArrayPrototypeShift,134,135,1
+block_hint,ArrayPrototypeShift,83,84,0
+block_hint,ArrayPrototypeShift,13,14,1
+block_hint,ArrayPrototypeShift,136,137,0
+block_hint,ArrayPrototypeShift,85,86,0
+block_hint,ArrayPrototypeShift,68,69,0
+block_hint,ArrayPrototypeShift,87,88,0
+block_hint,ArrayPrototypeShift,27,28,0
+block_hint,ArrayPrototypeShift,29,30,1
+block_hint,ArrayPrototypeShift,170,171,0
+block_hint,ArrayPrototypeShift,89,90,0
+block_hint,ArrayPrototypeShift,33,34,0
+block_hint,ArrayPrototypeShift,148,149,0
+block_hint,ArrayPrototypeShift,111,112,1
+block_hint,ArrayPrototypeShift,91,92,0
+block_hint,ArrayPrototypeShift,39,40,0
+block_hint,ArrayPrototypeShift,41,42,1
 block_hint,ArrayPrototypeSlice,288,289,1
 block_hint,ArrayPrototypeSlice,267,268,1
 block_hint,ArrayPrototypeSlice,245,246,1
@@ -3425,12 +3425,11 @@
 block_hint,ArrayPrototypeSlice,73,74,1
 block_hint,ArraySome,88,89,1
 block_hint,ArraySome,31,32,0
-block_hint,ArraySome,122,123,1
-block_hint,ArraySome,116,117,1
+block_hint,ArraySome,119,120,1
+block_hint,ArraySome,115,116,1
 block_hint,ArraySome,93,94,1
 block_hint,ArraySome,95,96,1
 block_hint,ArraySome,101,102,0
-block_hint,ArraySome,120,121,0
 block_hint,ArraySome,108,109,1
 block_hint,ArraySome,110,111,1
 block_hint,ArraySome,99,100,1
@@ -3448,98 +3447,101 @@
 block_hint,ArraySome,19,20,0
 block_hint,ArraySome,21,22,1
 block_hint,ArraySome,66,67,0
-block_hint,ArrayPrototypeSplice,605,606,1
-block_hint,ArrayPrototypeSplice,453,454,1
-block_hint,ArrayPrototypeSplice,455,456,1
-block_hint,ArrayPrototypeSplice,1201,1202,0
-block_hint,ArrayPrototypeSplice,1183,1184,0
-block_hint,ArrayPrototypeSplice,1159,1160,0
-block_hint,ArrayPrototypeSplice,1138,1139,0
-block_hint,ArrayPrototypeSplice,1104,1105,0
+block_hint,ArrayPrototypeSplice,599,600,1
+block_hint,ArrayPrototypeSplice,447,448,1
+block_hint,ArrayPrototypeSplice,449,450,1
+block_hint,ArrayPrototypeSplice,1195,1196,0
+block_hint,ArrayPrototypeSplice,1177,1178,0
+block_hint,ArrayPrototypeSplice,1153,1154,0
+block_hint,ArrayPrototypeSplice,1132,1133,0
+block_hint,ArrayPrototypeSplice,1098,1099,0
+block_hint,ArrayPrototypeSplice,1066,1067,0
+block_hint,ArrayPrototypeSplice,1025,1026,0
+block_hint,ArrayPrototypeSplice,1096,1097,0
+block_hint,ArrayPrototypeSplice,1064,1065,0
+block_hint,ArrayPrototypeSplice,1021,1022,1
+block_hint,ArrayPrototypeSplice,934,935,0
+block_hint,ArrayPrototypeSplice,1087,1088,0
+block_hint,ArrayPrototypeSplice,1220,1221,0
+block_hint,ArrayPrototypeSplice,1212,1213,0
+block_hint,ArrayPrototypeSplice,1199,1200,0
+block_hint,ArrayPrototypeSplice,1187,1188,1
+block_hint,ArrayPrototypeSplice,1156,1157,0
+block_hint,ArrayPrototypeSplice,1136,1137,0
+block_hint,ArrayPrototypeSplice,1115,1116,0
 block_hint,ArrayPrototypeSplice,1072,1073,0
-block_hint,ArrayPrototypeSplice,1031,1032,0
-block_hint,ArrayPrototypeSplice,1102,1103,0
+block_hint,ArrayPrototypeSplice,1032,1033,0
 block_hint,ArrayPrototypeSplice,1070,1071,0
-block_hint,ArrayPrototypeSplice,1027,1028,1
-block_hint,ArrayPrototypeSplice,940,941,0
-block_hint,ArrayPrototypeSplice,1093,1094,0
-block_hint,ArrayPrototypeSplice,1226,1227,0
-block_hint,ArrayPrototypeSplice,1218,1219,0
-block_hint,ArrayPrototypeSplice,1205,1206,0
-block_hint,ArrayPrototypeSplice,1193,1194,1
-block_hint,ArrayPrototypeSplice,1162,1163,0
-block_hint,ArrayPrototypeSplice,1142,1143,0
-block_hint,ArrayPrototypeSplice,1121,1122,0
-block_hint,ArrayPrototypeSplice,1078,1079,0
-block_hint,ArrayPrototypeSplice,1038,1039,0
-block_hint,ArrayPrototypeSplice,1076,1077,0
-block_hint,ArrayPrototypeSplice,1036,1037,0
-block_hint,ArrayPrototypeSplice,987,988,1
-block_hint,ArrayPrototypeSplice,877,878,0
-block_hint,ArrayPrototypeSplice,842,843,0
-block_hint,ArrayPrototypeSplice,810,811,0
-block_hint,ArrayPrototypeSplice,731,732,0
-block_hint,ArrayPrototypeSplice,677,678,0
-block_hint,ArrayPrototypeSplice,607,608,0
-block_hint,ArrayPrototypeSplice,513,514,1
-block_hint,ArrayPrototypeSplice,459,460,0
-block_hint,ArrayPrototypeSplice,229,230,0
-block_hint,ArrayPrototypeSplice,339,340,0
+block_hint,ArrayPrototypeSplice,1030,1031,0
+block_hint,ArrayPrototypeSplice,981,982,1
+block_hint,ArrayPrototypeSplice,871,872,0
+block_hint,ArrayPrototypeSplice,836,837,0
+block_hint,ArrayPrototypeSplice,804,805,0
+block_hint,ArrayPrototypeSplice,725,726,0
+block_hint,ArrayPrototypeSplice,671,672,0
+block_hint,ArrayPrototypeSplice,601,602,0
+block_hint,ArrayPrototypeSplice,507,508,1
+block_hint,ArrayPrototypeSplice,453,454,0
+block_hint,ArrayPrototypeSplice,223,224,0
+block_hint,ArrayPrototypeSplice,333,334,0
+block_hint,ArrayPrototypeSplice,335,336,0
+block_hint,ArrayPrototypeSplice,337,338,0
+block_hint,ArrayPrototypeSplice,225,226,1
+block_hint,ArrayPrototypeSplice,51,52,1
+block_hint,ArrayPrototypeSplice,339,340,1
 block_hint,ArrayPrototypeSplice,341,342,0
 block_hint,ArrayPrototypeSplice,343,344,0
-block_hint,ArrayPrototypeSplice,231,232,1
+block_hint,ArrayPrototypeSplice,388,389,1
+block_hint,ArrayPrototypeSplice,227,228,0
 block_hint,ArrayPrototypeSplice,53,54,1
-block_hint,ArrayPrototypeSplice,345,346,1
-block_hint,ArrayPrototypeSplice,347,348,0
-block_hint,ArrayPrototypeSplice,349,350,0
-block_hint,ArrayPrototypeSplice,394,395,1
-block_hint,ArrayPrototypeSplice,233,234,0
-block_hint,ArrayPrototypeSplice,55,56,1
-block_hint,ArrayPrototypeSplice,250,251,0
-block_hint,ArrayPrototypeSplice,91,92,1
-block_hint,ArrayPrototypeSplice,558,559,0
-block_hint,ArrayPrototypeSplice,407,408,0
-block_hint,ArrayPrototypeSplice,590,591,0
-block_hint,ArrayPrototypeSplice,530,531,1
-block_hint,ArrayPrototypeSplice,352,353,0
-block_hint,ArrayPrototypeSplice,354,355,1
-block_hint,ArrayPrototypeSplice,241,242,0
-block_hint,ArrayPrototypeSplice,263,264,1
-block_hint,ArrayPrototypeSplice,103,104,0
+block_hint,ArrayPrototypeSplice,244,245,0
+block_hint,ArrayPrototypeSplice,93,94,1
+block_hint,ArrayPrototypeSplice,552,553,0
+block_hint,ArrayPrototypeSplice,401,402,0
+block_hint,ArrayPrototypeSplice,584,585,0
+block_hint,ArrayPrototypeSplice,524,525,1
+block_hint,ArrayPrototypeSplice,346,347,0
+block_hint,ArrayPrototypeSplice,348,349,1
 block_hint,ArrayPrototypeSplice,235,236,0
-block_hint,ArrayPrototypeSplice,336,337,0
-block_hint,ArrayPrototypeSplice,334,335,0
-block_hint,ArrayPrototypeSplice,398,399,1
+block_hint,ArrayPrototypeSplice,257,258,1
+block_hint,ArrayPrototypeSplice,105,106,0
+block_hint,ArrayPrototypeSplice,229,230,0
+block_hint,ArrayPrototypeSplice,330,331,0
+block_hint,ArrayPrototypeSplice,328,329,0
+block_hint,ArrayPrototypeSplice,392,393,1
 block_hint,ArrayPrototypeSplice,65,66,1
-block_hint,ArrayPrototypeSplice,299,300,0
-block_hint,ArrayPrototypeSplice,145,146,0
+block_hint,ArrayPrototypeSplice,293,294,0
+block_hint,ArrayPrototypeSplice,143,144,0
 block_hint,ArrayPrototypeSplice,67,68,0
 block_hint,ArrayPrototypeSplice,69,70,0
-block_hint,ArrayPrototypeSplice,268,269,1
-block_hint,ArrayPrototypeSplice,184,185,1
-block_hint,ArrayPrototypeSplice,332,333,0
-block_hint,ArrayPrototypeSplice,428,429,1
-block_hint,ArrayPrototypeSplice,270,271,1
-block_hint,ArrayPrototypeSplice,430,431,1
+block_hint,ArrayPrototypeSplice,262,263,1
+block_hint,ArrayPrototypeSplice,178,179,1
+block_hint,ArrayPrototypeSplice,326,327,0
+block_hint,ArrayPrototypeSplice,422,423,1
+block_hint,ArrayPrototypeSplice,264,265,1
+block_hint,ArrayPrototypeSplice,111,112,0
+block_hint,ArrayPrototypeSplice,424,425,1
+block_hint,ArrayPrototypeSplice,266,267,0
 block_hint,ArrayPrototypeSplice,113,114,0
-block_hint,ArrayPrototypeSplice,188,189,1
+block_hint,ArrayPrototypeSplice,115,116,0
+block_hint,ArrayPrototypeSplice,182,183,1
 block_hint,ArrayPrototypeSplice,63,64,1
-block_hint,ArrayPrototypeSplice,129,130,1
-block_hint,ArrayPrototypeSplice,301,302,0
+block_hint,ArrayPrototypeSplice,131,132,1
+block_hint,ArrayPrototypeSplice,295,296,0
 block_hint,ArrayPrototypeSplice,71,72,1
-block_hint,ArrayPrototypeUnshift,188,189,1
-block_hint,ArrayPrototypeUnshift,161,162,1
-block_hint,ArrayPrototypeUnshift,143,144,1
-block_hint,ArrayPrototypeUnshift,99,100,1
-block_hint,ArrayPrototypeUnshift,58,59,1
-block_hint,ArrayPrototypeUnshift,11,12,1
-block_hint,ArrayPrototypeUnshift,132,133,1
-block_hint,ArrayPrototypeUnshift,101,102,0
-block_hint,ArrayPrototypeUnshift,60,61,0
-block_hint,ArrayPrototypeUnshift,103,104,1
-block_hint,ArrayPrototypeUnshift,62,63,0
-block_hint,ArrayPrototypeUnshift,19,20,1
-block_hint,ArrayPrototypeUnshift,21,22,0
+block_hint,ArrayPrototypeUnshift,185,186,1
+block_hint,ArrayPrototypeUnshift,158,159,1
+block_hint,ArrayPrototypeUnshift,140,141,1
+block_hint,ArrayPrototypeUnshift,96,97,1
+block_hint,ArrayPrototypeUnshift,55,56,1
+block_hint,ArrayPrototypeUnshift,10,11,1
+block_hint,ArrayPrototypeUnshift,129,130,1
+block_hint,ArrayPrototypeUnshift,98,99,0
+block_hint,ArrayPrototypeUnshift,57,58,0
+block_hint,ArrayPrototypeUnshift,100,101,1
+block_hint,ArrayPrototypeUnshift,59,60,0
+block_hint,ArrayPrototypeUnshift,20,21,1
+block_hint,ArrayPrototypeUnshift,22,23,0
 block_hint,ArrayBufferPrototypeGetByteLength,15,16,1
 block_hint,ArrayBufferPrototypeGetByteLength,10,11,1
 block_hint,ArrayBufferPrototypeGetByteLength,12,13,1
@@ -3606,16 +3608,14 @@
 block_hint,StringPrototypeCodePointAt,72,73,0
 block_hint,StringPrototypeCodePointAt,48,49,0
 block_hint,StringPrototypeCodePointAt,18,19,1
-block_hint,StringConstructor,65,66,1
+block_hint,StringConstructor,64,65,1
 block_hint,StringConstructor,49,50,0
-block_hint,StringConstructor,63,64,0
 block_hint,StringConstructor,36,37,0
-block_hint,StringConstructor,78,79,0
-block_hint,StringConstructor,83,84,1
-block_hint,StringConstructor,81,82,1
-block_hint,StringConstructor,75,76,1
-block_hint,StringConstructor,59,60,0
-block_hint,StringConstructor,61,62,1
+block_hint,StringConstructor,78,79,1
+block_hint,StringConstructor,76,77,1
+block_hint,StringConstructor,73,74,1
+block_hint,StringConstructor,60,61,0
+block_hint,StringConstructor,62,63,1
 block_hint,StringConstructor,45,46,0
 block_hint,StringConstructor,24,25,0
 block_hint,StringConstructor,26,27,1
@@ -3864,7 +3864,6 @@
 block_hint,CallIteratorWithFeedback,10,11,1
 block_hint,MathAbs,14,15,1
 block_hint,MathAbs,16,17,1
-block_hint,MathAbs,7,8,0
 block_hint,MathAbs,23,24,0
 block_hint,MathAbs,9,10,0
 block_hint,MathAbs,11,12,1
@@ -4016,11 +4015,9 @@
 block_hint,NumberParseInt,3,4,1
 block_hint,Add,66,67,1
 block_hint,Add,24,25,0
-block_hint,Add,52,53,1
 block_hint,Add,68,69,0
 block_hint,Add,35,36,0
 block_hint,Add,40,41,0
-block_hint,Add,29,30,1
 block_hint,Subtract,24,25,0
 block_hint,Subtract,9,10,0
 block_hint,Subtract,22,23,0
@@ -4179,73 +4176,71 @@
 block_hint,RegExpPrototypeExec,144,145,1
 block_hint,RegExpPrototypeExec,116,117,1
 block_hint,RegExpPrototypeExec,156,157,1
-block_hint,RegExpMatchFast,363,364,0
-block_hint,RegExpMatchFast,293,294,1
-block_hint,RegExpMatchFast,34,35,1
+block_hint,RegExpMatchFast,357,358,0
+block_hint,RegExpMatchFast,289,290,1
+block_hint,RegExpMatchFast,32,33,1
+block_hint,RegExpMatchFast,326,327,0
+block_hint,RegExpMatchFast,234,235,0
+block_hint,RegExpMatchFast,283,284,0
+block_hint,RegExpMatchFast,448,449,0
+block_hint,RegExpMatchFast,392,393,1
+block_hint,RegExpMatchFast,291,292,1
+block_hint,RegExpMatchFast,285,286,0
+block_hint,RegExpMatchFast,129,130,0
+block_hint,RegExpMatchFast,236,237,1
+block_hint,RegExpMatchFast,238,239,1
+block_hint,RegExpMatchFast,40,41,1
 block_hint,RegExpMatchFast,331,332,0
-block_hint,RegExpMatchFast,240,241,0
-block_hint,RegExpMatchFast,287,288,0
-block_hint,RegExpMatchFast,454,455,0
-block_hint,RegExpMatchFast,397,398,1
-block_hint,RegExpMatchFast,295,296,1
-block_hint,RegExpMatchFast,289,290,0
-block_hint,RegExpMatchFast,127,128,0
-block_hint,RegExpMatchFast,242,243,1
+block_hint,RegExpMatchFast,240,241,1
+block_hint,RegExpMatchFast,456,457,1
+block_hint,RegExpMatchFast,394,395,0
+block_hint,RegExpMatchFast,320,321,1
+block_hint,RegExpMatchFast,133,134,0
+block_hint,RegExpMatchFast,48,49,1
 block_hint,RegExpMatchFast,244,245,1
-block_hint,RegExpMatchFast,42,43,1
-block_hint,RegExpMatchFast,336,337,0
-block_hint,RegExpMatchFast,246,247,1
-block_hint,RegExpMatchFast,462,463,1
-block_hint,RegExpMatchFast,399,400,0
-block_hint,RegExpMatchFast,325,326,1
-block_hint,RegExpMatchFast,131,132,0
-block_hint,RegExpMatchFast,50,51,1
-block_hint,RegExpMatchFast,250,251,1
-block_hint,RegExpMatchFast,186,187,1
-block_hint,RegExpMatchFast,263,264,1
-block_hint,RegExpMatchFast,301,302,0
+block_hint,RegExpMatchFast,180,181,1
+block_hint,RegExpMatchFast,259,260,1
+block_hint,RegExpMatchFast,297,298,0
+block_hint,RegExpMatchFast,82,83,1
 block_hint,RegExpMatchFast,84,85,1
+block_hint,RegExpMatchFast,301,302,0
+block_hint,RegExpMatchFast,344,345,0
+block_hint,RegExpMatchFast,379,380,0
+block_hint,RegExpMatchFast,299,300,0
 block_hint,RegExpMatchFast,86,87,1
-block_hint,RegExpMatchFast,305,306,0
-block_hint,RegExpMatchFast,350,351,0
-block_hint,RegExpMatchFast,383,384,0
-block_hint,RegExpMatchFast,303,304,0
-block_hint,RegExpMatchFast,88,89,1
-block_hint,RegExpMatchFast,345,346,0
-block_hint,RegExpMatchFast,254,255,0
-block_hint,RegExpMatchFast,279,280,0
-block_hint,RegExpMatchFast,196,197,1
-block_hint,RegExpMatchFast,456,457,0
-block_hint,RegExpMatchFast,442,443,1
-block_hint,RegExpMatchFast,395,396,1
-block_hint,RegExpMatchFast,307,308,1
-block_hint,RegExpMatchFast,281,282,0
-block_hint,RegExpMatchFast,115,116,0
-block_hint,RegExpMatchFast,347,348,0
-block_hint,RegExpMatchFast,256,257,0
-block_hint,RegExpMatchFast,94,95,1
-block_hint,RegExpMatchFast,387,388,1
-block_hint,RegExpMatchFast,309,310,0
-block_hint,RegExpMatchFast,181,182,1
-block_hint,RegExpMatchFast,179,180,1
-block_hint,RegExpMatchFast,311,312,0
-block_hint,RegExpMatchFast,183,184,0
-block_hint,RegExpMatchFast,102,103,0
+block_hint,RegExpMatchFast,340,341,0
+block_hint,RegExpMatchFast,248,249,0
+block_hint,RegExpMatchFast,275,276,0
+block_hint,RegExpMatchFast,190,191,1
+block_hint,RegExpMatchFast,450,451,0
+block_hint,RegExpMatchFast,436,437,1
+block_hint,RegExpMatchFast,390,391,1
+block_hint,RegExpMatchFast,303,304,1
+block_hint,RegExpMatchFast,277,278,0
+block_hint,RegExpMatchFast,117,118,0
+block_hint,RegExpMatchFast,342,343,0
+block_hint,RegExpMatchFast,250,251,0
+block_hint,RegExpMatchFast,92,93,1
+block_hint,RegExpMatchFast,362,363,1
+block_hint,RegExpMatchFast,252,253,0
+block_hint,RegExpMatchFast,102,103,1
+block_hint,RegExpMatchFast,306,307,0
+block_hint,RegExpMatchFast,177,178,0
 block_hint,RegExpMatchFast,104,105,0
-block_hint,RegExpMatchFast,204,205,1
-block_hint,RegExpMatchFast,322,323,0
-block_hint,RegExpMatchFast,106,107,1
-block_hint,RegExpMatchFast,193,194,1
-block_hint,RegExpMatchFast,352,353,0
+block_hint,RegExpMatchFast,106,107,0
+block_hint,RegExpMatchFast,198,199,1
+block_hint,RegExpMatchFast,317,318,0
+block_hint,RegExpMatchFast,108,109,1
+block_hint,RegExpMatchFast,187,188,1
+block_hint,RegExpMatchFast,346,347,0
+block_hint,RegExpMatchFast,94,95,1
 block_hint,RegExpMatchFast,96,97,1
-block_hint,RegExpMatchFast,175,176,1
-block_hint,RegExpMatchFast,173,174,1
-block_hint,RegExpMatchFast,177,178,0
+block_hint,RegExpMatchFast,175,176,0
 block_hint,RegExpMatchFast,98,99,0
 block_hint,RegExpMatchFast,100,101,0
-block_hint,RegExpMatchFast,224,225,1
-block_hint,RegExpMatchFast,314,315,0
-block_hint,RegExpMatchFast,226,227,0
+block_hint,RegExpMatchFast,218,219,1
+block_hint,RegExpMatchFast,309,310,0
+block_hint,RegExpMatchFast,220,221,0
 block_hint,RegExpReplace,261,262,1
 block_hint,RegExpReplace,299,300,1
 block_hint,RegExpReplace,251,252,1
@@ -4754,34 +4749,39 @@
 block_hint,CreateTypedArray,391,392,0
 block_hint,CreateTypedArray,60,61,1
 block_hint,CreateTypedArray,62,63,1
-block_hint,TypedArrayFrom,156,157,1
-block_hint,TypedArrayFrom,140,141,0
-block_hint,TypedArrayFrom,124,125,1
-block_hint,TypedArrayFrom,95,96,1
-block_hint,TypedArrayFrom,56,57,1
-block_hint,TypedArrayFrom,58,59,1
-block_hint,TypedArrayFrom,117,118,1
-block_hint,TypedArrayFrom,109,110,0
-block_hint,TypedArrayFrom,88,89,0
-block_hint,TypedArrayFrom,69,70,1
-block_hint,TypedArrayFrom,71,72,1
-block_hint,TypedArrayFrom,163,164,1
-block_hint,TypedArrayFrom,165,166,0
-block_hint,TypedArrayFrom,167,168,0
-block_hint,TypedArrayFrom,180,181,1
-block_hint,TypedArrayFrom,173,174,0
-block_hint,TypedArrayFrom,175,176,1
-block_hint,TypedArrayFrom,161,162,1
-block_hint,TypedArrayFrom,152,153,0
-block_hint,TypedArrayFrom,143,144,1
-block_hint,TypedArrayFrom,111,112,0
-block_hint,TypedArrayFrom,73,74,1
-block_hint,TypedArrayFrom,75,76,1
-block_hint,TypedArrayFrom,24,25,0
-block_hint,TypedArrayFrom,91,92,0
-block_hint,TypedArrayFrom,26,27,0
-block_hint,TypedArrayFrom,82,83,1
-block_hint,TypedArrayFrom,28,29,0
+block_hint,TypedArrayFrom,234,235,1
+block_hint,TypedArrayFrom,214,215,0
+block_hint,TypedArrayFrom,195,196,1
+block_hint,TypedArrayFrom,154,155,1
+block_hint,TypedArrayFrom,87,88,1
+block_hint,TypedArrayFrom,89,90,1
+block_hint,TypedArrayFrom,184,185,1
+block_hint,TypedArrayFrom,176,177,0
+block_hint,TypedArrayFrom,139,140,0
+block_hint,TypedArrayFrom,100,101,1
+block_hint,TypedArrayFrom,102,103,1
+block_hint,TypedArrayFrom,248,249,1
+block_hint,TypedArrayFrom,250,251,0
+block_hint,TypedArrayFrom,236,237,0
+block_hint,TypedArrayFrom,223,224,1
+block_hint,TypedArrayFrom,225,226,0
+block_hint,TypedArrayFrom,204,205,1
+block_hint,TypedArrayFrom,186,187,1
+block_hint,TypedArrayFrom,164,165,0
+block_hint,TypedArrayFrom,166,167,0
+block_hint,TypedArrayFrom,244,245,0
+block_hint,TypedArrayFrom,217,218,1
+block_hint,TypedArrayFrom,178,179,0
+block_hint,TypedArrayFrom,106,107,1
+block_hint,TypedArrayFrom,108,109,1
+block_hint,TypedArrayFrom,171,172,0
+block_hint,TypedArrayFrom,144,145,0
+block_hint,TypedArrayFrom,118,119,0
+block_hint,TypedArrayFrom,55,56,0
+block_hint,TypedArrayFrom,150,151,0
+block_hint,TypedArrayFrom,57,58,0
+block_hint,TypedArrayFrom,133,134,1
+block_hint,TypedArrayFrom,59,60,0
 block_hint,TypedArrayPrototypeSet,189,190,1
 block_hint,TypedArrayPrototypeSet,104,105,1
 block_hint,TypedArrayPrototypeSet,106,107,1
@@ -4991,11 +4991,6 @@
 block_hint,MergeAt,85,86,1
 block_hint,MergeAt,87,88,1
 block_hint,MergeAt,89,90,1
-block_hint,MergeAt,147,148,0
-block_hint,MergeAt,91,92,1
-block_hint,MergeAt,93,94,1
-block_hint,MergeAt,95,96,1
-block_hint,MergeAt,107,108,1
 block_hint,MergeAt,194,195,1
 block_hint,MergeAt,97,98,1
 block_hint,MergeAt,99,100,1
@@ -5059,92 +5054,92 @@
 block_hint,GallopRight,39,40,0
 block_hint,GallopRight,17,18,1
 block_hint,GallopRight,61,62,0
-block_hint,ArrayTimSort,123,124,0
-block_hint,ArrayTimSort,243,244,0
+block_hint,ArrayTimSort,120,121,0
+block_hint,ArrayTimSort,240,241,0
+block_hint,ArrayTimSort,227,228,0
+block_hint,ArrayTimSort,122,123,0
+block_hint,ArrayTimSort,163,164,0
+block_hint,ArrayTimSort,140,141,0
+block_hint,ArrayTimSort,33,34,1
+block_hint,ArrayTimSort,93,94,0
+block_hint,ArrayTimSort,95,96,0
+block_hint,ArrayTimSort,143,144,0
+block_hint,ArrayTimSort,35,36,1
+block_hint,ArrayTimSort,37,38,1
+block_hint,ArrayTimSort,214,215,0
+block_hint,ArrayTimSort,145,146,1
+block_hint,ArrayTimSort,39,40,1
+block_hint,ArrayTimSort,218,219,0
+block_hint,ArrayTimSort,216,217,0
+block_hint,ArrayTimSort,41,42,1
+block_hint,ArrayTimSort,43,44,1
+block_hint,ArrayTimSort,45,46,1
+block_hint,ArrayTimSort,134,135,0
+block_hint,ArrayTimSort,47,48,1
+block_hint,ArrayTimSort,49,50,1
+block_hint,ArrayTimSort,222,223,0
+block_hint,ArrayTimSort,51,52,1
+block_hint,ArrayTimSort,53,54,1
+block_hint,ArrayTimSort,55,56,1
+block_hint,ArrayTimSort,57,58,1
+block_hint,ArrayTimSort,59,60,1
+block_hint,ArrayTimSort,61,62,1
+block_hint,ArrayTimSort,63,64,1
+block_hint,ArrayTimSort,65,66,1
+block_hint,ArrayTimSort,67,68,1
+block_hint,ArrayTimSort,69,70,1
+block_hint,ArrayTimSort,71,72,1
+block_hint,ArrayTimSort,157,158,1
+block_hint,ArrayTimSort,73,74,1
+block_hint,ArrayTimSort,75,76,1
+block_hint,ArrayTimSort,204,205,0
+block_hint,ArrayTimSort,77,78,1
+block_hint,ArrayTimSort,79,80,1
+block_hint,ArrayTimSort,209,210,0
+block_hint,ArrayTimSort,81,82,1
+block_hint,ArrayTimSort,83,84,1
+block_hint,ArrayTimSort,186,187,0
+block_hint,ArrayTimSort,236,237,1
+block_hint,ArrayTimSort,238,239,1
+block_hint,ArrayTimSort,211,212,1
+block_hint,ArrayTimSort,161,162,1
+block_hint,ArrayTimSort,85,86,1
+block_hint,ArrayTimSort,243,244,1
 block_hint,ArrayTimSort,230,231,0
-block_hint,ArrayTimSort,125,126,0
-block_hint,ArrayTimSort,164,165,0
-block_hint,ArrayTimSort,191,192,0
-block_hint,ArrayTimSort,32,33,1
-block_hint,ArrayTimSort,92,93,0
-block_hint,ArrayTimSort,94,95,0
-block_hint,ArrayTimSort,144,145,0
-block_hint,ArrayTimSort,34,35,1
-block_hint,ArrayTimSort,36,37,1
-block_hint,ArrayTimSort,217,218,0
-block_hint,ArrayTimSort,146,147,1
-block_hint,ArrayTimSort,38,39,1
-block_hint,ArrayTimSort,221,222,0
-block_hint,ArrayTimSort,219,220,0
-block_hint,ArrayTimSort,40,41,1
-block_hint,ArrayTimSort,42,43,1
-block_hint,ArrayTimSort,44,45,1
-block_hint,ArrayTimSort,137,138,0
-block_hint,ArrayTimSort,46,47,1
-block_hint,ArrayTimSort,48,49,1
-block_hint,ArrayTimSort,225,226,0
-block_hint,ArrayTimSort,50,51,1
-block_hint,ArrayTimSort,52,53,1
-block_hint,ArrayTimSort,54,55,1
-block_hint,ArrayTimSort,56,57,1
-block_hint,ArrayTimSort,58,59,1
-block_hint,ArrayTimSort,60,61,1
-block_hint,ArrayTimSort,62,63,1
-block_hint,ArrayTimSort,64,65,1
-block_hint,ArrayTimSort,66,67,1
-block_hint,ArrayTimSort,68,69,1
-block_hint,ArrayTimSort,70,71,1
-block_hint,ArrayTimSort,158,159,1
-block_hint,ArrayTimSort,72,73,1
-block_hint,ArrayTimSort,74,75,1
-block_hint,ArrayTimSort,207,208,0
-block_hint,ArrayTimSort,76,77,1
-block_hint,ArrayTimSort,78,79,1
-block_hint,ArrayTimSort,212,213,0
-block_hint,ArrayTimSort,80,81,1
-block_hint,ArrayTimSort,82,83,1
-block_hint,ArrayTimSort,187,188,0
-block_hint,ArrayTimSort,239,240,1
-block_hint,ArrayTimSort,241,242,1
-block_hint,ArrayTimSort,214,215,1
-block_hint,ArrayTimSort,162,163,1
-block_hint,ArrayTimSort,84,85,1
-block_hint,ArrayTimSort,246,247,1
-block_hint,ArrayTimSort,233,234,0
-block_hint,ArrayTimSort,189,190,1
-block_hint,ArrayTimSort,141,142,0
-block_hint,ArrayTimSort,86,87,1
-block_hint,ArrayTimSort,112,113,0
-block_hint,ArrayTimSort,88,89,0
-block_hint,ArrayPrototypeSort,109,110,1
-block_hint,ArrayPrototypeSort,83,84,0
-block_hint,ArrayPrototypeSort,42,43,1
-block_hint,ArrayPrototypeSort,73,74,0
-block_hint,ArrayPrototypeSort,44,45,1
-block_hint,ArrayPrototypeSort,85,86,1
-block_hint,ArrayPrototypeSort,87,88,1
-block_hint,ArrayPrototypeSort,66,67,0
-block_hint,ArrayPrototypeSort,26,27,0
-block_hint,ArrayPrototypeSort,123,124,0
-block_hint,ArrayPrototypeSort,104,105,1
-block_hint,ArrayPrototypeSort,76,77,1
-block_hint,ArrayPrototypeSort,54,55,1
-block_hint,ArrayPrototypeSort,16,17,1
-block_hint,ArrayPrototypeSort,98,99,1
-block_hint,ArrayPrototypeSort,78,79,0
-block_hint,ArrayPrototypeSort,56,57,0
-block_hint,ArrayPrototypeSort,139,140,0
-block_hint,ArrayPrototypeSort,142,143,0
-block_hint,ArrayPrototypeSort,133,134,0
-block_hint,ArrayPrototypeSort,125,126,0
-block_hint,ArrayPrototypeSort,106,107,0
-block_hint,ArrayPrototypeSort,117,118,0
-block_hint,ArrayPrototypeSort,120,121,1
-block_hint,ArrayPrototypeSort,80,81,1
-block_hint,ArrayPrototypeSort,36,37,0
+block_hint,ArrayTimSort,188,189,1
+block_hint,ArrayTimSort,138,139,0
+block_hint,ArrayTimSort,87,88,1
+block_hint,ArrayTimSort,113,114,0
+block_hint,ArrayTimSort,89,90,0
+block_hint,ArrayPrototypeSort,106,107,1
+block_hint,ArrayPrototypeSort,80,81,0
+block_hint,ArrayPrototypeSort,39,40,1
+block_hint,ArrayPrototypeSort,70,71,0
+block_hint,ArrayPrototypeSort,41,42,1
+block_hint,ArrayPrototypeSort,82,83,1
+block_hint,ArrayPrototypeSort,84,85,1
+block_hint,ArrayPrototypeSort,63,64,0
+block_hint,ArrayPrototypeSort,27,28,0
+block_hint,ArrayPrototypeSort,120,121,0
 block_hint,ArrayPrototypeSort,101,102,1
-block_hint,ArrayPrototypeSort,94,95,1
-block_hint,ArrayPrototypeSort,59,60,1
+block_hint,ArrayPrototypeSort,73,74,1
+block_hint,ArrayPrototypeSort,51,52,1
+block_hint,ArrayPrototypeSort,15,16,1
+block_hint,ArrayPrototypeSort,95,96,1
+block_hint,ArrayPrototypeSort,75,76,0
+block_hint,ArrayPrototypeSort,53,54,0
+block_hint,ArrayPrototypeSort,136,137,0
+block_hint,ArrayPrototypeSort,139,140,0
+block_hint,ArrayPrototypeSort,130,131,0
+block_hint,ArrayPrototypeSort,122,123,0
+block_hint,ArrayPrototypeSort,103,104,0
+block_hint,ArrayPrototypeSort,114,115,0
+block_hint,ArrayPrototypeSort,117,118,1
+block_hint,ArrayPrototypeSort,77,78,1
+block_hint,ArrayPrototypeSort,33,34,0
+block_hint,ArrayPrototypeSort,98,99,1
+block_hint,ArrayPrototypeSort,91,92,1
+block_hint,ArrayPrototypeSort,56,57,1
 block_hint,StringFastLocaleCompare,315,316,1
 block_hint,StringFastLocaleCompare,239,240,0
 block_hint,StringFastLocaleCompare,303,304,1
@@ -5254,20 +5249,20 @@
 block_hint,MulHandler,91,92,1
 block_hint,MulHandler,59,60,1
 block_hint,MulHandler,23,24,1
-block_hint,DivHandler,85,86,0
-block_hint,DivHandler,66,67,0
-block_hint,DivHandler,43,44,1
+block_hint,DivHandler,109,110,0
+block_hint,DivHandler,90,91,0
+block_hint,DivHandler,63,64,1
+block_hint,DivHandler,33,34,1
+block_hint,DivHandler,121,122,1
+block_hint,DivHandler,96,97,1
+block_hint,DivHandler,66,67,1
 block_hint,DivHandler,23,24,1
-block_hint,DivHandler,95,96,1
-block_hint,DivHandler,72,73,1
-block_hint,DivHandler,46,47,1
-block_hint,DivHandler,17,18,1
-block_hint,ModHandler,89,90,0
-block_hint,ModHandler,85,86,0
-block_hint,ModHandler,68,69,1
-block_hint,ModHandler,63,64,1
-block_hint,ModHandler,29,30,0
-block_hint,ModHandler,15,16,1
+block_hint,ModHandler,129,130,0
+block_hint,ModHandler,118,119,0
+block_hint,ModHandler,92,93,1
+block_hint,ModHandler,87,88,1
+block_hint,ModHandler,50,51,0
+block_hint,ModHandler,33,34,1
 block_hint,BitwiseOrHandler,42,43,0
 block_hint,BitwiseOrHandler,30,31,1
 block_hint,BitwiseOrHandler,8,9,1
@@ -5341,11 +5336,16 @@
 block_hint,DecHandler,18,19,1
 block_hint,NegateHandler,26,27,1
 block_hint,NegateHandler,24,25,1
-block_hint,NegateHandler,14,15,1
 block_hint,ToBooleanLogicalNotHandler,15,16,0
 block_hint,ToBooleanLogicalNotHandler,21,22,0
 block_hint,ToBooleanLogicalNotHandler,7,8,0
 block_hint,TypeOfHandler,20,21,0
+block_hint,FindNonDefaultConstructorOrConstructHandler,12,13,0
+block_hint,FindNonDefaultConstructorOrConstructHandler,6,7,0
+block_hint,FindNonDefaultConstructorOrConstructHandler,14,15,1
+block_hint,FindNonDefaultConstructorOrConstructHandler,16,17,0
+block_hint,FindNonDefaultConstructorOrConstructHandler,4,5,1
+block_hint,FindNonDefaultConstructorOrConstructHandler,18,19,1
 block_hint,CallAnyReceiverHandler,21,22,1
 block_hint,CallProperty0Handler,7,8,1
 block_hint,CallProperty0Handler,62,63,0
@@ -5695,720 +5695,721 @@
 block_hint,BitwiseAndSmiExtraWideHandler,18,19,1
 block_hint,CallUndefinedReceiver1ExtraWideHandler,68,69,0
 block_hint,CallUndefinedReceiver1ExtraWideHandler,19,20,0
-builtin_hash,RecordWriteSaveFP,-613048523
-builtin_hash,RecordWriteIgnoreFP,-613048523
-builtin_hash,EphemeronKeyBarrierSaveFP,-874028499
-builtin_hash,AdaptorWithBuiltinExitFrame,-50443338
-builtin_hash,Call_ReceiverIsNullOrUndefined_Baseline_Compact,277963652
-builtin_hash,Call_ReceiverIsNotNullOrUndefined_Baseline_Compact,739975018
-builtin_hash,Call_ReceiverIsAny_Baseline_Compact,739975018
-builtin_hash,CallProxy,265720458
-builtin_hash,CallWithSpread,690518666
-builtin_hash,CallWithSpread_Baseline,14944167
-builtin_hash,CallWithArrayLike,-463192950
-builtin_hash,ConstructWithSpread,1026222363
-builtin_hash,ConstructWithSpread_Baseline,-954447059
-builtin_hash,Construct_Baseline,242132798
-builtin_hash,FastNewObject,812115752
-builtin_hash,FastNewClosure,-22842529
-builtin_hash,StringEqual,552928703
-builtin_hash,StringGreaterThan,814990350
-builtin_hash,StringGreaterThanOrEqual,-931415038
-builtin_hash,StringLessThan,-931415038
-builtin_hash,StringLessThanOrEqual,814990350
-builtin_hash,StringSubstring,679034293
-builtin_hash,OrderedHashTableHealIndex,-480837431
-builtin_hash,CompileLazy,-913572652
-builtin_hash,CompileLazyDeoptimizedCode,797435305
-builtin_hash,InstantiateAsmJs,-824208537
-builtin_hash,AllocateInYoungGeneration,-589367571
-builtin_hash,AllocateRegularInYoungGeneration,549206964
-builtin_hash,AllocateRegularInOldGeneration,549206964
-builtin_hash,CopyFastSmiOrObjectElements,-664166620
-builtin_hash,GrowFastDoubleElements,-794207344
-builtin_hash,GrowFastSmiOrObjectElements,-727031326
-builtin_hash,ToNumber,87194511
-builtin_hash,ToNumber_Baseline,-245107362
-builtin_hash,ToNumeric_Baseline,765738096
-builtin_hash,ToNumberConvertBigInt,-809735249
-builtin_hash,Typeof,554300746
-builtin_hash,KeyedLoadIC_PolymorphicName,808866465
-builtin_hash,KeyedStoreIC_Megamorphic,355428822
-builtin_hash,DefineKeyedOwnIC_Megamorphic,-254774567
-builtin_hash,LoadGlobalIC_NoFeedback,567497889
-builtin_hash,LoadIC_FunctionPrototype,440547932
-builtin_hash,LoadIC_StringLength,631981109
-builtin_hash,LoadIC_StringWrapperLength,957410129
-builtin_hash,LoadIC_NoFeedback,-673925088
-builtin_hash,StoreIC_NoFeedback,599149807
-builtin_hash,DefineNamedOwnIC_NoFeedback,-684443605
-builtin_hash,KeyedLoadIC_SloppyArguments,732273933
-builtin_hash,StoreFastElementIC_Standard,301200009
-builtin_hash,StoreFastElementIC_GrowNoTransitionHandleCOW,-894353505
-builtin_hash,StoreFastElementIC_NoTransitionHandleCOW,-684092303
-builtin_hash,ElementsTransitionAndStore_Standard,-313637466
-builtin_hash,ElementsTransitionAndStore_GrowNoTransitionHandleCOW,887654385
-builtin_hash,ElementsTransitionAndStore_NoTransitionHandleCOW,-730942180
-builtin_hash,KeyedHasIC_PolymorphicName,-900991969
-builtin_hash,EnqueueMicrotask,-201594324
-builtin_hash,RunMicrotasks,226014440
-builtin_hash,HasProperty,-179991880
-builtin_hash,DeleteProperty,-417791504
-builtin_hash,SetDataProperties,-676389083
-builtin_hash,ReturnReceiver,-253986889
-builtin_hash,ArrayConstructor,-132723945
-builtin_hash,ArrayConstructorImpl,-940010648
-builtin_hash,ArrayNoArgumentConstructor_PackedSmi_DontOverride,-419508170
-builtin_hash,ArrayNoArgumentConstructor_HoleySmi_DontOverride,-419508170
-builtin_hash,ArrayNoArgumentConstructor_PackedSmi_DisableAllocationSites,605372040
-builtin_hash,ArrayNoArgumentConstructor_Packed_DisableAllocationSites,605372040
-builtin_hash,ArrayNoArgumentConstructor_Holey_DisableAllocationSites,605372040
-builtin_hash,ArrayNoArgumentConstructor_PackedDouble_DisableAllocationSites,-118459699
-builtin_hash,ArraySingleArgumentConstructor_HoleySmi_DontOverride,-533026482
-builtin_hash,ArraySingleArgumentConstructor_HoleySmi_DisableAllocationSites,276667194
-builtin_hash,ArraySingleArgumentConstructor_Holey_DisableAllocationSites,276667194
-builtin_hash,ArraySingleArgumentConstructor_HoleyDouble_DisableAllocationSites,-533601049
-builtin_hash,ArrayIncludesSmi,-836179006
-builtin_hash,ArrayIncludesSmiOrObject,162670788
-builtin_hash,ArrayIncludes,508610041
-builtin_hash,ArrayIndexOfSmi,-144958716
-builtin_hash,ArrayIndexOfSmiOrObject,-560665373
-builtin_hash,ArrayIndexOf,659016893
-builtin_hash,ArrayPrototypePop,-672159034
-builtin_hash,ArrayPrototypePush,-828516926
-builtin_hash,CloneFastJSArray,330962956
-builtin_hash,CloneFastJSArrayFillingHoles,-114093580
-builtin_hash,ExtractFastJSArray,-899029625
-builtin_hash,ArrayPrototypeEntries,-846534049
-builtin_hash,ArrayPrototypeKeys,-432117890
-builtin_hash,ArrayPrototypeValues,-846534049
-builtin_hash,ArrayIteratorPrototypeNext,167355436
-builtin_hash,AsyncFunctionEnter,-860415031
-builtin_hash,AsyncFunctionResolve,910332485
-builtin_hash,AsyncFunctionAwaitCaught,-619125883
-builtin_hash,AsyncFunctionAwaitUncaught,-619125883
-builtin_hash,AsyncFunctionAwaitResolveClosure,-441313583
-builtin_hash,DatePrototypeGetDate,596885245
-builtin_hash,DatePrototypeGetDay,596885245
-builtin_hash,DatePrototypeGetFullYear,596885245
-builtin_hash,DatePrototypeGetHours,596885245
-builtin_hash,DatePrototypeGetMilliseconds,-147735130
-builtin_hash,DatePrototypeGetMinutes,596885245
-builtin_hash,DatePrototypeGetMonth,596885245
-builtin_hash,DatePrototypeGetSeconds,596885245
-builtin_hash,DatePrototypeGetTime,842589209
-builtin_hash,DatePrototypeGetTimezoneOffset,-147735130
-builtin_hash,DatePrototypeValueOf,842589209
-builtin_hash,DatePrototypeToPrimitive,-469261030
-builtin_hash,CreateIterResultObject,-236239497
-builtin_hash,CreateGeneratorObject,-989601020
-builtin_hash,GeneratorPrototypeNext,-532167070
-builtin_hash,GeneratorPrototypeReturn,204056688
-builtin_hash,SuspendGeneratorBaseline,-716242694
-builtin_hash,ResumeGeneratorBaseline,600643426
-builtin_hash,GlobalIsFinite,-28742852
-builtin_hash,GlobalIsNaN,-414427038
-builtin_hash,LoadIC,-1028921753
-builtin_hash,LoadIC_Megamorphic,604208967
-builtin_hash,LoadIC_Noninlined,-411987614
-builtin_hash,LoadICTrampoline,800274028
-builtin_hash,LoadICBaseline,470944725
-builtin_hash,LoadICTrampoline_Megamorphic,800274028
-builtin_hash,LoadSuperIC,-145652312
-builtin_hash,LoadSuperICBaseline,-463763660
-builtin_hash,KeyedLoadIC,-400473566
-builtin_hash,KeyedLoadIC_Megamorphic,41817838
-builtin_hash,KeyedLoadICTrampoline,800274028
-builtin_hash,KeyedLoadICBaseline,470944725
-builtin_hash,KeyedLoadICTrampoline_Megamorphic,800274028
-builtin_hash,StoreGlobalIC,-985598929
-builtin_hash,StoreGlobalICTrampoline,800274028
-builtin_hash,StoreGlobalICBaseline,470944725
-builtin_hash,StoreIC,107868822
-builtin_hash,StoreICTrampoline,515324941
-builtin_hash,StoreICBaseline,-463763660
-builtin_hash,DefineNamedOwnIC,293425336
-builtin_hash,DefineNamedOwnICBaseline,-463763660
-builtin_hash,KeyedStoreIC,-634858106
-builtin_hash,KeyedStoreICTrampoline,515324941
-builtin_hash,KeyedStoreICBaseline,-463763660
-builtin_hash,DefineKeyedOwnIC,-567510982
-builtin_hash,StoreInArrayLiteralIC,336733574
-builtin_hash,StoreInArrayLiteralICBaseline,-463763660
-builtin_hash,LoadGlobalIC,-994002095
-builtin_hash,LoadGlobalICInsideTypeof,131610143
-builtin_hash,LoadGlobalICTrampoline,-356577892
-builtin_hash,LoadGlobalICBaseline,-87390287
-builtin_hash,LoadGlobalICInsideTypeofTrampoline,-356577892
-builtin_hash,LoadGlobalICInsideTypeofBaseline,-87390287
-builtin_hash,LookupGlobalICBaseline,195819709
-builtin_hash,LookupGlobalICInsideTypeofBaseline,195819709
-builtin_hash,KeyedHasIC,-581893205
-builtin_hash,KeyedHasICBaseline,470944725
-builtin_hash,KeyedHasIC_Megamorphic,-179991880
-builtin_hash,IterableToList,-847583682
-builtin_hash,IterableToListWithSymbolLookup,639766325
-builtin_hash,IterableToListMayPreserveHoles,915672519
-builtin_hash,FindOrderedHashMapEntry,257985360
-builtin_hash,MapConstructor,173900465
-builtin_hash,MapPrototypeSet,-909373880
-builtin_hash,MapPrototypeDelete,-182536468
-builtin_hash,MapPrototypeGet,-10028336
-builtin_hash,MapPrototypeHas,-139761843
-builtin_hash,MapPrototypeEntries,-344495525
-builtin_hash,MapPrototypeGetSize,1002199563
-builtin_hash,MapPrototypeForEach,666422496
-builtin_hash,MapPrototypeKeys,-344495525
-builtin_hash,MapPrototypeValues,-344495525
-builtin_hash,MapIteratorPrototypeNext,824163271
-builtin_hash,MapIteratorToList,-171739571
-builtin_hash,SameValueNumbersOnly,-385008716
-builtin_hash,Add_Baseline,-279802821
-builtin_hash,AddSmi_Baseline,-180294218
-builtin_hash,Subtract_Baseline,422911741
-builtin_hash,SubtractSmi_Baseline,593938918
-builtin_hash,Multiply_Baseline,-390820476
-builtin_hash,MultiplySmi_Baseline,325873812
-builtin_hash,Divide_Baseline,-303206156
-builtin_hash,DivideSmi_Baseline,-760734875
-builtin_hash,Modulus_Baseline,-56419644
-builtin_hash,ModulusSmi_Baseline,-723448
-builtin_hash,Exponentiate_Baseline,-897267514
-builtin_hash,BitwiseAnd_Baseline,368212144
-builtin_hash,BitwiseAndSmi_Baseline,-1040430105
-builtin_hash,BitwiseOr_Baseline,-468458668
-builtin_hash,BitwiseOrSmi_Baseline,688726246
-builtin_hash,BitwiseXor_Baseline,-113074811
-builtin_hash,BitwiseXorSmi_Baseline,601401020
-builtin_hash,ShiftLeft_Baseline,-775732772
-builtin_hash,ShiftLeftSmi_Baseline,-78665210
-builtin_hash,ShiftRight_Baseline,748634885
-builtin_hash,ShiftRightSmi_Baseline,886941283
-builtin_hash,ShiftRightLogical_Baseline,561208446
-builtin_hash,ShiftRightLogicalSmi_Baseline,-31850172
-builtin_hash,Add_WithFeedback,-713508648
-builtin_hash,Subtract_WithFeedback,-1006518356
-builtin_hash,Modulus_WithFeedback,673708690
-builtin_hash,BitwiseOr_WithFeedback,-71811840
-builtin_hash,Equal_Baseline,-449571287
-builtin_hash,StrictEqual_Baseline,-311709296
-builtin_hash,LessThan_Baseline,-1041710075
-builtin_hash,GreaterThan_Baseline,763769306
-builtin_hash,LessThanOrEqual_Baseline,-289600196
-builtin_hash,GreaterThanOrEqual_Baseline,-964000144
-builtin_hash,Equal_WithFeedback,-804822195
-builtin_hash,StrictEqual_WithFeedback,316409561
-builtin_hash,LessThan_WithFeedback,-1041748847
-builtin_hash,GreaterThan_WithFeedback,208079969
-builtin_hash,GreaterThanOrEqual_WithFeedback,50039232
-builtin_hash,BitwiseNot_Baseline,574212378
-builtin_hash,Decrement_Baseline,740961552
-builtin_hash,Increment_Baseline,-482954167
-builtin_hash,Negate_Baseline,257429052
-builtin_hash,ObjectAssign,415745977
-builtin_hash,ObjectCreate,152352347
-builtin_hash,ObjectEntries,-267361188
-builtin_hash,ObjectGetOwnPropertyDescriptor,-1005546404
-builtin_hash,ObjectGetOwnPropertyNames,-10249982
-builtin_hash,ObjectIs,947042700
-builtin_hash,ObjectKeys,276395735
-builtin_hash,ObjectPrototypeHasOwnProperty,-366540189
-builtin_hash,ObjectToString,-680252272
-builtin_hash,InstanceOf_WithFeedback,-814385450
-builtin_hash,InstanceOf_Baseline,-567095434
-builtin_hash,ForInEnumerate,329908035
-builtin_hash,ForInPrepare,731557174
-builtin_hash,ForInFilter,884185984
-builtin_hash,RegExpConstructor,-1029370119
-builtin_hash,RegExpExecAtom,181372809
-builtin_hash,RegExpExecInternal,317900879
-builtin_hash,FindOrderedHashSetEntry,482436035
-builtin_hash,SetConstructor,692235107
-builtin_hash,SetPrototypeHas,-139761843
-builtin_hash,SetPrototypeAdd,-596680080
-builtin_hash,SetPrototypeDelete,331633635
-builtin_hash,SetPrototypeEntries,-344495525
-builtin_hash,SetPrototypeGetSize,1002199563
-builtin_hash,SetPrototypeForEach,97244170
-builtin_hash,SetPrototypeValues,-344495525
-builtin_hash,SetIteratorPrototypeNext,-441725951
-builtin_hash,SetOrSetIteratorToList,623342942
-builtin_hash,StringFromCharCode,-123751380
-builtin_hash,StringPrototypeReplace,-921072145
-builtin_hash,StringPrototypeSplit,-419686814
-builtin_hash,TypedArrayConstructor,32466415
-builtin_hash,TypedArrayPrototypeByteLength,864895308
-builtin_hash,TypedArrayPrototypeLength,539604699
-builtin_hash,WeakMapConstructor,814764494
-builtin_hash,WeakMapLookupHashIndex,-464287185
-builtin_hash,WeakMapGet,925651553
-builtin_hash,WeakMapPrototypeHas,947465532
-builtin_hash,WeakMapPrototypeSet,-976760951
-builtin_hash,WeakSetConstructor,694246453
-builtin_hash,WeakSetPrototypeHas,947465532
-builtin_hash,WeakSetPrototypeAdd,-160318733
-builtin_hash,WeakCollectionSet,578996244
-builtin_hash,AsyncGeneratorResolve,-83028412
-builtin_hash,AsyncGeneratorYieldWithAwait,-366463177
-builtin_hash,AsyncGeneratorResumeNext,220127321
-builtin_hash,AsyncGeneratorPrototypeNext,1069549757
-builtin_hash,AsyncGeneratorAwaitUncaught,-628599896
-builtin_hash,AsyncGeneratorAwaitResolveClosure,1062097477
-builtin_hash,AsyncGeneratorYieldWithAwaitResolveClosure,793122606
-builtin_hash,StringAdd_CheckNone,113370168
-builtin_hash,SubString,895503589
-builtin_hash,GetProperty,1052862169
-builtin_hash,GetPropertyWithReceiver,1045827042
-builtin_hash,SetProperty,908643608
-builtin_hash,CreateDataProperty,-314133834
-builtin_hash,ArrayPrototypeConcat,-557766770
-builtin_hash,ArrayEvery,-740699383
-builtin_hash,ArrayFilterLoopLazyDeoptContinuation,-463893516
-builtin_hash,ArrayFilterLoopContinuation,-636224543
-builtin_hash,ArrayFilter,-1006837550
-builtin_hash,ArrayPrototypeFind,358067331
-builtin_hash,ArrayForEachLoopLazyDeoptContinuation,-227856192
-builtin_hash,ArrayForEachLoopContinuation,498815593
-builtin_hash,ArrayForEach,-465472618
-builtin_hash,ArrayFrom,559791774
-builtin_hash,ArrayIsArray,556045869
-builtin_hash,LoadJoinElement_FastSmiOrObjectElements_0,4464260
-builtin_hash,LoadJoinElement_FastDoubleElements_0,-669389930
-builtin_hash,JoinStackPush,932509525
-builtin_hash,JoinStackPop,97051696
-builtin_hash,ArrayPrototypeJoin,638420418
-builtin_hash,ArrayPrototypeToString,571363693
-builtin_hash,ArrayPrototypeLastIndexOf,-262998450
-builtin_hash,ArrayMapLoopLazyDeoptContinuation,992596139
-builtin_hash,ArrayMapLoopContinuation,852679435
-builtin_hash,ArrayMap,237015696
-builtin_hash,ArrayReduceLoopLazyDeoptContinuation,-1021360101
-builtin_hash,ArrayReduceLoopContinuation,736239909
-builtin_hash,ArrayReduce,550306639
-builtin_hash,ArrayPrototypeReverse,-848939503
-builtin_hash,ArrayPrototypeShift,510698980
-builtin_hash,ArrayPrototypeSlice,-226926113
-builtin_hash,ArraySome,616986483
-builtin_hash,ArrayPrototypeSplice,318122997
-builtin_hash,ArrayPrototypeUnshift,942293281
-builtin_hash,ArrayBufferPrototypeGetByteLength,8155127
-builtin_hash,ArrayBufferIsView,-92420774
-builtin_hash,ToInteger,-772950786
-builtin_hash,FastCreateDataProperty,683077437
-builtin_hash,BooleanConstructor,104847507
-builtin_hash,BooleanPrototypeToString,496844333
-builtin_hash,ToString,-492204321
-builtin_hash,StringPrototypeToString,232130928
-builtin_hash,StringPrototypeValueOf,232130928
-builtin_hash,StringPrototypeCharAt,-493882295
-builtin_hash,StringPrototypeCharCodeAt,-70476469
-builtin_hash,StringPrototypeCodePointAt,958343749
-builtin_hash,StringPrototypeConcat,122908250
-builtin_hash,StringConstructor,36941296
-builtin_hash,StringAddConvertLeft,895631940
-builtin_hash,StringAddConvertRight,620894196
-builtin_hash,StringCharAt,-771156702
-builtin_hash,FastNewClosureBaseline,-345301780
-builtin_hash,FastNewFunctionContextFunction,393493853
-builtin_hash,CreateRegExpLiteral,1052274841
-builtin_hash,CreateShallowArrayLiteral,758569216
-builtin_hash,CreateEmptyArrayLiteral,-244361805
-builtin_hash,CreateShallowObjectLiteral,429596211
-builtin_hash,ObjectConstructor,792071103
-builtin_hash,CreateEmptyLiteralObject,792021411
-builtin_hash,NumberConstructor,-545912408
-builtin_hash,StringToNumber,-567475001
-builtin_hash,NonNumberToNumber,-75339598
-builtin_hash,NonNumberToNumeric,-163611573
-builtin_hash,ToNumeric,1067114169
-builtin_hash,NumberToString,808056721
-builtin_hash,ToBoolean,474893826
-builtin_hash,ToBooleanForBaselineJump,-1000387172
-builtin_hash,ToLength,-1031135247
-builtin_hash,ToName,-893589751
-builtin_hash,ToObject,-995611522
-builtin_hash,NonPrimitiveToPrimitive_Default,-741936834
-builtin_hash,NonPrimitiveToPrimitive_Number,-741936834
-builtin_hash,NonPrimitiveToPrimitive_String,-741936834
-builtin_hash,OrdinaryToPrimitive_Number,940682530
-builtin_hash,OrdinaryToPrimitive_String,940682530
-builtin_hash,DataViewPrototypeGetByteLength,-344862281
-builtin_hash,DataViewPrototypeGetFloat64,-710736378
-builtin_hash,DataViewPrototypeSetUint32,561326289
-builtin_hash,DataViewPrototypeSetFloat64,224815643
-builtin_hash,FunctionPrototypeHasInstance,-159239165
-builtin_hash,FastFunctionPrototypeBind,-835190429
-builtin_hash,ForInNext,-628108871
-builtin_hash,GetIteratorWithFeedback,412632852
-builtin_hash,GetIteratorBaseline,878549031
-builtin_hash,CallIteratorWithFeedback,-173921836
-builtin_hash,MathAbs,-418374171
-builtin_hash,MathCeil,-83433093
-builtin_hash,MathFloor,963617939
-builtin_hash,MathRound,739741009
-builtin_hash,MathPow,510691647
-builtin_hash,MathMax,45115699
-builtin_hash,MathMin,-996382942
-builtin_hash,MathAsin,261451622
-builtin_hash,MathAtan2,605332815
-builtin_hash,MathCos,515079504
-builtin_hash,MathExp,551351922
-builtin_hash,MathFround,564706237
-builtin_hash,MathImul,685265173
-builtin_hash,MathLog,-553256829
-builtin_hash,MathSin,302395292
-builtin_hash,MathSign,611819739
-builtin_hash,MathSqrt,55107225
-builtin_hash,MathTan,-332405887
-builtin_hash,MathTanh,939045985
-builtin_hash,MathRandom,-504157126
-builtin_hash,NumberPrototypeToString,145247584
-builtin_hash,NumberIsInteger,632098621
-builtin_hash,NumberIsNaN,343619286
-builtin_hash,NumberParseFloat,-745268146
-builtin_hash,ParseInt,423449565
-builtin_hash,NumberParseInt,348325306
-builtin_hash,Add,-712082634
-builtin_hash,Subtract,860006498
-builtin_hash,Multiply,966938552
-builtin_hash,Divide,501339465
-builtin_hash,Modulus,556264773
-builtin_hash,CreateObjectWithoutProperties,911390056
-builtin_hash,ObjectIsExtensible,-376770424
-builtin_hash,ObjectPreventExtensions,-675757061
-builtin_hash,ObjectGetPrototypeOf,-694816240
-builtin_hash,ObjectSetPrototypeOf,-335823538
-builtin_hash,ObjectPrototypeToString,158685312
-builtin_hash,ObjectPrototypeValueOf,-993024104
-builtin_hash,FulfillPromise,-68874675
-builtin_hash,NewPromiseCapability,-880232666
-builtin_hash,PromiseCapabilityDefaultResolve,694927325
-builtin_hash,PerformPromiseThen,-238303189
-builtin_hash,PromiseAll,-121414633
-builtin_hash,PromiseAllResolveElementClosure,797273436
-builtin_hash,PromiseConstructor,-424149894
-builtin_hash,PromisePrototypeCatch,235262026
-builtin_hash,PromiseFulfillReactionJob,927825363
-builtin_hash,PromiseResolveTrampoline,-549629094
-builtin_hash,PromiseResolve,-366429795
-builtin_hash,ResolvePromise,526061379
-builtin_hash,PromisePrototypeThen,959282415
-builtin_hash,PromiseResolveThenableJob,-977786068
-builtin_hash,ProxyConstructor,-54504231
-builtin_hash,ProxyGetProperty,-692505715
-builtin_hash,ProxyIsExtensible,-120987472
-builtin_hash,ProxyPreventExtensions,739592105
-builtin_hash,ReflectGet,1006327680
-builtin_hash,ReflectHas,-549629094
-builtin_hash,RegExpPrototypeExec,866694176
-builtin_hash,RegExpMatchFast,556779044
-builtin_hash,RegExpReplace,1037671691
-builtin_hash,RegExpPrototypeReplace,-488505709
-builtin_hash,RegExpSearchFast,744647901
-builtin_hash,RegExpPrototypeSourceGetter,-69902772
-builtin_hash,RegExpSplit,418335022
-builtin_hash,RegExpPrototypeTest,-893509849
-builtin_hash,RegExpPrototypeTestFast,-541676085
-builtin_hash,RegExpPrototypeGlobalGetter,612394650
-builtin_hash,RegExpPrototypeIgnoreCaseGetter,-595775382
-builtin_hash,RegExpPrototypeMultilineGetter,368200363
-builtin_hash,RegExpPrototypeHasIndicesGetter,99570183
-builtin_hash,RegExpPrototypeDotAllGetter,99570183
-builtin_hash,RegExpPrototypeStickyGetter,471291660
-builtin_hash,RegExpPrototypeUnicodeGetter,471291660
-builtin_hash,RegExpPrototypeFlagsGetter,-493351549
-builtin_hash,StringPrototypeEndsWith,-140669855
-builtin_hash,StringPrototypeIncludes,-538712449
-builtin_hash,StringPrototypeIndexOf,-279080867
-builtin_hash,StringPrototypeIterator,-906814404
-builtin_hash,StringIteratorPrototypeNext,-459023719
-builtin_hash,StringPrototypeMatch,950777323
-builtin_hash,StringPrototypeSearch,950777323
-builtin_hash,StringRepeat,333496990
-builtin_hash,StringPrototypeSlice,147923310
-builtin_hash,StringPrototypeStartsWith,-916453690
-builtin_hash,StringPrototypeSubstr,93046303
-builtin_hash,StringPrototypeSubstring,-486167723
-builtin_hash,StringPrototypeTrim,-537839064
-builtin_hash,SymbolPrototypeToString,-331094885
-builtin_hash,CreateTypedArray,946007034
-builtin_hash,TypedArrayFrom,-383816322
-builtin_hash,TypedArrayPrototypeSet,183639399
-builtin_hash,TypedArrayPrototypeSubArray,-654743264
-builtin_hash,NewSloppyArgumentsElements,-733955106
-builtin_hash,NewStrictArgumentsElements,27861461
-builtin_hash,NewRestArgumentsElements,-158196826
-builtin_hash,FastNewSloppyArguments,701807193
-builtin_hash,FastNewStrictArguments,-400637158
-builtin_hash,FastNewRestArguments,771398605
-builtin_hash,StringSlowFlatten,758688335
-builtin_hash,StringIndexOf,893861646
-builtin_hash,Load_FastSmiElements_0,41377987
-builtin_hash,Load_FastObjectElements_0,41377987
-builtin_hash,Store_FastSmiElements_0,987491586
-builtin_hash,Store_FastObjectElements_0,-907039137
-builtin_hash,SortCompareDefault,-712046902
-builtin_hash,SortCompareUserFn,-498446944
-builtin_hash,Copy,1005972100
-builtin_hash,MergeAt,-238184884
-builtin_hash,GallopLeft,-228579918
-builtin_hash,GallopRight,508662767
-builtin_hash,ArrayTimSort,-584574007
-builtin_hash,ArrayPrototypeSort,-446345392
-builtin_hash,StringFastLocaleCompare,-805723901
-builtin_hash,WasmInt32ToHeapNumber,186218317
-builtin_hash,WasmTaggedNonSmiToInt32,644195797
-builtin_hash,WasmTriggerTierUp,-448026998
-builtin_hash,WasmStackGuard,929375954
-builtin_hash,CanUseSameAccessor_FastSmiElements_0,333215288
-builtin_hash,CanUseSameAccessor_FastObjectElements_0,333215288
-builtin_hash,StringPrototypeToLowerCaseIntl,325118773
-builtin_hash,StringToLowerCaseIntl,729618594
-builtin_hash,WideHandler,-985531040
-builtin_hash,ExtraWideHandler,-985531040
-builtin_hash,LdarHandler,1066069071
-builtin_hash,LdaZeroHandler,697098880
-builtin_hash,LdaSmiHandler,-92763154
-builtin_hash,LdaUndefinedHandler,94159659
-builtin_hash,LdaNullHandler,94159659
-builtin_hash,LdaTheHoleHandler,94159659
-builtin_hash,LdaTrueHandler,66190034
-builtin_hash,LdaFalseHandler,66190034
-builtin_hash,LdaConstantHandler,-234672240
-builtin_hash,LdaContextSlotHandler,999512170
-builtin_hash,LdaImmutableContextSlotHandler,999512170
-builtin_hash,LdaCurrentContextSlotHandler,-705029165
-builtin_hash,LdaImmutableCurrentContextSlotHandler,-705029165
-builtin_hash,StarHandler,-825981541
-builtin_hash,MovHandler,-222623368
-builtin_hash,PushContextHandler,239039195
-builtin_hash,PopContextHandler,663403390
-builtin_hash,TestReferenceEqualHandler,107959616
-builtin_hash,TestUndetectableHandler,768306054
-builtin_hash,TestNullHandler,317848228
-builtin_hash,TestUndefinedHandler,317848228
-builtin_hash,TestTypeOfHandler,-585531608
-builtin_hash,LdaGlobalHandler,680542536
-builtin_hash,LdaGlobalInsideTypeofHandler,-812384965
-builtin_hash,StaGlobalHandler,-849976646
-builtin_hash,StaContextSlotHandler,-642236485
-builtin_hash,StaCurrentContextSlotHandler,515612512
-builtin_hash,LdaLookupGlobalSlotHandler,328181263
-builtin_hash,LdaLookupGlobalSlotInsideTypeofHandler,-152487163
-builtin_hash,StaLookupSlotHandler,1043986971
-builtin_hash,GetNamedPropertyHandler,-918198086
-builtin_hash,GetNamedPropertyFromSuperHandler,-605958764
-builtin_hash,GetKeyedPropertyHandler,-368783501
-builtin_hash,SetNamedPropertyHandler,512867069
-builtin_hash,DefineNamedOwnPropertyHandler,512867069
-builtin_hash,SetKeyedPropertyHandler,-529790650
-builtin_hash,DefineKeyedOwnPropertyHandler,-529790650
-builtin_hash,StaInArrayLiteralHandler,-529790650
-builtin_hash,DefineKeyedOwnPropertyInLiteralHandler,519916231
-builtin_hash,AddHandler,-1014428769
-builtin_hash,SubHandler,-971645828
-builtin_hash,MulHandler,-1072793455
-builtin_hash,DivHandler,-162323805
-builtin_hash,ModHandler,-485795098
-builtin_hash,ExpHandler,795159955
-builtin_hash,BitwiseOrHandler,-974394049
-builtin_hash,BitwiseXorHandler,580834482
-builtin_hash,BitwiseAndHandler,614318128
-builtin_hash,ShiftLeftHandler,-423182377
-builtin_hash,ShiftRightHandler,-255081510
-builtin_hash,ShiftRightLogicalHandler,735938776
-builtin_hash,AddSmiHandler,107839307
-builtin_hash,SubSmiHandler,-363881533
-builtin_hash,MulSmiHandler,169761579
-builtin_hash,DivSmiHandler,-681265328
-builtin_hash,ModSmiHandler,861935655
-builtin_hash,BitwiseOrSmiHandler,-680303745
-builtin_hash,BitwiseXorSmiHandler,576458108
-builtin_hash,BitwiseAndSmiHandler,-994511503
-builtin_hash,ShiftLeftSmiHandler,-728693655
-builtin_hash,ShiftRightSmiHandler,975905832
-builtin_hash,ShiftRightLogicalSmiHandler,686146238
-builtin_hash,IncHandler,117772531
-builtin_hash,DecHandler,-691015839
-builtin_hash,NegateHandler,212889736
-builtin_hash,BitwiseNotHandler,-960473652
-builtin_hash,ToBooleanLogicalNotHandler,-997041363
-builtin_hash,LogicalNotHandler,-404436240
-builtin_hash,TypeOfHandler,-868029172
-builtin_hash,DeletePropertyStrictHandler,-310645655
-builtin_hash,DeletePropertySloppyHandler,-884621901
-builtin_hash,GetSuperConstructorHandler,-336144805
-builtin_hash,CallAnyReceiverHandler,-483788286
-builtin_hash,CallPropertyHandler,-483788286
-builtin_hash,CallProperty0Handler,234175094
-builtin_hash,CallProperty1Handler,354307341
-builtin_hash,CallProperty2Handler,968021051
-builtin_hash,CallUndefinedReceiverHandler,472718464
-builtin_hash,CallUndefinedReceiver0Handler,1020191467
-builtin_hash,CallUndefinedReceiver1Handler,785762305
-builtin_hash,CallUndefinedReceiver2Handler,-921863582
-builtin_hash,CallWithSpreadHandler,-483788286
-builtin_hash,CallRuntimeHandler,575543766
-builtin_hash,CallJSRuntimeHandler,-279970155
-builtin_hash,InvokeIntrinsicHandler,315814934
-builtin_hash,ConstructHandler,750653559
-builtin_hash,ConstructWithSpreadHandler,-950529667
-builtin_hash,TestEqualHandler,469957169
-builtin_hash,TestEqualStrictHandler,774972588
-builtin_hash,TestLessThanHandler,876731233
-builtin_hash,TestGreaterThanHandler,854370589
-builtin_hash,TestLessThanOrEqualHandler,-616820445
-builtin_hash,TestGreaterThanOrEqualHandler,128578007
-builtin_hash,TestInstanceOfHandler,437146777
-builtin_hash,TestInHandler,-595986293
-builtin_hash,ToNameHandler,-388837341
-builtin_hash,ToNumberHandler,172727215
-builtin_hash,ToNumericHandler,518340123
-builtin_hash,ToObjectHandler,-388837341
-builtin_hash,ToStringHandler,-736791596
-builtin_hash,CreateRegExpLiteralHandler,-387261303
-builtin_hash,CreateArrayLiteralHandler,544722821
-builtin_hash,CreateArrayFromIterableHandler,-590862374
-builtin_hash,CreateEmptyArrayLiteralHandler,-215104396
-builtin_hash,CreateObjectLiteralHandler,536615992
-builtin_hash,CreateEmptyObjectLiteralHandler,810635729
-builtin_hash,CreateClosureHandler,-899658211
-builtin_hash,CreateBlockContextHandler,-125556632
-builtin_hash,CreateCatchContextHandler,551209828
-builtin_hash,CreateFunctionContextHandler,-65684761
-builtin_hash,CreateMappedArgumentsHandler,67709625
-builtin_hash,CreateUnmappedArgumentsHandler,608258279
-builtin_hash,CreateRestParameterHandler,1042430952
-builtin_hash,JumpLoopHandler,77742379
-builtin_hash,JumpHandler,-420188660
-builtin_hash,JumpConstantHandler,-998825364
-builtin_hash,JumpIfUndefinedConstantHandler,-326209739
-builtin_hash,JumpIfNotUndefinedConstantHandler,37208057
-builtin_hash,JumpIfUndefinedOrNullConstantHandler,-104381115
-builtin_hash,JumpIfTrueConstantHandler,-326209739
-builtin_hash,JumpIfFalseConstantHandler,-326209739
-builtin_hash,JumpIfToBooleanTrueConstantHandler,-234142841
-builtin_hash,JumpIfToBooleanFalseConstantHandler,-602774868
-builtin_hash,JumpIfToBooleanTrueHandler,-297635325
-builtin_hash,JumpIfToBooleanFalseHandler,1015367976
-builtin_hash,JumpIfTrueHandler,862147447
-builtin_hash,JumpIfFalseHandler,862147447
-builtin_hash,JumpIfNullHandler,862147447
-builtin_hash,JumpIfNotNullHandler,-481058680
-builtin_hash,JumpIfUndefinedHandler,862147447
-builtin_hash,JumpIfNotUndefinedHandler,-481058680
-builtin_hash,JumpIfUndefinedOrNullHandler,14126870
-builtin_hash,JumpIfJSReceiverHandler,-723850389
-builtin_hash,SwitchOnSmiNoFeedbackHandler,-902670490
-builtin_hash,ForInEnumerateHandler,-322331924
-builtin_hash,ForInPrepareHandler,20034175
-builtin_hash,ForInContinueHandler,827732360
-builtin_hash,ForInNextHandler,119110335
-builtin_hash,ForInStepHandler,757646701
-builtin_hash,SetPendingMessageHandler,996401409
-builtin_hash,ThrowHandler,122680912
-builtin_hash,ReThrowHandler,122680912
-builtin_hash,ReturnHandler,47039723
-builtin_hash,ThrowReferenceErrorIfHoleHandler,-342650955
-builtin_hash,ThrowSuperNotCalledIfHoleHandler,-285583864
-builtin_hash,ThrowSuperAlreadyCalledIfNotHoleHandler,-827541184
-builtin_hash,ThrowIfNotSuperConstructorHandler,1018623070
-builtin_hash,SwitchOnGeneratorStateHandler,717471818
-builtin_hash,SuspendGeneratorHandler,547514791
-builtin_hash,ResumeGeneratorHandler,-860485588
-builtin_hash,GetIteratorHandler,-6630463
-builtin_hash,ShortStarHandler,721894508
-builtin_hash,LdarWideHandler,-978392409
-builtin_hash,LdaSmiWideHandler,-366656871
-builtin_hash,LdaConstantWideHandler,972813981
-builtin_hash,LdaContextSlotWideHandler,628329787
-builtin_hash,LdaImmutableContextSlotWideHandler,628329787
-builtin_hash,LdaImmutableCurrentContextSlotWideHandler,489858159
-builtin_hash,StarWideHandler,-1038662456
-builtin_hash,MovWideHandler,483803273
-builtin_hash,PushContextWideHandler,216419588
-builtin_hash,PopContextWideHandler,272986324
-builtin_hash,TestReferenceEqualWideHandler,-4739833
-builtin_hash,LdaGlobalWideHandler,-434470564
-builtin_hash,LdaGlobalInsideTypeofWideHandler,888730933
-builtin_hash,StaGlobalWideHandler,459118950
-builtin_hash,StaContextSlotWideHandler,888275701
-builtin_hash,StaCurrentContextSlotWideHandler,-317584552
-builtin_hash,LdaLookupGlobalSlotWideHandler,1026575020
-builtin_hash,GetNamedPropertyWideHandler,664403992
-builtin_hash,GetKeyedPropertyWideHandler,322108853
-builtin_hash,SetNamedPropertyWideHandler,784668777
-builtin_hash,DefineNamedOwnPropertyWideHandler,784668777
-builtin_hash,SetKeyedPropertyWideHandler,1015904043
-builtin_hash,DefineKeyedOwnPropertyWideHandler,1015904043
-builtin_hash,StaInArrayLiteralWideHandler,1015904043
-builtin_hash,AddWideHandler,1006647977
-builtin_hash,SubWideHandler,212325320
-builtin_hash,MulWideHandler,-922622067
-builtin_hash,DivWideHandler,145054418
-builtin_hash,BitwiseOrWideHandler,805505097
-builtin_hash,BitwiseAndWideHandler,563101073
-builtin_hash,ShiftLeftWideHandler,448918085
-builtin_hash,AddSmiWideHandler,-135072104
-builtin_hash,SubSmiWideHandler,-169078418
-builtin_hash,MulSmiWideHandler,793690226
-builtin_hash,DivSmiWideHandler,-657180043
-builtin_hash,ModSmiWideHandler,335754550
-builtin_hash,BitwiseOrSmiWideHandler,-1067934836
-builtin_hash,BitwiseXorSmiWideHandler,-668709153
-builtin_hash,BitwiseAndSmiWideHandler,-90084544
-builtin_hash,ShiftLeftSmiWideHandler,-381247703
-builtin_hash,ShiftRightSmiWideHandler,-38676513
-builtin_hash,ShiftRightLogicalSmiWideHandler,-1026231042
-builtin_hash,IncWideHandler,389395178
-builtin_hash,DecWideHandler,1062128797
-builtin_hash,NegateWideHandler,375542705
-builtin_hash,CallPropertyWideHandler,479651507
-builtin_hash,CallProperty0WideHandler,402451236
-builtin_hash,CallProperty1WideHandler,864866147
-builtin_hash,CallProperty2WideHandler,672960581
-builtin_hash,CallUndefinedReceiverWideHandler,633606056
-builtin_hash,CallUndefinedReceiver0WideHandler,-782323787
-builtin_hash,CallUndefinedReceiver1WideHandler,52355318
-builtin_hash,CallUndefinedReceiver2WideHandler,297430331
-builtin_hash,CallWithSpreadWideHandler,479651507
-builtin_hash,ConstructWideHandler,-923801363
-builtin_hash,TestEqualWideHandler,745861994
-builtin_hash,TestEqualStrictWideHandler,982796365
-builtin_hash,TestLessThanWideHandler,665221830
-builtin_hash,TestGreaterThanWideHandler,776130121
-builtin_hash,TestLessThanOrEqualWideHandler,-299580558
-builtin_hash,TestGreaterThanOrEqualWideHandler,-356242933
-builtin_hash,TestInstanceOfWideHandler,406240218
-builtin_hash,TestInWideHandler,-754759119
-builtin_hash,ToNumericWideHandler,1034444948
-builtin_hash,CreateRegExpLiteralWideHandler,1015965077
-builtin_hash,CreateArrayLiteralWideHandler,238187057
-builtin_hash,CreateEmptyArrayLiteralWideHandler,-21075025
-builtin_hash,CreateObjectLiteralWideHandler,570835533
-builtin_hash,CreateClosureWideHandler,912422636
-builtin_hash,CreateBlockContextWideHandler,499748521
-builtin_hash,CreateFunctionContextWideHandler,-887672919
-builtin_hash,JumpLoopWideHandler,714317010
-builtin_hash,JumpWideHandler,-420188660
-builtin_hash,JumpIfToBooleanTrueWideHandler,230302934
-builtin_hash,JumpIfToBooleanFalseWideHandler,237768975
-builtin_hash,JumpIfTrueWideHandler,814624851
-builtin_hash,JumpIfFalseWideHandler,814624851
-builtin_hash,SwitchOnSmiNoFeedbackWideHandler,623977068
-builtin_hash,ForInPrepareWideHandler,430965432
-builtin_hash,ForInNextWideHandler,-899950637
-builtin_hash,ThrowReferenceErrorIfHoleWideHandler,-575574526
-builtin_hash,GetIteratorWideHandler,-626454663
-builtin_hash,LdaSmiExtraWideHandler,465680004
-builtin_hash,LdaGlobalExtraWideHandler,1016564513
-builtin_hash,AddSmiExtraWideHandler,585533206
-builtin_hash,SubSmiExtraWideHandler,-88717151
-builtin_hash,MulSmiExtraWideHandler,-508453390
-builtin_hash,DivSmiExtraWideHandler,-542490757
-builtin_hash,BitwiseOrSmiExtraWideHandler,776661340
-builtin_hash,BitwiseXorSmiExtraWideHandler,276228867
-builtin_hash,BitwiseAndSmiExtraWideHandler,739058259
-builtin_hash,CallUndefinedReceiverExtraWideHandler,488508421
-builtin_hash,CallUndefinedReceiver1ExtraWideHandler,700320270
-builtin_hash,CallUndefinedReceiver2ExtraWideHandler,-7276189
+builtin_hash,RecordWriteSaveFP,-787985789
+builtin_hash,RecordWriteIgnoreFP,-787985789
+builtin_hash,EphemeronKeyBarrierSaveFP,-762846067
+builtin_hash,AdaptorWithBuiltinExitFrame,245562366
+builtin_hash,Call_ReceiverIsNullOrUndefined_Baseline_Compact,-701969451
+builtin_hash,Call_ReceiverIsNotNullOrUndefined_Baseline_Compact,-324308522
+builtin_hash,Call_ReceiverIsAny_Baseline_Compact,-324308522
+builtin_hash,CallProxy,1028339399
+builtin_hash,CallWithSpread,535056033
+builtin_hash,CallWithSpread_Baseline,-119914143
+builtin_hash,CallWithArrayLike,-122249728
+builtin_hash,ConstructWithSpread,246592083
+builtin_hash,ConstructWithSpread_Baseline,150379974
+builtin_hash,Construct_Baseline,62706048
+builtin_hash,FastNewObject,958443730
+builtin_hash,FastNewClosure,344670909
+builtin_hash,StringEqual,747283806
+builtin_hash,StringGreaterThan,-181364078
+builtin_hash,StringGreaterThanOrEqual,-462881432
+builtin_hash,StringLessThan,-462881432
+builtin_hash,StringLessThanOrEqual,-181364078
+builtin_hash,StringSubstring,-615814018
+builtin_hash,OrderedHashTableHealIndex,-1059061674
+builtin_hash,CompileLazy,-1040787392
+builtin_hash,CompileLazyDeoptimizedCode,254075260
+builtin_hash,InstantiateAsmJs,-162781474
+builtin_hash,AllocateInYoungGeneration,504130749
+builtin_hash,AllocateRegularInYoungGeneration,-967770913
+builtin_hash,AllocateRegularInOldGeneration,-967770913
+builtin_hash,CopyFastSmiOrObjectElements,-184201389
+builtin_hash,GrowFastDoubleElements,933478036
+builtin_hash,GrowFastSmiOrObjectElements,62812155
+builtin_hash,ToNumber,-536181652
+builtin_hash,ToNumber_Baseline,-361624131
+builtin_hash,ToNumeric_Baseline,-968362129
+builtin_hash,ToNumberConvertBigInt,-484303877
+builtin_hash,Typeof,-292943239
+builtin_hash,KeyedLoadIC_PolymorphicName,-445639640
+builtin_hash,KeyedStoreIC_Megamorphic,228109775
+builtin_hash,DefineKeyedOwnIC_Megamorphic,587942691
+builtin_hash,LoadGlobalIC_NoFeedback,-506168140
+builtin_hash,LoadIC_FunctionPrototype,-217294724
+builtin_hash,LoadIC_StringLength,876788958
+builtin_hash,LoadIC_StringWrapperLength,-105737329
+builtin_hash,LoadIC_NoFeedback,796730020
+builtin_hash,StoreIC_NoFeedback,-771215689
+builtin_hash,DefineNamedOwnIC_NoFeedback,610029223
+builtin_hash,KeyedLoadIC_SloppyArguments,1037341519
+builtin_hash,StoreFastElementIC_Standard,-1073045042
+builtin_hash,StoreFastElementIC_GrowNoTransitionHandleCOW,-733182579
+builtin_hash,StoreFastElementIC_NoTransitionHandleCOW,14002747
+builtin_hash,ElementsTransitionAndStore_Standard,-303995099
+builtin_hash,ElementsTransitionAndStore_GrowNoTransitionHandleCOW,-620039698
+builtin_hash,ElementsTransitionAndStore_NoTransitionHandleCOW,387221171
+builtin_hash,KeyedHasIC_PolymorphicName,481900135
+builtin_hash,EnqueueMicrotask,987190055
+builtin_hash,RunMicrotasks,-606800144
+builtin_hash,HasProperty,-958876308
+builtin_hash,DeleteProperty,-583543539
+builtin_hash,SetDataProperties,-633970258
+builtin_hash,ReturnReceiver,386533367
+builtin_hash,ArrayConstructor,-862505040
+builtin_hash,ArrayConstructorImpl,-772732436
+builtin_hash,ArrayNoArgumentConstructor_PackedSmi_DontOverride,546753803
+builtin_hash,ArrayNoArgumentConstructor_HoleySmi_DontOverride,546753803
+builtin_hash,ArrayNoArgumentConstructor_PackedSmi_DisableAllocationSites,76921937
+builtin_hash,ArrayNoArgumentConstructor_Packed_DisableAllocationSites,76921937
+builtin_hash,ArrayNoArgumentConstructor_Holey_DisableAllocationSites,76921937
+builtin_hash,ArrayNoArgumentConstructor_PackedDouble_DisableAllocationSites,-916490644
+builtin_hash,ArraySingleArgumentConstructor_HoleySmi_DontOverride,924187471
+builtin_hash,ArraySingleArgumentConstructor_HoleySmi_DisableAllocationSites,-90166804
+builtin_hash,ArraySingleArgumentConstructor_Holey_DisableAllocationSites,-90166804
+builtin_hash,ArraySingleArgumentConstructor_HoleyDouble_DisableAllocationSites,377718997
+builtin_hash,ArrayIncludesSmi,833613331
+builtin_hash,ArrayIncludesSmiOrObject,-439120197
+builtin_hash,ArrayIncludes,-557378221
+builtin_hash,ArrayIndexOfSmi,818318721
+builtin_hash,ArrayIndexOfSmiOrObject,1027851539
+builtin_hash,ArrayIndexOf,344845802
+builtin_hash,ArrayPrototypePop,127416215
+builtin_hash,ArrayPrototypePush,611743176
+builtin_hash,CloneFastJSArray,1060615555
+builtin_hash,CloneFastJSArrayFillingHoles,1003395618
+builtin_hash,ExtractFastJSArray,-517393151
+builtin_hash,ArrayPrototypeEntries,-332667431
+builtin_hash,ArrayPrototypeKeys,110264383
+builtin_hash,ArrayPrototypeValues,-332667431
+builtin_hash,ArrayIteratorPrototypeNext,-858892834
+builtin_hash,AsyncFunctionEnter,423723147
+builtin_hash,AsyncFunctionResolve,265196636
+builtin_hash,AsyncFunctionAwaitCaught,960969853
+builtin_hash,AsyncFunctionAwaitUncaught,960969853
+builtin_hash,AsyncFunctionAwaitResolveClosure,-1057297202
+builtin_hash,DatePrototypeGetDate,905028372
+builtin_hash,DatePrototypeGetDay,905028372
+builtin_hash,DatePrototypeGetFullYear,905028372
+builtin_hash,DatePrototypeGetHours,905028372
+builtin_hash,DatePrototypeGetMilliseconds,-707287527
+builtin_hash,DatePrototypeGetMinutes,905028372
+builtin_hash,DatePrototypeGetMonth,905028372
+builtin_hash,DatePrototypeGetSeconds,905028372
+builtin_hash,DatePrototypeGetTime,665014006
+builtin_hash,DatePrototypeGetTimezoneOffset,-707287527
+builtin_hash,DatePrototypeValueOf,665014006
+builtin_hash,DatePrototypeToPrimitive,23745105
+builtin_hash,CreateIterResultObject,833507199
+builtin_hash,CreateGeneratorObject,-898656785
+builtin_hash,GeneratorPrototypeNext,-29771038
+builtin_hash,GeneratorPrototypeReturn,-279661376
+builtin_hash,SuspendGeneratorBaseline,-49499079
+builtin_hash,ResumeGeneratorBaseline,145201245
+builtin_hash,GlobalIsFinite,805204024
+builtin_hash,GlobalIsNaN,413622277
+builtin_hash,LoadIC,79924816
+builtin_hash,LoadIC_Megamorphic,682925528
+builtin_hash,LoadIC_Noninlined,-767250044
+builtin_hash,LoadICTrampoline,-803254542
+builtin_hash,LoadICBaseline,-628874782
+builtin_hash,LoadICTrampoline_Megamorphic,-803254542
+builtin_hash,LoadSuperIC,-238282119
+builtin_hash,LoadSuperICBaseline,841397561
+builtin_hash,KeyedLoadIC,78355712
+builtin_hash,KeyedLoadIC_Megamorphic,-391277039
+builtin_hash,KeyedLoadICTrampoline,-803254542
+builtin_hash,KeyedLoadICBaseline,-628874782
+builtin_hash,KeyedLoadICTrampoline_Megamorphic,-803254542
+builtin_hash,StoreGlobalIC,-33330877
+builtin_hash,StoreGlobalICTrampoline,-803254542
+builtin_hash,StoreGlobalICBaseline,-628874782
+builtin_hash,StoreIC,-959753225
+builtin_hash,StoreICTrampoline,756382466
+builtin_hash,StoreICBaseline,841397561
+builtin_hash,DefineNamedOwnIC,464622021
+builtin_hash,DefineNamedOwnICBaseline,841397561
+builtin_hash,KeyedStoreIC,-538069768
+builtin_hash,KeyedStoreICTrampoline,756382466
+builtin_hash,KeyedStoreICBaseline,841397561
+builtin_hash,DefineKeyedOwnIC,458562905
+builtin_hash,StoreInArrayLiteralIC,-604069917
+builtin_hash,StoreInArrayLiteralICBaseline,841397561
+builtin_hash,LoadGlobalIC,274757270
+builtin_hash,LoadGlobalICInsideTypeof,303475129
+builtin_hash,LoadGlobalICTrampoline,-833311190
+builtin_hash,LoadGlobalICBaseline,-77255126
+builtin_hash,LoadGlobalICInsideTypeofTrampoline,-833311190
+builtin_hash,LoadGlobalICInsideTypeofBaseline,-77255126
+builtin_hash,LookupGlobalICBaseline,-1021507359
+builtin_hash,LookupGlobalICInsideTypeofBaseline,-1021507359
+builtin_hash,KeyedHasIC,-204183308
+builtin_hash,KeyedHasICBaseline,-628874782
+builtin_hash,KeyedHasIC_Megamorphic,-958876308
+builtin_hash,IterableToList,-4651130
+builtin_hash,IterableToListWithSymbolLookup,977588013
+builtin_hash,IterableToListMayPreserveHoles,908990960
+builtin_hash,FindOrderedHashMapEntry,196242182
+builtin_hash,MapConstructor,127220366
+builtin_hash,MapPrototypeSet,529910141
+builtin_hash,MapPrototypeDelete,-553855034
+builtin_hash,MapPrototypeGet,-312429732
+builtin_hash,MapPrototypeHas,-908577859
+builtin_hash,MapPrototypeEntries,898519671
+builtin_hash,MapPrototypeGetSize,548120946
+builtin_hash,MapPrototypeForEach,600253966
+builtin_hash,MapPrototypeKeys,898519671
+builtin_hash,MapPrototypeValues,898519671
+builtin_hash,MapIteratorPrototypeNext,581031622
+builtin_hash,MapIteratorToList,-668334452
+builtin_hash,SameValueNumbersOnly,1046023669
+builtin_hash,Add_Baseline,-819537320
+builtin_hash,AddSmi_Baseline,-468458532
+builtin_hash,Subtract_Baseline,65596691
+builtin_hash,SubtractSmi_Baseline,-149584042
+builtin_hash,Multiply_Baseline,294831898
+builtin_hash,MultiplySmi_Baseline,996262660
+builtin_hash,Divide_Baseline,-446061441
+builtin_hash,DivideSmi_Baseline,-447600168
+builtin_hash,Modulus_Baseline,-832082339
+builtin_hash,ModulusSmi_Baseline,413347859
+builtin_hash,Exponentiate_Baseline,129594833
+builtin_hash,BitwiseAnd_Baseline,807317245
+builtin_hash,BitwiseAndSmi_Baseline,-299694524
+builtin_hash,BitwiseOr_Baseline,517046253
+builtin_hash,BitwiseOrSmi_Baseline,986547189
+builtin_hash,BitwiseXor_Baseline,-23876279
+builtin_hash,BitwiseXorSmi_Baseline,-1002138133
+builtin_hash,ShiftLeft_Baseline,500850188
+builtin_hash,ShiftLeftSmi_Baseline,-633960771
+builtin_hash,ShiftRight_Baseline,-32080745
+builtin_hash,ShiftRightSmi_Baseline,315819990
+builtin_hash,ShiftRightLogical_Baseline,479447240
+builtin_hash,ShiftRightLogicalSmi_Baseline,-519393226
+builtin_hash,Add_WithFeedback,-206794177
+builtin_hash,Subtract_WithFeedback,347362352
+builtin_hash,Modulus_WithFeedback,920841751
+builtin_hash,BitwiseOr_WithFeedback,-74343708
+builtin_hash,Equal_Baseline,-896951542
+builtin_hash,StrictEqual_Baseline,87581778
+builtin_hash,LessThan_Baseline,-374004445
+builtin_hash,GreaterThan_Baseline,-368668942
+builtin_hash,LessThanOrEqual_Baseline,301132954
+builtin_hash,GreaterThanOrEqual_Baseline,756925202
+builtin_hash,Equal_WithFeedback,-1040295188
+builtin_hash,StrictEqual_WithFeedback,-1052414211
+builtin_hash,LessThan_WithFeedback,948983301
+builtin_hash,GreaterThan_WithFeedback,-258688563
+builtin_hash,GreaterThanOrEqual_WithFeedback,691471117
+builtin_hash,BitwiseNot_Baseline,182142082
+builtin_hash,Decrement_Baseline,-544743600
+builtin_hash,Increment_Baseline,-307783174
+builtin_hash,Negate_Baseline,434902398
+builtin_hash,ObjectAssign,-786777006
+builtin_hash,ObjectCreate,-543317475
+builtin_hash,ObjectEntries,-465524320
+builtin_hash,ObjectGetOwnPropertyDescriptor,862856609
+builtin_hash,ObjectGetOwnPropertyNames,409260893
+builtin_hash,ObjectIs,-428110665
+builtin_hash,ObjectKeys,-711238005
+builtin_hash,ObjectPrototypeHasOwnProperty,-338192343
+builtin_hash,ObjectToString,993745228
+builtin_hash,InstanceOf_WithFeedback,-50284518
+builtin_hash,InstanceOf_Baseline,992223159
+builtin_hash,ForInEnumerate,-857152067
+builtin_hash,ForInPrepare,-602567485
+builtin_hash,ForInFilter,-142224411
+builtin_hash,RegExpConstructor,-862541618
+builtin_hash,RegExpExecAtom,-837574121
+builtin_hash,RegExpExecInternal,549675176
+builtin_hash,FindOrderedHashSetEntry,-166628054
+builtin_hash,SetConstructor,-778640968
+builtin_hash,SetPrototypeHas,-908577859
+builtin_hash,SetPrototypeAdd,-427333429
+builtin_hash,SetPrototypeDelete,-871946847
+builtin_hash,SetPrototypeEntries,898519671
+builtin_hash,SetPrototypeGetSize,548120946
+builtin_hash,SetPrototypeForEach,-501810916
+builtin_hash,SetPrototypeValues,898519671
+builtin_hash,SetIteratorPrototypeNext,182871241
+builtin_hash,SetOrSetIteratorToList,-33118696
+builtin_hash,StringFromCharCode,-971392951
+builtin_hash,StringPrototypeReplace,211421001
+builtin_hash,StringPrototypeSplit,-575300599
+builtin_hash,TypedArrayConstructor,618386097
+builtin_hash,TypedArrayPrototypeByteLength,-587563610
+builtin_hash,TypedArrayPrototypeLength,-163278974
+builtin_hash,WeakMapConstructor,-808541690
+builtin_hash,WeakMapLookupHashIndex,-619048905
+builtin_hash,WeakMapGet,276986520
+builtin_hash,WeakMapPrototypeHas,-285904254
+builtin_hash,WeakMapPrototypeSet,629680419
+builtin_hash,WeakSetConstructor,-367435631
+builtin_hash,WeakSetPrototypeHas,-285904254
+builtin_hash,WeakSetPrototypeAdd,-301255294
+builtin_hash,WeakCollectionSet,217583952
+builtin_hash,AsyncGeneratorResolve,242317686
+builtin_hash,AsyncGeneratorYieldWithAwait,302667528
+builtin_hash,AsyncGeneratorResumeNext,-265907726
+builtin_hash,AsyncGeneratorPrototypeNext,-194499830
+builtin_hash,AsyncGeneratorAwaitUncaught,-398074132
+builtin_hash,AsyncGeneratorAwaitResolveClosure,-245656056
+builtin_hash,AsyncGeneratorYieldWithAwaitResolveClosure,-649252259
+builtin_hash,StringAdd_CheckNone,1037172071
+builtin_hash,SubString,-701927326
+builtin_hash,GetProperty,-433765894
+builtin_hash,GetPropertyWithReceiver,636771451
+builtin_hash,SetProperty,-985618808
+builtin_hash,CreateDataProperty,952942021
+builtin_hash,FindNonDefaultConstructorOrConstruct,1020851957
+builtin_hash,ArrayPrototypeConcat,-711562967
+builtin_hash,ArrayEvery,732127203
+builtin_hash,ArrayFilterLoopLazyDeoptContinuation,782264259
+builtin_hash,ArrayFilterLoopContinuation,292635770
+builtin_hash,ArrayFilter,-585622372
+builtin_hash,ArrayPrototypeFind,410534083
+builtin_hash,ArrayForEachLoopLazyDeoptContinuation,-299794382
+builtin_hash,ArrayForEachLoopContinuation,350033182
+builtin_hash,ArrayForEach,729108989
+builtin_hash,ArrayFrom,1055630901
+builtin_hash,ArrayIsArray,-970031738
+builtin_hash,LoadJoinElement_FastSmiOrObjectElements_0,228167807
+builtin_hash,LoadJoinElement_FastDoubleElements_0,580988969
+builtin_hash,JoinStackPush,751439150
+builtin_hash,JoinStackPop,128574663
+builtin_hash,ArrayPrototypeJoin,89295304
+builtin_hash,ArrayPrototypeToString,-66500098
+builtin_hash,ArrayPrototypeLastIndexOf,1073113005
+builtin_hash,ArrayMapLoopLazyDeoptContinuation,-47088981
+builtin_hash,ArrayMapLoopContinuation,-794603673
+builtin_hash,ArrayMap,-326417675
+builtin_hash,ArrayReduceLoopLazyDeoptContinuation,-1014597388
+builtin_hash,ArrayReduceLoopContinuation,-1067144759
+builtin_hash,ArrayReduce,-407776620
+builtin_hash,ArrayPrototypeReverse,-121874294
+builtin_hash,ArrayPrototypeShift,-928108750
+builtin_hash,ArrayPrototypeSlice,214735037
+builtin_hash,ArraySome,466290774
+builtin_hash,ArrayPrototypeSplice,1001942992
+builtin_hash,ArrayPrototypeUnshift,-1052845134
+builtin_hash,ArrayBufferPrototypeGetByteLength,445258508
+builtin_hash,ArrayBufferIsView,-78532109
+builtin_hash,ToInteger,-64770826
+builtin_hash,FastCreateDataProperty,-278611029
+builtin_hash,BooleanConstructor,-809457299
+builtin_hash,BooleanPrototypeToString,-798757106
+builtin_hash,ToString,436846720
+builtin_hash,StringPrototypeToString,-794700080
+builtin_hash,StringPrototypeValueOf,-794700080
+builtin_hash,StringPrototypeCharAt,915103217
+builtin_hash,StringPrototypeCharCodeAt,-272108096
+builtin_hash,StringPrototypeCodePointAt,-596824984
+builtin_hash,StringPrototypeConcat,-577571398
+builtin_hash,StringConstructor,-65593142
+builtin_hash,StringAddConvertLeft,51926197
+builtin_hash,StringAddConvertRight,115066033
+builtin_hash,StringCharAt,959950211
+builtin_hash,FastNewClosureBaseline,-532908706
+builtin_hash,FastNewFunctionContextFunction,977993537
+builtin_hash,CreateRegExpLiteral,64770172
+builtin_hash,CreateShallowArrayLiteral,866949735
+builtin_hash,CreateEmptyArrayLiteral,-862242730
+builtin_hash,CreateShallowObjectLiteral,991590480
+builtin_hash,ObjectConstructor,-384944316
+builtin_hash,CreateEmptyLiteralObject,-310219292
+builtin_hash,NumberConstructor,-974450450
+builtin_hash,StringToNumber,-446317754
+builtin_hash,NonNumberToNumber,504608456
+builtin_hash,NonNumberToNumeric,-570033562
+builtin_hash,ToNumeric,-772194204
+builtin_hash,NumberToString,674929388
+builtin_hash,ToBoolean,856538717
+builtin_hash,ToBooleanForBaselineJump,-446512949
+builtin_hash,ToLength,-155953797
+builtin_hash,ToName,645844037
+builtin_hash,ToObject,119745243
+builtin_hash,NonPrimitiveToPrimitive_Default,-151838227
+builtin_hash,NonPrimitiveToPrimitive_Number,-151838227
+builtin_hash,NonPrimitiveToPrimitive_String,-151838227
+builtin_hash,OrdinaryToPrimitive_Number,-337334591
+builtin_hash,OrdinaryToPrimitive_String,-337334591
+builtin_hash,DataViewPrototypeGetByteLength,750091486
+builtin_hash,DataViewPrototypeGetFloat64,544637297
+builtin_hash,DataViewPrototypeSetUint32,366892025
+builtin_hash,DataViewPrototypeSetFloat64,267831220
+builtin_hash,FunctionPrototypeHasInstance,-911487777
+builtin_hash,FastFunctionPrototypeBind,-29755211
+builtin_hash,ForInNext,547638943
+builtin_hash,GetIteratorWithFeedback,935596039
+builtin_hash,GetIteratorBaseline,-124236956
+builtin_hash,CallIteratorWithFeedback,174322508
+builtin_hash,MathAbs,111472406
+builtin_hash,MathCeil,466078480
+builtin_hash,MathFloor,-900013988
+builtin_hash,MathRound,981339685
+builtin_hash,MathPow,-432438626
+builtin_hash,MathMax,-914923816
+builtin_hash,MathMin,-435430851
+builtin_hash,MathAsin,-865319143
+builtin_hash,MathAtan2,-706534972
+builtin_hash,MathCos,705415335
+builtin_hash,MathExp,1065131032
+builtin_hash,MathFround,-135252655
+builtin_hash,MathImul,773832811
+builtin_hash,MathLog,540909033
+builtin_hash,MathSin,-688911662
+builtin_hash,MathSign,-523407079
+builtin_hash,MathSqrt,-794868693
+builtin_hash,MathTan,537052027
+builtin_hash,MathTanh,-300840302
+builtin_hash,MathRandom,966867537
+builtin_hash,NumberPrototypeToString,-382822730
+builtin_hash,NumberIsInteger,-693598207
+builtin_hash,NumberIsNaN,788813704
+builtin_hash,NumberParseFloat,-741561968
+builtin_hash,ParseInt,998287919
+builtin_hash,NumberParseInt,-382916138
+builtin_hash,Add,-136527337
+builtin_hash,Subtract,-213501900
+builtin_hash,Multiply,7472525
+builtin_hash,Divide,-344347312
+builtin_hash,Modulus,-582417614
+builtin_hash,CreateObjectWithoutProperties,339671006
+builtin_hash,ObjectIsExtensible,-329082141
+builtin_hash,ObjectPreventExtensions,940542631
+builtin_hash,ObjectGetPrototypeOf,157540923
+builtin_hash,ObjectSetPrototypeOf,187356384
+builtin_hash,ObjectPrototypeToString,-483254038
+builtin_hash,ObjectPrototypeValueOf,193287106
+builtin_hash,FulfillPromise,272197869
+builtin_hash,NewPromiseCapability,-508522709
+builtin_hash,PromiseCapabilityDefaultResolve,-402797269
+builtin_hash,PerformPromiseThen,330989248
+builtin_hash,PromiseAll,697437536
+builtin_hash,PromiseAllResolveElementClosure,-862999565
+builtin_hash,PromiseConstructor,762524591
+builtin_hash,PromisePrototypeCatch,756171957
+builtin_hash,PromiseFulfillReactionJob,-630924263
+builtin_hash,PromiseResolveTrampoline,-167249272
+builtin_hash,PromiseResolve,-412690059
+builtin_hash,ResolvePromise,756044362
+builtin_hash,PromisePrototypeThen,3713531
+builtin_hash,PromiseResolveThenableJob,-14213172
+builtin_hash,ProxyConstructor,459230341
+builtin_hash,ProxyGetProperty,1054163992
+builtin_hash,ProxyIsExtensible,308384776
+builtin_hash,ProxyPreventExtensions,399450299
+builtin_hash,ReflectGet,-434221017
+builtin_hash,ReflectHas,-167249272
+builtin_hash,RegExpPrototypeExec,963999476
+builtin_hash,RegExpMatchFast,384654261
+builtin_hash,RegExpReplace,-475275041
+builtin_hash,RegExpPrototypeReplace,860372377
+builtin_hash,RegExpSearchFast,907750005
+builtin_hash,RegExpPrototypeSourceGetter,-747085084
+builtin_hash,RegExpSplit,-607180644
+builtin_hash,RegExpPrototypeTest,-585829947
+builtin_hash,RegExpPrototypeTestFast,-1071276448
+builtin_hash,RegExpPrototypeGlobalGetter,-718555192
+builtin_hash,RegExpPrototypeIgnoreCaseGetter,1070990033
+builtin_hash,RegExpPrototypeMultilineGetter,216999873
+builtin_hash,RegExpPrototypeHasIndicesGetter,390292067
+builtin_hash,RegExpPrototypeDotAllGetter,390292067
+builtin_hash,RegExpPrototypeStickyGetter,1055105538
+builtin_hash,RegExpPrototypeUnicodeGetter,1055105538
+builtin_hash,RegExpPrototypeFlagsGetter,-646009057
+builtin_hash,StringPrototypeEndsWith,565371891
+builtin_hash,StringPrototypeIncludes,480948081
+builtin_hash,StringPrototypeIndexOf,619068194
+builtin_hash,StringPrototypeIterator,-532566456
+builtin_hash,StringIteratorPrototypeNext,-1034386014
+builtin_hash,StringPrototypeMatch,127768813
+builtin_hash,StringPrototypeSearch,127768813
+builtin_hash,StringRepeat,92491602
+builtin_hash,StringPrototypeSlice,111174165
+builtin_hash,StringPrototypeStartsWith,-951440779
+builtin_hash,StringPrototypeSubstr,716425893
+builtin_hash,StringPrototypeSubstring,769385864
+builtin_hash,StringPrototypeTrim,-151587513
+builtin_hash,SymbolPrototypeToString,697341238
+builtin_hash,CreateTypedArray,100324164
+builtin_hash,TypedArrayFrom,-508079252
+builtin_hash,TypedArrayPrototypeSet,241292735
+builtin_hash,TypedArrayPrototypeSubArray,-638094120
+builtin_hash,NewSloppyArgumentsElements,745494442
+builtin_hash,NewStrictArgumentsElements,-81425804
+builtin_hash,NewRestArgumentsElements,-823345459
+builtin_hash,FastNewSloppyArguments,-174863955
+builtin_hash,FastNewStrictArguments,-75939795
+builtin_hash,FastNewRestArguments,-680285498
+builtin_hash,StringSlowFlatten,108774605
+builtin_hash,StringIndexOf,119327941
+builtin_hash,Load_FastSmiElements_0,-418523514
+builtin_hash,Load_FastObjectElements_0,-418523514
+builtin_hash,Store_FastSmiElements_0,975980653
+builtin_hash,Store_FastObjectElements_0,311513691
+builtin_hash,SortCompareDefault,842664214
+builtin_hash,SortCompareUserFn,1059126141
+builtin_hash,Copy,-750738169
+builtin_hash,MergeAt,944447896
+builtin_hash,GallopLeft,368113946
+builtin_hash,GallopRight,186729557
+builtin_hash,ArrayTimSort,-475205137
+builtin_hash,ArrayPrototypeSort,-366911589
+builtin_hash,StringFastLocaleCompare,15452983
+builtin_hash,WasmInt32ToHeapNumber,751194511
+builtin_hash,WasmTaggedNonSmiToInt32,-202443862
+builtin_hash,WasmTriggerTierUp,-980759280
+builtin_hash,WasmStackGuard,-1024124053
+builtin_hash,CanUseSameAccessor_FastSmiElements_0,-756700379
+builtin_hash,CanUseSameAccessor_FastObjectElements_0,-756700379
+builtin_hash,StringPrototypeToLowerCaseIntl,-966367732
+builtin_hash,StringToLowerCaseIntl,-481509366
+builtin_hash,WideHandler,-298201266
+builtin_hash,ExtraWideHandler,-298201266
+builtin_hash,LdarHandler,-745598094
+builtin_hash,LdaZeroHandler,368748633
+builtin_hash,LdaSmiHandler,-545227529
+builtin_hash,LdaUndefinedHandler,1011673901
+builtin_hash,LdaNullHandler,1011673901
+builtin_hash,LdaTheHoleHandler,1011673901
+builtin_hash,LdaTrueHandler,827753247
+builtin_hash,LdaFalseHandler,827753247
+builtin_hash,LdaConstantHandler,407548785
+builtin_hash,LdaContextSlotHandler,506452989
+builtin_hash,LdaImmutableContextSlotHandler,506452989
+builtin_hash,LdaCurrentContextSlotHandler,327557270
+builtin_hash,LdaImmutableCurrentContextSlotHandler,327557270
+builtin_hash,StarHandler,305217552
+builtin_hash,MovHandler,-283701884
+builtin_hash,PushContextHandler,177425195
+builtin_hash,PopContextHandler,-1044986385
+builtin_hash,TestReferenceEqualHandler,-651544719
+builtin_hash,TestUndetectableHandler,-830971105
+builtin_hash,TestNullHandler,1005522396
+builtin_hash,TestUndefinedHandler,1005522396
+builtin_hash,TestTypeOfHandler,-1028477858
+builtin_hash,LdaGlobalHandler,965344129
+builtin_hash,LdaGlobalInsideTypeofHandler,585777250
+builtin_hash,StaGlobalHandler,1056951542
+builtin_hash,StaContextSlotHandler,-675927710
+builtin_hash,StaCurrentContextSlotHandler,-997669083
+builtin_hash,LdaLookupGlobalSlotHandler,-84752131
+builtin_hash,LdaLookupGlobalSlotInsideTypeofHandler,49834142
+builtin_hash,StaLookupSlotHandler,-381579342
+builtin_hash,GetNamedPropertyHandler,-27764824
+builtin_hash,GetNamedPropertyFromSuperHandler,-724989944
+builtin_hash,GetKeyedPropertyHandler,-56635454
+builtin_hash,SetNamedPropertyHandler,448782548
+builtin_hash,DefineNamedOwnPropertyHandler,448782548
+builtin_hash,SetKeyedPropertyHandler,941278116
+builtin_hash,DefineKeyedOwnPropertyHandler,941278116
+builtin_hash,StaInArrayLiteralHandler,941278116
+builtin_hash,DefineKeyedOwnPropertyInLiteralHandler,1045494813
+builtin_hash,AddHandler,-518783725
+builtin_hash,SubHandler,505104408
+builtin_hash,MulHandler,-222850853
+builtin_hash,DivHandler,-1028262634
+builtin_hash,ModHandler,143526297
+builtin_hash,ExpHandler,-727777022
+builtin_hash,BitwiseOrHandler,-522781712
+builtin_hash,BitwiseXorHandler,-419955523
+builtin_hash,BitwiseAndHandler,530208341
+builtin_hash,ShiftLeftHandler,-804444955
+builtin_hash,ShiftRightHandler,-104335215
+builtin_hash,ShiftRightLogicalHandler,1050635494
+builtin_hash,AddSmiHandler,-161508067
+builtin_hash,SubSmiHandler,-609360326
+builtin_hash,MulSmiHandler,282822605
+builtin_hash,DivSmiHandler,292906952
+builtin_hash,ModSmiHandler,-917212490
+builtin_hash,BitwiseOrSmiHandler,172148322
+builtin_hash,BitwiseXorSmiHandler,1046550901
+builtin_hash,BitwiseAndSmiHandler,-808862341
+builtin_hash,ShiftLeftSmiHandler,862845296
+builtin_hash,ShiftRightSmiHandler,183483372
+builtin_hash,ShiftRightLogicalSmiHandler,31369673
+builtin_hash,IncHandler,-318834355
+builtin_hash,DecHandler,938496699
+builtin_hash,NegateHandler,-590726041
+builtin_hash,BitwiseNotHandler,322709376
+builtin_hash,ToBooleanLogicalNotHandler,-972724513
+builtin_hash,LogicalNotHandler,-706273800
+builtin_hash,TypeOfHandler,-751823
+builtin_hash,DeletePropertyStrictHandler,-724253277
+builtin_hash,DeletePropertySloppyHandler,-476722269
+builtin_hash,FindNonDefaultConstructorOrConstructHandler,-746857468
+builtin_hash,CallAnyReceiverHandler,87393745
+builtin_hash,CallPropertyHandler,87393745
+builtin_hash,CallProperty0Handler,956548008
+builtin_hash,CallProperty1Handler,-471075746
+builtin_hash,CallProperty2Handler,-1043814952
+builtin_hash,CallUndefinedReceiverHandler,126620186
+builtin_hash,CallUndefinedReceiver0Handler,-286191860
+builtin_hash,CallUndefinedReceiver1Handler,-357856703
+builtin_hash,CallUndefinedReceiver2Handler,798828847
+builtin_hash,CallWithSpreadHandler,87393745
+builtin_hash,CallRuntimeHandler,624123308
+builtin_hash,CallJSRuntimeHandler,1005113218
+builtin_hash,InvokeIntrinsicHandler,-566159390
+builtin_hash,ConstructHandler,543386518
+builtin_hash,ConstructWithSpreadHandler,595837553
+builtin_hash,TestEqualHandler,-157366914
+builtin_hash,TestEqualStrictHandler,998643852
+builtin_hash,TestLessThanHandler,1046936290
+builtin_hash,TestGreaterThanHandler,-369508260
+builtin_hash,TestLessThanOrEqualHandler,-412750652
+builtin_hash,TestGreaterThanOrEqualHandler,-364267636
+builtin_hash,TestInstanceOfHandler,-607728916
+builtin_hash,TestInHandler,539847065
+builtin_hash,ToNameHandler,701699245
+builtin_hash,ToNumberHandler,-512585428
+builtin_hash,ToNumericHandler,459707132
+builtin_hash,ToObjectHandler,701699245
+builtin_hash,ToStringHandler,620423288
+builtin_hash,CreateRegExpLiteralHandler,848340986
+builtin_hash,CreateArrayLiteralHandler,101333771
+builtin_hash,CreateArrayFromIterableHandler,-18783057
+builtin_hash,CreateEmptyArrayLiteralHandler,-289337896
+builtin_hash,CreateObjectLiteralHandler,-711473910
+builtin_hash,CreateEmptyObjectLiteralHandler,795228443
+builtin_hash,CreateClosureHandler,877324634
+builtin_hash,CreateBlockContextHandler,-344466857
+builtin_hash,CreateCatchContextHandler,-214012965
+builtin_hash,CreateFunctionContextHandler,729147868
+builtin_hash,CreateMappedArgumentsHandler,-124182926
+builtin_hash,CreateUnmappedArgumentsHandler,758781228
+builtin_hash,CreateRestParameterHandler,-10099522
+builtin_hash,JumpLoopHandler,-166037043
+builtin_hash,JumpHandler,-79617432
+builtin_hash,JumpConstantHandler,906507762
+builtin_hash,JumpIfUndefinedConstantHandler,250257394
+builtin_hash,JumpIfNotUndefinedConstantHandler,-587816710
+builtin_hash,JumpIfUndefinedOrNullConstantHandler,53751011
+builtin_hash,JumpIfTrueConstantHandler,250257394
+builtin_hash,JumpIfFalseConstantHandler,250257394
+builtin_hash,JumpIfToBooleanTrueConstantHandler,15176103
+builtin_hash,JumpIfToBooleanFalseConstantHandler,422983862
+builtin_hash,JumpIfToBooleanTrueHandler,635201116
+builtin_hash,JumpIfToBooleanFalseHandler,408147223
+builtin_hash,JumpIfTrueHandler,801953084
+builtin_hash,JumpIfFalseHandler,801953084
+builtin_hash,JumpIfNullHandler,801953084
+builtin_hash,JumpIfNotNullHandler,1026829001
+builtin_hash,JumpIfUndefinedHandler,801953084
+builtin_hash,JumpIfNotUndefinedHandler,1026829001
+builtin_hash,JumpIfUndefinedOrNullHandler,1021601552
+builtin_hash,JumpIfJSReceiverHandler,65469341
+builtin_hash,SwitchOnSmiNoFeedbackHandler,807681990
+builtin_hash,ForInEnumerateHandler,510063374
+builtin_hash,ForInPrepareHandler,630466074
+builtin_hash,ForInContinueHandler,-691562887
+builtin_hash,ForInNextHandler,512834227
+builtin_hash,ForInStepHandler,942618821
+builtin_hash,SetPendingMessageHandler,401946975
+builtin_hash,ThrowHandler,50431783
+builtin_hash,ReThrowHandler,50431783
+builtin_hash,ReturnHandler,-117530186
+builtin_hash,ThrowReferenceErrorIfHoleHandler,512852920
+builtin_hash,ThrowSuperNotCalledIfHoleHandler,717642155
+builtin_hash,ThrowSuperAlreadyCalledIfNotHoleHandler,-546144205
+builtin_hash,ThrowIfNotSuperConstructorHandler,-460002303
+builtin_hash,SwitchOnGeneratorStateHandler,10710931
+builtin_hash,SuspendGeneratorHandler,-500612975
+builtin_hash,ResumeGeneratorHandler,1068636019
+builtin_hash,GetIteratorHandler,-71006498
+builtin_hash,ShortStarHandler,356943682
+builtin_hash,LdarWideHandler,-249230336
+builtin_hash,LdaSmiWideHandler,-31881096
+builtin_hash,LdaConstantWideHandler,-758989820
+builtin_hash,LdaContextSlotWideHandler,687146226
+builtin_hash,LdaImmutableContextSlotWideHandler,687146226
+builtin_hash,LdaImmutableCurrentContextSlotWideHandler,-836770052
+builtin_hash,StarWideHandler,501248040
+builtin_hash,MovWideHandler,-871657303
+builtin_hash,PushContextWideHandler,844522230
+builtin_hash,PopContextWideHandler,744748597
+builtin_hash,TestReferenceEqualWideHandler,-118913544
+builtin_hash,LdaGlobalWideHandler,-661487412
+builtin_hash,LdaGlobalInsideTypeofWideHandler,-572343212
+builtin_hash,StaGlobalWideHandler,555909381
+builtin_hash,StaContextSlotWideHandler,478877471
+builtin_hash,StaCurrentContextSlotWideHandler,-615279276
+builtin_hash,LdaLookupGlobalSlotWideHandler,-1002268065
+builtin_hash,GetNamedPropertyWideHandler,-241462706
+builtin_hash,GetKeyedPropertyWideHandler,641533107
+builtin_hash,SetNamedPropertyWideHandler,-58064714
+builtin_hash,DefineNamedOwnPropertyWideHandler,-58064714
+builtin_hash,SetKeyedPropertyWideHandler,686171362
+builtin_hash,DefineKeyedOwnPropertyWideHandler,686171362
+builtin_hash,StaInArrayLiteralWideHandler,686171362
+builtin_hash,AddWideHandler,-617481681
+builtin_hash,SubWideHandler,145242966
+builtin_hash,MulWideHandler,166175890
+builtin_hash,DivWideHandler,829768719
+builtin_hash,BitwiseOrWideHandler,-671352735
+builtin_hash,BitwiseAndWideHandler,-748389668
+builtin_hash,ShiftLeftWideHandler,-722355824
+builtin_hash,AddSmiWideHandler,-503151286
+builtin_hash,SubSmiWideHandler,266762310
+builtin_hash,MulSmiWideHandler,767307001
+builtin_hash,DivSmiWideHandler,1050619977
+builtin_hash,ModSmiWideHandler,-653636504
+builtin_hash,BitwiseOrSmiWideHandler,905206733
+builtin_hash,BitwiseXorSmiWideHandler,1044063990
+builtin_hash,BitwiseAndSmiWideHandler,-376485258
+builtin_hash,ShiftLeftSmiWideHandler,-1004091795
+builtin_hash,ShiftRightSmiWideHandler,-397666497
+builtin_hash,ShiftRightLogicalSmiWideHandler,54662547
+builtin_hash,IncWideHandler,331971916
+builtin_hash,DecWideHandler,279024516
+builtin_hash,NegateWideHandler,-781916260
+builtin_hash,CallPropertyWideHandler,-998392170
+builtin_hash,CallProperty0WideHandler,54487119
+builtin_hash,CallProperty1WideHandler,-147592428
+builtin_hash,CallProperty2WideHandler,-58614287
+builtin_hash,CallUndefinedReceiverWideHandler,400495181
+builtin_hash,CallUndefinedReceiver0WideHandler,-1000686597
+builtin_hash,CallUndefinedReceiver1WideHandler,-299347389
+builtin_hash,CallUndefinedReceiver2WideHandler,525189648
+builtin_hash,CallWithSpreadWideHandler,-998392170
+builtin_hash,ConstructWideHandler,193926631
+builtin_hash,TestEqualWideHandler,-797631551
+builtin_hash,TestEqualStrictWideHandler,753248660
+builtin_hash,TestLessThanWideHandler,-210582608
+builtin_hash,TestGreaterThanWideHandler,543018087
+builtin_hash,TestLessThanOrEqualWideHandler,-1053789276
+builtin_hash,TestGreaterThanOrEqualWideHandler,-582678107
+builtin_hash,TestInstanceOfWideHandler,-280937039
+builtin_hash,TestInWideHandler,817647574
+builtin_hash,ToNumericWideHandler,868695670
+builtin_hash,CreateRegExpLiteralWideHandler,-1006765965
+builtin_hash,CreateArrayLiteralWideHandler,-826485513
+builtin_hash,CreateEmptyArrayLiteralWideHandler,559300434
+builtin_hash,CreateObjectLiteralWideHandler,455963528
+builtin_hash,CreateClosureWideHandler,1061873155
+builtin_hash,CreateBlockContextWideHandler,271729622
+builtin_hash,CreateFunctionContextWideHandler,527181803
+builtin_hash,JumpLoopWideHandler,941891518
+builtin_hash,JumpWideHandler,-79617432
+builtin_hash,JumpIfToBooleanTrueWideHandler,923993949
+builtin_hash,JumpIfToBooleanFalseWideHandler,145370961
+builtin_hash,JumpIfTrueWideHandler,-1042889789
+builtin_hash,JumpIfFalseWideHandler,-1042889789
+builtin_hash,SwitchOnSmiNoFeedbackWideHandler,-773907277
+builtin_hash,ForInPrepareWideHandler,-483036360
+builtin_hash,ForInNextWideHandler,-173595160
+builtin_hash,ThrowReferenceErrorIfHoleWideHandler,-254407930
+builtin_hash,GetIteratorWideHandler,-412149326
+builtin_hash,LdaSmiExtraWideHandler,65806156
+builtin_hash,LdaGlobalExtraWideHandler,411460668
+builtin_hash,AddSmiExtraWideHandler,553152400
+builtin_hash,SubSmiExtraWideHandler,446395338
+builtin_hash,MulSmiExtraWideHandler,105494980
+builtin_hash,DivSmiExtraWideHandler,-317292269
+builtin_hash,BitwiseOrSmiExtraWideHandler,604681516
+builtin_hash,BitwiseXorSmiExtraWideHandler,-91329781
+builtin_hash,BitwiseAndSmiExtraWideHandler,150048166
+builtin_hash,CallUndefinedReceiverExtraWideHandler,423421950
+builtin_hash,CallUndefinedReceiver1ExtraWideHandler,168432499
+builtin_hash,CallUndefinedReceiver2ExtraWideHandler,524973830
diff -r -u --color up/v8/tools/clusterfuzz/foozzie/v8_fuzz_flags.json nw/v8/tools/clusterfuzz/foozzie/v8_fuzz_flags.json
--- up/v8/tools/clusterfuzz/foozzie/v8_fuzz_flags.json	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/clusterfuzz/foozzie/v8_fuzz_flags.json	2023-01-19 16:46:37.868942454 -0500
@@ -31,6 +31,5 @@
   [0.1, "--no-wasm-generic-wrapper"],
   [0.1, "--turbo-force-mid-tier-regalloc"],
   [0.0001, "--simulate-errors"],
-  [0.25, "--no-use-map-space"],
   [0.1, "--minor-mc"]
 ]
diff -r -u --color up/v8/tools/clusterfuzz/trials/clusterfuzz_trials_config.json nw/v8/tools/clusterfuzz/trials/clusterfuzz_trials_config.json
--- up/v8/tools/clusterfuzz/trials/clusterfuzz_trials_config.json	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/clusterfuzz/trials/clusterfuzz_trials_config.json	2023-01-19 16:46:37.879775784 -0500
@@ -1,7 +1,6 @@
 [
     {"app_args": "--assert-types", "app_name": "d8", "probability": 0.25, "contradicts": ["--stress-concurrent-inlining", "--stress-concurrent-inlining-attach-code"]},
     {"app_args": "--interrupt-budget-for-feedback-vector-allocation=0", "app_name": "d8", "probability": 0.05},
-    {"app_args": "--compact-maps", "app_name": "d8", "probability": 0.25},
     {"app_args": "--force-slow-path", "app_name": "d8", "probability": 0.05},
     {"app_args": "--future", "app_name": "d8", "probability": 0.25},
     {"app_args": "--interrupt-budget=1000", "app_name": "d8", "probability": 0.25},
@@ -24,7 +23,6 @@
     {"app_args": "--no-lazy-feedback-allocation", "app_name": "d8", "probability": 0.35},
     {"app_args": "--no-regexp-tier-up", "app_name": "d8", "probability": 0.2},
     {"app_args": "--no-use-ic", "app_name": "d8", "probability": 0.25},
-    {"app_args": "--no-use-map-space", "app_name": "d8", "probability": 0.25},
     {"app_args": "--no-wasm-generic-wrapper", "app_name": "d8", "probability": 0.1},
     {"app_args": "--regexp-interpret-all", "app_name": "d8", "probability": 0.1},
     {"app_args": "--simulate-errors", "app_name": "d8", "probability": 0.001},
Only in nw/v8/tools/memory: rss.py
diff -r -u --color up/v8/tools/profiling/linux-perf-d8.py nw/v8/tools/profiling/linux-perf-d8.py
--- up/v8/tools/profiling/linux-perf-d8.py	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/profiling/linux-perf-d8.py	2023-01-19 16:46:37.901442447 -0500
@@ -39,6 +39,11 @@
     metavar="OUT_DIR",
     help="Output directory for linux perf profile files")
 parser.add_option("--timeout", type=float, help="Stop d8 after N seconds")
+parser.add_option(
+    "--skip-pprof",
+    action="store_true",
+    default=False,
+    help="Skip pprof upload (relevant for Googlers only)")
 
 d8_options = optparse.OptionGroup(
     parser, "d8-forwarded Options",
@@ -232,14 +237,14 @@
 result = inject_v8_symbols(perf_data_file)
 if result is None:
   print("No perf files were successfully processed"
-        " Check for errors or partial results in '{options.perf_data_dir}'")
+        f" Check for errors or partial results in '{options.perf_data_dir}'")
   exit(1)
 log(f"RESULTS in '{options.perf_data_dir}'")
 BYTES_TO_MIB = 1 / 1024 / 1024
 print(f"{result.name:67}{(result.stat().st_size*BYTES_TO_MIB):10.2f}MiB")
 
 # ==============================================================================
-if not shutil.which('gcertstatus'):
+if not shutil.which('gcertstatus') or options.skip_pprof:
   log("ANALYSIS")
   print(f"perf report --input='{result}'")
   print(f"pprof '{result}'")
diff -r -u --color up/v8/tools/system-analyzer/index.html nw/v8/tools/system-analyzer/index.html
--- up/v8/tools/system-analyzer/index.html	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/system-analyzer/index.html	2023-01-19 16:46:37.912275777 -0500
@@ -120,13 +120,13 @@
         <h3><code>LOG_FLAGS</code>:</h3>
         <dl class="d8-options">
           <dt>
-            <a href="https://source.chromium.org/search?q=FLAG_log_all">
+            <a href="https://source.chromium.org/search?q=v8_flags.log_all">
               <code>--log-all</code>
             </a>
           </dt>
           <dd>Enable all V8 logging options.</dd>
           <dt>
-            <a href="https://source.chromium.org/search?q=FLAG_trace_maps">
+            <a href="https://source.chromium.org/search?q=v8_flags.trace_maps">
               <code>--log-maps</code>
             </a>
           </dt>
@@ -134,7 +134,7 @@
               Log <a href="https://v8.dev/blog/fast-properties">Maps</a>
           </dd>
           <dt>
-            <a href="https://source.chromium.org/search?q=FLAG_log_ic">
+            <a href="https://source.chromium.org/search?q=v8_flags.log_ic">
               <code>--log-ic</code>
             </a>
           </dt>
@@ -142,31 +142,31 @@
             Log <a href="https://mathiasbynens.be/notes/shapes-ics">ICs</a>
           </dd>
           <dt>
-            <a href="https://source.chromium.org/search?q=FLAG_log_source_code">
+            <a href="https://source.chromium.org/search?q=v8_flags.log_source_code">
               <code>--log-source-code</code>
             </a>
           </dt>
           <dd>Log source code</dd>
           <dt>
-            <a href="https://source.chromium.org/search?q=FLAG_log_code_disassemble">
+            <a href="https://source.chromium.org/search?q=v8_flags.log_code_disassemble">
               <code>--log-code-disassemble</code>
             </a>
           </dt>
           <dd>Log detailed generated generated code</dd>
           <dt>
-            <a href="https://source.chromium.org/search?q=FLAG_log_code">
+            <a href="https://source.chromium.org/search?q=v8_flags.log_code">
               <code>--log-code</code>
             </a>
           </dt>
           <dd>Log details about deoptimized code</dd>
           <dt>
-            <a href="https://source.chromium.org/search?q=FLAG_log_deopt">
+            <a href="https://source.chromium.org/search?q=v8_flags.log_deopt">
               <code>--log-deopt</code>
             </a>
           </dt>
           <dd>Log various API uses.</dd>
           <dt>
-            <a href="https://source.chromium.org/search?q=FLAG_prof">
+            <a href="https://source.chromium.org/search?q=v8_flags.prof">
               <code>--prof</code>
             </a>
           </dt>
@@ -181,11 +181,14 @@
 
         <h3>Keyboard Shortcuts for Navigation</h3>
         <dl>
+          <dt><kbd>CTRL</kbd> + <kbd>Mouse Move</kbd></dt>
+          <dd>Show tooltips immediately</dd>
+
           <dt><kbd>A</kbd></dt>
           <dd>Scroll left</dd>
 
           <dt><kbd>D</kbd></dt>
-          <dd>Sroll right</dd>
+          <dd>Scroll right</dd>
 
           <dt><kbd>SHIFT</kbd> + <kbd>Arrow Up</kbd></dt>
           <dd>Follow Map transition forward (first child)</dd>
diff -r -u --color up/v8/tools/system-analyzer/index.mjs nw/v8/tools/system-analyzer/index.mjs
--- up/v8/tools/system-analyzer/index.mjs	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/system-analyzer/index.mjs	2023-01-19 16:46:37.912275777 -0500
@@ -328,12 +328,11 @@
       throw new Error(
           `Unknown tooltip content type: ${content.constructor?.name}`);
     }
-    this.setToolTip(content, event.positionOrTargetNode);
-  }
-
-  setToolTip(content, positionOrTargetNode) {
-    this._view.toolTip.positionOrTargetNode = positionOrTargetNode;
-    this._view.toolTip.content = content;
+    this._view.toolTip.data = {
+      content: content,
+      positionOrTargetNode: event.positionOrTargetNode,
+      immediate: event.immediate,
+    };
   }
 
   restartApp() {
diff -r -u --color up/v8/tools/system-analyzer/view/events.mjs nw/v8/tools/system-analyzer/view/events.mjs
--- up/v8/tools/system-analyzer/view/events.mjs	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/system-analyzer/view/events.mjs	2023-01-19 16:46:37.912275777 -0500
@@ -76,13 +76,14 @@
     return 'showtooltip';
   }
 
-  constructor(content, positionOrTargetNode) {
+  constructor(content, positionOrTargetNode, immediate) {
     super(ToolTipEvent.name);
     if (!positionOrTargetNode) {
       throw Error('Either provide a valid position or targetNode');
     }
     this._content = content;
     this._positionOrTargetNode = positionOrTargetNode;
+    this._immediate = immediate;
   }
 
   get content() {
@@ -92,4 +93,8 @@
   get positionOrTargetNode() {
     return this._positionOrTargetNode;
   }
+
+  get immediate() {
+    return this._immediate;
+  }
 }
diff -r -u --color up/v8/tools/system-analyzer/view/helper.mjs nw/v8/tools/system-analyzer/view/helper.mjs
--- up/v8/tools/system-analyzer/view/helper.mjs	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/system-analyzer/view/helper.mjs	2023-01-19 16:46:37.912275777 -0500
@@ -322,5 +322,27 @@
   return stops;
 }
 
+export class Debouncer {
+  constructor(callback, timeout = 250) {
+    this._callback = callback;
+    this._timeout = timeout;
+    this._timeoutId = 0;
+  }
+
+  callNow(...args) {
+    this.clear();
+    return this._callback(...args);
+  }
+
+  call(...args) {
+    this.clear() this._timeoutId =
+        window.setTimeout(this._callback, this._timeout, ...args)
+  }
+
+  clear() {
+    clearTimeout(this._timeoutId);
+  }
+}
+
 export * from '../helper.mjs';
 export * from '../../js/web-api-helper.mjs'
diff -r -u --color up/v8/tools/system-analyzer/view/list-panel.mjs nw/v8/tools/system-analyzer/view/list-panel.mjs
--- up/v8/tools/system-analyzer/view/list-panel.mjs	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/system-analyzer/view/list-panel.mjs	2023-01-19 16:46:37.912275777 -0500
@@ -127,7 +127,7 @@
 
   _logEntryMouseOverHandler(e) {
     const group = e.currentTarget.group;
-    this.dispatchEvent(new ToolTipEvent(group.key, e.currentTarget));
+    this.dispatchEvent(new ToolTipEvent(group.key, e.currentTarget, e.ctrlKey));
   }
 
   _handleDetailsClick(event) {
diff -r -u --color up/v8/tools/system-analyzer/view/map-panel/map-transitions.mjs nw/v8/tools/system-analyzer/view/map-panel/map-transitions.mjs
--- up/v8/tools/system-analyzer/view/map-panel/map-transitions.mjs	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/system-analyzer/view/map-panel/map-transitions.mjs	2023-01-19 16:46:37.912275777 -0500
@@ -147,8 +147,8 @@
   }
 
   _handleMouseoverMap(event) {
-    this.dispatchEvent(
-        new ToolTipEvent(event.currentTarget.map, event.currentTarget));
+    this.dispatchEvent(new ToolTipEvent(
+        event.currentTarget.map, event.currentTarget, event.ctrlKey));
   }
 
   _handleToggleSubtree(event) {
diff -r -u --color up/v8/tools/system-analyzer/view/profiler-panel.mjs nw/v8/tools/system-analyzer/view/profiler-panel.mjs
--- up/v8/tools/system-analyzer/view/profiler-panel.mjs	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/system-analyzer/view/profiler-panel.mjs	2023-01-19 16:46:37.912275777 -0500
@@ -295,7 +295,7 @@
     const profileNode = e.target.data;
     if (!profileNode) return;
     const logEntry = profileNode.codeEntry.logEntry;
-    this.dispatchEvent(new ToolTipEvent(logEntry, e.target));
+    this.dispatchEvent(new ToolTipEvent(logEntry, e.target, e.ctrlKey));
   }
 
   _handleFlameChartClick(e) {
diff -r -u --color up/v8/tools/system-analyzer/view/property-link-table.mjs nw/v8/tools/system-analyzer/view/property-link-table.mjs
--- up/v8/tools/system-analyzer/view/property-link-table.mjs	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/system-analyzer/view/property-link-table.mjs	2023-01-19 16:46:37.912275777 -0500
@@ -111,16 +111,20 @@
     if (this._object === undefined) return;
     if (!this._instanceLinkButtons) return;
     const footer = DOM.div('footer');
-    let showButton = footer.appendChild(DOM.button('Show', this._showHandler));
+    let showButton =
+        footer.appendChild(DOM.button('🔍 Details', this._showHandler));
     showButton.data = this._object;
+    showButton.title = `Show details for ${this._object}`
     if (this._object.sourcePosition) {
       let showSourcePositionButton = footer.appendChild(
-          DOM.button('Source Position', this._showSourcePositionHandler));
+          DOM.button('📍 Source Position', this._showSourcePositionHandler));
       showSourcePositionButton.data = this._object;
+      showSourcePositionButton.title = 'Open the source position';
     }
     let showRelatedButton = footer.appendChild(
-        DOM.button('Show Related', this._showRelatedHandler));
+        DOM.button('🕸 Related', this._showRelatedHandler));
     showRelatedButton.data = this._object;
+    showRelatedButton.title = 'Show all related events in all panels';
     this._fragment.appendChild(footer);
   }
 
diff -r -u --color up/v8/tools/system-analyzer/view/script-panel.mjs nw/v8/tools/system-analyzer/view/script-panel.mjs
--- up/v8/tools/system-analyzer/view/script-panel.mjs	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/system-analyzer/view/script-panel.mjs	2023-01-19 16:46:37.923109107 -0500
@@ -198,7 +198,7 @@
         break;
     }
     toolTipContent.appendChild(sourceMapContent);
-    this.dispatchEvent(new ToolTipEvent(toolTipContent, e.target));
+    this.dispatchEvent(new ToolTipEvent(toolTipContent, e.target, e.ctrlKey));
   }
 
   handleShowToolTipEntries(event) {
@@ -233,8 +233,8 @@
     tr.appendChild(DOM.td(name));
     tr.appendChild(DOM.td(subtypeName));
     tr.appendChild(DOM.td(entries.length));
-    const button =
-        DOM.button('Show', this._scriptPanel.showToolTipEntriesHandler);
+    const button = DOM.button('🔎', this._scriptPanel.showToolTipEntriesHandler);
+    button.title = `Show all ${entries.length} ${name || subtypeName} entries.`
     button.data = entries;
     tr.appendChild(DOM.td(button));
     this.tableNode.appendChild(tr);
diff -r -u --color up/v8/tools/system-analyzer/view/timeline/timeline-overview.mjs nw/v8/tools/system-analyzer/view/timeline/timeline-overview.mjs
--- up/v8/tools/system-analyzer/view/timeline/timeline-overview.mjs	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/system-analyzer/view/timeline/timeline-overview.mjs	2023-01-19 16:46:37.923109107 -0500
@@ -110,7 +110,8 @@
     if (!toolTipContent) {
       toolTipContent = `Time ${formatDurationMicros(timeMicros)}`;
     }
-    this.dispatchEvent(new ToolTipEvent(toolTipContent, this._indicatorNode));
+    this.dispatchEvent(
+        new ToolTipEvent(toolTipContent, this._indicatorNode, e.ctrlKey));
   }
 
   _findLogEntryAtTime(time, maxTimeDistance) {
diff -r -u --color up/v8/tools/system-analyzer/view/timeline/timeline-track-base.mjs nw/v8/tools/system-analyzer/view/timeline/timeline-track-base.mjs
--- up/v8/tools/system-analyzer/view/timeline/timeline-track-base.mjs	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/system-analyzer/view/timeline/timeline-track-base.mjs	2023-01-19 16:46:37.923109107 -0500
@@ -30,7 +30,7 @@
     this.timelineChunks = this.$('#timelineChunks');
     this.timelineSamples = this.$('#timelineSamples');
     this.timelineNode = this.$('#timeline');
-    this.toolTipTargetNode = this.$('#toolTipTarget');
+    this._toolTipTargetNode = undefined;
     this.hitPanelNode = this.$('#hitPanel');
     this.timelineAnnotationsNode = this.$('#timelineAnnotations');
     this.timelineMarkersNode = this.$('#timelineMarkers');
@@ -356,8 +356,8 @@
 
   _updateToolTip(event) {
     if (!this._focusedEntry) return false;
-    this.dispatchEvent(
-        new ToolTipEvent(this._focusedEntry, this.toolTipTargetNode));
+    this.dispatchEvent(new ToolTipEvent(
+        this._focusedEntry, this._toolTipTargetNode, event.ctrlKey));
     event.stopImmediatePropagation();
   }
 
@@ -419,13 +419,27 @@
         (kTimelineHeight - event.layerY) / chunk.height * (chunk.size() - 1));
     if (relativeIndex > chunk.size()) return false;
     const logEntry = chunk.at(relativeIndex);
-    const style = this.toolTipTargetNode.style;
+    const node = this.getToolTipTargetNode(logEntry);
+    if (!node) return logEntry;
+    const style = node.style;
     style.left = `${chunk.index * kChunkWidth}px`;
     style.top = `${kTimelineHeight - chunk.height}px`;
     style.height = `${chunk.height}px`;
     style.width = `${kChunkVisualWidth}px`;
     return logEntry;
   }
+
+  getToolTipTargetNode(logEntry) {
+    let node = this._toolTipTargetNode;
+    if (node) {
+      if (node.logEntry === logEntry) return undefined;
+      node.parentNode.removeChild(node);
+    }
+    node = this._toolTipTargetNode = DOM.div('toolTipTarget');
+    node.logEntry = logEntry;
+    this.$('#cropper').appendChild(node);
+    return node;
+  }
 };
 
 class SelectionHandler {
diff -r -u --color up/v8/tools/system-analyzer/view/timeline/timeline-track-stacked-base.mjs nw/v8/tools/system-analyzer/view/timeline/timeline-track-stacked-base.mjs
--- up/v8/tools/system-analyzer/view/timeline/timeline-track-stacked-base.mjs	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/system-analyzer/view/timeline/timeline-track-stacked-base.mjs	2023-01-19 16:46:37.923109107 -0500
@@ -61,7 +61,9 @@
     const item = this._getDrawableItemForEvent(event);
     const logEntry = this._drawableItemToLogEntry(item);
     if (item === undefined) return undefined;
-    const style = this.toolTipTargetNode.style;
+    const node = this.getToolTipTargetNode(logEntry);
+    if (!node) return logEntry;
+    const style = node.style;
     style.left = `${event.layerX}px`;
     style.top = `${(item.depth + 1) * kItemHeight}px`;
     style.height = `${kItemHeight}px`
diff -r -u --color up/v8/tools/system-analyzer/view/timeline/timeline-track-template.html nw/v8/tools/system-analyzer/view/timeline/timeline-track-template.html
--- up/v8/tools/system-analyzer/view/timeline/timeline-track-template.html	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/system-analyzer/view/timeline/timeline-track-template.html	2023-01-19 16:46:37.923109107 -0500
@@ -40,7 +40,7 @@
     pointer-events: none;
   }
 
-  #toolTipTarget {
+  .toolTipTarget {
     position: absolute;
   }
 
@@ -237,7 +237,6 @@
       </svg>
       <svg id="timelineAnnotations" xmlns="http://www.w3.org/2000/svg" class="dataSized noPointerEvents"></svg>
       <svg id="timelineMarkers" xmlns="http://www.w3.org/2000/svg" class="dataSized noPointerEvents"></svg>
-      <div id="toolTipTarget"></div>
     </div>
     <!-- Use a div element covering all complex items to prevent slow hit test-->
     <div id="hitPanel" class="dataSized"></div>
diff -r -u --color up/v8/tools/system-analyzer/view/tool-tip.mjs nw/v8/tools/system-analyzer/view/tool-tip.mjs
--- up/v8/tools/system-analyzer/view/tool-tip.mjs	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/system-analyzer/view/tool-tip.mjs	2023-01-19 16:46:37.923109107 -0500
@@ -2,137 +2,146 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-import {DOM, V8CustomElement} from './helper.mjs';
+import {Debouncer, DOM, V8CustomElement} from './helper.mjs';
 
-DOM.defineCustomElement(
-    'view/tool-tip', (templateText) => class Tooltip extends V8CustomElement {
-      _targetNode;
-      _content;
-      _isHidden = true;
-
-      constructor() {
-        super(templateText);
-        this._intersectionObserver = new IntersectionObserver((entries) => {
-          if (entries[0].intersectionRatio <= 0) {
-            this.hide();
-          } else {
-            this.show();
-            this.requestUpdate(true);
-          }
-        });
-        document.addEventListener('click', (event) => {
-          // Only hide the tooltip if we click anywhere outside of it.
-          let target = event.target;
-          while (target) {
-            if (target == this) return;
-            target = target.parentNode;
-          }
-          this.hide()
-        });
-      }
-
-      _update() {
-        if (!this._targetNode || this._isHidden) return;
-        const rect = this._targetNode.getBoundingClientRect();
-        rect.x += rect.width / 2;
-        let atRight = this._useRight(rect.x);
-        let atBottom = this._useBottom(rect.y);
-        if (atBottom) rect.y += rect.height;
-        this._setPosition(rect, atRight, atBottom);
-        this.requestUpdate(true);
-      }
-
-      set positionOrTargetNode(positionOrTargetNode) {
-        if (positionOrTargetNode.nodeType === undefined) {
-          this.position = positionOrTargetNode;
-        } else {
-          this.targetNode = positionOrTargetNode;
-        }
-      }
-
-      set targetNode(targetNode) {
-        this._intersectionObserver.disconnect();
-        this._targetNode = targetNode;
-        if (targetNode === undefined) return;
-        if (!(targetNode instanceof SVGElement)) {
-          this._intersectionObserver.observe(targetNode);
-        }
-        this.requestUpdate(true);
-      }
-
-      set position(position) {
-        this._targetNode = undefined;
-        this._setPosition(
-            position, this._useRight(position.x), this._useBottom(position.y));
-      }
-
-      _setPosition(viewportPosition, atRight, atBottom) {
-        const horizontalMode = atRight ? 'right' : 'left';
-        const verticalMode = atBottom ? 'bottom' : 'top';
-        this.bodyNode.className = horizontalMode + ' ' + verticalMode;
-        const pageX = viewportPosition.x + window.scrollX;
-        this.style.left = `${pageX}px`;
-        const pageY = viewportPosition.y + window.scrollY;
-        this.style.top = `${pageY}px`;
-      }
-
-      _useBottom(viewportY) {
-        return viewportY <= 400;
-      }
-
-      _useRight(viewportX) {
-        return viewportX < document.documentElement.clientWidth / 2;
-      }
-
-      set content(content) {
-        if (!content) return this.hide();
+DOM.defineCustomElement('view/tool-tip',
+                        (templateText) =>
+                            class Tooltip extends V8CustomElement {
+  _targetNode;
+  _content;
+  _isHidden = true;
+  _debouncedSetData = new Debouncer((...args) => this._setData(...args), 500)
+
+  constructor() {
+    super(templateText);
+    this._intersectionObserver = new IntersectionObserver((entries) => {
+      if (entries[0].intersectionRatio <= 0) {
+        this.hide();
+      } else {
         this.show();
-        if (this._content === content) return;
-        this._content = content;
-
-        if (typeof content === 'string') {
-          this.contentNode.innerHTML = content;
-          this.contentNode.className = 'textContent';
-        } else if (content?.nodeType && content?.nodeName) {
-          this._setContentNode(content);
-        } else {
-          if (this.contentNode.firstChild?.localName == 'property-link-table') {
-            this.contentNode.firstChild.propertyDict = content;
-          } else {
-            const node = DOM.element('property-link-table');
-            node.instanceLinkButtons = true;
-            node.propertyDict = content;
-            this._setContentNode(node);
-          }
-        }
-      }
-
-      _setContentNode(content) {
-        const newContent = DOM.div();
-        newContent.appendChild(content);
-        this.contentNode.replaceWith(newContent);
-        newContent.id = 'content';
-      }
-
-      hide() {
-        this._content = undefined;
-        if (this._isHidden) return;
-        this._isHidden = true;
-        this.bodyNode.style.display = 'none';
-        this.targetNode = undefined;
-      }
-
-      show() {
-        if (!this._isHidden) return;
-        this.bodyNode.style.display = 'block';
-        this._isHidden = false;
-      }
-
-      get bodyNode() {
-        return this.$('#body');
+        this.requestUpdate(true);
       }
-
-      get contentNode() {
-        return this.$('#content');
+    });
+    document.addEventListener('click', (event) => {
+      // Only hide the tooltip if we click anywhere outside of it.
+      let target = event.target;
+      while (target) {
+        if (target == this) return;
+        target = target.parentNode;
       }
+      this.hide()
     });
+  }
+
+  _update() {
+    if (!this._targetNode || this._isHidden) return;
+    if (!this._targetNode.parentNode) return;
+    const rect = this._targetNode.getBoundingClientRect();
+    rect.x += rect.width / 2;
+    let atRight = this._useRight(rect.x);
+    let atBottom = this._useBottom(rect.y);
+    if (atBottom) rect.y += rect.height;
+    this._setPosition(rect, atRight, atBottom);
+    this.requestUpdate(true);
+  }
+
+  set data({content, positionOrTargetNode, immediate}) {
+    if (immediate) {
+      this._debouncedSetData.callNow(content, positionOrTargetNode)
+    } else {
+      this._debouncedSetData.call(content, positionOrTargetNode)
+    }
+  }
+
+  _setData(content, positionOrTargetNode) {
+    if (positionOrTargetNode.nodeType === undefined) {
+      this._targetNode = undefined;
+      const position = positionOrTargetNode;
+      this._setPosition(
+          position, this._useRight(position.x), this._useBottom(position.y));
+    } else {
+      this._setTargetNode(positionOrTargetNode);
+    }
+    this._setContent(content);
+  }
+
+  _setTargetNode(targetNode) {
+    this._intersectionObserver.disconnect();
+    this._targetNode = targetNode;
+    if (targetNode === undefined) return;
+    if (!(targetNode instanceof SVGElement)) {
+      this._intersectionObserver.observe(targetNode);
+    }
+    this.requestUpdate(true);
+  }
+
+  _setPosition(viewportPosition, atRight, atBottom) {
+    const horizontalMode = atRight ? 'right' : 'left';
+    const verticalMode = atBottom ? 'bottom' : 'top';
+    this.bodyNode.className = horizontalMode + ' ' + verticalMode;
+    const pageX = viewportPosition.x + window.scrollX;
+    this.style.left = `${pageX}px`;
+    const pageY = viewportPosition.y + window.scrollY;
+    this.style.top = `${pageY}px`;
+  }
+
+  _useBottom(viewportY) {
+    return viewportY <= 400;
+  }
+
+  _useRight(viewportX) {
+    return viewportX < document.documentElement.clientWidth / 2;
+  }
+
+  _setContent(content) {
+    if (!content) return this.hide();
+    this.show();
+    if (this._content === content) return;
+    this._content = content;
+
+    if (typeof content === 'string') {
+      this.contentNode.innerHTML = content;
+      this.contentNode.className = 'textContent';
+    } else if (content?.nodeType && content?.nodeName) {
+      this._setContentNode(content);
+    } else {
+      if (this.contentNode.firstChild?.localName == 'property-link-table') {
+        this.contentNode.firstChild.propertyDict = content;
+      } else {
+        const node = DOM.element('property-link-table');
+        node.instanceLinkButtons = true;
+        node.propertyDict = content;
+        this._setContentNode(node);
+      }
+    }
+  }
+
+  _setContentNode(content) {
+    const newContent = DOM.div();
+    newContent.appendChild(content);
+    this.contentNode.replaceWith(newContent);
+    newContent.id = 'content';
+  }
+
+  hide() {
+    this._content = undefined;
+    if (this._isHidden) return;
+    this._isHidden = true;
+    this.bodyNode.style.display = 'none';
+    this.targetNode = undefined;
+  }
+
+  show() {
+    if (!this._isHidden) return;
+    this.bodyNode.style.display = 'block';
+    this._isHidden = false;
+  }
+
+  get bodyNode() {
+    return this.$('#body');
+  }
+
+  get contentNode() {
+    return this.$('#content');
+  }
+});
diff -r -u --color up/v8/tools/testrunner/base_runner.py nw/v8/tools/testrunner/base_runner.py
--- up/v8/tools/testrunner/base_runner.py	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/testrunner/base_runner.py	2023-01-19 16:46:37.923109107 -0500
@@ -547,6 +547,10 @@
             sys.byteorder,
         "cfi_vptr":
             self.build_config.cfi_vptr,
+        "component_build":
+            self.build_config.component_build,
+        "conservative_stack_scanning":
+            self.build_config.conservative_stack_scanning,
         "control_flow_integrity":
             self.build_config.control_flow_integrity,
         "concurrent_marking":
diff -r -u --color up/v8/tools/testrunner/build_config.py nw/v8/tools/testrunner/build_config.py
--- up/v8/tools/testrunner/build_config.py	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/testrunner/build_config.py	2023-01-19 16:46:37.923109107 -0500
@@ -23,6 +23,9 @@
 
     self.asan = build_config['is_asan']
     self.cfi_vptr = build_config['is_cfi']
+    self.component_build = build_config['is_component_build']
+    self.conservative_stack_scanning = build_config[
+        'v8_enable_conservative_stack_scanning']
     self.control_flow_integrity = build_config['v8_control_flow_integrity']
     self.concurrent_marking = build_config['v8_enable_concurrent_marking']
     self.single_generation = build_config['v8_enable_single_generation']
diff -r -u --color up/v8/tools/testrunner/testdata/testroot1/out/build/v8_build_config.json nw/v8/tools/testrunner/testdata/testroot1/out/build/v8_build_config.json
--- up/v8/tools/testrunner/testdata/testroot1/out/build/v8_build_config.json	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/testrunner/testdata/testroot1/out/build/v8_build_config.json	2023-01-19 16:46:37.923109107 -0500
@@ -17,6 +17,7 @@
   "v8_enable_i18n_support": true,
   "v8_enable_verify_predictable": false,
   "v8_target_cpu": "x64",
+  "v8_enable_conservative_stack_scanning": false,
   "v8_enable_concurrent_marking": true,
   "v8_enable_verify_csa": false,
   "v8_enable_lite_mode": false,
diff -r -u --color up/v8/tools/testrunner/testdata/testroot2/out/build/v8_build_config.json nw/v8/tools/testrunner/testdata/testroot2/out/build/v8_build_config.json
--- up/v8/tools/testrunner/testdata/testroot2/out/build/v8_build_config.json	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/testrunner/testdata/testroot2/out/build/v8_build_config.json	2023-01-19 16:46:37.923109107 -0500
@@ -17,6 +17,7 @@
   "v8_enable_i18n_support": true,
   "v8_enable_verify_predictable": false,
   "v8_target_cpu": "x64",
+  "v8_enable_conservative_stack_scanning": false,
   "v8_enable_concurrent_marking": true,
   "v8_enable_verify_csa": false,
   "v8_enable_lite_mode": false,
diff -r -u --color up/v8/tools/testrunner/testdata/testroot3/out/build/v8_build_config.json nw/v8/tools/testrunner/testdata/testroot3/out/build/v8_build_config.json
--- up/v8/tools/testrunner/testdata/testroot3/out/build/v8_build_config.json	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/testrunner/testdata/testroot3/out/build/v8_build_config.json	2023-01-19 16:46:37.923109107 -0500
@@ -17,6 +17,7 @@
   "v8_enable_i18n_support": true,
   "v8_enable_verify_predictable": false,
   "v8_target_cpu": "x64",
+  "v8_enable_conservative_stack_scanning": false,
   "v8_enable_concurrent_marking": true,
   "v8_enable_verify_csa": false,
   "v8_enable_lite_mode": false,
diff -r -u --color up/v8/tools/testrunner/testdata/testroot5/out.gn/build/v8_build_config.json nw/v8/tools/testrunner/testdata/testroot5/out.gn/build/v8_build_config.json
--- up/v8/tools/testrunner/testdata/testroot5/out.gn/build/v8_build_config.json	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/testrunner/testdata/testroot5/out.gn/build/v8_build_config.json	2023-01-19 16:46:37.923109107 -0500
@@ -17,6 +17,7 @@
   "v8_enable_i18n_support": true,
   "v8_enable_verify_predictable": false,
   "v8_target_cpu": "x64",
+  "v8_enable_conservative_stack_scanning": false,
   "v8_enable_concurrent_marking": true,
   "v8_enable_verify_csa": false,
   "v8_enable_lite_mode": false,
diff -r -u --color up/v8/tools/testrunner/testdata/testroot6/out/build/v8_build_config.json nw/v8/tools/testrunner/testdata/testroot6/out/build/v8_build_config.json
--- up/v8/tools/testrunner/testdata/testroot6/out/build/v8_build_config.json	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/testrunner/testdata/testroot6/out/build/v8_build_config.json	2023-01-19 16:46:37.933942436 -0500
@@ -17,6 +17,7 @@
   "v8_enable_i18n_support": true,
   "v8_enable_verify_predictable": false,
   "v8_target_cpu": "x64",
+  "v8_enable_conservative_stack_scanning": false,
   "v8_enable_concurrent_marking": true,
   "v8_enable_verify_csa": false,
   "v8_enable_lite_mode": false,
diff -r -u --color up/v8/tools/testrunner/testproc/fuzzer.py nw/v8/tools/testrunner/testproc/fuzzer.py
--- up/v8/tools/testrunner/testproc/fuzzer.py	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/testrunner/testproc/fuzzer.py	2023-01-19 16:46:37.933942436 -0500
@@ -13,7 +13,6 @@
     (0.1, '--assert-types'),
     (0.1, '--interrupt-budget-for-feedback-allocation=0'),
     (0.1, '--cache=code'),
-    (0.25, '--compact-maps'),
     (0.1, '--force-slow-path'),
     (0.2, '--future'),
     (0.1, '--interrupt-budget=100'),
@@ -39,7 +38,6 @@
     (0.1, '--no-liftoff'),
     (0.1, '--no-turbofan'),
     (0.2, '--no-regexp-tier-up'),
-    (0.25, '--no-use-map-space'),
     (0.1, '--no-wasm-tier-up'),
     (0.1, '--regexp-interpret-all'),
     (0.1, '--regexp-tier-up-ticks=10'),
diff -r -u --color up/v8/tools/testrunner/testproc/progress.py nw/v8/tools/testrunner/testproc/progress.py
--- up/v8/tools/testrunner/testproc/progress.py	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/testrunner/testproc/progress.py	2023-01-19 16:46:37.933942436 -0500
@@ -6,7 +6,7 @@
 from . import base
 from testrunner.local import utils
 from testrunner.testproc.indicators import JsonTestProgressIndicator, PROGRESS_INDICATORS
-from testrunner.testproc.resultdb import ResultDBIndicator
+from testrunner.testproc.resultdb import rdb_sink, ResultDBIndicator
 
 
 class ResultsTracker(base.TestProcObserver):
@@ -67,7 +67,9 @@
           0,
           JsonTestProgressIndicator(context, options, test_count,
                                     framework_name))
-    self.procs.append(ResultDBIndicator(context, options, test_count))
+    sink = rdb_sink()
+    if sink:
+      self.procs.append(ResultDBIndicator(context, options, test_count, sink))
     self._requirement = max(proc._requirement for proc in self.procs)
 
   def _on_result_for(self, test, result):
diff -r -u --color up/v8/tools/testrunner/testproc/resultdb.py nw/v8/tools/testrunner/testproc/resultdb.py
--- up/v8/tools/testrunner/testproc/resultdb.py	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/testrunner/testproc/resultdb.py	2023-01-19 16:46:37.933942436 -0500
@@ -5,7 +5,6 @@
 import json
 import logging
 import pprint
-import requests
 import os
 
 from . import base
@@ -22,10 +21,10 @@
 
 class ResultDBIndicator(ProgressIndicator):
 
-  def __init__(self, context, options, test_count):
+  def __init__(self, context, options, test_count, sink):
     super(ResultDBIndicator, self).__init__(context, options, test_count)
     self._requirement = base.DROP_PASS_OUTPUT
-    self.rpc = ResultDB_RPC()
+    self.rpc = ResultDB_RPC(sink)
 
   def on_test_result(self, test, result):
     for run, sub_result in enumerate(result.as_list):
@@ -57,24 +56,33 @@
     self.rpc.send(rdb_result)
 
 
+def rdb_sink():
+  try:
+    import requests
+  except:
+    log_instantiation_failure('Failed to import requests module.')
+    return None
+  luci_context = os.environ.get('LUCI_CONTEXT')
+  if not luci_context:
+    log_instantiation_failure('No LUCI_CONTEXT found.')
+    return None
+  with open(luci_context, mode="r", encoding="utf-8") as f:
+    config = json.load(f)
+  sink = config.get('result_sink', None)
+  if not sink:
+    log_instantiation_failure('No ResultDB sink found.')
+    return None
+  return sink
+
+
+def log_instantiation_failure(error_message):
+  logging.info(f'{error_message} No results will be sent to ResultDB.')
+
+
 class ResultDB_RPC:
 
-  def __init__(self):
-    self.session = None
-    luci_context = os.environ.get('LUCI_CONTEXT')
-    # TODO(liviurau): use a factory method and return None in absence of
-    # necessary context.
-    if not luci_context:
-      logging.warning(
-          f'No LUCI_CONTEXT found. No results will be sent to ResutDB.')
-      return
-    with open(luci_context, mode="r", encoding="utf-8") as f:
-      config = json.load(f)
-    sink = config.get('result_sink', None)
-    if not sink:
-      logging.warning(
-          f'No ResultDB sink found. No results will be sent to ResutDB.')
-      return
+  def __init__(self, sink):
+    import requests
     self.session = requests.Session()
     self.session.headers = {
         'Authorization': f'ResultSink {sink.get("auth_token")}',
@@ -82,14 +90,12 @@
     self.url = f'http://{sink.get("address")}/prpc/luci.resultsink.v1.Sink/ReportTestResults'
 
   def send(self, result):
-    if self.session:
-      payload = dict(testResults=[result])
-      try:
-        self.session.post(self.url, json=payload).raise_for_status()
-      except Exception as e:
-        logging.error(f'Request failed: {payload}')
-        raise e
+    payload = dict(testResults=[result])
+    try:
+      self.session.post(self.url, json=payload).raise_for_status()
+    except Exception as e:
+      logging.error(f'Request failed: {payload}')
+      raise e
 
   def __del__(self):
-    if self.session:
-      self.session.close()
+    self.session.close()
diff -r -u --color up/v8/tools/v8heapconst.py nw/v8/tools/v8heapconst.py
--- up/v8/tools/v8heapconst.py	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/v8heapconst.py	2023-01-19 16:46:37.955609099 -0500
@@ -59,59 +59,59 @@
   148: "ASYNC_GENERATOR_REQUEST_TYPE",
   149: "BREAK_POINT_TYPE",
   150: "BREAK_POINT_INFO_TYPE",
-  151: "CACHED_TEMPLATE_OBJECT_TYPE",
-  152: "CALL_SITE_INFO_TYPE",
-  153: "CLASS_POSITIONS_TYPE",
-  154: "DEBUG_INFO_TYPE",
-  155: "ENUM_CACHE_TYPE",
-  156: "ERROR_STACK_DATA_TYPE",
-  157: "FEEDBACK_CELL_TYPE",
-  158: "FUNCTION_TEMPLATE_RARE_DATA_TYPE",
-  159: "INTERCEPTOR_INFO_TYPE",
-  160: "INTERPRETER_DATA_TYPE",
-  161: "MODULE_REQUEST_TYPE",
-  162: "PROMISE_CAPABILITY_TYPE",
-  163: "PROMISE_ON_STACK_TYPE",
-  164: "PROMISE_REACTION_TYPE",
-  165: "PROPERTY_DESCRIPTOR_OBJECT_TYPE",
-  166: "PROTOTYPE_INFO_TYPE",
-  167: "REG_EXP_BOILERPLATE_DESCRIPTION_TYPE",
-  168: "SCRIPT_TYPE",
-  169: "SCRIPT_OR_MODULE_TYPE",
-  170: "SOURCE_TEXT_MODULE_INFO_ENTRY_TYPE",
-  171: "STACK_FRAME_INFO_TYPE",
-  172: "TEMPLATE_OBJECT_DESCRIPTION_TYPE",
-  173: "TUPLE2_TYPE",
-  174: "WASM_EXCEPTION_TAG_TYPE",
-  175: "WASM_INDIRECT_FUNCTION_TABLE_TYPE",
-  176: "FIXED_ARRAY_TYPE",
-  177: "HASH_TABLE_TYPE",
-  178: "EPHEMERON_HASH_TABLE_TYPE",
-  179: "GLOBAL_DICTIONARY_TYPE",
-  180: "NAME_DICTIONARY_TYPE",
-  181: "NAME_TO_INDEX_HASH_TABLE_TYPE",
-  182: "NUMBER_DICTIONARY_TYPE",
-  183: "ORDERED_HASH_MAP_TYPE",
-  184: "ORDERED_HASH_SET_TYPE",
-  185: "ORDERED_NAME_DICTIONARY_TYPE",
-  186: "REGISTERED_SYMBOL_TABLE_TYPE",
-  187: "SIMPLE_NUMBER_DICTIONARY_TYPE",
-  188: "CLOSURE_FEEDBACK_CELL_ARRAY_TYPE",
-  189: "OBJECT_BOILERPLATE_DESCRIPTION_TYPE",
-  190: "SCRIPT_CONTEXT_TABLE_TYPE",
-  191: "BYTE_ARRAY_TYPE",
-  192: "BYTECODE_ARRAY_TYPE",
-  193: "FIXED_DOUBLE_ARRAY_TYPE",
-  194: "INTERNAL_CLASS_WITH_SMI_ELEMENTS_TYPE",
-  195: "SLOPPY_ARGUMENTS_ELEMENTS_TYPE",
-  196: "TURBOFAN_BITSET_TYPE_TYPE",
-  197: "TURBOFAN_HEAP_CONSTANT_TYPE_TYPE",
-  198: "TURBOFAN_OTHER_NUMBER_CONSTANT_TYPE_TYPE",
-  199: "TURBOFAN_RANGE_TYPE_TYPE",
-  200: "TURBOFAN_UNION_TYPE_TYPE",
-  201: "EXPORTED_SUB_CLASS_BASE_TYPE",
-  202: "EXPORTED_SUB_CLASS_TYPE",
-  203: "EXPORTED_SUB_CLASS2_TYPE",
+  151: "CALL_SITE_INFO_TYPE",
+  152: "CLASS_POSITIONS_TYPE",
+  153: "DEBUG_INFO_TYPE",
+  154: "ENUM_CACHE_TYPE",
+  155: "ERROR_STACK_DATA_TYPE",
+  156: "FEEDBACK_CELL_TYPE",
+  157: "FUNCTION_TEMPLATE_RARE_DATA_TYPE",
+  158: "INTERCEPTOR_INFO_TYPE",
+  159: "INTERPRETER_DATA_TYPE",
+  160: "MODULE_REQUEST_TYPE",
+  161: "PROMISE_CAPABILITY_TYPE",
+  162: "PROMISE_ON_STACK_TYPE",
+  163: "PROMISE_REACTION_TYPE",
+  164: "PROPERTY_DESCRIPTOR_OBJECT_TYPE",
+  165: "PROTOTYPE_INFO_TYPE",
+  166: "REG_EXP_BOILERPLATE_DESCRIPTION_TYPE",
+  167: "SCRIPT_TYPE",
+  168: "SCRIPT_OR_MODULE_TYPE",
+  169: "SOURCE_TEXT_MODULE_INFO_ENTRY_TYPE",
+  170: "STACK_FRAME_INFO_TYPE",
+  171: "TEMPLATE_OBJECT_DESCRIPTION_TYPE",
+  172: "TUPLE2_TYPE",
+  173: "WASM_EXCEPTION_TAG_TYPE",
+  174: "WASM_INDIRECT_FUNCTION_TABLE_TYPE",
+  175: "FIXED_ARRAY_TYPE",
+  176: "HASH_TABLE_TYPE",
+  177: "EPHEMERON_HASH_TABLE_TYPE",
+  178: "GLOBAL_DICTIONARY_TYPE",
+  179: "NAME_DICTIONARY_TYPE",
+  180: "NAME_TO_INDEX_HASH_TABLE_TYPE",
+  181: "NUMBER_DICTIONARY_TYPE",
+  182: "ORDERED_HASH_MAP_TYPE",
+  183: "ORDERED_HASH_SET_TYPE",
+  184: "ORDERED_NAME_DICTIONARY_TYPE",
+  185: "REGISTERED_SYMBOL_TABLE_TYPE",
+  186: "SIMPLE_NUMBER_DICTIONARY_TYPE",
+  187: "CLOSURE_FEEDBACK_CELL_ARRAY_TYPE",
+  188: "OBJECT_BOILERPLATE_DESCRIPTION_TYPE",
+  189: "SCRIPT_CONTEXT_TABLE_TYPE",
+  190: "BYTE_ARRAY_TYPE",
+  191: "BYTECODE_ARRAY_TYPE",
+  192: "FIXED_DOUBLE_ARRAY_TYPE",
+  193: "INTERNAL_CLASS_WITH_SMI_ELEMENTS_TYPE",
+  194: "SLOPPY_ARGUMENTS_ELEMENTS_TYPE",
+  195: "TURBOFAN_BITSET_TYPE_TYPE",
+  196: "TURBOFAN_HEAP_CONSTANT_TYPE_TYPE",
+  197: "TURBOFAN_OTHER_NUMBER_CONSTANT_TYPE_TYPE",
+  198: "TURBOFAN_RANGE_TYPE_TYPE",
+  199: "TURBOFAN_UNION_TYPE_TYPE",
+  200: "UNCOMPILED_DATA_WITH_PREPARSE_DATA_TYPE",
+  201: "UNCOMPILED_DATA_WITH_PREPARSE_DATA_AND_JOB_TYPE",
+  202: "UNCOMPILED_DATA_WITHOUT_PREPARSE_DATA_TYPE",
+  203: "UNCOMPILED_DATA_WITHOUT_PREPARSE_DATA_WITH_JOB_TYPE",
   204: "FOREIGN_TYPE",
   205: "AWAIT_CONTEXT_TYPE",
   206: "BLOCK_CONTEXT_TYPE",
@@ -123,65 +123,64 @@
   212: "NATIVE_CONTEXT_TYPE",
   213: "SCRIPT_CONTEXT_TYPE",
   214: "WITH_CONTEXT_TYPE",
-  215: "UNCOMPILED_DATA_WITH_PREPARSE_DATA_TYPE",
-  216: "UNCOMPILED_DATA_WITH_PREPARSE_DATA_AND_JOB_TYPE",
-  217: "UNCOMPILED_DATA_WITHOUT_PREPARSE_DATA_TYPE",
-  218: "UNCOMPILED_DATA_WITHOUT_PREPARSE_DATA_WITH_JOB_TYPE",
-  219: "WASM_FUNCTION_DATA_TYPE",
-  220: "WASM_CAPI_FUNCTION_DATA_TYPE",
-  221: "WASM_EXPORTED_FUNCTION_DATA_TYPE",
-  222: "WASM_JS_FUNCTION_DATA_TYPE",
-  223: "SMALL_ORDERED_HASH_MAP_TYPE",
-  224: "SMALL_ORDERED_HASH_SET_TYPE",
-  225: "SMALL_ORDERED_NAME_DICTIONARY_TYPE",
-  226: "ABSTRACT_INTERNAL_CLASS_SUBCLASS1_TYPE",
-  227: "ABSTRACT_INTERNAL_CLASS_SUBCLASS2_TYPE",
-  228: "DESCRIPTOR_ARRAY_TYPE",
-  229: "STRONG_DESCRIPTOR_ARRAY_TYPE",
-  230: "SOURCE_TEXT_MODULE_TYPE",
-  231: "SYNTHETIC_MODULE_TYPE",
-  232: "WEAK_FIXED_ARRAY_TYPE",
-  233: "TRANSITION_ARRAY_TYPE",
-  234: "ACCESSOR_INFO_TYPE",
-  235: "CALL_HANDLER_INFO_TYPE",
-  236: "CELL_TYPE",
-  237: "CODE_TYPE",
-  238: "CODE_DATA_CONTAINER_TYPE",
-  239: "COVERAGE_INFO_TYPE",
-  240: "EMBEDDER_DATA_ARRAY_TYPE",
-  241: "FEEDBACK_METADATA_TYPE",
-  242: "FEEDBACK_VECTOR_TYPE",
-  243: "FILLER_TYPE",
-  244: "FREE_SPACE_TYPE",
-  245: "INTERNAL_CLASS_TYPE",
-  246: "INTERNAL_CLASS_WITH_STRUCT_ELEMENTS_TYPE",
-  247: "MAP_TYPE",
-  248: "MEGA_DOM_HANDLER_TYPE",
-  249: "ON_HEAP_BASIC_BLOCK_PROFILER_DATA_TYPE",
-  250: "PREPARSE_DATA_TYPE",
-  251: "PROPERTY_ARRAY_TYPE",
-  252: "PROPERTY_CELL_TYPE",
-  253: "SCOPE_INFO_TYPE",
-  254: "SHARED_FUNCTION_INFO_TYPE",
-  255: "SMI_BOX_TYPE",
-  256: "SMI_PAIR_TYPE",
-  257: "SORT_STATE_TYPE",
-  258: "SWISS_NAME_DICTIONARY_TYPE",
-  259: "WASM_API_FUNCTION_REF_TYPE",
-  260: "WASM_CONTINUATION_OBJECT_TYPE",
-  261: "WASM_INTERNAL_FUNCTION_TYPE",
-  262: "WASM_RESUME_DATA_TYPE",
-  263: "WASM_STRING_VIEW_ITER_TYPE",
-  264: "WASM_TYPE_INFO_TYPE",
-  265: "WEAK_ARRAY_LIST_TYPE",
-  266: "WEAK_CELL_TYPE",
-  267: "WASM_ARRAY_TYPE",
-  268: "WASM_STRUCT_TYPE",
-  269: "JS_PROXY_TYPE",
+  215: "WASM_FUNCTION_DATA_TYPE",
+  216: "WASM_CAPI_FUNCTION_DATA_TYPE",
+  217: "WASM_EXPORTED_FUNCTION_DATA_TYPE",
+  218: "WASM_JS_FUNCTION_DATA_TYPE",
+  219: "EXPORTED_SUB_CLASS_BASE_TYPE",
+  220: "EXPORTED_SUB_CLASS_TYPE",
+  221: "EXPORTED_SUB_CLASS2_TYPE",
+  222: "SMALL_ORDERED_HASH_MAP_TYPE",
+  223: "SMALL_ORDERED_HASH_SET_TYPE",
+  224: "SMALL_ORDERED_NAME_DICTIONARY_TYPE",
+  225: "ABSTRACT_INTERNAL_CLASS_SUBCLASS1_TYPE",
+  226: "ABSTRACT_INTERNAL_CLASS_SUBCLASS2_TYPE",
+  227: "DESCRIPTOR_ARRAY_TYPE",
+  228: "STRONG_DESCRIPTOR_ARRAY_TYPE",
+  229: "SOURCE_TEXT_MODULE_TYPE",
+  230: "SYNTHETIC_MODULE_TYPE",
+  231: "WEAK_FIXED_ARRAY_TYPE",
+  232: "TRANSITION_ARRAY_TYPE",
+  233: "ACCESSOR_INFO_TYPE",
+  234: "CALL_HANDLER_INFO_TYPE",
+  235: "CELL_TYPE",
+  236: "CODE_TYPE",
+  237: "CODE_DATA_CONTAINER_TYPE",
+  238: "COVERAGE_INFO_TYPE",
+  239: "EMBEDDER_DATA_ARRAY_TYPE",
+  240: "FEEDBACK_METADATA_TYPE",
+  241: "FEEDBACK_VECTOR_TYPE",
+  242: "FILLER_TYPE",
+  243: "FREE_SPACE_TYPE",
+  244: "INTERNAL_CLASS_TYPE",
+  245: "INTERNAL_CLASS_WITH_STRUCT_ELEMENTS_TYPE",
+  246: "MAP_TYPE",
+  247: "MEGA_DOM_HANDLER_TYPE",
+  248: "ON_HEAP_BASIC_BLOCK_PROFILER_DATA_TYPE",
+  249: "PREPARSE_DATA_TYPE",
+  250: "PROPERTY_ARRAY_TYPE",
+  251: "PROPERTY_CELL_TYPE",
+  252: "SCOPE_INFO_TYPE",
+  253: "SHARED_FUNCTION_INFO_TYPE",
+  254: "SMI_BOX_TYPE",
+  255: "SMI_PAIR_TYPE",
+  256: "SORT_STATE_TYPE",
+  257: "SWISS_NAME_DICTIONARY_TYPE",
+  258: "WASM_API_FUNCTION_REF_TYPE",
+  259: "WASM_CONTINUATION_OBJECT_TYPE",
+  260: "WASM_INTERNAL_FUNCTION_TYPE",
+  261: "WASM_RESUME_DATA_TYPE",
+  262: "WASM_STRING_VIEW_ITER_TYPE",
+  263: "WASM_TYPE_INFO_TYPE",
+  264: "WEAK_ARRAY_LIST_TYPE",
+  265: "WEAK_CELL_TYPE",
+  266: "WASM_ARRAY_TYPE",
+  267: "WASM_STRUCT_TYPE",
+  268: "JS_PROXY_TYPE",
   1057: "JS_OBJECT_TYPE",
-  270: "JS_GLOBAL_OBJECT_TYPE",
-  271: "JS_GLOBAL_PROXY_TYPE",
-  272: "JS_MODULE_NAMESPACE_TYPE",
+  269: "JS_GLOBAL_OBJECT_TYPE",
+  270: "JS_GLOBAL_PROXY_TYPE",
+  271: "JS_MODULE_NAMESPACE_TYPE",
   1040: "JS_SPECIAL_API_OBJECT_TYPE",
   1041: "JS_PRIMITIVE_WRAPPER_TYPE",
   1058: "JS_API_OBJECT_TYPE",
@@ -286,88 +285,88 @@
 
 # List of known V8 maps.
 KNOWN_MAPS = {
-    ("read_only_space", 0x02141): (247, "MetaMap"),
+    ("read_only_space", 0x02141): (246, "MetaMap"),
     ("read_only_space", 0x02169): (131, "NullMap"),
-    ("read_only_space", 0x02191): (229, "StrongDescriptorArrayMap"),
-    ("read_only_space", 0x021b9): (265, "WeakArrayListMap"),
-    ("read_only_space", 0x021fd): (155, "EnumCacheMap"),
-    ("read_only_space", 0x02231): (176, "FixedArrayMap"),
+    ("read_only_space", 0x02191): (228, "StrongDescriptorArrayMap"),
+    ("read_only_space", 0x021b9): (264, "WeakArrayListMap"),
+    ("read_only_space", 0x021fd): (154, "EnumCacheMap"),
+    ("read_only_space", 0x02231): (175, "FixedArrayMap"),
     ("read_only_space", 0x0227d): (8, "OneByteInternalizedStringMap"),
-    ("read_only_space", 0x022c9): (244, "FreeSpaceMap"),
-    ("read_only_space", 0x022f1): (243, "OnePointerFillerMap"),
-    ("read_only_space", 0x02319): (243, "TwoPointerFillerMap"),
+    ("read_only_space", 0x022c9): (243, "FreeSpaceMap"),
+    ("read_only_space", 0x022f1): (242, "OnePointerFillerMap"),
+    ("read_only_space", 0x02319): (242, "TwoPointerFillerMap"),
     ("read_only_space", 0x02341): (131, "UninitializedMap"),
     ("read_only_space", 0x023b9): (131, "UndefinedMap"),
     ("read_only_space", 0x023fd): (130, "HeapNumberMap"),
     ("read_only_space", 0x02431): (131, "TheHoleMap"),
     ("read_only_space", 0x02491): (131, "BooleanMap"),
-    ("read_only_space", 0x02535): (191, "ByteArrayMap"),
-    ("read_only_space", 0x0255d): (176, "FixedCOWArrayMap"),
-    ("read_only_space", 0x02585): (177, "HashTableMap"),
+    ("read_only_space", 0x02535): (190, "ByteArrayMap"),
+    ("read_only_space", 0x0255d): (175, "FixedCOWArrayMap"),
+    ("read_only_space", 0x02585): (176, "HashTableMap"),
     ("read_only_space", 0x025ad): (128, "SymbolMap"),
     ("read_only_space", 0x025d5): (40, "OneByteStringMap"),
-    ("read_only_space", 0x025fd): (253, "ScopeInfoMap"),
-    ("read_only_space", 0x02625): (254, "SharedFunctionInfoMap"),
-    ("read_only_space", 0x0264d): (237, "CodeMap"),
-    ("read_only_space", 0x02675): (236, "CellMap"),
-    ("read_only_space", 0x0269d): (252, "GlobalPropertyCellMap"),
+    ("read_only_space", 0x025fd): (252, "ScopeInfoMap"),
+    ("read_only_space", 0x02625): (253, "SharedFunctionInfoMap"),
+    ("read_only_space", 0x0264d): (236, "CodeMap"),
+    ("read_only_space", 0x02675): (235, "CellMap"),
+    ("read_only_space", 0x0269d): (251, "GlobalPropertyCellMap"),
     ("read_only_space", 0x026c5): (204, "ForeignMap"),
-    ("read_only_space", 0x026ed): (233, "TransitionArrayMap"),
+    ("read_only_space", 0x026ed): (232, "TransitionArrayMap"),
     ("read_only_space", 0x02715): (45, "ThinOneByteStringMap"),
-    ("read_only_space", 0x0273d): (242, "FeedbackVectorMap"),
+    ("read_only_space", 0x0273d): (241, "FeedbackVectorMap"),
     ("read_only_space", 0x02775): (131, "ArgumentsMarkerMap"),
     ("read_only_space", 0x027d5): (131, "ExceptionMap"),
     ("read_only_space", 0x02831): (131, "TerminationExceptionMap"),
     ("read_only_space", 0x02899): (131, "OptimizedOutMap"),
     ("read_only_space", 0x028f9): (131, "StaleRegisterMap"),
-    ("read_only_space", 0x02959): (190, "ScriptContextTableMap"),
-    ("read_only_space", 0x02981): (188, "ClosureFeedbackCellArrayMap"),
-    ("read_only_space", 0x029a9): (241, "FeedbackMetadataArrayMap"),
-    ("read_only_space", 0x029d1): (176, "ArrayListMap"),
+    ("read_only_space", 0x02959): (189, "ScriptContextTableMap"),
+    ("read_only_space", 0x02981): (187, "ClosureFeedbackCellArrayMap"),
+    ("read_only_space", 0x029a9): (240, "FeedbackMetadataArrayMap"),
+    ("read_only_space", 0x029d1): (175, "ArrayListMap"),
     ("read_only_space", 0x029f9): (129, "BigIntMap"),
-    ("read_only_space", 0x02a21): (189, "ObjectBoilerplateDescriptionMap"),
-    ("read_only_space", 0x02a49): (192, "BytecodeArrayMap"),
-    ("read_only_space", 0x02a71): (238, "CodeDataContainerMap"),
-    ("read_only_space", 0x02a99): (239, "CoverageInfoMap"),
-    ("read_only_space", 0x02ac1): (193, "FixedDoubleArrayMap"),
-    ("read_only_space", 0x02ae9): (179, "GlobalDictionaryMap"),
-    ("read_only_space", 0x02b11): (157, "ManyClosuresCellMap"),
-    ("read_only_space", 0x02b39): (248, "MegaDomHandlerMap"),
-    ("read_only_space", 0x02b61): (176, "ModuleInfoMap"),
-    ("read_only_space", 0x02b89): (180, "NameDictionaryMap"),
-    ("read_only_space", 0x02bb1): (157, "NoClosuresCellMap"),
-    ("read_only_space", 0x02bd9): (182, "NumberDictionaryMap"),
-    ("read_only_space", 0x02c01): (157, "OneClosureCellMap"),
-    ("read_only_space", 0x02c29): (183, "OrderedHashMapMap"),
-    ("read_only_space", 0x02c51): (184, "OrderedHashSetMap"),
-    ("read_only_space", 0x02c79): (181, "NameToIndexHashTableMap"),
-    ("read_only_space", 0x02ca1): (186, "RegisteredSymbolTableMap"),
-    ("read_only_space", 0x02cc9): (185, "OrderedNameDictionaryMap"),
-    ("read_only_space", 0x02cf1): (250, "PreparseDataMap"),
-    ("read_only_space", 0x02d19): (251, "PropertyArrayMap"),
-    ("read_only_space", 0x02d41): (234, "AccessorInfoMap"),
-    ("read_only_space", 0x02d69): (235, "SideEffectCallHandlerInfoMap"),
-    ("read_only_space", 0x02d91): (235, "SideEffectFreeCallHandlerInfoMap"),
-    ("read_only_space", 0x02db9): (235, "NextCallSideEffectFreeCallHandlerInfoMap"),
-    ("read_only_space", 0x02de1): (187, "SimpleNumberDictionaryMap"),
-    ("read_only_space", 0x02e09): (223, "SmallOrderedHashMapMap"),
-    ("read_only_space", 0x02e31): (224, "SmallOrderedHashSetMap"),
-    ("read_only_space", 0x02e59): (225, "SmallOrderedNameDictionaryMap"),
-    ("read_only_space", 0x02e81): (230, "SourceTextModuleMap"),
-    ("read_only_space", 0x02ea9): (258, "SwissNameDictionaryMap"),
-    ("read_only_space", 0x02ed1): (231, "SyntheticModuleMap"),
-    ("read_only_space", 0x02ef9): (259, "WasmApiFunctionRefMap"),
-    ("read_only_space", 0x02f21): (220, "WasmCapiFunctionDataMap"),
-    ("read_only_space", 0x02f49): (221, "WasmExportedFunctionDataMap"),
-    ("read_only_space", 0x02f71): (261, "WasmInternalFunctionMap"),
-    ("read_only_space", 0x02f99): (222, "WasmJSFunctionDataMap"),
-    ("read_only_space", 0x02fc1): (262, "WasmResumeDataMap"),
-    ("read_only_space", 0x02fe9): (264, "WasmTypeInfoMap"),
-    ("read_only_space", 0x03011): (260, "WasmContinuationObjectMap"),
-    ("read_only_space", 0x03039): (232, "WeakFixedArrayMap"),
-    ("read_only_space", 0x03061): (178, "EphemeronHashTableMap"),
-    ("read_only_space", 0x03089): (240, "EmbedderDataArrayMap"),
-    ("read_only_space", 0x030b1): (266, "WeakCellMap"),
+    ("read_only_space", 0x02a21): (188, "ObjectBoilerplateDescriptionMap"),
+    ("read_only_space", 0x02a49): (191, "BytecodeArrayMap"),
+    ("read_only_space", 0x02a71): (237, "CodeDataContainerMap"),
+    ("read_only_space", 0x02a99): (238, "CoverageInfoMap"),
+    ("read_only_space", 0x02ac1): (192, "FixedDoubleArrayMap"),
+    ("read_only_space", 0x02ae9): (178, "GlobalDictionaryMap"),
+    ("read_only_space", 0x02b11): (156, "ManyClosuresCellMap"),
+    ("read_only_space", 0x02b39): (247, "MegaDomHandlerMap"),
+    ("read_only_space", 0x02b61): (175, "ModuleInfoMap"),
+    ("read_only_space", 0x02b89): (179, "NameDictionaryMap"),
+    ("read_only_space", 0x02bb1): (156, "NoClosuresCellMap"),
+    ("read_only_space", 0x02bd9): (181, "NumberDictionaryMap"),
+    ("read_only_space", 0x02c01): (156, "OneClosureCellMap"),
+    ("read_only_space", 0x02c29): (182, "OrderedHashMapMap"),
+    ("read_only_space", 0x02c51): (183, "OrderedHashSetMap"),
+    ("read_only_space", 0x02c79): (180, "NameToIndexHashTableMap"),
+    ("read_only_space", 0x02ca1): (185, "RegisteredSymbolTableMap"),
+    ("read_only_space", 0x02cc9): (184, "OrderedNameDictionaryMap"),
+    ("read_only_space", 0x02cf1): (249, "PreparseDataMap"),
+    ("read_only_space", 0x02d19): (250, "PropertyArrayMap"),
+    ("read_only_space", 0x02d41): (233, "AccessorInfoMap"),
+    ("read_only_space", 0x02d69): (234, "SideEffectCallHandlerInfoMap"),
+    ("read_only_space", 0x02d91): (234, "SideEffectFreeCallHandlerInfoMap"),
+    ("read_only_space", 0x02db9): (234, "NextCallSideEffectFreeCallHandlerInfoMap"),
+    ("read_only_space", 0x02de1): (186, "SimpleNumberDictionaryMap"),
+    ("read_only_space", 0x02e09): (222, "SmallOrderedHashMapMap"),
+    ("read_only_space", 0x02e31): (223, "SmallOrderedHashSetMap"),
+    ("read_only_space", 0x02e59): (224, "SmallOrderedNameDictionaryMap"),
+    ("read_only_space", 0x02e81): (229, "SourceTextModuleMap"),
+    ("read_only_space", 0x02ea9): (257, "SwissNameDictionaryMap"),
+    ("read_only_space", 0x02ed1): (230, "SyntheticModuleMap"),
+    ("read_only_space", 0x02ef9): (258, "WasmApiFunctionRefMap"),
+    ("read_only_space", 0x02f21): (216, "WasmCapiFunctionDataMap"),
+    ("read_only_space", 0x02f49): (217, "WasmExportedFunctionDataMap"),
+    ("read_only_space", 0x02f71): (260, "WasmInternalFunctionMap"),
+    ("read_only_space", 0x02f99): (218, "WasmJSFunctionDataMap"),
+    ("read_only_space", 0x02fc1): (261, "WasmResumeDataMap"),
+    ("read_only_space", 0x02fe9): (263, "WasmTypeInfoMap"),
+    ("read_only_space", 0x03011): (259, "WasmContinuationObjectMap"),
+    ("read_only_space", 0x03039): (231, "WeakFixedArrayMap"),
+    ("read_only_space", 0x03061): (177, "EphemeronHashTableMap"),
+    ("read_only_space", 0x03089): (239, "EmbedderDataArrayMap"),
+    ("read_only_space", 0x030b1): (265, "WeakCellMap"),
     ("read_only_space", 0x030d9): (32, "StringMap"),
     ("read_only_space", 0x03101): (41, "ConsOneByteStringMap"),
     ("read_only_space", 0x03129): (33, "ConsStringMap"),
@@ -394,77 +393,76 @@
     ("read_only_space", 0x03471): (131, "SelfReferenceMarkerMap"),
     ("read_only_space", 0x03499): (131, "BasicBlockCountersMarkerMap"),
     ("read_only_space", 0x034dd): (146, "ArrayBoilerplateDescriptionMap"),
-    ("read_only_space", 0x035dd): (159, "InterceptorInfoMap"),
-    ("read_only_space", 0x075c9): (132, "PromiseFulfillReactionJobTaskMap"),
-    ("read_only_space", 0x075f1): (133, "PromiseRejectReactionJobTaskMap"),
-    ("read_only_space", 0x07619): (134, "CallableTaskMap"),
-    ("read_only_space", 0x07641): (135, "CallbackTaskMap"),
-    ("read_only_space", 0x07669): (136, "PromiseResolveThenableJobTaskMap"),
-    ("read_only_space", 0x07691): (139, "FunctionTemplateInfoMap"),
-    ("read_only_space", 0x076b9): (140, "ObjectTemplateInfoMap"),
-    ("read_only_space", 0x076e1): (141, "AccessCheckInfoMap"),
-    ("read_only_space", 0x07709): (142, "AccessorPairMap"),
-    ("read_only_space", 0x07731): (143, "AliasedArgumentsEntryMap"),
-    ("read_only_space", 0x07759): (144, "AllocationMementoMap"),
-    ("read_only_space", 0x07781): (147, "AsmWasmDataMap"),
-    ("read_only_space", 0x077a9): (148, "AsyncGeneratorRequestMap"),
-    ("read_only_space", 0x077d1): (149, "BreakPointMap"),
-    ("read_only_space", 0x077f9): (150, "BreakPointInfoMap"),
-    ("read_only_space", 0x07821): (151, "CachedTemplateObjectMap"),
-    ("read_only_space", 0x07849): (152, "CallSiteInfoMap"),
-    ("read_only_space", 0x07871): (153, "ClassPositionsMap"),
-    ("read_only_space", 0x07899): (154, "DebugInfoMap"),
-    ("read_only_space", 0x078c1): (156, "ErrorStackDataMap"),
-    ("read_only_space", 0x078e9): (158, "FunctionTemplateRareDataMap"),
-    ("read_only_space", 0x07911): (160, "InterpreterDataMap"),
-    ("read_only_space", 0x07939): (161, "ModuleRequestMap"),
-    ("read_only_space", 0x07961): (162, "PromiseCapabilityMap"),
-    ("read_only_space", 0x07989): (163, "PromiseOnStackMap"),
-    ("read_only_space", 0x079b1): (164, "PromiseReactionMap"),
-    ("read_only_space", 0x079d9): (165, "PropertyDescriptorObjectMap"),
-    ("read_only_space", 0x07a01): (166, "PrototypeInfoMap"),
-    ("read_only_space", 0x07a29): (167, "RegExpBoilerplateDescriptionMap"),
-    ("read_only_space", 0x07a51): (168, "ScriptMap"),
-    ("read_only_space", 0x07a79): (169, "ScriptOrModuleMap"),
-    ("read_only_space", 0x07aa1): (170, "SourceTextModuleInfoEntryMap"),
-    ("read_only_space", 0x07ac9): (171, "StackFrameInfoMap"),
-    ("read_only_space", 0x07af1): (172, "TemplateObjectDescriptionMap"),
-    ("read_only_space", 0x07b19): (173, "Tuple2Map"),
-    ("read_only_space", 0x07b41): (174, "WasmExceptionTagMap"),
-    ("read_only_space", 0x07b69): (175, "WasmIndirectFunctionTableMap"),
-    ("read_only_space", 0x07b91): (195, "SloppyArgumentsElementsMap"),
-    ("read_only_space", 0x07bb9): (228, "DescriptorArrayMap"),
-    ("read_only_space", 0x07be1): (217, "UncompiledDataWithoutPreparseDataMap"),
-    ("read_only_space", 0x07c09): (215, "UncompiledDataWithPreparseDataMap"),
-    ("read_only_space", 0x07c31): (218, "UncompiledDataWithoutPreparseDataWithJobMap"),
-    ("read_only_space", 0x07c59): (216, "UncompiledDataWithPreparseDataAndJobMap"),
-    ("read_only_space", 0x07c81): (249, "OnHeapBasicBlockProfilerDataMap"),
-    ("read_only_space", 0x07ca9): (196, "TurbofanBitsetTypeMap"),
-    ("read_only_space", 0x07cd1): (200, "TurbofanUnionTypeMap"),
-    ("read_only_space", 0x07cf9): (199, "TurbofanRangeTypeMap"),
-    ("read_only_space", 0x07d21): (197, "TurbofanHeapConstantTypeMap"),
-    ("read_only_space", 0x07d49): (198, "TurbofanOtherNumberConstantTypeMap"),
-    ("read_only_space", 0x07d71): (245, "InternalClassMap"),
-    ("read_only_space", 0x07d99): (256, "SmiPairMap"),
-    ("read_only_space", 0x07dc1): (255, "SmiBoxMap"),
-    ("read_only_space", 0x07de9): (201, "ExportedSubClassBaseMap"),
-    ("read_only_space", 0x07e11): (202, "ExportedSubClassMap"),
-    ("read_only_space", 0x07e39): (226, "AbstractInternalClassSubclass1Map"),
-    ("read_only_space", 0x07e61): (227, "AbstractInternalClassSubclass2Map"),
-    ("read_only_space", 0x07e89): (194, "InternalClassWithSmiElementsMap"),
-    ("read_only_space", 0x07eb1): (246, "InternalClassWithStructElementsMap"),
-    ("read_only_space", 0x07ed9): (203, "ExportedSubClass2Map"),
-    ("read_only_space", 0x07f01): (257, "SortStateMap"),
-    ("read_only_space", 0x07f29): (263, "WasmStringViewIterMap"),
-    ("read_only_space", 0x07f51): (145, "AllocationSiteWithWeakNextMap"),
-    ("read_only_space", 0x07f79): (145, "AllocationSiteWithoutWeakNextMap"),
-    ("read_only_space", 0x08045): (137, "LoadHandler1Map"),
-    ("read_only_space", 0x0806d): (137, "LoadHandler2Map"),
-    ("read_only_space", 0x08095): (137, "LoadHandler3Map"),
-    ("read_only_space", 0x080bd): (138, "StoreHandler0Map"),
-    ("read_only_space", 0x080e5): (138, "StoreHandler1Map"),
-    ("read_only_space", 0x0810d): (138, "StoreHandler2Map"),
-    ("read_only_space", 0x08135): (138, "StoreHandler3Map"),
+    ("read_only_space", 0x035dd): (158, "InterceptorInfoMap"),
+    ("read_only_space", 0x075e9): (132, "PromiseFulfillReactionJobTaskMap"),
+    ("read_only_space", 0x07611): (133, "PromiseRejectReactionJobTaskMap"),
+    ("read_only_space", 0x07639): (134, "CallableTaskMap"),
+    ("read_only_space", 0x07661): (135, "CallbackTaskMap"),
+    ("read_only_space", 0x07689): (136, "PromiseResolveThenableJobTaskMap"),
+    ("read_only_space", 0x076b1): (139, "FunctionTemplateInfoMap"),
+    ("read_only_space", 0x076d9): (140, "ObjectTemplateInfoMap"),
+    ("read_only_space", 0x07701): (141, "AccessCheckInfoMap"),
+    ("read_only_space", 0x07729): (142, "AccessorPairMap"),
+    ("read_only_space", 0x07751): (143, "AliasedArgumentsEntryMap"),
+    ("read_only_space", 0x07779): (144, "AllocationMementoMap"),
+    ("read_only_space", 0x077a1): (147, "AsmWasmDataMap"),
+    ("read_only_space", 0x077c9): (148, "AsyncGeneratorRequestMap"),
+    ("read_only_space", 0x077f1): (149, "BreakPointMap"),
+    ("read_only_space", 0x07819): (150, "BreakPointInfoMap"),
+    ("read_only_space", 0x07841): (151, "CallSiteInfoMap"),
+    ("read_only_space", 0x07869): (152, "ClassPositionsMap"),
+    ("read_only_space", 0x07891): (153, "DebugInfoMap"),
+    ("read_only_space", 0x078b9): (155, "ErrorStackDataMap"),
+    ("read_only_space", 0x078e1): (157, "FunctionTemplateRareDataMap"),
+    ("read_only_space", 0x07909): (159, "InterpreterDataMap"),
+    ("read_only_space", 0x07931): (160, "ModuleRequestMap"),
+    ("read_only_space", 0x07959): (161, "PromiseCapabilityMap"),
+    ("read_only_space", 0x07981): (162, "PromiseOnStackMap"),
+    ("read_only_space", 0x079a9): (163, "PromiseReactionMap"),
+    ("read_only_space", 0x079d1): (164, "PropertyDescriptorObjectMap"),
+    ("read_only_space", 0x079f9): (165, "PrototypeInfoMap"),
+    ("read_only_space", 0x07a21): (166, "RegExpBoilerplateDescriptionMap"),
+    ("read_only_space", 0x07a49): (167, "ScriptMap"),
+    ("read_only_space", 0x07a71): (168, "ScriptOrModuleMap"),
+    ("read_only_space", 0x07a99): (169, "SourceTextModuleInfoEntryMap"),
+    ("read_only_space", 0x07ac1): (170, "StackFrameInfoMap"),
+    ("read_only_space", 0x07ae9): (171, "TemplateObjectDescriptionMap"),
+    ("read_only_space", 0x07b11): (172, "Tuple2Map"),
+    ("read_only_space", 0x07b39): (173, "WasmExceptionTagMap"),
+    ("read_only_space", 0x07b61): (174, "WasmIndirectFunctionTableMap"),
+    ("read_only_space", 0x07b89): (194, "SloppyArgumentsElementsMap"),
+    ("read_only_space", 0x07bb1): (227, "DescriptorArrayMap"),
+    ("read_only_space", 0x07bd9): (202, "UncompiledDataWithoutPreparseDataMap"),
+    ("read_only_space", 0x07c01): (200, "UncompiledDataWithPreparseDataMap"),
+    ("read_only_space", 0x07c29): (203, "UncompiledDataWithoutPreparseDataWithJobMap"),
+    ("read_only_space", 0x07c51): (201, "UncompiledDataWithPreparseDataAndJobMap"),
+    ("read_only_space", 0x07c79): (248, "OnHeapBasicBlockProfilerDataMap"),
+    ("read_only_space", 0x07ca1): (195, "TurbofanBitsetTypeMap"),
+    ("read_only_space", 0x07cc9): (199, "TurbofanUnionTypeMap"),
+    ("read_only_space", 0x07cf1): (198, "TurbofanRangeTypeMap"),
+    ("read_only_space", 0x07d19): (196, "TurbofanHeapConstantTypeMap"),
+    ("read_only_space", 0x07d41): (197, "TurbofanOtherNumberConstantTypeMap"),
+    ("read_only_space", 0x07d69): (244, "InternalClassMap"),
+    ("read_only_space", 0x07d91): (255, "SmiPairMap"),
+    ("read_only_space", 0x07db9): (254, "SmiBoxMap"),
+    ("read_only_space", 0x07de1): (219, "ExportedSubClassBaseMap"),
+    ("read_only_space", 0x07e09): (220, "ExportedSubClassMap"),
+    ("read_only_space", 0x07e31): (225, "AbstractInternalClassSubclass1Map"),
+    ("read_only_space", 0x07e59): (226, "AbstractInternalClassSubclass2Map"),
+    ("read_only_space", 0x07e81): (193, "InternalClassWithSmiElementsMap"),
+    ("read_only_space", 0x07ea9): (245, "InternalClassWithStructElementsMap"),
+    ("read_only_space", 0x07ed1): (221, "ExportedSubClass2Map"),
+    ("read_only_space", 0x07ef9): (256, "SortStateMap"),
+    ("read_only_space", 0x07f21): (262, "WasmStringViewIterMap"),
+    ("read_only_space", 0x07f49): (145, "AllocationSiteWithWeakNextMap"),
+    ("read_only_space", 0x07f71): (145, "AllocationSiteWithoutWeakNextMap"),
+    ("read_only_space", 0x0803d): (137, "LoadHandler1Map"),
+    ("read_only_space", 0x08065): (137, "LoadHandler2Map"),
+    ("read_only_space", 0x0808d): (137, "LoadHandler3Map"),
+    ("read_only_space", 0x080b5): (138, "StoreHandler0Map"),
+    ("read_only_space", 0x080dd): (138, "StoreHandler1Map"),
+    ("read_only_space", 0x08105): (138, "StoreHandler2Map"),
+    ("read_only_space", 0x0812d): (138, "StoreHandler3Map"),
     ("old_space", 0x043a5): (2116, "ExternalMap"),
     ("old_space", 0x043d5): (2120, "JSMessageObjectMap"),
 }
@@ -556,30 +554,30 @@
   ("old_space", 0x045a1): "StringSplitCache",
   ("old_space", 0x049a9): "RegExpMultipleCache",
   ("old_space", 0x04db1): "BuiltinsConstantsTable",
-  ("old_space", 0x05211): "AsyncFunctionAwaitRejectSharedFun",
-  ("old_space", 0x05235): "AsyncFunctionAwaitResolveSharedFun",
-  ("old_space", 0x05259): "AsyncGeneratorAwaitRejectSharedFun",
-  ("old_space", 0x0527d): "AsyncGeneratorAwaitResolveSharedFun",
-  ("old_space", 0x052a1): "AsyncGeneratorYieldWithAwaitResolveSharedFun",
-  ("old_space", 0x052c5): "AsyncGeneratorReturnResolveSharedFun",
-  ("old_space", 0x052e9): "AsyncGeneratorReturnClosedRejectSharedFun",
-  ("old_space", 0x0530d): "AsyncGeneratorReturnClosedResolveSharedFun",
-  ("old_space", 0x05331): "AsyncIteratorValueUnwrapSharedFun",
-  ("old_space", 0x05355): "PromiseAllResolveElementSharedFun",
-  ("old_space", 0x05379): "PromiseAllSettledResolveElementSharedFun",
-  ("old_space", 0x0539d): "PromiseAllSettledRejectElementSharedFun",
-  ("old_space", 0x053c1): "PromiseAnyRejectElementSharedFun",
-  ("old_space", 0x053e5): "PromiseCapabilityDefaultRejectSharedFun",
-  ("old_space", 0x05409): "PromiseCapabilityDefaultResolveSharedFun",
-  ("old_space", 0x0542d): "PromiseCatchFinallySharedFun",
-  ("old_space", 0x05451): "PromiseGetCapabilitiesExecutorSharedFun",
-  ("old_space", 0x05475): "PromiseThenFinallySharedFun",
-  ("old_space", 0x05499): "PromiseThrowerFinallySharedFun",
-  ("old_space", 0x054bd): "PromiseValueThunkFinallySharedFun",
-  ("old_space", 0x054e1): "ProxyRevokeSharedFun",
-  ("old_space", 0x05505): "ShadowRealmImportValueFulfilledSFI",
-  ("old_space", 0x05529): "SourceTextModuleExecuteAsyncModuleFulfilledSFI",
-  ("old_space", 0x0554d): "SourceTextModuleExecuteAsyncModuleRejectedSFI",
+  ("old_space", 0x05215): "AsyncFunctionAwaitRejectSharedFun",
+  ("old_space", 0x05239): "AsyncFunctionAwaitResolveSharedFun",
+  ("old_space", 0x0525d): "AsyncGeneratorAwaitRejectSharedFun",
+  ("old_space", 0x05281): "AsyncGeneratorAwaitResolveSharedFun",
+  ("old_space", 0x052a5): "AsyncGeneratorYieldWithAwaitResolveSharedFun",
+  ("old_space", 0x052c9): "AsyncGeneratorReturnResolveSharedFun",
+  ("old_space", 0x052ed): "AsyncGeneratorReturnClosedRejectSharedFun",
+  ("old_space", 0x05311): "AsyncGeneratorReturnClosedResolveSharedFun",
+  ("old_space", 0x05335): "AsyncIteratorValueUnwrapSharedFun",
+  ("old_space", 0x05359): "PromiseAllResolveElementSharedFun",
+  ("old_space", 0x0537d): "PromiseAllSettledResolveElementSharedFun",
+  ("old_space", 0x053a1): "PromiseAllSettledRejectElementSharedFun",
+  ("old_space", 0x053c5): "PromiseAnyRejectElementSharedFun",
+  ("old_space", 0x053e9): "PromiseCapabilityDefaultRejectSharedFun",
+  ("old_space", 0x0540d): "PromiseCapabilityDefaultResolveSharedFun",
+  ("old_space", 0x05431): "PromiseCatchFinallySharedFun",
+  ("old_space", 0x05455): "PromiseGetCapabilitiesExecutorSharedFun",
+  ("old_space", 0x05479): "PromiseThenFinallySharedFun",
+  ("old_space", 0x0549d): "PromiseThrowerFinallySharedFun",
+  ("old_space", 0x054c1): "PromiseValueThunkFinallySharedFun",
+  ("old_space", 0x054e5): "ProxyRevokeSharedFun",
+  ("old_space", 0x05509): "ShadowRealmImportValueFulfilledSFI",
+  ("old_space", 0x0552d): "SourceTextModuleExecuteAsyncModuleFulfilledSFI",
+  ("old_space", 0x05551): "SourceTextModuleExecuteAsyncModuleRejectedSFI",
 }
 
 # Lower 32 bits of first page addresses for various heap spaces.
@@ -601,7 +599,7 @@
   "WASM_DEBUG_BREAK",
   "C_WASM_ENTRY",
   "WASM_EXIT",
-  "WASM_COMPILE_LAZY",
+  "WASM_LIFTOFF_SETUP",
   "INTERPRETED",
   "BASELINE",
   "MAGLEV",
diff -r -u --color up/v8/tools/wasm/module-inspector.cc nw/v8/tools/wasm/module-inspector.cc
--- up/v8/tools/wasm/module-inspector.cc	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/wasm/module-inspector.cc	2023-01-19 16:46:37.966442429 -0500
@@ -242,17 +242,18 @@
     }
     uint32_t total_length = 0;
     uint32_t length;
-    uint32_t entries = read_u32v<validate>(pc_, &length);
+    uint32_t entries = read_u32v<ValidationTag>(pc_, &length);
     PrintHexBytes(out, length, pc_, 4);
     out << " // " << entries << " entries in locals list";
     out.NextLine(kWeDontCareAboutByteCodeOffsetsHere);
     total_length += length;
     while (entries-- > 0) {
       uint32_t count_length;
-      uint32_t count = read_u32v<validate>(pc_ + total_length, &count_length);
+      uint32_t count =
+          read_u32v<ValidationTag>(pc_ + total_length, &count_length);
       uint32_t type_length;
-      ValueType type = value_type_reader::read_value_type<validate>(
-          this, pc_ + total_length + count_length, &type_length, nullptr,
+      ValueType type = value_type_reader::read_value_type<ValidationTag>(
+          this, pc_ + total_length + count_length, &type_length,
           WasmFeatures::All());
       PrintHexBytes(out, count_length + type_length, pc_ + total_length, 4);
       out << " // " << count << (count != 1 ? " locals" : " local")
@@ -340,10 +341,10 @@
     while (pc_ < end_) {
       WasmOpcode opcode = GetOpcode();
       if (opcode == kExprI32Const) {
-        ImmI32Immediate<Decoder::kNoValidation> imm(this, pc_ + 1);
+        ImmI32Immediate imm(this, pc_ + 1, Decoder::kNoValidation);
         stats.RecordImmediate(opcode, imm.value);
       } else if (opcode == kExprLocalGet || opcode == kExprGlobalGet) {
-        IndexImmediate<Decoder::kNoValidation> imm(this, pc_ + 1, "");
+        IndexImmediate imm(this, pc_ + 1, "", Decoder::kNoValidation);
         stats.RecordImmediate(opcode, static_cast<int>(imm.index));
       }
       uint32_t length = WasmDecoder::OpcodeLength(this, pc_);
@@ -866,7 +867,9 @@
     DCHECK_EQ(status_, kModuleReady);
     MultiLineStringBuilder sb;
     ModuleDisassembler md(sb, module(), names(), wire_bytes_, &allocator_);
-    md.PrintModule({0, 2});
+    // 100 GB is an approximation of "unlimited".
+    size_t max_mb = 100'000;
+    md.PrintModule({0, 2}, max_mb);
     sb.WriteTo(out_);
   }
 
diff -r -u --color up/v8/tools/whitespace.txt nw/v8/tools/whitespace.txt
--- up/v8/tools/whitespace.txt	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/whitespace.txt	2023-01-19 16:46:37.966442429 -0500
@@ -6,7 +6,7 @@
 "I'm so deoptimized today!"
 The doubles heard this and started to unbox.
 The Smi looked at them when a crazy v8-autoroll account showed up...
-The autoroller bought a round of Himbeerbrause. Suddenly.......
+The autoroller bought a round of Himbeerbrause. Suddenly........
 The bartender starts to shake the bottles.............................
 I can't add trailing whitespaces, so I'm adding this line............
 I'm starting to think that just adding trailing whitespaces might not be bad.
@@ -15,4 +15,5 @@
 Today's answer to life the universe and everything is 12950!
 Today's answer to life the universe and everything is 6728!
 Today's answer to life the universe and everything is 6728!!
+Off-course, this is wrong ...
 .
diff -r -u --color up/v8/tools/zone-stats/categories.js nw/v8/tools/zone-stats/categories.js
--- up/v8/tools/zone-stats/categories.js	2022-12-01 12:14:12.000000000 -0500
+++ nw/v8/tools/zone-stats/categories.js	2023-01-19 16:46:37.966442429 -0500
@@ -48,6 +48,7 @@
       'V8.TFAllocateGeneralRegisters',
       'V8.TFAssembleCode',
       'V8.TFAssignSpillSlots',
+      'V8.TFBitcastElision',
       'V8.TFBuildLiveRangeBundles',
       'V8.TFBuildLiveRanges',
       'V8.TFBytecodeGraphBuilder',
